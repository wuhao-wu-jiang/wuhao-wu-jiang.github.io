<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/12/Fourier-Transformation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/12/Fourier-Transformation/" class="post-title-link" itemprop="url">Fourier Transformation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-12 16:21:21" itemprop="dateCreated datePublished" datetime="2021-01-12T16:21:21+11:00">2021-01-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-26 23:51:21" itemprop="dateModified" datetime="2021-01-26T23:51:21+11:00">2021-01-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $f: [-\pi, \pi]: \rightarrow \mathbb{R}$ be a real function. As $f$ itself can be complicated, we want to decompose it into a linear combination of simpler functions. </p>
<blockquote>
<p><strong>Example.</strong> A typical family of simple functions are polynomials:<br>$$<br>    { 1, t, t^2, t^3, … }.<br>$$<br>Under proper condition, e.g., if $f$ is infinitely differentiable, it can be indeed decomposed into a linear combination of polynomials, such as:<br>$$<br>    f = f(0) + f’(0)  t + \frac{f’’(t)}{2!} t^2 + …<br>$$  </p>
</blockquote>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a><strong>Problem</strong></h2><p>Here we study the problem of approximating $f$ with trigonometric functions:<br>$$<br>    \mathfrak{T} = { 1, \sin t, \cos t, \sin 2t, \cos 2t, …. }.<br>$$</p>
<p>Although $f: [-\pi, \pi]: \rightarrow \mathbb{R}$ is a function with output on $\mathbb{R}$, we view it as a function $[-\pi, \pi]: \rightarrow \mathbb{C}$ to take advantage of complex exponential. Now, we approximate $f$ with a set of sinusoids:<br>$$<br>   \Gamma = { 1, \exp(it), \exp(-it), \exp(i2t), \exp(-i2t), \exp(i3t), \exp(-i3t)… }.<br>$$ </p>
<p>Observe that $\Gamma$ generalizes $\mathfrak{T}$, as for $\forall k \in \mathbb{N}$,<br>$$<br>2 \cos kt =  \exp(ikt) + \exp(-ikt), \<br>2 \sin kt =  \exp(ikt) - \exp(-ikt).<br>$$</p>
<p>Define the span $S(\Gamma)$ as the set of all possible linear combinations of functions in $\Gamma$:<br>$$<br>S(\Gamma) \doteq \left{ \sum_{k \in \mathbb{Z} } c_k \exp(ikt) : \forall k \in \mathbb{Z}, c_k \in \mathbb{C} \right}.<br>$$ </p>
<p><strong><em>The goal is to find the closest function to $f$ in $S(\Gamma)$</em></strong>.  </p>
<p>The key issues with the approximation are </p>
<ol>
<li><p>How do we measure the closeness between two functions?   </p>
</li>
<li><p>Under what condition, can $f$ be rewrite as a linear combination of functions in $\Gamma$?</p>
</li>
<li><p>What is the approximation error, between $f$ and the linear combination?</p>
</li>
</ol>
<h2 id="Norm"><a href="#Norm" class="headerlink" title="Norm"></a><strong>Norm</strong></h2><p>Despite the goal stated above, the closeness between $f$ and a function in $S(\Gamma)$ has not yet been defined. This raises the question of measuring the distance between two functions in $[-\pi, \pi] \rightarrow \mathcal{C}$. </p>
<p>In $\mathbb{R}^n$, the Euclidean distance between two vectors $u, v$ is defined as<br>$$<br>    || u - v || = \sqrt{\sum_{i = 1}^n (u_i - v_i)^2 }.<br>$$</p>
<p>If we define $w = u - v$, then the problem of measuring the distance between two vectors reduces to the one of measuring the length of a vector<br>$$<br>|| u - v || = || w || = \sqrt{\sum_{i = 1}^n w_i^2 }.<br>$$</p>
<!-- We call the function $||\cdot || : \mathbb{R}^n \rightarrow \mathbb{R}$ as norm and it satisfies the following three properties:

1. Positive-definite: $||w|| = 0 \rightarrow w = 0, \forall w \in \mathbb{R}^n$;
2. Absolutely homogeneous: $|| a w|| = |a| \cdot ||w||, \forall a \in \mathbb{R}, \forall w \in \mathbb{R}^n$; 
3. Triangle inequality: $||w + w'|| \le ||w|| + ||w'||, \forall w, w' \in \mathbb{R}^n$. -->

<p>In a similar manner, if we view a function $h: [-\pi, \pi] \rightarrow \mathcal{C}$ as an (uncountable) infinite dimension vector, it is natural to defined its length<br>$$<br>    || h || \doteq \sqrt{ \int_{ [-\pi, \pi] }   ||h(t) ||^2 } = \sqrt{ \int_{ [-\pi, \pi] }   \overline{h(t) } \cdot h(t)  }.<br>$$</p>
<blockquote>
<p><em>Remark:</em> </p>
<ol>
<li><em>$|| h ||^2$ needs to be integrable.</em>   </li>
<li><em>Verify that $|| \cdot ||$ is a norm.</em> </li>
</ol>
</blockquote>
<p>As $f - g$ is also a function in $[-\pi, \pi] \rightarrow \mathcal{C}$, the distance between two functions $f$ and $g$ is given by<br>$$<br>\begin{aligned}<br>    ||f - g||<br>        &amp;= \sqrt{ \int_{ [-\pi, \pi] }   ||f(t) - g(t)||^2  } \<br>        &amp;= \sqrt{ \int_{ [-\pi, \pi] }   \overline{ (f(t) - g(t) ) } \cdot (f(t) - g(t) )  } \<br>        &amp;= \sqrt{ \int_{ [-\pi, \pi] }   \overline{ f(t) } \cdot f(t) + \int_{ [-\pi, \pi] }  \overline{ g(t) } \cdot g(t) - \int_{ [-\pi, \pi] }  \overline{ f(t) } \cdot g(t) - \int_{ [-\pi, \pi] }  \overline{ g(t) } \cdot f(t) \   } \<br>        &amp;= \sqrt{ ||f||^2 + || g ||^2 - \int_{ [-\pi, \pi] }  \overline{ f(t) } \cdot g(t) - \int_{ [-\pi, \pi] }  \overline{ g(t) } \cdot f(t) \   }. \<br>\end{aligned}<br>$$</p>
<blockquote>
<p><em>Remark:</em> </p>
<ol>
<li>$|| f - g ||^2$ needs to be integrable. </li>
<li>The integration operation is required to be linear. </li>
</ol>
</blockquote>
<h3 id="Hermitian-Product"><a href="#Hermitian-Product" class="headerlink" title="Hermitian Product"></a><strong>Hermitian Product</strong></h3><p>To simplify the expansion of $||f - g||$, we introduce another operation, termed Hermitian product, as follows:<br>$$<br>\left&lt; g, f \right&gt;<em>H \doteq \int</em>{ [-\pi, \pi] }  \overline{ g(t) } \cdot f(t).<br>$$</p>
<p>The operation is not symmetric. By definition,<br>$$<br>\left&lt; f, g \right&gt;<em>H \doteq \int</em>{ [-\pi, \pi] }  \overline{ f(t) } \cdot g(t).<br>$$</p>
<p>$\left&lt; g, f \right&gt;_H$ is the complex conjugate of $\left&lt; f, g \right&gt;_H$:<br>$$<br>\left&lt; g, f \right&gt;_H = \overline{ \left&lt; f, g \right&gt;_H }.<br>$$</p>
<p>For any $c \in \mathbb{C}$, it holds that<br>$$<br>\left&lt; c \cdot g, f \right&gt;_H = \bar c \cdot \left&lt; g, f \right&gt;_H,  \<br>\left&lt; g, c \cdot f \right&gt;_H = c  \cdot \left&lt; g, f \right&gt;_H. \<br>$$</p>
<p>Now the expansion of $||f - g||$ is simplified as<br>$$<br>\begin{aligned}<br>    ||f - g||<br>        &amp;= \sqrt{ ||f||^2 + ||g||^2 - ( \overline{ \left&lt; g, f \right&gt;_H } + \left&lt; g, f \right&gt;_H ) } \<br>        &amp;= \sqrt{ ||f||^2 + ||g||^2 - 2 \cdot \mathbb{Re}( \left&lt; g, f \right&gt;_H ) }. \<br>\end{aligned}<br>$$</p>
<h4 id="Geometric-Interpretation"><a href="#Geometric-Interpretation" class="headerlink" title="Geometric Interpretation"></a><strong>Geometric Interpretation</strong></h4><p>We have just introduced the notation of Hermitian product to simplify the expression. Its “geometric” interpretation is related to the idea of “projection”. We start by reviewing the ideas of “Projection, Dot Product and Orthogonality” in $\mathbb{R}^2$. </p>
<blockquote>
<p><strong>Projection</strong> in $\mathbb{R}^2$. Given $u, v \in \mathbb{R}^2$, the projection of $u$ to $v$ is the vector in the subspace<br>$$<br>    S(v) \doteq { c \cdot v : c \in \mathbb{R} }<br>$$<br>that is closest to $u$. </p>
<p><strong>Dot Product</strong> in $\mathbb{R}^2$. Given $u, v \in \mathbb{R}^2$, their dot product is defined as<br> $$<br>    \left&lt; u, v \right&gt; = u_1 v_1 + u_2 v_2.<br>$$<br>The projection of $u$ to $v$ (closest vector to $u$ in $S(v)$) is given by<br>$$<br>    \frac{1}{ ||v||^2 } \left&lt; u, v \right&gt; \cdot v = \frac{ \left&lt; u, v \right&gt; }{ ||v|| }  \cdot \frac{ v }{ ||v|| }.<br>$$<br>The first term of the final product $\frac{ \left&lt; u, v \right&gt; }{ ||v|| } = \frac{ ||u|| \cdot ||v|| \cdot \cos \angle (u, v) }{ ||v|| } = ||u|| \cdot \cos \angle (u, v)$ gives the length of $u$’s projection to $v$, and the second term $\frac{ v }{ ||v|| }$ is a unit vector that shares the same direction as $v$. </p>
<p><strong>Orthogonality</strong> in $\mathbb{R}^2$.  Given $u, v \in \mathbb{R}^2$, they are orthogonal if the projection of $u$ to $v$ is $\vec 0$. In this case,<br>$$<br>    \left&lt; u, v \right&gt; = 0.<br>$$</p>
</blockquote>
<p>We can extend these ideas to $[-\pi, \pi] \rightarrow \mathbb{C}$.  </p>
<h4 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a><strong>Projection</strong></h4><p>Given $f, g:[-\pi, \pi] \rightarrow \mathcal{C}$, the projection of $f$ to $g$ is the function in the linear span of $g$<br>$$<br>    S(g) \doteq { c \cdot g : c \in \mathbb{C} }<br>$$<br>that minimize $||f - c \cdot g||, \forall c \in \mathbb{C}$.</p>
<h4 id="Hermitian-Product-1"><a href="#Hermitian-Product-1" class="headerlink" title="Hermitian Product"></a><strong>Hermitian Product</strong></h4><p>Hermitian product is related to projection and arises naturally when we try to solve<br>$$<br>    \min_{c \in \mathbb{C} }  ||f - c \cdot g||,<br>$$</p>
<p>or equivalently,<br>$$<br>    \min_{c \in \mathbb{C} }  ||f - c \cdot g||^2.<br>$$</p>
<p>Expanding $||f - c \cdot g||^2$ and let $c = x + iy$, $\left&lt; g, f \right&gt;_H = a + i b$, we get<br>$$<br>\begin{aligned}<br>    ||f - c \cdot g||^2<br>        &amp;= ||f||^2 + ||c||^2 \cdot ||g||^2 -  2 \cdot \mathbb{Re}( \left&lt; c \cdot g, f \right&gt;_H )  \<br>        &amp;= ||f||^2 + ||c||^2 \cdot ||g||^2 - 2 \cdot \mathbb{Re} ( \bar c \cdot \left&lt; g, f \right&gt;_H ) \<br>        &amp;= ||f||^2 + (x^2 + y^2) \cdot ||g||^2 - 2 \cdot \mathbb{Re} ( ( x- i y)  \cdot (a + i b) ) \<br>        &amp;= ||f||^2 + (x^2 + y^2) \cdot ||g||^2 - 2 \cdot (a x + by) \<br>\end{aligned}<br>$$</p>
<blockquote>
<p><em>Remark:</em> <em>The derivation above requires the integration operation to be linear.</em> </p>
</blockquote>
<p>Taking the derivate of RHS with respect to $x$, setting it to zero and assuming that $||g|| \neq 0$, we see<br>$$<br>    2x\cdot ||g||^2 - 2 a = 0 \Rightarrow x = \frac{ a }{ ||g||^2 }.<br>$$</p>
<p>Similarly,<br>$$<br>    2y\cdot ||g||^2 - 2 b = 0 \Rightarrow y = \frac{ b }{ ||g||^2 }.<br>$$</p>
<p>Combined, $c$ is given by<br>$$<br>    c = x + iy = \frac{ \left&lt; g, f \right&gt;_H  }{ ||g||^2 } = \left&lt; \frac{g }{ ||g|| }, f \right&gt;_H \cdot \frac{ 1 }{ ||g|| }.<br>$$</p>
<p>This implies that<br>$$<br>    c \cdot g = \frac{ \left&lt; g, f \right&gt;_H  }{ ||g||^2 } \cdot g = \left&lt; \frac{g }{ ||g|| }, f \right&gt;_H \cdot \frac{g }{ ||g|| }.<br>$$</p>
<p>Now it is clear that the Hermitian product computes the coefficient for the projection of $f$ to $g$, scaled by a factor of $1/ ||g||^2$. When $|| g ||$ has unit length,<br>$$<br>    c \cdot g = \left&lt; g, f \right&gt;_H \cdot g.<br>$$</p>
<p>The Hermitian product $\in \mathbb{C}$ is the just the coefficient for the projection.</p>
<h4 id="Orthogonality"><a href="#Orthogonality" class="headerlink" title="Orthogonality"></a><strong>Orthogonality</strong></h4><p>Given $f, g:[-\pi, \pi] \rightarrow \mathcal{C}$, $f$ is orthogonal to $g$ if the projection of $f$ to $g$ is the zero function, i.e., the closest function (measured by $|| \cdot ||$) to $f$ in the linear span $S(g)$ is the constant function zero. It holds that, either $||g||^2 = 0$ or<br>$$<br>    \arg\min_{ c \in \mathbb{C} } \left| \left|f - c \cdot g \right| \right|^2 = \frac{ \left&lt; g, f \right&gt;_H  }{ ||g||^2 } = 0.<br>$$</p>
<p>In both cases,<br>$$<br>    \overline{ \left&lt; g, f \right&gt;_H } = \left&lt; f, g \right&gt; = 0.<br>$$</p>
<h2 id="Properties-of-Gamma"><a href="#Properties-of-Gamma" class="headerlink" title="Properties of $\Gamma$"></a><strong>Properties of $\Gamma$</strong></h2><p>We are now ready to discuss the properties of $\Gamma = { 1, \exp(it), \exp(-it), \exp(i2t), \exp(-i2t), … }$. </p>
<ol>
<li><p>For $k \in \mathbb{Z}$,<br>$$<br>|| \exp(ikt) || = \sqrt{ \int_{ [-\pi, \pi] }  \overline{\exp(ikt) } \exp(ikt) \  } = \sqrt{2 \pi }.<br>$$</p>
</li>
<li><p>For $k, l \in \mathbb{Z}$, $k \neq l$, $\exp(ikt)$ and $\exp(ilt)$ are orthogonal.<br>$$<br>\begin{aligned}</p>
<pre><code>\left&lt; \exp(ikt), \exp(ilt) \right&gt;_H 
 &amp;= \int_&#123; [ -\pi, \pi] &#125; \exp(-ikt) \cdot \exp(ilt)  \\
 &amp;= \int_&#123; [ -\pi, \pi] &#125; \exp(i (l - k) t) \\
 &amp;= \frac&#123;1&#125;&#123; i(l - k) &#125; \exp(i (l - k) t) \mid_&#123;- \pi&#125;^\pi \\
 &amp;= 0.
</code></pre>
<p>\end{aligned}<br>$$</p>
</li>
</ol>
<p>If we normalize the function in $\Gamma$ by $1 / \sqrt{2 \pi}$, then each function has norm 1. We call this family an orthonormal family, and rewrite it as<br>$$<br>\mathfrak{F} = { e_0,  e_1, e_2, e_3, …, },<br>$$</p>
<p>where $e_0 = 1$, and for $k \in \mathbb{N}$,   </p>
<ul>
<li><p>$e_{2k + 1} = \exp(ikt) / \sqrt{2 \pi}$;  </p>
</li>
<li><p>$e_{2k + 2} = \exp(- ikt) / \sqrt{2 \pi}$.  </p>
</li>
</ul>
<h2 id="Approximating-f-with-mathfrak-F"><a href="#Approximating-f-with-mathfrak-F" class="headerlink" title="Approximating $f$ with $\mathfrak{F}$"></a><strong>Approximating $f$ with $\mathfrak{F}$</strong></h2><p>As discuss before, we have construct an orthogonal family of $\mathfrak{F}$, such that </p>
<ol>
<li>$||e_i|| = 1$ for $e_i \in \mathfrak{F}$. </li>
<li>$\left&lt; e_i, e_j \right&gt;_H = 0$ for $e_i, e_j \in \mathfrak{F}, e_i \neq e_j$. </li>
</ol>
<!-- We continue to study the following two questions. 

2. Under what condition, can $f$ be rewrite as a linear combination of functions in $\mathfrak{F}$?
   
3. What is the approximation error, between $f$ and the linear combination?

For the second question, we provide a sufficient condition as an answer:

> $f^2$ is integrable. 

Hence,  -->

<p><strong>Definition.</strong> Define $S(\mathfrak{F}_k)$ the span of the first $k + 1$ elements in $\mathfrak{F}$:<br>$$<br>    S(\mathfrak{F}<em>k ) \doteq \left{ \sum</em>{i = 0}^k c_i \cdot e_i : \forall \ 0 \le i \le k, c_i \in \mathbb{C} \right}.<br>$$</p>
<blockquote>
<p><strong>Theorem.</strong> The closest function to $f$ in $S(\mathfrak{F}<em>k )$, denoted as $g_k$, is given by<br>$$<br>    g_k \doteq \sum</em>{i = 0 }^k a_i \cdot e_i.<br>$$<br>where $a_i \doteq \left&lt; e_i, f \right&gt;_H, 0 \le i \le k$.</p>
</blockquote>
<blockquote>
<p><em>Remark: implicitly, we assume that $f^2$, $\bar e_i f$ is integrable.</em> </p>
</blockquote>
<p><em>Proof:</em> For $0 \le i \le k$, it holds that<br>$$<br>\left&lt; e_i, g_k \right&gt;<em>H = \sum</em>{j = 0 }^k \left&lt; e_i, a_j \cdot e_j \right&gt;_H =  a_i = \left&lt; e_i, f \right&gt;_H.<br>$$</p>
<p>Hence,<br>$$<br>\left&lt; e_i, g_k - f\right&gt;_H = \overline{ \left&lt; g_k - f, e_i \right&gt;_H  } = 0.<br>$$</p>
<p>Let<br>$$<br>    f_k = \sum_{i = 0}^k c_i \cdot e_i,<br>$$</p>
<p>be a function in $S(\mathfrak{F}_k )$. Then,<br>$$<br>\begin{aligned}<br>    || f - f_k||^2<br>        &amp;= ||f - g_k + g_k - f_k ||^2 \<br>        &amp;= ||f - g_k||^2 + ||g_k - f_k||^2 + \left&lt; f - g_k, g_k - f_k\right&gt;_H + \left&lt; g_k - f_k, f - g_k \right&gt;_H.<br>\end{aligned}<br>$$</p>
<p>It holds that<br>$$<br>\left&lt; f - g_k, g_k - f_k\right&gt;<em>H = \sum</em>{i = 0}^k (a_i - c_i) \cdot \left&lt; f - g_k,  e_i \right&gt;_H = 0.<br>$$</p>
<p>Similarly, $\left&lt; g_k - f_k, f - g_k \right&gt;_H = 0$. So,<br>$$<br>\begin{aligned}<br>    || f - f_k||^2<br>        &amp;= ||f - g_k||^2 + ||g_k - f_k||^2,<br>\end{aligned}<br>$$</p>
<p>which is minimized when $f_k = g_k$. </p>
<p>$\square$</p>
<!-- $$
\begin{aligned}
    ||f - f_k||^2 
        &= \left< f - f_k, f - f_k \right>_H \\
        &= \left<  f, f\right>_H + \left<  f_k, f_k \right>_H - \left<  f, f_k \right>_H - \left<  f_k , f\right>_H \\
        &= ||f||^2 + \sum_{i = 0}^k \left( \left< c_i \cdot e_i, c_i \cdot e_i \right>_H - \left<  f, c_i \cdot e_i \right>_H - \left< c_i \cdot e_i , f \right>_H \right) \\
        &= ||f||^2 + \sum_{i = 0}^k \left( \bar c_i \cdot c_i - c_i \cdot \left<  f, e_i \right>_H - \bar c_i \cdot \left< e_i , f \right>_H + \bar a_i \cdot a_i \right) - \sum_{i = 0}^k ||a_i||^2 \\
        &= ||f||^2 + \sum_{i = 0}^k \left( \bar c_i \cdot c_i - c_i \cdot \bar a_i - \bar c_i \cdot a_i + \bar a_i \cdot a_i \right) - \sum_{i = 0}^k ||a_i||^2 \\
        &= ||f||^2 + \sum_{i = 0}^k ||c_i - a_i||^2 - \sum_{i = 0}^k ||a_i||^2. \\
\end{aligned}
$$ -->

<!-- Thus, when $c_i = a_i$, the distance between $f_k$ and $f$ is minimized.  -->

<blockquote>
<p><strong>Corollary. (Bessel Ineuqality)</strong>.<br>$$<br>    \sum_{i = 0}^\infty ||a_i||^2 \le ||f||^2.<br>$$</p>
</blockquote>
<p><em>Proof.</em> We see that<br>$$<br>\begin{aligned}<br>    || f - g_k ||^2<br>        &amp;= \left&lt; f - g_k, f \right&gt;_H - \left&lt; f - g_k, g_k \right&gt;<em>H \<br>        &amp;= \left&lt; f , f \right&gt;_H - \left&lt;  g_k, f \right&gt;_H \<br>        &amp;= ||f||^2 - \sum</em>{i = 0}^k ||a_i||^2 \<br>        &amp;\ge 0.<br>\end{aligned}<br>$$</p>
<p>Therefore,<br>$$<br>    \sum_{i = 0}^k ||a_i||^2 \le ||f||^2.<br>$$</p>
<p>Taking the limit of $k$ we get the desired result. </p>
<p>$\square$</p>
<p>The function series $g_1, g_2, g_3, …$ is called a Cauchy series, as $\forall \epsilon &gt; 0$, $\exists N \ge 0$, $\forall N \le i &lt; j$, it holds that<br>$$<br>    ||g_j - g_i|| = || \sum_{t = i + 1}^j a_t \cdot e_t ||<br>    = \sqrt{ \sum_{t = i + 1}^j ||a_t||^2 } \le \epsilon.<br>$$</p>
<p>Suppose that we are using some kind of integration and let $I^2([-\pi, \pi])$ be the set of square integrable functions. Further, assume that $g_1, g_2, g_3, …$ are also elements in $I^2([-\pi, \pi])$. We are interested in </p>
<blockquote>
<p>Whether a Cauchy series in $I^2([-\pi, \pi])$ converges to a function in $I^2([-\pi, \pi])$? </p>
</blockquote>
<p>Define<br>$$<br>    g = \lim_{k \rightarrow \infty} g_k = \sum_{i = 0}^\infty a_i \cdot e_i.<br>$$</p>
<p>It is important that $g \in I^2([-\pi, \pi])$, as we need the length of $|| g ||$ to be well-defined. Further, $f - g$ should also be in $I^2([-\pi, \pi])$. </p>
<p>However, if we are using Riemann integration, the answer is no. Let $r_1, r_2, ….$ be the rational number in the interval of $[0, 1]$ and define<br>$$<br>    h_i(x) = \begin{cases}<br>        \begin{aligned}<br>            &amp;1,  &amp;\text{if } x \in \cup_{t = 1}^i { r_t } \<br>            &amp;0,  &amp;\text{otherwise}<br>        \end{aligned}<br>    \end{cases}<br>$$</p>
<p>Then $h_i \in I^2([-\pi, \pi])$ but $\lim_{i \rightarrow \infty} h_i$ is not Riemann integrable, hence not in $I^2([-\pi, \pi])$.</p>
<p>If we are using Lebesgue integration, the answer is yes. </p>
<blockquote>
<p>Fact. If we are using Lebesgue integration, then $I^2([-\pi, \pi])$ is complete, i.e., if $g_1, g_2, …, \in I^2([-\pi, \pi])$ is Cauchy, then $\lim_k g_k \in I^2([-\pi, \pi])$. </p>
</blockquote>
<h2 id="Approximating-Error"><a href="#Approximating-Error" class="headerlink" title="Approximating Error"></a><strong>Approximating Error</strong></h2><p>It is left to study the error of approximating $f$ with $g$. </p>
<blockquote>
<p><strong>Lemma. Cauchy-Schwartz Inequality.</strong> Let $h_1, h_2 \in I^2([-\pi, \pi])$, then<br>$$<br>    | \left&lt; h_1, h_2 \right&gt;_H | \le ||h_1|| \cdot ||h_2 ||<br>$$</p>
</blockquote>
<p><em>Proof.</em> If $||h_2|| = 0$, then $h_2 = 0$ almost everywhere and the case is trivial. Otherwise, $\forall c \in \mathbb{C}$, we have<br>$$<br>\begin{aligned}<br>    || h_1 - c \cdot h_2 ||^2<br>        &amp;= || h_1||^2 + ||c||^2 ||h_2||^2 -  \left&lt; h_1, c \cdot h_2 \right&gt;_H -   \left&lt; c \cdot h_2, h_1 \right&gt;_H \<br>        &amp;= || h_1||^2 + ||c||^2 ||h_2||^2 - c \cdot \left&lt; h_1, h_2 \right&gt;_H - \bar c \cdot \left&lt; h_2, h_1 \right&gt;_H \<br>\end{aligned}<br>$$</p>
<p>Let $c = \frac{1}{ ||h_2||^2 } \cdot \left&lt; h_2, h_1 \right&gt;_H$, then<br>$$<br>\begin{aligned}<br>    || h_1 - c \cdot h_2 ||^2<br>        &amp;= || h_1||^2 - \frac{1}{ ||h_2||^2 } \cdot \left| \left&lt; h_2, h_1 \right&gt;_H \right|^2<br>        &amp;\ge 0.<br>\end{aligned}<br>$$</p>
<!-- As  -->
<!-- $$
    || \left< h_1, h_2 \right>_H || \le ||h_1|| \cdot ||h_2 || \longleftrightarrow
    \left|\left| \left< \frac{h_1}{ ||h_1|| }, \frac{h_2}{ ||h_2 || } \right>_H \right|\right| \le  1
$$

Without lose of generality, we assume that $||h_1|| = 1$ and $||h_2|| = 1$. 

$$
\begin{aligned}
    2 \cdot \mathbb{Re} \left( \left< h_1, h_2 \right>_H \right) 
        &= \left< h_1, h_2 \right>_H  + \overline{ \left< h_2, h_1 \right>_H } \\
        &= \int_{ [ -\pi, \pi] } \bar h_1 h_2  + \bar h_2 h_1 \\
        &\le \int_{ [ -\pi, \pi] } 2 \left( ||h_1||^2 + ||h_2||^2 \right)
\end{aligned}
$$ -->

<!-- We will prove that 
$$
    || \left< h_1, h_2 \right>_H ||^2 \le 1.
$$

$$
    \begin{aligned}
        \left|\left| \int_{ [ -\pi, \pi] } \bar h_1 h_2 \right|\right|^2 
        &= \left|\left| \int_{ [ -\pi, \pi] } \mathbb{Re} (\bar h_1 h_2) + i \mathbb{Im} (\bar h_1 h_2) \right|\right|^2 \\
        &= \left( \int_{ [ -\pi, \pi] } \mathbb{Re} (\bar h_1 h_2) \right)^2 +  \left( \int_{ [ -\pi, \pi] } \mathbb{Im} (\bar h_1 h_2) \right)^2 \\
        &\le \left( \int_{ [ -\pi, \pi] } \mathbb{Re} (\bar h_1 h_2) \right)^2 +  \left( \int_{ [ -\pi, \pi] } \mathbb{Im} (\bar h_1 h_2) \right)^2
    \end{aligned}
$$ -->


<p>$\square$</p>
<blockquote>
<p><strong>Lemma.</strong> $\forall i \in \mathbb{N}$, $\left&lt;  e_i, g \right&gt;_H =a_i$. </p>
</blockquote>
<p><em>Proof.</em> By Cauchy-Schwartz inequality, </p>
<p>$$<br>\begin{aligned}<br>    \left|\left| \int_{ [ -\pi, \pi] } \bar e_i (g_k - g) \right|\right|^2<br>        &amp;\le \left( \int_{ [ -\pi, \pi] } ||e_i||^2 \right) \cdot \left( \int_{ [ -\pi, \pi] } ||g_k - g||^2 \right) \<br>        &amp;= \int_{ [ -\pi, \pi] } ||g_k - g||^2 \<br>        &amp;= \int_{ [ -\pi, \pi] } \lim_l ||g_k - g_l ||^2<br>\end{aligned}<br>$$</p>
<p>By <strong>Fatou’s lemma</strong>,<br>$$<br>    \int_{ [ -\pi, \pi] } \lim_l ||g_k - g_l ||^2 \le \liminf_l \int_{ [ -\pi, \pi] } ||g_k - g_l ||^2<br>$$</p>
<p>Finally,<br>$$<br>    \lim_k  \left|\left|  \left&lt;  e_i, g_k - g \right&gt;<em>H \right|\right|^2 = \lim_k \left|\left|  \int_{ [ -\pi, \pi] } \bar e_i (g_k - g) \right|\right|^2 \le \lim_k \liminf_l \int</em>{ [ -\pi, \pi] } ||g_k - g_l ||^2  = 0.<br>$$</p>
<p>$\square$</p>
<blockquote>
<p>Remark: the above proof involves the operation of exchanging the order of two limiting operations, namely $\lim$ and $\int$. Exchanging order like this is dangerous for Riemann integration, but we can do it safely under Lebesgue integration, under proper condition. Here we use <strong>Fatou’s lemma</strong>. </p>
</blockquote>
<p>By the above lemma, the function $g$ has an important property, such that  $\forall i \in \mathbb{N}$,<br>$$<br>    \left&lt; e_i, f - g \right&gt;_H = 0.<br>$$ </p>
<blockquote>
<p><strong>Lemma.</strong> Let $h \in I^2([-\pi, \pi])$. If $\forall i \in N$,<br>$$<br>    \left&lt; e_i, h \right&gt;_H = 0,<br>$$<br>then $h = 0$ almost everywhere. </p>
</blockquote>
<p><em>Proof.</em> First, we assume that $h$ is a continuous function on $[-\pi, \pi]$. If $h \not\equiv 0$, then $|h(x)|$ achieves maximum value at some point $x^* \in [-\pi, \pi]$. Without lose of generality, we assume that<br>$$<br>    h(x^*) = M &gt; 0.<br>$$</p>
<p>By continuity of $h$, $\exists \delta \in (0, \pi)$, such that $h(x) &gt; M / 2$ in the interval<br>$$<br>    I \doteq (x^* - \delta, x^* + \delta) \cap [-\pi, \pi].<br>$$</p>
<p>We will capture the maximum value of $f$, by taking Hermitian product of $f$ with a signal function that </p>
<ol>
<li>is a linear combination of functions in $\mathfrak{F}$;</li>
<li>is $\ge 1$ in $I$;</li>
<li>is $&lt; 1$ in $[-\pi, \pi] \setminus I$.</li>
</ol>
<p>The signal function is given by<br>$$<br>    s(x) = 1 + \cos \left( \frac{1}{2} \cdot (x - x^*) \right) - \cos \frac{\delta}{2}.<br>$$</p>
<p>Observe that $s$ </p>
<ol>
<li>achieves maximum value $s(x^*) = 2 - \cos \frac{\delta}{2}$;</li>
<li>is $\ge 1$ for $x \in I$;</li>
<li>has period $4 \pi$. </li>
</ol>
<p>Hence, it increases on the interval $[x^* - 2\pi, x^*]$ and decreases on $[x^*, x^* + 2\pi]$. So</p>
<ol start="4">
<li>$s(x) &lt; 1$ for $x \in [-\pi, \pi] \setminus I$.</li>
</ol>
<p>For $n \ge 1$, $s^n(x)$ is also a linear combination of functions in  $\mathfrak{F}$. It always holds that $\left&lt; s^n(x), f \right&gt;_H \in \mathbb{R}$ and by assumption on $f$, it holds that<br>$$<br>    \left&lt; s^n(x), f \right&gt;_H = 0.<br>$$</p>
<p>But this is impossible. By monotonicity of integration,<br>$$<br>\int_{ [-\pi, \pi] \setminus I } f(x) s^n(x) \ dx \ge \int_{ [-\pi, \pi] \setminus I } (-M) \cdot 1 \ dx = - 2 \cdot \pi \cdot M.<br>$$</p>
<p>Define<br>$$<br>    I’ \doteq (x^* - \delta / 2, x^* + \delta / 2) \cap [-\pi, \pi].<br>$$</p>
<p>Then $|I’| \ge \delta / 2$ and $m \doteq \min_{x \in I’} s(x) &gt; 1$.<br>$$<br>    \int_{ I } f(x) s^n(x) \ dx \ge \int_{ I’ } \frac{M}{2} \cdot m^n \ dx \ge \frac{M \cdot m^n \cdot \delta }{ 4 }.<br>$$</p>
<p>This implies as $n \rightarrow \infty$,<br>$$<br>    \left&lt; s^n(x), f \right&gt;_H \ge - 2 \cdot \pi \cdot M + \frac{M \cdot m^n \cdot \delta }{ 4 } \rightarrow \infty,<br>$$</p>
<p>which is a contradiction. </p>
<p>Next, we consider the general case of $h \in I^2([-\pi, \pi])$. Define<br>$$<br>    w(x) = \int_{-\pi}^x h(t) \ dt.<br>$$</p>
<p>The function is absolutely continuous on $[-\pi, \pi]$. Moreover,<br>$w(0) = 0$ and by assumption, $w(\pi) = \left&lt; e_0, h \right&gt;<em>H = 0$. For $k \in \mathbb{N}$, integrating by part gives<br>$$<br>    \begin{aligned}<br>        \sqrt{2 \cdot \pi} \cdot \left&lt; e_{2k + 1}, w \right&gt;<em>H<br>            &amp;= \int</em>{ [-\pi, \pi] } \exp(-ikx) w(x) \ dx \<br>            &amp;= \frac{1}{ik} \exp(-ikx) w(x) \mid</em>{-\pi}^\pi - \frac{1}{ik}  \int_{ [-\pi, \pi] } \exp(-ikx) w’(x) \ dx \<br>            &amp;=  - \frac{1}{ik}  \int_{ [-\pi, \pi] } \exp(-ikx) h(x) \ dx \<br>            &amp;=  - \frac{1}{ik}  \left&lt; e_{2k + 1}, h\right&gt;_H \<br>            &amp;= 0.<br>    \end{aligned}<br>$$</p>
<p>Similarly, we can prove that $\left&lt; e_{2k + 2}, w \right&gt;_H = 0$. </p>
<p>Hence, $\forall k \ge 1$, $\left&lt; e_k, w \right&gt;_H = 0$. But it is not guaranteed that $\left&lt; e_0, w \right&gt;_H = 0$. To fix this, we construct another function<br>$$<br>    W(x) \doteq w(x) - B.<br>$$</p>
<p>where $B = \left&lt; e_0, w \right&gt;_H$ is a constant. It holds that </p>
<ol>
<li>$\forall k \ge 1$, $\left&lt; e_k, W \right&gt;_H = \left&lt; e_k, w \right&gt;_H - \left&lt; e_k, B \right&gt;_H = - B \cdot \left&lt; e_k, e_0 \right&gt;_H = 0$.   </li>
<li>$\left&lt; e_0, W \right&gt;_H = \left&lt; e_0, w \right&gt;_H + \left&lt; e_0, B \right&gt;_H = B - B \cdot \left&lt; e_0, e_0 \right&gt;_H = 0$.   </li>
</ol>
<p>By previous result, $W(x) \equiv 0$ and $w(x) \equiv B$. This implies<br>$$<br>w’(x) = h(x) = 0, \quad a.e.<br>$$</p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/24/Gamma-Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/24/Gamma-Function/" class="post-title-link" itemprop="url">Gamma Function</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-12-24 21:32:57 / Modified: 21:47:33" itemprop="dateCreated datePublished" datetime="2020-12-24T21:32:57+11:00">2020-12-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Consider the integral<br>$$<br>    \int_0^\infty e^{- st} \ dt = \frac{1}{s} e^{- st} \mid_0^\infty  = \frac{1}{s}.<br>$$</p>
<p>Taking derivative with respect to $s$ of both side, we get<br>$$<br>    \int_0^\infty t e^{- t s} \ dt = \frac{1}{s^2}.<br>$$</p>
<p>Repeating this process one more step,<br>$$<br>    \int_0^\infty t^2 e^{- st} \ dt = \frac{2}{s^3}.<br>$$</p>
<p>By induction on $k \in \mathbb{N}^+$, we obtain<br>$$<br>    \int_0^\infty t^k e^{- st} \ dt = \frac{k!}{s^{k + 1} }.<br>$$</p>
<p>Setting $s = 1$, we have<br>$$<br>    k! = \int_0^\infty t^k e^{ -t } \ dt.<br>$$</p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/19/Hermitian-Inner-Products/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/19/Hermitian-Inner-Products/" class="post-title-link" itemprop="url">Hermitian Inner Products</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-19 23:17:13" itemprop="dateCreated datePublished" datetime="2020-12-19T23:17:13+11:00">2020-12-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-23 00:41:33" itemprop="dateModified" datetime="2020-12-23T00:41:33+11:00">2020-12-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The inner product of two complex vectors $\vec u, \vec v \in \mathbb{C}^n$ is defined as<br>$$<br>    \vec u^H \vec v = [ \bar u_1, \bar u_2, …, \bar u_n ] \begin{bmatrix} v_1 \ v_2 \ . \ . \ . \ v_n \end{bmatrix} = \bar u_1 v_1 + \bar u_2 v_2 … + \bar u_n v_n.<br>$$</p>
<p>It is tempting to think the inner product should be $\vec u^T \vec v$, as the one for real vectors. Why do we take the conjugate of $\vec u$? To answer this, we need to study the geometric meaning of inner product. We begin from the inner product in $\mathbb{R}^n$. </p>
<h1 id="Inner-Product-in-mathbb-R-n"><a href="#Inner-Product-in-mathbb-R-n" class="headerlink" title="Inner Product in $\mathbb{R}^n$"></a>Inner Product in $\mathbb{R}^n$</h1><p>The inner product is closely related to the concept of “length”. Generally, we express the length of a vector in term of inner product. Here we take a reverse approach. We consider the length of a vector as a more fundamental concept. Once the length of a vector is defined, it leads naturally the definition of inner product. </p>
<p>For a vector $\vec u \in \mathbb{R}^n$, we define its length as<br>$$<br>||\vec u || = \sqrt{ \sum_{i \in [n] } u_i^2 }.<br>$$</p>
<p>Given two vector $\vec u, \vec v \in \mathbb{R}^n$, the definition of the inner product of $\vec u^T \vec v$ stems from the following problem: </p>
<blockquote>
<p>Find a vector in the subspace generated by $\vec u$, such that it distance to $\vec v$ is minimized. </p>
</blockquote>
<p>The subspace generated by $\vec u$ is the line that passes through $\vec u$ and can be represented as<br>$$<br>{ t \vec u : t \in \mathbb{R} }.<br>$$</p>
<p>The problem gives rise to the optimization problem:<br>$$<br>\min_{ t \in \mathbb{R} } ||\vec v - t \vec u||^2.<br>$$</p>
<p>Define<br>$$<br>y \doteq || \vec v - t \vec u||^2 = || \vec u ||^2 t^2 - 2 ( \vec v^T \vec u) t + || \vec  v||^2.<br>$$ </p>
<p>By differentiating and setting the derivative equal to zero, we get<br>$$<br>y’ = 2 || \vec u ||^2 t - 2 (\vec  v^T  \vec  u) = 0 \implies t = \frac{ \vec  v^T \vec u }{ || \vec u ||^2 }.<br>$$</p>
<p>The inner product appears exactly in the numerator. Its meaning is clear now: it is related to the magnitude of the closest vector in the subspace spanned by $\vec u$. In case that $\vec u$ is a unit vector, $t = \vec v^T \vec u$ is exactly the length of this closest vector. </p>
<p>This also sheds light on the meaning of orthogonality:</p>
<blockquote>
<p>If $\vec u$ is orthogonal to $\vec v$, then the best approximation (measured by $|| \cdot ||$) of $\vec v$ in the subspace spanned by $\vec u$ is the zero vector. </p>
</blockquote>
<h1 id="Inner-Product-in-mathbb-C-n"><a href="#Inner-Product-in-mathbb-C-n" class="headerlink" title="Inner Product in $\mathbb{C}^n$"></a>Inner Product in $\mathbb{C}^n$</h1><p>To get the desired expression of inner product $\mathbb{C}^n$, we first extend the definition of length in $\mathbb{R}^n$ to $\mathbb{C}^n$. For $\vec u \in \mathbb{C}^n$, define<br>$$<br>|| \vec u || = \sqrt{ \sum_{i \in [n]}  |u_i|^2 }.<br>$$</p>
<p>Given $\vec u, \vec v \in \mathbb{C}^n$, we consider the similar problem: </p>
<blockquote>
<p>Find a vector in the subspace generated by $\vec u$, such that it distance to $\vec v$ is minimized. </p>
</blockquote>
<p>However, this time the subspace generated by $\vec u$ is given by<br>$$<br>{ (t^r + i t^i ) \vec u: t^r, t^i \in \mathbb{R} }.<br>$$</p>
<p>Let $\vec u^r \in \mathbb{R}^n$ be the real part of $\vec u$ and $\vec u^i \in \mathbb{R}^n$ be its imaginary part. Then $\vec u = \vec u^r + i \vec u^i$. Similarly, we can decompose $\vec v$ into $\vec v = \vec v^r + i \vec v^i$. </p>
<p>We want to minimize<br>$$<br>\begin{aligned}<br>    y   &amp;\doteq ||(t^r + i t^i ) \vec u - \vec v||^2 \<br>        &amp;=      ||(t^r + i t^i ) (\vec u^r + i \vec u^i) - \vec v^r - i \vec v^i ||^2 \<br>        &amp;=      || t^r \vec u^r - t^i \vec u^i - \vec v^r + i ( t^i \vec u^r + t^r \vec u^i - \vec v^i ) ||^2 \<br>        &amp;=      || t^r \vec u^r - t^i \vec u^i - \vec v^r ||^2 + || t^i \vec u^r + t^r \vec u^i - \vec v^i ||^2 \<br>\end{aligned}<br>$$</p>
<p>Taking the derivative with respect to $t^r$, we get<br>$$<br>    \frac{\partial y}{ \partial t^r } = 2 ||\vec u^r||^2 t^r - 2 ( \vec u^r \cdot \vec u^i) t^i - 2 ( \vec u^r \cdot \vec v^r) + 2 ||\vec u^i||^2 t^r + 2 (\vec u^r \cdot \vec u^i ) t^i - 2 ( \vec u^i \cdot \vec v^i )<br>$$</p>
<p>where $\vec u^r \cdot \vec u^i$ denotes their dot product. Setting the derivative to zero, we have<br>$$<br>    t^r = \frac{ \vec u^r \cdot \vec v^r + \vec u^i \cdot \vec v^i }{ ||\vec u^r||^2 + ||\vec u^i||^2 } = \frac{ \vec u^r \cdot \vec v^r + \vec u^i \cdot \vec v^i }{ ||\vec u||^2  }.<br>$$</p>
<p>Also,<br>$$<br>    \frac{\partial y}{ \partial t^i } = 2 ||\vec u^i||^2 t^i - 2 ( \vec u^r \cdot \vec u^i) t^r + 2 ( \vec u^i \cdot \vec v^r) + 2 ||\vec u^r||^2 t^i + 2 (\vec u^r \cdot \vec u^i ) t^r - 2 ( \vec u^r \cdot \vec v^i ).<br>$$</p>
<p>Setting the derivative to zero, we obtain<br>$$<br>    t^i = \frac{ -\vec u^i \cdot \vec v^r + \vec u^r \cdot \vec v^i }{ ||\vec u^r||^2 + ||\vec u^i||^2 } = \frac{ -\vec u^i \cdot \vec v^r + \vec u^r \cdot \vec v^i }{ ||\vec u||^2  }.<br>$$</p>
<p>On the other hand, the Hermitian inner product is<br>$$<br>    \overline{(\vec u^r + i \vec u^i)}  \cdot (\vec v^r + i \vec v^i ) = (\vec u^r \cdot \vec v^r + \vec u^i \cdot \vec v^i) + i ( -\vec u^i \cdot \vec v^r + \vec u^r \cdot \vec v^i ).<br>$$</p>
<p>Therefore,<br>$$<br>    t^r + i t^i = \frac{1}{||\vec u||^2} \vec u^H \vec v.<br>$$</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/HermitainProduct.png?raw=true"></p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/18/Complex-Numbers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/18/Complex-Numbers/" class="post-title-link" itemprop="url">Complex Numbers</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-18 11:06:31" itemprop="dateCreated datePublished" datetime="2020-12-18T11:06:31+11:00">2020-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-21 00:43:06" itemprop="dateModified" datetime="2020-12-21T00:43:06+11:00">2020-12-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>I feel uncomfortable with complex numbers for a long time, partially because of the way they are introduced in the textbooks. They are considered necessary for solving equations such as<br>$$<br>x^2 + 1= 0.<br>$$</p>
<p>Somehow magically, a symbol $i$ is introduced, and its square $i^2$ is manually defined as $-1$. Following this, typically a set of operations defined on complex numbers as well as their properties are listed by the textbooks, leaving the question of why we can have such “magical symbol” unsolved.  </p>
<p>In this blog, I am trying to introduce complex numbers in a more natural manner, by studying <em>rotations</em>, <em>dilations</em> and <em>additions</em> of points in $\mathbb{R}^2$. I will show that these operations satisfy some properties, such that we can manipulate points in $\mathbb{R}^2$ in a similar way as real numbers (by addition and multiplication). </p>
<p>We first reflect on the real numbers $\mathbb{R}$. They are concrete, since they can be represented by points on the real axis. We can perform addition ($+$) and multiplication ($\cdot$) on them, that satisfy the following properties (referred to as <strong>field axioms</strong>). Let $a, b, c \in \mathbb{R}$, </p>
<ol>
<li><em>Associativity of addition:</em> $a + (b + c) = (a + b) + c$. </li>
<li><em>Commutativity of addition:</em> $a + b = b + a$. </li>
<li><em>Additive identity:</em> there exists a ‘zero’ element $0$ such that $a + 0 = a$.  </li>
<li><em>Additive inverse:</em> there is an element, denoted as $-a$, such that $a + (-a) = 0$.  </li>
<li><em>Associativity of  multiplication:</em> $a \cdot (b \cdot c) = (a \cdot b) \cdot c$. </li>
<li><em>Commutativity of addition:</em> $a  \cdot b = b  \cdot a$. </li>
<li><em>Multiplicative identity:</em> there exists a ‘one’ element  $1$ such that $a \cdot 1 = a$.</li>
<li><em>Multiplicative inverse:</em> there exists a element $a^{-1}$ for $a \neq 0$ such that $a \cdot a^{-1} = 1$.  </li>
<li><em>Distributivity:</em> $a \cdot (b + c) = a \cdot b + a \cdot c$.  </li>
</ol>
<p>Any set with two operations that satisfy similar properties as above is called a field. In the remainder of the blog, we show how to construct a field from $\mathbb{R}^2$. </p>
<p>We can view $\mathbb{R}^2$ as the set of vectors that originates from the origin. Hence, each element in $\mathbb{R}^2$ is considered as both a point and a vector. We will define addition and multiplication for $\mathbb{R}^2$. For the moment, we will use $\oplus$ and $\odot$ to denote addition and multiplications in $\mathbb{R}^2$, to distinguish the operations $+$ and $\cdot$ for $\mathbb{R}$. Lastly, we show that the set system $(\mathbb{R}^2, \oplus, \odot)$ constitutes a field.</p>
<p>In the remainder of the blog, when we refer to a vector, we refer to the one that originate from the origin.</p>
<h1 id="Addition-oplus"><a href="#Addition-oplus" class="headerlink" title="Addition $\oplus$"></a>Addition $\oplus$</h1><p>Addition in $\mathbb{R}^2$ is defined the same way as vector addition. When points are specified by Cartesian coordinates, it is easy to get a closed form expression of addition. Let $a = (a_1, a_2), b = (b_1, b_2) \in \mathbb{R}^2$, then<br>$$<br>a \oplus b = (a_1 + b_1, a_2 + b_2).<br>$$</p>
<!-- The figure below shows that addition is commutative.  -->

<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/addition.png?raw=true" width="400" height="340" />
</div>

<h1 id="Multiplication-odot"><a href="#Multiplication-odot" class="headerlink" title="Multiplication $\odot$"></a>Multiplication $\odot$</h1><p>When multiplying two vectors, their lengths multiply and their angles add. It is an operation that involves both dilation and rotation.</p>
<p>We investigate multiplication in the polar coordinates. For $a \in \mathbb{R}^2$, let $|a| \in \mathbb{R}$ be its length and $\theta_a$ be its angle with respect to the $x$-axis. We can write $a$ as:<br>$$<br>a = |a| \angle \theta_a.<br>$$</p>
<p>For $a, b \in \mathbb{R}^2$,<br>$$<br>a \odot b \doteq (|a| \cdot |b|) \angle (\theta_{v_1} + \theta_{v_2} )<br>$$ </p>
<p>The result is a vector (from the origin) with length $|a| \cdot |b|$ and angle $\theta_a + \theta_b$. </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/multiplication.png?raw=true" width="400" height="340" />
</div>

<p><em>Remark: we have use different coordinate systems to get closed form formulas for addition and multiplication separately. This is immaterial. We need only to make sure both addition and multiplication are functions in $\mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}^2$. Writing the closed form of a function in a coordinate system is just a specific way to describe it. For a specific function $f$ in $\mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}^2$, it maps each pair of $a, b \in \mathbb{R}^2 \times \mathbb{R}^2$ to a unique point $f(a, b) \in \mathbb{R}^2$, whether this function is described in Cartesian or polar coordinates. If we want, we can even describe it by language. For example, the sentence “$f$ takes every pair of $\forall a, b \in \mathbb{R}^2$ to the origin.” defines a constant function.</em> </p>
<h1 id="Field-Verification"><a href="#Field-Verification" class="headerlink" title="Field Verification"></a>Field Verification</h1><p>We need to verify that the set system $(\mathbb{R}^2, \oplus, \odot)$ is a field.</p>
<h2 id="1-Associativity-of-addition-a-oplus-b-oplus-c-a-oplus-b-oplus-c"><a href="#1-Associativity-of-addition-a-oplus-b-oplus-c-a-oplus-b-oplus-c" class="headerlink" title="1. Associativity of addition: $a \oplus (b \oplus c) = (a \oplus b) \oplus c$"></a>1. <em>Associativity of addition:</em> $a \oplus (b \oplus c) = (a \oplus b) \oplus c$</h2><p>The figure below shows that addition is associative. </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/addition-associativity.png?raw=true" width="400" height="340" />
</div>

<p>Associativity is also easy to verify in Cartesian coordinates. Let $a = (a_1, a_2), b = (b_1, b_2), c = (c_1, c_2)$, we get<br>$$<br>a \oplus ( b \oplus c) = (a_1 + b_1 + c_1, a_2 + b_2 + c_2) = (a \oplus b) \oplus c.<br>$$</p>
<p>$\square$</p>
<h2 id="2-Commutativity-of-addition-a-oplus-b-b-oplus-a"><a href="#2-Commutativity-of-addition-a-oplus-b-b-oplus-a" class="headerlink" title="2. Commutativity of addition: $a \oplus b = b \oplus a$"></a>2. <em>Commutativity of addition:</em> $a \oplus b = b \oplus a$</h2><p>This is verified when we define addition or we can check it easily in Cartesian coordinate system. </p>
<p>$\square$</p>
<h2 id="3-Additive-identity-exists-vec-0-s-t-a-oplus-vec-0-a"><a href="#3-Additive-identity-exists-vec-0-s-t-a-oplus-vec-0-a" class="headerlink" title="3. Additive identity: $\exists$ $\vec 0$, s.t. $a \oplus \vec 0 = a$"></a>3. <em>Additive identity:</em> $\exists$ $\vec 0$, s.t. $a \oplus \vec 0 = a$</h2><p>$(0, 0) \in \mathbb{R}^2$ serves as the zero element. </p>
<p>$\square$</p>
<h2 id="4-Additive-inverse-exists-a-in-mathbb-R-2-s-t-a-oplus-a-vec-0"><a href="#4-Additive-inverse-exists-a-in-mathbb-R-2-s-t-a-oplus-a-vec-0" class="headerlink" title="4. Additive inverse: $\exists (-a) \in \mathbb{R}^2$, s.t. $a \oplus (-a) = \vec 0$"></a>4. <em>Additive inverse:</em> $\exists (-a) \in \mathbb{R}^2$, s.t. $a \oplus (-a) = \vec 0$</h2><p>Let $(-a)$ be the vector that is symmetric to $a$ with respect to the origin. Then $a \oplus (-a) = 0$. Indeed, if $a = (a_1, a_2)$,<br>$$<br>(-a) = (-a_1, -a_2).<br>$$</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/inverse.png?raw=true" width="400" height="340" />
</div>

<p>$\square$</p>
<h2 id="5-Associativity-of-multiplication-a-odot-b-odot-c-a-odot-b-odot-c"><a href="#5-Associativity-of-multiplication-a-odot-b-odot-c-a-odot-b-odot-c" class="headerlink" title="5. Associativity of  multiplication: $a \odot (b \odot c) = (a \odot b) \odot c$"></a>5. <em>Associativity of  multiplication:</em> $a \odot (b \odot c) = (a \odot b) \odot c$</h2><p>It is more convenient to check the properties involved multiplication via polar coordinates. </p>
<p>$$<br>a \odot b \odot c = ( |a| \cdot |b| \cdot |c| ) \angle (\theta_a + \theta_b + \theta_c ) = a\odot (b \odot c).<br>$$</p>
<p>$\square$</p>
<h2 id="6-Commutativity-of-addition-a-odot-b-b-odot-a"><a href="#6-Commutativity-of-addition-a-odot-b-b-odot-a" class="headerlink" title="6. Commutativity of addition: $a  \odot b = b  \odot a$"></a>6. <em>Commutativity of addition:</em> $a  \odot b = b  \odot a$</h2><p>Similarly,<br>$$<br>a \odot b = (|a| \cdot |b|) \angle (\theta_a + \theta_b ) = (|b| \cdot |a|) \angle (\theta_a + \theta_b ) = b \odot a.<br>$$</p>
<p>$\square$</p>
<h2 id="7-Multiplicative-identity-exists-vec-1-s-t-a-odot-vec-1-a"><a href="#7-Multiplicative-identity-exists-vec-1-s-t-a-odot-vec-1-a" class="headerlink" title="7. Multiplicative identity: $\exists \vec 1$ s.t. $a \odot \vec 1 = a$"></a>7. <em>Multiplicative identity:</em> $\exists \vec 1$ s.t. $a \odot \vec 1 = a$</h2><p>$(1, 0) = |1|\angle 0$ is the ‘one’ vector.</p>
<p>$\square$</p>
<h2 id="8-Multiplicative-inverse-If-a-neq-vec-0-exists-a-1-s-t-a-odot-a-1-vec-1"><a href="#8-Multiplicative-inverse-If-a-neq-vec-0-exists-a-1-s-t-a-odot-a-1-vec-1" class="headerlink" title="8. Multiplicative inverse: If $a \neq \vec 0$, $\exists a^{-1}$, s.t. $a \odot a^{-1} = \vec 1$"></a>8. <em>Multiplicative inverse:</em> If $a \neq \vec 0$, $\exists a^{-1}$, s.t. $a \odot a^{-1} = \vec 1$</h2><p>The multiplicative inverse is given by<br>$$<br>a^{-1} = \frac{1}{|a|} \angle (-\theta_a).<br>$$</p>
<p>$\square$</p>
<h2 id="9-Distributivity-a-odot-b-oplus-c-a-odot-b-oplus-a-odot-c"><a href="#9-Distributivity-a-odot-b-oplus-c-a-odot-b-oplus-a-odot-c" class="headerlink" title="9. Distributivity: $a \odot (b \oplus c) = a \odot b \oplus a \odot c$"></a>9. <em>Distributivity:</em> $a \odot (b \oplus c) = a \odot b \oplus a \odot c$</h2><p>Distributivity involves both addition and multiplication. We prove it geometrically.</p>
<p>Let $w = b \oplus c$. </p>
<p>First suppose $|a| = 1$. In such case, $a = |1| \angle \theta_a$ and<br>$$<br>w \odot a = |w| \angle (\theta_w + \theta_a)<br>$$ </p>
<p>Multiplying $w$ by $a$ is equivalent to rotating it by an angle of $\theta_a$. Let $w’, b’, c’$ be the vectors obtained by rotating $b, c, w$ by angle $\theta_a$. Then<br>$$<br>w’ = a \odot w, \quad b’ = a \odot b, \quad c’ = a \odot c.<br>$$</p>
<p>The figure belows show that<br>$$<br>w’ = b’ \oplus c’ \implies a \odot w = a \odot b \oplus a \odot c.<br>$$</p>
<p>In case $|a| &gt; 1$, $|w’|$, $|b’|$ and $|c’|$ are magnified by a factor of $|a|$ and the same conclusion apply. </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/distributivity.png?raw=true" width="600" height="340" />
</div>

<p>$\square$</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>We have shown that $(\mathbb{R}^2, \oplus, \odot)$ is indeed a field. For convenience, we use $\mathbb{C}$ to denote this set system. If we define<br>$$<br>\vec j = (1, 0), \qquad \vec i = (0, 1),<br>$$</p>
<p>Hence, $\vec j = \vec 1$ and </p>
<ol>
<li>$\vec j \odot \vec j = \vec 1$, </li>
<li>$\vec i \odot \vec j = \vec j \odot \vec i = \vec i$, </li>
<li>$\vec i \odot \vec i = |1| \angle ( \pi / 2 + \pi / 2) = (-1, 0) = -\vec j$.</li>
</ol>
<p>For $a = (a_1, a_2), b = (b_1, b_2) \in \mathbb{R}^2$, we can write<br>$$<br>a = a_1 \vec j \oplus a_2 \vec i, \qquad b = b_1 \vec j \oplus b_2 \vec i,<br>$$</p>
<p>and<br>$$<br>    a \oplus b = (a_1 + b_1) \vec j \oplus (a_2 + b_2) \vec i.<br>$$</p>
<p>To derive a formula for multiplication, we use the <em>distributivity</em> property of the two operations<br>$$<br>\begin{aligned}<br>    a \odot b<br>        &amp;= a \odot ( b_1 \vec j \oplus b_2 \vec i ) \<br>        &amp;= a \odot (b_1 \vec j) \oplus a \odot (b_2 \vec i) \<br>        &amp;= (a_1 \vec j \oplus a_2 \vec i) \odot (b_1 \vec j) \oplus (a_1 \vec j \oplus a_2 \vec i) \odot (b_2 \vec i) \<br>        &amp;=  (a_1 \cdot b_1) (\vec j \odot \vec j)  \oplus (a_2 \cdot b_1) (\vec i \odot \vec j) \oplus (a_1 \cdot b_2) (\vec j \odot \vec i) \oplus (a_2 \cdot b_2)  (\vec i \odot \vec i) \<br>        &amp;=  (a_1 \cdot b_1) \vec j  \oplus (a_2 \cdot b_1) \vec i \oplus (a_1 \cdot b_2) \vec i \oplus (a_2 \cdot b_2)  ( - \vec j ) \<br>        &amp;=  (a_1 \cdot b_1 - a_2 \cdot b_2) \vec j  \oplus (a_2 \cdot b_1 + a_1 \cdot b_2) \vec i.  \<br>\end{aligned}<br>$$</p>
<p>We are almost done. We now further simplify the notations. As $\vec j$ plays the role of $\vec 1$, we can omit it. For $a = (a_1, a_2)$, we use only $\vec i$ to distinguish its $y$-coordinate from the $x$-coordinate:<br>$$<br>a = a_1 \oplus a_2 \vec i.<br>$$</p>
<p>Addition and multiplication can be performed correctly in this representation:</p>
<ol>
<li>$a \oplus b = a_1 \oplus a_2 \vec i \oplus b_1 \oplus b_2 \vec i = (a_1 + b_1) \oplus (a_2 + b_2) \vec i$.   </li>
<li>$$<br> \begin{aligned}<pre><code> a \odot b 
     &amp;= a \odot ( b_1  \oplus b_2 \vec i ) \\
     &amp;= a \odot b_1 \oplus a \odot (b_2 \vec i) \\
     &amp;= (a_1  \oplus a_2 \vec i) \odot b_1 \oplus (a_1  \oplus a_2 \vec i) \odot (b_2 \vec i) \\
     &amp;=  (a_1 \cdot b_1) \oplus (a_2 \cdot b_1) \vec i \oplus (a_1 \cdot b_2) \vec i \oplus (a_2 \cdot b_2)  (\vec i \odot \vec i) \\
     &amp;=  (a_1 \cdot b_1)   \oplus (a_2 \cdot b_1) \vec i \oplus (a_1 \cdot b_2) \vec i \oplus (- a_2 \cdot b_2) \\
     &amp;=  (a_1 \cdot b_1 - a_2 \cdot b_2)   \oplus (a_2 \cdot b_1 + a_1 \cdot b_2) \vec i.  \\
</code></pre>
 \end{aligned}<br>$$</li>
</ol>
<p>Finally, observe that if we restrict $\oplus$ and $\odot$ to the points on the $x$-axis, they perform similarly as $+$ and $\cdot$ do for $\mathbb{R}$. If we do not distinguish a number $(a_1, 0) \in \mathbb{R}^2$ and $a_1 \in \mathbb{R}$, replace $\oplus$ and $\odot$ with $+$ and $\cdot$, we get exactly the same formula for standard complex number addition and multiplication: $\forall a, b \in \mathbb{R}^2$, </p>
<ol>
<li>$a + b = (a_1 + b_1) + (a_2 + b_2) \vec i$, </li>
<li>$a \cdot b = (a_1 + a_2 \vec i) \cdot (b_1 + b_2 \vec i) = (a_1 \cdot b_1 - a_2 \cdot b_2) + (a_1 \cdot b_2 + a_2 \cdot b_1) \vec i$. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/10/Stirling-s-Approximation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/10/Stirling-s-Approximation/" class="post-title-link" itemprop="url">Stirling's Approximation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-10 10:22:35" itemprop="dateCreated datePublished" datetime="2020-12-10T10:22:35+11:00">2020-12-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-01 12:40:39" itemprop="dateModified" datetime="2021-01-01T12:40:39+11:00">2021-01-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The Stirling’s approximation [1] states that $\forall n \in \mathbb{N}^+$,<br>$$<br>\sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ \frac{1}{ 12n + 1 } } \le n! \le \sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ \frac{1}{ 12n } }.<br>$$</p>
<p>As $\forall x \in R, e^x \ge 1 + x$, and for $0 &lt; x &lt; 0.5$, $e^x \le 1 + 2 x$, we get $\forall n \in \mathbb{N}^+$,<br>$$<br>\sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 +  \frac{1}{ 12n + 1 } \right) \le n! \le \sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 +  \frac{1}{ 6n } \right).<br>$$</p>
<p>Asymptotically,<br>$$<br>n! = \sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 +  \Theta \left( \frac{1}{n} \right) \right).<br>$$</p>
<p>In the remainder of the blog, we try to approach this estimation incrementally. First we have two trivial facts. </p>
<h3 id="1-n-le-n-n"><a href="#1-n-le-n-n" class="headerlink" title="1. $n! \le n^n$."></a>1. $n! \le n^n$.</h3><h3 id="2-n-ge-n-2-n-2"><a href="#2-n-ge-n-2-n-2" class="headerlink" title="2. $n! \ge (n / 2)^{n / 2}$."></a>2. $n! \ge (n / 2)^{n / 2}$.</h3><p>   <em>Proof:</em> Clearly this holds for even numbers. When $n$ is odd, $\lceil n / 2 \rceil = (n + 1) / 2$. There are $(n + 1) / 2$ numbers in $[n]$ that are $\ge (n + 1) / 2$.<br>   $$<br>   n! \ge \prod_{i = (n + 1) / 2)}^n i \ge \big( (n + 1) / 2) \big)^{ (n + 1) / 2) } \ge (n / 2)^{n / 2}<br>   $$</p>
<p>   $\square$</p>
<p>Somewhat unexpectedly, we can combine inequality (1) and (2) to obtain the asymptotical trend of $\log n!$ up to some constant:<br>$$<br>(n / 2) \log_2 n - (n / 2) \le \log_2 n! \le n \log_2 n.<br>$$</p>
<p>This implies that<br>$$<br>\log_2 n! = \Theta( n \log n).<br>$$</p>
<p>We proceed to get even sharper bounds. </p>
<h3 id="3-n-ge-frac-n-n-exp-n-n-e-n"><a href="#3-n-ge-frac-n-n-exp-n-n-e-n" class="headerlink" title="3. $n! \ge \frac{n^n}{ \exp(n) } = (n / e)^n$."></a>3. $n! \ge \frac{n^n}{ \exp(n) } = (n / e)^n$.</h3><p><em>Proof:</em> Using the Taylor expansion for $\exp(x)$ for $\forall x \in \mathbb{R}$, we get<br>$$<br>    \exp(x) = 1 + x + \frac{x^2}{2!} + … + \frac{x^n}{n!} + …<br>$$</p>
<p>Keeping only the $n$-th term of the expansion, we have<br>$$<br>    n! \ge \frac{x^n}{\exp(x) }.<br>$$</p>
<p>Taking $x = n$ maximizes the RHS. </p>
<p>$\square$</p>
<h3 id="4-n-ge-e-n-e-n"><a href="#4-n-ge-e-n-e-n" class="headerlink" title="4. $n! \ge e ( n / e)^n$."></a>4. $n! \ge e ( n / e)^n$.</h3><p><em>Proof:</em> Using the Taylor expansion for $\exp(x)$ for $\forall x \in \mathbb{R}$, we get<br>$$<br>    \exp(x) = 1 + x + \frac{x^2}{2!} + … + \frac{x^n}{n!} + …<br>$$</p>
<p>Setting $x = n$, we get<br>$$<br>\exp(n) \ge \frac{n^{n - 1} }{ (n - 1)!} + \frac{ n^n }{n !} = 2 \frac{ n^n }{n !},<br>$$<br>which finishes our proof. </p>
<p>$\square$</p>
<p>Next we resort to calculus to get finer estimation.</p>
<h3 id="5-n-ge-e-n-e-n"><a href="#5-n-ge-e-n-e-n" class="headerlink" title="5. $n! \ge e ( n / e)^n$."></a>5. $n! \ge e ( n / e)^n$.</h3><p><em>Proof:</em> As the figure below shows, $\sum_{i \in [n] } \ln i$ is the sum of areas of a set of rectangles and it upper bounds the area under the curve $y = \log t$:<br>$$<br>\sum_{i \in [n]} \ln i \ge \int_1^n \ln t \ dt = t \ln t \mid_1^n - \int_1^n 1 \ dt = n \ln n - n + 1.<br>$$</p>
<p>Hence, $n! \ge e (n / e)^n$.<br><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/StirlingApproximation/LowerBound.png?raw=true"></p>
<p>$\square$</p>
<h3 id="6-n-le-e-sqrt-n-n-e-n"><a href="#6-n-le-e-sqrt-n-n-e-n" class="headerlink" title="6. $n! \le e \sqrt{n} (n / e)^n$."></a>6. $n! \le e \sqrt{n} (n / e)^n$.</h3><p><em>Proof:</em> For a rectangle, its area can be decomposed into the curvilinear area under $y = \ln t$ and a curvy triangle. To obtain an upper bound for $\sum_{i \in [n] } \ln i$, we want to upper bound the areas of the curvy triangles. </p>
<p>How are we going to do that? Image that we  put a wall at $t = 1$, and line the curvy triangles up against the wall. By the figure, they take up at most half the area of the rectangle with width $1$ and height $\ln n$. Then</p>
<p>$$<br>\sum_{i \in [n]} \ln i \le \int_1^n \ln t \ dt + \frac{1}{2} \ln n = n \ln n - n + 1 + \frac{1}{2} \ln n.<br>$$</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/StirlingApproximation/UpperBound1.png?raw=true"></p>
<p>$\square$</p>
<p>By inequality (4) and (5), we get<br>$$<br>e ( n / e)^n \le n! \le e \sqrt{n} (n / e)^n.<br>$$</p>
<p>So<br>$$<br>n! = \Theta ( (n / e)^n ).<br>$$</p>
<p>Indeed the approximation of $e \sqrt n (n / e)^n$ is quite sharp. If we compute its ratio with the near accurate estimation $\sqrt{2 \pi} \sqrt n (n / e)^n$, we see that the ratio is very close to 1:<br>$$<br>\frac{ e \sqrt n (n / e)^n  }{ \sqrt{2 \pi} \sqrt n (n / e)^n } = \frac{e }{\sqrt{ 2 \pi} } \approx 1.084.<br>$$ </p>
<p>Therefore, when $n$ is large, $e \sqrt n (n / e)^n$ is within $10%$ of the real value of $n!$. </p>
<h3 id="7-n-le-e-sqrt-n-n-e-n"><a href="#7-n-le-e-sqrt-n-n-e-n" class="headerlink" title="7. $n! \le e \sqrt{n} (n / e)^n$."></a>7. $n! \le e \sqrt{n} (n / e)^n$.</h3><p>The same inequality can be obtained in the following manner. </p>
<p><em>Proof:</em> We lower bound the integral by the areas of trapezoids. The trapezoid between $t$ and $t + 1$ has base heights $\ln t$ and $\ln (t + 1)$, and area $\frac{1}{2} (\ln t + \ln (t + 1) )$. Hence, </p>
<p>$$<br>\begin{aligned}<br>    \sum_{i = 1}^n \ln i &amp;= \frac{1}{2} \sum_{t = 1}^{n - 1} (\ln t + \ln (t + 1) ) + \frac{1}{2} \ln n \<br>    &amp;\le \int_1^n \ln t \ dt + \frac{1}{2} \ln n \<br>    &amp;= n \ln n - n + 1 + \frac{1}{2} \ln n<br>\end{aligned}<br>$$<br><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/StirlingApproximation/UpperBound0.png?raw=true"></p>
<p>$\square$</p>
<h3 id="8-n-in-e-11-12-sqrt-n-left-frac-n-e-right-n-e-1-12n-1-e-12-13-sqrt-n-left-frac-n-e-right-n-e-1-12-n"><a href="#8-n-in-e-11-12-sqrt-n-left-frac-n-e-right-n-e-1-12n-1-e-12-13-sqrt-n-left-frac-n-e-right-n-e-1-12-n" class="headerlink" title="8. $n! \in [e^{11 / 12} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ 1 / (12n + 1)}, e^{12 / 13} \sqrt{n} \left( \frac{n}{e} \right)^n e^{1 / (12 n) }]$"></a>8. $n! \in [e^{11 / 12} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ 1 / (12n + 1)}, e^{12 / 13} \sqrt{n} \left( \frac{n}{e} \right)^n e^{1 / (12 n) }]$</h3><p><em>Proof:</em> Let<br>$$<br>\begin{aligned}<br>    U_k &amp;= \int_k^{k + 1} \ln t \ dt, \<br>    L_k &amp;= \frac{1}{2} \left(\ln k + \ln (k + 1) \right).<br>\end{aligned}<br>$$</p>
<p>The estimation error in the previous section is determined by the gap between $U_k$ and $L_k$, for $k \in [n - 1]$. To investigate it carefully, define $\forall k \in [n - 1]$,<br>$$<br>    \epsilon_k \doteq U_k - L_k .<br>$$</p>
<p>Denote $S = \sum_{i \in [n] } \ln i$ and $\hat S = n \ln n - n + 1 + \frac{1}{2} \ln n$. The derivation in the previous section shows<br>$$<br>\hat S - S = \sum_{i = 1}^{n - 1} \epsilon_k.<br>$$</p>
<p>For a fixed $k$, we can compute $\epsilon_k$ in closed form:<br>$$<br>\begin{aligned}<br>    \epsilon_k<br>        &amp;= t \ln t \mid_k^{k + 1} - t \mid_k^{k + 1} - \frac{1}{2} \left( \ln k + \ln (k + 1) \right) \<br>        &amp;= \ln \frac{ (k + 1)^{k + 1} }{ k^k } \cdot \frac{1}{ k^{\frac{1}{2} } (k + 1)^{\frac{1}{2} } } - 1 \<br>        &amp;= \ln \left( \frac{k + 1}{k } \right)^{ k + \frac{1}{2} } - 1 \<br>        &amp;= \left(  \frac{2k + 1}{2} \right) \cdot \ln \left( \frac{k + 1}{k } \right)- 1 \<br>\end{aligned}<br>$$</p>
<p>Let $x = \frac{1}{2k + 1}$. As $k + 1 = \frac{1}{2}( (2k + 1) + 1)$, and $k = \frac{1}{2}( (2k + 1) - 1)$, we can write<br>$$<br>    \epsilon_k = \left(  \frac{2k + 1}{2} \right) \cdot \ln \left( \frac{1 + k }{k } \right)- 1 = \frac{1}{2 x} \ln \frac{1 + x}{1 - x} - 1.<br>$$</p>
<p>Using the Taylor’s expansions for $y = \ln(1 -x)$ and $y = \ln(1 + x)$, we have<br>$$<br>\ln(1 - x) = -(x + \frac{x^2}{2} + \frac{x^3}{3} + …) \<br>\ln(1 + x) = -(-x + \frac{x^2}{2} - \frac{x^3}{3} + …) \<br>$$</p>
<p>and<br>$$<br>\ln \frac{1 + x}{1 - x} = 2 (x + \frac{x^3}{3} + \frac{x^5}{5} + \frac{x^7}{7} + … ).<br>$$</p>
<p>It follows that<br>$$<br>\epsilon_k = \frac{x^2}{3} + \frac{x^4}{5} + \frac{x^6}{7} + …<br>$$</p>
<p>We can upper bound $\epsilon_k$ by<br>$$<br>\begin{aligned}<br>    \epsilon_k<br>        &amp;\le \frac{x^2}{3} ( 1 + x^2 +x^4 + … ) \<br>        &amp;= \frac{x^2}{3} \frac{1}{1 - x^2} \<br>        &amp;= \frac{1}{3} \frac{1}{ (2k + 1)^2 - 1} \<br>        &amp;= \frac{1}{12} \left(\frac{1}{k } - \frac{1}{k + 1} \right).<br>\end{aligned}<br>$$</p>
<p>Also, we can lower bound $\epsilon_k$ by<br>$$<br>\begin{aligned}<br>    \epsilon_k<br>        &amp;\ge \frac{x^2}{3} ( 1 + x^2 / 3 +x^4 / 3^2 + … ) \<br>        &amp;= \frac{x^2}{3} \frac{1}{1 - x^2 / 3} \<br>        &amp;= \frac{1}{ 3(2k + 1)^2 - 1} \<br>        &amp;= \frac{1}{ 12(k^2 + k + \frac{2}{12} ) } \<br>        &amp;\ge \frac{1}{ 12(k + 1 + \frac{1}{12}) ( k + \frac{1}{12} ) } \<br>        &amp;= \frac{1}{12} \left(\frac{1}{k + \frac{1}{12} } - \frac{1}{k + 1 + \frac{1}{12} } \right).<br>\end{aligned}<br>$$</p>
<p>Let $A = \sum_{k = 1}^\infty \epsilon_k$ and $R = \sum_{k = n}^\infty \epsilon_k$. Then<br>$$<br>\begin{aligned}<br>    \hat S - S &amp;= A - R \<br>    \implies S &amp;= \hat S - A + R \<br>    \implies S &amp;= n \ln n - n + 1 + \frac{1}{2} \ln n - A + R \<br>    \implies n! &amp;= e^{1 - A} \sqrt{n} \left( \frac{n}{e} \right)^n e^R.<br>\end{aligned}<br>$$</p>
<p>By previous inequalities, we know<br>$$<br>    \begin{aligned}<br>        A &amp;\ge \frac{1}{12} \sum_{k = 1}^\infty \left( \frac{1}{k + \frac{1}{12} } - \frac{1}{k + 1 + \frac{1}{12} } \right) = \frac{1}{12} \frac{1}{1 + \frac{1}{12} } = \frac{1}{13} \<br>        A &amp;\le \frac{1}{12} \sum_{k = 1}^\infty \left(\frac{1}{k } - \frac{1}{k + 1} \right) = \frac{1}{12}.<br>    \end{aligned}<br>$$</p>
<p>Similarly, we can get $\frac{1}{12n + 1 } = \frac{1}{12} \frac{1}{n + \frac{1}{12} } \le R \le \frac{1}{12n}$. To finish our proof, we have<br>$$<br>n! \in [e^{11 / 12} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ 1 / (12n + 1)}, e^{12 / 13} \sqrt{n} \left( \frac{n}{e} \right)^n e^{1 / (12 n) }]<br>$$</p>
<p>$\square$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] H. Robbins, “A Remark on Stirling’s Formula,” The American Mathematical Monthly, vol. 62, no. 1, pp. 26–29, 1955</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/09/Inverting-Functions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/09/Inverting-Functions/" class="post-title-link" itemprop="url">Inverting Functions</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-12-09 10:21:46 / Modified: 11:07:15" itemprop="dateCreated datePublished" datetime="2020-12-09T10:21:46+11:00">2020-12-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We come across the problem of inverting a function often in computer science. Instead of solving it by brute force, sometimes it would be much easier to do it with the help of $\Theta( \cdot )$, if we only care about the asymptotic trend. See two examples below. </p>
<ol>
<li>If $t \ln t = n$, then $t = \Theta( n / \ln n )$. </li>
</ol>
<p>$$<br>\begin{aligned}<br>            &amp;\ln t + \ln \ln t = \ln n \<br>\implies    &amp;\ln t = \Theta( \ln n) \<br>\implies    &amp;t \Theta (\ln n) = n \<br>\implies    &amp;t = \Theta \left( \frac{n}{\ln n} \right)<br>\end{aligned}<br>$$</p>
<ol start="2">
<li>If $t^2 \ln t = n^3$, then $t = \Theta( n / \ln n )$.<br>$$<br>\begin{aligned}<pre><code>     &amp;2\ln t + \ln \ln t = 3\ln n \\
</code></pre>
\implies    &amp;\ln t = \Theta( \ln n) \<br>\implies    &amp;t^2 \Theta (\ln n) = n^3 \<br>\implies    &amp;t^2 = \Theta \left( \frac{n^3}{\ln n} \right) \<br>\implies    &amp;t = \Theta \left( \sqrt \frac{n^3 }{\ln n} \right)<br>\end{aligned}<br>$$</li>
</ol>
<p><strong>Application</strong></p>
<p>Suppose there is an algorithm that has running time $O(n^3 / t) + O(t \log t)$, for any $t &gt; 0$. We want to choose a $t$ that minimizes the asymptotic time complexity of the algorithm. </p>
<p>As for any $a, b &gt; 0$,<br>$$<br>\max {a, b } \le a + b \le 2 \cdot \max{ a, b }.<br>$$</p>
<p>It suffices to choose a $t$ that minimizes $\max{ n^3 / t, t \log t}$. By the result of previous section, we can choose<br>$$<br>t = \Theta \left( \sqrt \frac{n^3}{\log n} \right),<br>$$</p>
<p>and the time complexity is<br>$$<br>O(\sqrt{n^3 \log n}).<br>$$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1]. Ryan O’Donnell, Lecture 2, CS Theory Toolkit, CARNEGIE MELLON UNIVERSITY</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/27/The-Prime-Number-Theory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/27/The-Prime-Number-Theory/" class="post-title-link" itemprop="url">The Prime Number Theory</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-27 21:59:17" itemprop="dateCreated datePublished" datetime="2020-11-27T21:59:17+11:00">2020-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-01 23:33:18" itemprop="dateModified" datetime="2020-12-01T23:33:18+11:00">2020-12-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>For $n \in \mathbb{N}^+$, define $\pi(n)$ be the number of prime numbers that are at most $n$. The prime number theory states that<br>$$<br>\lim_{n} \pi(n) \frac{\ln n}{n} = 1.<br>$$</p>
<p>We show here a weaker version of<br>$$<br>\frac{\pi(n)}{ n } = \Theta (\frac{1}{\log n}).<br>$$</p>
<p>That is, the density of the prime number in $[1, n]$ is $\Theta(1 / \log_2 n)$, which is inversely proportional to the number of bits required to represent $n$. </p>
<blockquote>
<p>Theorem. $n \in \mathbb{N}^+, n \ge 3$, it holds that<br>$$<br>    \frac{1}{2 \log_2 n} \le \frac{\pi(n)}{n} \le  \frac{2}{\ln n}<br>$$</p>
</blockquote>
<p>For the first inequality, we will show that $\forall k \in [n]$,<br>$$<br>    \binom{n}{ k} \le n^{ \pi(n) }.<br>$$</p>
<p>Roughly speaking, the prime factorization of $\binom{n}{k}$ consists of at most $\pi(n)$ different primes $\le n$. The inequality follows if we can prove that the highest power of a prime in the factorization is at most $n$. We can choose $k = \lfloor n / 2 \rfloor$ to maximize the lower bound. </p>
<p>For the second inequality, we use<br>$$<br>    (k + 2)^{ \pi(2k + 1) - \pi(k + 1) } \le \binom{ 2k + 1}{ k }<br>$$<br>for $k \in \mathbb{N}^+$ and upper bound $\binom{ 2k + 1}{ k }$ by $2^{2k}$. We only consider odd numbers $2k + 1$ here because $\pi(2k + 2) = \pi(2k + 1), \forall k \in \mathbb{N}^+$. </p>
<h3 id="Proof-of-The-Theorem-Part-1"><a href="#Proof-of-The-Theorem-Part-1" class="headerlink" title="Proof of The Theorem, Part 1."></a>Proof of The Theorem, Part 1.</h3><p>We need a few lemmas for the proof. </p>
<p><strong><em>Definition.</em></strong> Given a prime number $p$ and $n \in \mathbb{N}^+$, $m_p(n)$ is the number of large integer $k$ such that $p^k$ divides $n$. </p>
<h4 id="Lemma-1"><a href="#Lemma-1" class="headerlink" title="Lemma 1."></a>Lemma 1.</h4><blockquote>
<p>$$<br>m_p(n!) = \sum_{j = 1}^\infty \lfloor \frac{n}{p^j} \rfloor<br>$$</p>
</blockquote>
<p><em>Proof.</em> We count the number of time $p$ divides $n!$. Each multiple of $p$ that is in $[n]$ contributes a factor. Each multiple of $p^2$ that is in $[n]$ contributes another factor,  and so on. </p>
<p>$\square$</p>
<h4 id="Lemma-2"><a href="#Lemma-2" class="headerlink" title="Lemma 2."></a>Lemma 2.</h4><blockquote>
<p>Let $a, b \in \mathbb{N}^+$, $a &gt; b &gt; 0$. Given a prime $p$ and an integer $k \in \mathbb{N}^+$, if $p^k$ divided $\binom{a}{b}$, then $p^k \le a$.  </p>
</blockquote>
<p><em>Proof.</em> Let $k’ = m_p(\binom{a}{b})$. It suffices to show that $p^{k’} \le a$.<br>$$<br>\begin{aligned}<br>    k’<br>        &amp;= m_p( \frac{a!}{b! (a - b)!} ) \<br>        &amp;= m_p(a!) - m_p(b!) - m_p( (a - b)! ) \<br>        &amp;= \sum_{j = 1}^\infty \left( \lfloor \frac{a}{p^j} \rfloor - \lfloor \frac{b}{p^j} \rfloor - \lfloor \frac{b - a}{p^j} \rfloor \right)<br>\end{aligned}<br>$$</p>
<p>Each summand is either 1 or 0. Further, when $p^j &gt; a$ (i.e., $j &gt; \log_p a$), the summand is 0. It concludes that $k’ \le \log_p a$. </p>
<p>$\square$   </p>
<h4 id="Lemma-3"><a href="#Lemma-3" class="headerlink" title="Lemma 3."></a>Lemma 3.</h4><blockquote>
<p>$$<br>    \binom{n}{ \lfloor n / 2 \rfloor} \le n^{ \pi(n) }<br>$$<br><em>Proof.</em> Consider the unique prime factorization of $\binom{n }{ \lfloor n / 2 \rfloor}$,<br>$$<br>\binom{n}{ \lfloor n / 2 \rfloor } = p_1^{k_1} p_2^{k_2} … p_t^{k_t},<br>$$<br>where $p_i \neq p_j$, for $1 \le i &lt; j \le t$. By Lemma 2 we have $\forall i \in [t]$, $p_i^{k_i} \le n$ and $p_i$ is a prime number at most $n$. Hence $t \le \pi(n)$. It follows that<br>$$<br>\binom{n}{ \lfloor n / 2 \rfloor } = p_1^{k_1} p_2^{k_2} … p_t^{k_t} \le n^t \le n^{ \pi(n) }.<br>$$</p>
</blockquote>
<p>$\square$</p>
<h4 id="Lemma-4"><a href="#Lemma-4" class="headerlink" title="Lemma 4."></a>Lemma 4.</h4><blockquote>
<p>$$<br>\frac{1}{n + 1} 2^{n} \le \binom{n}{ \lfloor n / 2 \rfloor } \le n^{\pi(n)}<br>$$</p>
</blockquote>
<p><em>Proof.</em> Given $n \in \mathbb{N}^+$, as $\binom{n}{k}$ is maximized when $k = \lfloor n / 2 \rfloor$ for ($k =0, 1, 2, …, n$), it holds that<br>$$<br>1 = (\frac{1}{2} + \frac{1}{2})^n \le (n + 1) (\frac{1}{2})^n \binom{n}{ \lfloor n / 2 \rfloor }.<br>$$</p>
<p>This establishes the lower bound. </p>
<p>$\square$</p>
<p>By Lemma 4,<br>$$<br>\pi(n) \ge \frac{n  - \log (n + 1) }{\log n} \ge \frac{1}{2} \frac{n}{\log n}.<br>$$</p>
<p>The final inequality holds when $\log (n + 1) \le 0.5 n$. It suffices to take $n \ge 7$, as $\log (7 + 1) = 3 &lt; 3.5$ but $\log (3 + 1) = 2 &gt; 1.5$. When $n &lt; 7$, we can check manually that $\pi(n) / n \ge 0.5 / \log n$ holds. </p>
<p>$\blacksquare$</p>
<h3 id="Proof-of-The-Theorem-Part-2"><a href="#Proof-of-The-Theorem-Part-2" class="headerlink" title="Proof of The Theorem, Part 2."></a>Proof of The Theorem, Part 2.</h3><h4 id="Lemma-5-For-k-in-mathbb-N"><a href="#Lemma-5-For-k-in-mathbb-N" class="headerlink" title="Lemma 5. For $k \in \mathbb{N}^+$,"></a>Lemma 5. For $k \in \mathbb{N}^+$,</h4><blockquote>
<p>$$<br>\binom{2k + 1}{ k } \ge \prod_{k + 2 \le p \le 2k + 1 \wedge p \text{ is prime}} p<br>$$</p>
</blockquote>
<p><em>Proof.</em><br>First,<br>$$<br>\binom{2k + 1}{k} = \frac{(2k + 1) (2k) (2k - 1) … (k + 2)}{ k (k - 1) … 1}.<br>$$</p>
<p>For a prime $p \in [k + 2, 2k + 1]$, it divides the numerator but not the denominator. </p>
<p>$\square$</p>
<p>We prove that $\pi(n) \le 2 n / \ln n$ by induction. When $n &gt; 2$ is even, $\pi(n) = \pi(n - 1)$. As $2 n / \ln n$ increases with $n$, it suffices to prove this for odd numbers of $n$. Suppose that $n = 2k + 1$ for some $k \in \mathbb{N}^+$, then </p>
<p>$$<br> (k + 2)^{\pi(2k + 1) - \pi(k + 1) }  \le \prod_{k + 2 \le p \le 2k + 1 \wedge p \text{ is prime}} p \le \binom{2k + 1}{k}.<br>$$</p>
<!-- Observe that 
$$
\binom{2k + 1}{k} = \binom{2k + 1}{k - 1} \frac{k + 2}{k} = \binom{2k + 1}{k - 2} \frac{(k + 2) (k + 3)}{k(k - 1)}.
$$

When $k \ge 2$, $(k + 2) / k \le 2$. Solving 
$$
\frac{(k + 2) (k + 3)}{k(k - 1)} \le 2
$$
gives 
$$
k^2 + 5k + 6 \le 2k^2 - 2k \longleftrightarrow k^2 - 7k - 6 \ge 0.
$$
It suffices to take $k \ge 8$. 

So when $k \ge 8$, 
$$
\begin{aligned}
    \binom{2k + 1}{k} 
        &\le \frac{1}{4} \left( \binom{2k + 1}{k - 2}+ \binom{2k + 1}{k - 1} + \binom{2k + 1}{k} + \binom{2k + 1}{k + 1} + \binom{2k + 1}{k + 2} + \binom{2k + 1}{k + 3} \right) \\
        &\le \frac{1}{4} \sum_{j = 0}^{2k + 1} \binom{2k + 1}{j}\\
        &\le \frac{1}{4} \cdot 2^{2k + 1} = 2^{2k - 1}
\end{aligned}
$$ -->

<p>Further<br>$$<br>\binom{2k + 1}{k} = \frac{1}{2} \left( \binom{2k + 1}{k} + \binom{2k + 1}{k + 1}\right) \le \frac{1}{2} \cdot 2^{2k + 1} = 2^{2k}.<br>$$</p>
<p>Therefore,<br>$$<br>\pi(2k + 1) - \pi(k + 1) \le \frac{ 2k \cdot \ln 2 }{ \ln (k + 2)}.<br>$$</p>
<p>By induction on $\pi(k + 1)$, we arrive at<br>$$<br>\pi(2k + 1) \le \frac{ 2k \cdot \ln 2 }{ \ln (k + 2)} + 2 \frac{ k + 1}{\ln (k + 1)} &lt; \frac{ (n - 1) \cdot \ln 2 + n + 1 }{\ln (k + 1) } &lt; \frac{  (1 + \ln 2) \cdot n + 1}{\ln (n / 2) }.<br>$$</p>
<p>In order that<br>$$<br>\frac{  (1 + \ln 2) \cdot n + 1}{\ln (n / 2) } \le \frac{2 n }{ \ln n},<br>$$</p>
<p>we need<br>$$<br>(1 + \ln 2) \cdot n \cdot \ln n + \ln n \le 2 n \cdot \ln n - 2 n \cdot \ln 2 \<br>\Longleftrightarrow (2 \ln 2) \cdot n + \ln n \le (1 - \ln 2) \cdot n \cdot \ln n.<br>$$</p>
<p>This holds when $n \ge 107$. It is left to verify manually that $\pi \le 2 n / \ln n$ holds for all $n &lt; 107$.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/The-Prime-Number-Theory.PNG?raw=true"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Jan-Hendrik, “Chapter 1: Introduction to prime number theory”, ANALYTIC NUMBER THEORY</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/25/SmallDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/25/SmallDB/" class="post-title-link" itemprop="url">SmallDB</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-25 22:34:48" itemprop="dateCreated datePublished" datetime="2020-11-25T22:34:48+11:00">2020-11-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-26 11:53:10" itemprop="dateModified" datetime="2020-11-26T11:53:10+11:00">2020-11-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $U = {x_1, x_2, … , x_n } \subset \mathcal{X}^n$ be a set of $n$ points from a domain $\mathcal{X}$ and $Q = { f_1, f_2, … \mid f_i : \mathcal{X} \rightarrow [0, 1] }$ be a finite set of queries (functions). For any $f \in Q$ and finite set $S$ with domain $\mathcal{X}$, define<br>$$<br>f(S) \doteq \frac{1}{ |S| }\sum_{x \in S} f(x)<br>$$</p>
<p>as the average over the values ${ f(x) : x \in S }$. Our goal is to find a subset $S$ with size<br>$$<br>O( \frac{\log |Q| }{\lambda^2})<br>$$</p>
<p>in a $(2 \epsilon, 0)$ differentially private manner, such that with probability at least $1 - \delta$, it holds simultaneously $\forall f \in Q$,<br>$$<br>|f(S) - f(U) | \le \lambda + \frac{\frac{ }{} \log |Q| \log |\mathcal{X}|  } { \lambda^2 \epsilon n } + \frac{ \log (1 / \delta) }{ \epsilon n  }<br>$$</p>
<h3 id="Existence-of-Small-Representative"><a href="#Existence-of-Small-Representative" class="headerlink" title="Existence of Small Representative"></a>Existence of Small Representative</h3><p>Let $X$ be a random element sampled uniformly from $U$. Then for a fixed $f \in Q$,<br>$$<br>\mathbb{E}[ f(X) ] = f(U)<br>$$</p>
<p>By Hoeffding, if we take $m = \frac{\ln |Q|}{\lambda^2}$ independent copies of $X$, denoted as $X_1, X_2, …, X_m$, then<br>$$<br>\Pr[ |\frac{1}{m} \sum_{i \in [m]} X_i - f(U) | \ge \lambda ] \le 2 \exp(- 2m \lambda^2 ) = \frac{2}{e^{2 \ln |Q|}} = \frac{2}{|Q|^2} &lt; \frac{1}{ |Q| }.<br>$$</p>
<p>By union bound, this happens for some $f \in Q$ with probability less than 1. Therefore, if we enumerate over all sets $S \subset \mathcal{X}^m$ (which takes exponential time), then it is guaranteed that we can find one such that<br>$$<br>|f(S) - f(U) | \le \lambda.<br>$$</p>
<p>This is tighter than the bound we would like to prove. The additional error is due to the privacy protection mechanism. </p>
<h3 id="Release-Representative-Differential-Privately"><a href="#Release-Representative-Differential-Privately" class="headerlink" title="Release Representative Differential-Privately"></a>Release Representative Differential-Privately</h3><p>We can just release the $S$ blatantly. </p>
<h4 id="Counter-Example"><a href="#Counter-Example" class="headerlink" title="Counter Example"></a>Counter Example</h4><p>Suppose $\mathcal{X} = {0, 1}$, and $U = {0, 0, …, 0} \subset \mathcal{X}^n$. Further, the set $Q$ contains two copies of the function $f$, defined as<br>$$<br>f(0) = 0, f(1) = 1.<br>$$</p>
<p>Assume that $\lambda = \sqrt{\frac{ 2 \ln 2 }{ n } }$. Then $m = \frac{n \ln 2}{2 \ln 2} = n / 2$. Any $S \subset \mathcal{X}^m$ that contains no more than<br>$$<br>\lfloor m \sqrt{\frac{ 2 \ln 2 }{ n } } \rfloor = \lfloor \sqrt{\frac{ 2 \ln 2 }{ n } } \frac{n}{2} \rfloor = \lfloor \sqrt{\frac{ n \ln 2 }{ 2 } } \rfloor<br>$$<br>ones, will satisfies<br>$$<br>|f(S) - f(U) | \le \lambda.<br>$$</p>
<p>Now, consider a neighboring set $U’ = {0, 0, …, 1}$, obtained by replacing the last zero with one in $U$. Any $S$ with no more than $\lfloor \sqrt{\frac{ n \ln 2 }{ 2 } } + 1 \rfloor$ ones satisfies $|f(S) - f(U’) | \le \lambda$. </p>
<p>Consider the scenario where we know the underlying set is either $U$ or $U’$. If a set in $\mathcal{X}^m$ with exactly $\lfloor \sqrt{\frac{ n \ln 2 }{ 2 } } + 1 \rfloor$ is released as an representative of the underlying set, we can infer immediately that the underlying set is $U’$. </p>
<h4 id="Exponential-Mechanism"><a href="#Exponential-Mechanism" class="headerlink" title="Exponential Mechanism"></a>Exponential Mechanism</h4><p>One remedy for the above problem is that, for a set $S \subset \mathcal{X}^m$, even if $\exists f \in Q$, s.t., $|f(S) - f(U)| &gt; \lambda$, we output $S$ as the representative of $U$ with some probability, because $S$ might be a feasible solution for $U$’s neighbors. This motivates the <em>exponential mechanism</em>. In particular, </p>
<ol>
<li>It outputs the $S \in \mathcal{X}^m$ that minimizes $\max_{f \in Q} |f(S) - f(U)|$ with highest probability. </li>
<li>It outputs other $S \in \mathcal{X}^m$ with probability exponentially and inversely proportional to its error $\max_{f \in Q} | f(S) - f(U) |$.</li>
</ol>
<p>The mechanism, termed $M$,  is as follows. </p>
<blockquote>
<ol>
<li>For $S \in \mathcal{X}^m$, define weight $w(U, S) \doteq \exp(- \epsilon n \max_{f \in Q} | f(S) - f(U) | )$.   </li>
<li>Sample and output an $S \in \mathcal{X}^m$ with probability<br>$$<br> \frac{ w(U, S) }{ \sum_{T \in \mathcal{X}^m} w(U, T)}<br>$$</li>
</ol>
</blockquote>
<h5 id="Privacy"><a href="#Privacy" class="headerlink" title="Privacy"></a><strong><em>Privacy</em></strong></h5><p>The mechanism $M$ is $(2\epsilon, 0)$ differentially private. </p>
<p><em>Proof.</em> Let $U’ \in \mathcal{X}^n$ be obtained by replacing exactly one element in $U$. For any $T \in \mathcal{X}^m$, by definition,<br>$$<br>\begin{aligned}<br>    \max_{f \in Q} |f(U) - f(T)| - \max_{f \in Q} |f(T) - f(U’) |<br>        &amp;\le \max_{f \in Q} \big( |f(U) - f(T)| - |f(T) - f(U’) | \big) \<br>        &amp;\le \max_{f \in Q} \big( |f(U) - f(T) + f(T) - f(U’) | \big) \<br>        &amp;\le \max_{f \in Q} \big( |f(U) - f(U’) | \big) \<br>        &amp;\le 1 / n \<br>\end{aligned}<br>$$</p>
<p>By symmetry, $\max_{f \in Q} |f(U) - f(T)| - \max_{f \in Q} |f(T) - f(U’) | \in [- 1 / n, 1 / n ]$ and<br>$$<br>\frac{w(U, T)}{ w(U’, T)} \in [e^{-\epsilon}, e^\epsilon]<br>$$</p>
<p>For an $S \in \mathcal{X}^m$, it holds that<br>$$<br>\begin{aligned}<br>    \frac{\Pr[M(U) = S]}{ \Pr[M(U’) = S]}<br>        &amp;= \frac{ w(U, S) }{ w(U, S’) } \frac{ \sum_{T \in \mathcal{X}^m} w(U’, T) }{ \sum_{T \in \mathcal{X}^m} w(U, T) } \<br>        &amp;\le e^\epsilon \cdot \frac{ \sum_{T \in \mathcal{X}^m} e^\epsilon \cdot w(U, T) }{ \sum_{T \in \mathcal{X}^m} w(U, T) } \<br>        &amp;= e^{2\epsilon}<br>\end{aligned}<br>$$</p>
<h5 id="Error"><a href="#Error" class="headerlink" title="Error"></a><strong><em>Error</em></strong></h5><p>For a set $U$, although we might assign probabilities to bad $S \in \mathcal{X}^m$, as long as the probability mass assigned is small, we are unlikely to get a bad representative. </p>
<blockquote>
<p>Theorem. With probability at most $\delta$, $M$ returns an $S$, such that<br>$$<br>    \max_{f \in Q} | f(S) - f(U) | \ge \lambda + \frac{m \log |\mathcal{X}| + \log (1 / \delta) } {\epsilon n }<br>$$</p>
</blockquote>
<p><em>Proof.</em> Let $S^* \in \mathcal{X}^m$ be the set that minimizes $\max_{f \in Q} |f(S) - f(U)|$. </p>
<p>For a $S \in \mathcal{X}^m$, if $\max_{f \in Q} |f(S) - f(U)| \ge \lambda + t$ for some $t &gt; 0$, then<br>$$<br>\begin{aligned}<br>    \Pr[M(U) = S]<br>        &amp;= \frac{ w(U, S) }{ \sum_{T \in \mathcal{X}^m} w(U, T)} \<br>        &amp;\le \frac{ w(U, S) }{ w(U, S^*) } \<br>        &amp;\le \exp( - \epsilon n (\max_{f \in Q} |f(S) - f(U)| - \max_{f \in Q} |f(S^*) - f(U)|)) \<br>        &amp;\le \exp( - \epsilon n t)<br>\end{aligned}<br>$$</p>
<p>By union bound,<br>$$<br>\begin{aligned}<br>    \Pr[ M(U) \in { S : \max_{f \in Q} |f(S) - f(U)| \ge \lambda + t } ]<br>        &amp;\le |{ S : \max_{f \in Q} |f(S) - f(U)| \ge \lambda + t } | \exp( - \epsilon n t) \<br>        &amp;\le |\mathcal{X}|^m \exp( - \epsilon n t)<br>\end{aligned}<br>$$</p>
<p>By requiring this probability to be bounded by some $\delta &gt; 0$, we get<br>$$<br>t = \frac{m \log |\mathcal{X}| + \log (1 / \delta) } {\epsilon n }<br>$$</p>
<p>$\square$. </p>
<p>Substituting $m$ with $\frac{\log |Q| }{\lambda^2}$, the error is then<br>$$<br>\lambda + \frac{\frac{\log |Q| }{\lambda^2} \log |\mathcal{X}| + \log (1 / \delta) } {\epsilon n }<br>$$</p>
<p>When $\lambda = (\frac{\log |Q| \log |\mathcal{X}| }{ \epsilon n})^{1 / 3}$, the error is given by<br>$$<br>2(\frac{\log |Q| \log |\mathcal{X}| }{ \epsilon n})^{1 / 3} + \frac{\log (1 / \delta) }{ \epsilon n }<br>$$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] S. Vadhan, “The Complexity of Differential Privacy,” in Tutorials on the Foundations of Cryptography, Y. Lindell, Ed. Cham: Springer International Publishing, 2017, pp. 347–450.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/25/Releasing-Histogram-Privately/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/25/Releasing-Histogram-Privately/" class="post-title-link" itemprop="url">Releasing Histogram Privately</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-11-25 10:19:09 / Modified: 22:11:21" itemprop="dateCreated datePublished" datetime="2020-11-25T10:19:09+11:00">2020-11-25</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given a set of $n$ elements $S$ that belongs to some finite domain $\mathcal{X}= { x_1, x_2, …, x_m }$, we want to release the (normalized) histogram in a differentially private manner. Define $\forall x_i \in \mathcal{X}$:</p>
<ol>
<li>$F_S(x_i) \doteq$ the number of times $x_i$ appears in $S$;  </li>
<li>$f_S(x_i) \doteq \frac{F_S(x_i) }{ |S| } = \frac{F_S(x_i) }{ n }$, the frequency of $x_i$ in $S$.</li>
</ol>
<p>The (normalized) histogram of $h_S$ is a frequency vector:<br>$$<br>h_S \doteq \left&lt; f_S(x_1), f_S(x_2), …, f_S(x_m) \right&gt; \in \mathbb{R}^m<br>$$</p>
<p>Consider another set $S’$ obtained by replacing an element $x_k$ in $S$ to another element $x_l$, such that </p>
<ol>
<li>$F_{S’} (x_k) = F_{S} (x_k) - 1$;</li>
<li>$F_{S’} (x_l) = F_{S} (x_l) + 1$;</li>
<li>$F_{S’} (x_i) = F_{S} (x_i), \forall i \notin {k, l}$. </li>
</ol>
<p>Let $h_{S’}$ be the histogram of $S’$. Our goal is to design some mechanism $M$ that adds noise to the normalized so that the output distribution of $M(h_S)$ and $M(h_{S’})$ are indistinguishable. </p>
<h3 id="Naive-Laplacian"><a href="#Naive-Laplacian" class="headerlink" title="Naive Laplacian"></a>Naive Laplacian</h3><p>One way is to add Laplacian noise directly to each dimension of the histogram. For any vector $v \in \mathbb{R}^m$, $M$ outputs a new perturbed vector:<br>$$<br>M(v) \doteq v + L<br>$$</p>
<p>where $L \doteq \left&lt; l_1, l_2, …, l_m\right&gt;$ and each $l_i$ is sampled independently from $Lap( \frac{2 }{ n \epsilon } )$. </p>
<h4 id="Privacy"><a href="#Privacy" class="headerlink" title="Privacy"></a>Privacy</h4><p>We claim this mechanism is $(\epsilon, 0)$-differentially private. For $u \in \mathbb{R}^n$, we have<br>$$<br>\begin{aligned}<br>    \frac{ p(M(h_S) = u) }{ p(M(h_{S’}) = u) }<br>        &amp;= \frac{ \prod_{i \in [m] } p( f_S (x_i) + l_i = u_i) }{ \prod_{i \in [m] } p( f_{S’} (x_i) + l_i = u_i) } \<br>        &amp;= \frac{ \prod_{i \in {k, l} } p( f_S (x_i) + l_i = u_i) }{ \prod_{i \in {k, l} } p( f_{S’} (x_i) + l_i = u_i) } \<br>        &amp;\le \exp( - (|u_k - f_S(x_k)| - |u_k - f_{S’} (x_k) |) \frac{n\epsilon}{2} ) \<br>        &amp;\quad \cdot \exp( - (|u_l - f_S(x_l)| - |u_l - f_{S’} (x_l) |) \frac{n\epsilon}{2} ) \<br>        &amp;\le \exp( |f_S(x_k) - f_{S’} (x_k) | \frac{n\epsilon}{2} ) \cdot \exp( |f_S(x_l) - f_{S’} (x_l) | \frac{n\epsilon}{2} ) \<br>        &amp;= \exp( \frac{1}{n} \frac{ n \epsilon}{2} ) \cdot \exp( \frac{1}{n} \frac{n\epsilon}{2} ) \<br>        &amp;= \exp( \epsilon).<br>\end{aligned}<br>$$</p>
<h4 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h4><p>We prove that $|M(h_S) - h_S |_\infty = O (\frac{1}{ n \epsilon } \log \frac{m}{\delta})$ with probability at least $1 - \delta$. </p>
<p>Let $b = \frac{2 }{ n \epsilon }$. For a failure probability $\delta$,<br>$$<br>\Pr[ |f_S(x_i) + l_i - f_S(x_i) | \ge b \log \frac{m}{\delta} ] = \int_{ b \log \frac{m}{\delta} }^\infty \frac{1}{b} \exp( - x / b ) \ dx = \exp(- \log \frac{m}{\delta} ) = \frac{\delta}{m}<br>$$</p>
<p>By union bound, it holds that<br>$$<br>\Pr[ |M(h_S) - h_S |_\infty \ge b \log \frac{m}{\delta} ] \le \delta.<br>$$</p>
<p>$\square$</p>
<h3 id="Rounding-Small-Values"><a href="#Rounding-Small-Values" class="headerlink" title="Rounding Small Values"></a>Rounding Small Values</h3><p>When $|\mathcal{X}| = m \gg n$, we could improve the release mechanism. As shown in the previous section, we would like to achieve error $O(\frac{1}{n})$. This inspire to round the small frequencies (roughly $O(\frac{1}{n})$) to zero. Of course, we should carry out this in a differentially private manner. </p>
<blockquote>
<p>Mechanism $M$<br>$===============================$<br>INPUT: a set $S$ of $n$ elements.<br>OUTPUT: a histogram of $S$ in differentially private manner.<br>$————————$<br>For $i \in [m]$ do<br>$\qquad$ If $f_S(x_i) = 0$, then $M(h_S)_i \leftarrow 0$<br>$\qquad$ Else<br>$\qquad$ $\qquad$ $M(h_S)_i \leftarrow f_S(x_i) + l_i$, where $l_i \sim Lap(\frac{2}{n \epsilon} )$.<br>$\qquad$ $\qquad$ If $M(h_S)_i &lt; \frac{1}{n} +\frac{2 \log 2 / \delta }{ \epsilon n}$, then $M(h_S)_i \leftarrow 0$.<br>Release $M(h_S) \doteq \left&lt; M(h_S)_1, M(h_S)_2, … , M(h_S)_n \right&gt;$<br>$————————$   </p>
</blockquote>
<h4 id="Privacy-1"><a href="#Privacy-1" class="headerlink" title="Privacy"></a>Privacy</h4><p>We claim this mechanism is $(\epsilon, \delta)$-differentially private. </p>
<ul>
<li><p>Case 1. If $S$ and $S’$ have the same set of supports, i.e., $f_S(x_i) &gt; 0 \leftrightarrow f_{S’}(x_i) &gt; 0$, then $M$ is $(\epsilon, 0)$ differentially private.</p>
</li>
<li><p>Case 2. </p>
<ol>
<li><p>$F_{S} (x_k) = 1$, then $F_{S’} (x_k) = F_{S} (x_k) - 1 = 0$. For $S’$, it is guaranteed that $M(h_{S’})_k = 0$. For $S$, as $f_S(x_k) = \frac{1}{n}$,<br> $$</p>
<pre><code> \Pr[ M(h_S)_k \neq 0 ] = \Pr[ M(h_S)_k \neq M(h_&#123;S&#39;&#125;)_k ] = \frac&#123;\delta&#125;&#123;2&#125;
</code></pre>
<p> $$</p>
</li>
<li><p>$F_{S} (x_l) = 0$, then  $F_{S’} (x_l) = F_{S} (x_l) + 1 = 1$. For $S$, it is guaranteed that $M(h_S)<em>k = 0$. For $S’$, by similar argument, we have $\Pr[ M(h</em>{S’})<em>k \neq 0 ] = \Pr[ M(h_S)_k \neq M(h</em>{S’})_k ] = \frac{\delta}{2}$. </p>
<p>There are three combinations such that either $F_S(x_k) = 1$ or $F_{S} (x_l) = 0$ holds, namely </p>
</li>
<li><p>$F_S(x_k) = 1, F_{S} (x_l) = 0$,</p>
</li>
<li><p>$F_S(x_k) = 1, F_{S} (x_l) &gt; 0$,</p>
</li>
<li><p>$F_S(x_k) &gt; 1, F_{S} (x_l) = 0$.</p>
<p>We give a proof for the first case. The proofs for the other two are similar.<br>Now, for $\forall$ measurable $E \subset \mathbb{R}^n$,<br>$$<br>\begin{aligned}<br> \Pr[ M(h_S) \in E] </p>
<pre><code> &amp;= \Pr[ M(h_S) \in E \mid M(h_S)_k = 0, M(h_&#123;S&#39;&#125;)_l = 0] \\
 &amp;\ + \Pr[ M(h_S) \in E \mid M(h_S)_k \neq 0 \vee M(h_&#123;S&#39;&#125;)_l \neq 0] \\
 &amp;\le \Pr[ M(h_S) \in E \mid M(h_S)_k = 0, M(h_&#123;S&#39;&#125;)_l = 0] \\
 &amp;\ + \Pr[M(h_S)_k \neq 0 ] + \Pr[ M(h_&#123;S&#39;&#125;)_l \neq 0] \\
 &amp;= \Pr[ M(h_&#123;S&#39;&#125;) \in E \mid M(h_S)_k = 0, M(h_&#123;S&#39;&#125;)_l = 0] + \delta \\
 &amp;\le \Pr[ M(h_&#123;S&#39;&#125;) \in E] + \delta
</code></pre>
<p>\end{aligned}<br>$$</p>
</li>
</ol>
</li>
</ul>
<h4 id="Accuracy-1"><a href="#Accuracy-1" class="headerlink" title="Accuracy"></a>Accuracy</h4><p>We prove that $|M(h_S) - h_S |_\infty = O (\frac{1}{ n \epsilon } \log \frac{1}{\delta})$ with probability at least $1 - \delta$, if $\delta \le \frac{1}{n}$.</p>
<ol>
<li><p>If $f_S(x_i) = 0$, then there is no error.   </p>
</li>
<li><p>Otherwise, if $M(h_S)_i$ is not truncated, with probability at most $\frac{\delta}{n}$, it $|M(h_S)_i - f_S(x_i)|\le \frac{2\log 1 / \delta}{n\epsilon}$. If it is truncated, this will introduce an additional error of at most $\frac{1}{n} +\frac{2 \log 2 / \delta }{ \epsilon n}$. The overall error is at most $\frac{1}{n} +\frac{4 \log 2 / \delta }{ \epsilon n}$, which is $O( \frac{ \log 1 / \delta }{ \epsilon n})$  if $\epsilon \le \log n$. </p>
<!-- Let $b = \frac{2 }{ n \epsilon }$. For a failure probability $\delta$, 
$$
\Pr[ |f_S(x_i) + l_i - f_S(x_i) | \ge b \log \frac{m}{\delta} ] = \int_{ b \log \frac{m}{\delta} }^\infty \frac{1}{b} \exp( - x / b ) \ dx = \exp(- \log \frac{m}{\delta} ) = \frac{\delta}{m}
$$

</li>
</ol>
<p>By union bound, it holds that<br>$$<br>\Pr[ |M(h_S) - h_S |_\infty \ge b \log \frac{m}{\delta} ] \le \delta.<br>$$ –&gt;</p>
<p>$\square$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] S. Vadhan, “The Complexity of Differential Privacy,” in Tutorials on the Foundations of Cryptography, Y. Lindell, Ed. Cham: Springer International Publishing, 2017, pp. 347–450.  </p>
<!-- Now, for $\forall$ measurable $E \subset \mathbb{R}^n$, define  
$$
    E_1 \doteq \{ v \in E : v_k = 0, v_l = 0 \}
$$
be the set of points with the $k$-th and $l$-th dimensions equal to 0. Let $E_2 = E \setminus E_1$. 
$$
\begin{aligned}
    \Pr[ M(h_S) \in E] &= \Pr[ M(h_S) \in E_1] + \Pr[ M(h_S) \in E_2] \\
    &= \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot \Pr[M(h_S)_k = 0] + \Pr[ M(h_S) \in E_2] \\
    &= \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot (1 - \frac{\delta}{2}) + \Pr[ M(h_S) \in E_2] \\
    &\le \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot (1 - \frac{\delta}{2}) + \Pr[ M(h_S)_k = 0]  \\
    &= \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot (1 - \frac{\delta}{2}) + \frac{\delta}{2}  \\
    &= \Pr[ M(h_{S'}) \in E_1 \mid M(h_{S'})_l = 0] \cdot (1 - \frac{\delta}{2}) + \frac{\delta}{2}  \\
    &\le \Pr[M(h_{S'}) \in E ] + \frac{\delta}{2}
\end{aligned}
$$

The last inequality follows from $\Pr[ M(h_{S'}) \in E_1 \mid M(h_{S'})_l = 0] = \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0]$.  -->



      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/23/Propose-Test-Release/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/23/Propose-Test-Release/" class="post-title-link" itemprop="url">Propose-Test-Release</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-23 21:53:14" itemprop="dateCreated datePublished" datetime="2020-11-23T21:53:14+11:00">2020-11-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-18 10:32:42" itemprop="dateModified" datetime="2020-12-18T10:32:42+11:00">2020-12-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $(\mathcal{X}, d)$ be a metric space and $q : \mathcal{X} \rightarrow \mathbb{R}$ be a function defined on $\mathcal{X}$. The metric $d: \mathcal{X} \rightarrow \mathbb{N}$ takes only integer values. For an $x \in \mathcal{X}$, it neighbors an $x’ \in \mathcal{X}$, termed $x \sim x’$, if $d(x, x’) = 1$. </p>
<p>Further, define the global sensitivity $\Delta$ of $q$ as:<br>$$<br>    \Delta \doteq \max_{x \sim x’ } | q(x) - q(x’) |<br>$$</p>
<p>The Laplace mechanism $M$ of $q$ adds random noise $N \sim Lap(\frac{\Delta}{\epsilon})$ to the output of $q$:<br>$$<br>M(x) \doteq q(x) + N<br>$$</p>
<p>where $Lap(\frac{\Delta}{\epsilon})$ is the Laplacian distribution with parameter $\frac{\Delta}{\epsilon}$. It guarantees that if $x \sim x’$, the output distributions of $M(x)$ and $M(x’)$ are indistinguishable in the sense that $\forall S \subset \mathbb{R}$, if $S$ is measurable,<br>$$<br>\Pr[ M(x) \in S] \in [\exp(-\epsilon), \exp(\epsilon) ] \cdot \Pr[ M(x’) \in S]<br>$$</p>
<p>The value of $\Delta$ could be large and is independent of an $x \in \mathcal{X}$. We would like to investigate the possibility of adding a smaller noise than $Lap(\frac{\Delta}{\epsilon})$. </p>
<p>For a fixed $x$, we can define the local sensitivity with respect to $x$ as<br>$$<br>    \delta(x) \doteq \max_{x’ \sim x} | q(x) - q(x’) |.<br>$$</p>
<p>Here the maximum is over the neighbors of a fixed $x$. The value between $\Delta$ and $\delta(x)$ is quite different. </p>
<blockquote>
<p>Example. Let </p>
<ul>
<li><p>$\mathcal{X} \doteq [0, 100]^3$.   </p>
</li>
<li><p>$d(x, x’)\doteq #\text{ different values between } x \text{ and } x’$. </p>
</li>
<li><p>$q(x) \doteq \min_{1 \le i &lt; j \le 3} |x_i - x_j|$, where $x_i$ is the $i$-dimension of $x$.    </p>
<p>Then for the point $x = (0, 0, 0) \in [0, 100]^3$, if $x’ \sim x$, $x’$ contains at least two zeros in some of its dimensions. Therefore, $q(x’) = 0$ and $\delta(x) = 0$. But for $x = (0, 0, 100)$, it has a neighbor $x’ = (0, 50, 100)$. Now $q(x) = 0$ but $q(x’) = 50$. So $\Delta \ge \delta(x) \ge 50$. </p>
</li>
</ul>
</blockquote>
<p>We are interested in whether adding smaller noise $N \sim Lap(\frac{\delta(x) }{\epsilon})$ to the output of $q(x)$ would preserve privacy:<br>$$<br>M(x) = q(x) + N<br>$$</p>
<p>It is not. In the previous example, $(0, 0, 0)$ and $(0, 0, 100)$ are neighbors. But for the former we add noise $N \sim Lap(0)$ and for the latter we add noise $N \sim Lap(\frac{50}{\epsilon})$. So the output distributions of them are very different and this would lead to information leakage.</p>
<p>Perhaps, we should propose some fixed parameter $\beta &gt; 0$ (which could be much smaller than $\Delta$), and add a noise $N \sim Lap(\frac{\beta}{\epsilon})$ for whatever $x \in \mathcal{X}$. For a pair of neighboring $x, x’$, when $|q(x) - q(x’)| \le \beta$, then the distributions of $q(x) + N$ and $q(x’) + N$ are indeed $(\epsilon, 0)$ indistinguishable. The only concern is for the case where $|q(x) - q(x’)| &gt; \beta$ and adding a noise $N \sim Lap(\frac{\beta}{\epsilon})$ fails to protect the privacy. </p>
<p>One simple remedy is simply to refuse to output $q(x) + N$ (or $q(x) + N’$). This should be done in a differentially private manner. Let<br>$$<br>{ y \in \mathcal{X} : \delta(y) &gt; \beta }<br>$$</p>
<p>be the set of points with local sensitivity greater than $\beta$. For a fixed $x$, define $d(x, { y : \delta(y) &gt; \beta })$ be its distance to the set. It follows that for $x \sim x’$,<br>$$<br>|d(x, { y \in \mathcal{X} : \delta(y) &gt; \beta }) - d(x’, { y \in \mathcal{X} : \delta(y) &gt; \beta })| \le 1.<br>$$</p>
<p>This motivates the following <em>propose-test-release</em> algorithm. </p>
<blockquote>
<ol>
<li>Propose a bound $\beta$ on local sensitivity.   </li>
<li>Compute $\hat d \doteq d(x, { y \in \mathcal{X} : \delta(y) &gt; \beta }) + Lap(1 / \epsilon)$.   </li>
<li>If $\hat d \le \frac{1}{\epsilon} \ln \frac{1}{\gamma}$ then<br>$\qquad$ output $\bot$.</li>
<li>Else<br>$\qquad$ output $q(x) + Lap(\beta / \epsilon)$.  </li>
</ol>
</blockquote>
<p><strong>Theorem.</strong> The algorithm is $(2\epsilon, \gamma / 2)$ differentially private. </p>
<p><em>Proof.</em>  </p>
<p>First observe that for two neighboring $x, x’$, their probabilities of outputting $\bot$ are similar.<br>$$<br> \Pr[ M(x) = \bot ] \le e^{ \epsilon} \cdot \Pr[ M(x’) = \bot ] \<br> \Pr[ M(x) \neq \bot ] \le e^{ \epsilon} \cdot \Pr[ M(x’) \neq \bot ]<br>$$</p>
<p>Now consider a measurable set $S \subset \mathbb{R}$. $M(x) \in S$ is only possible when $\hat d &gt; \frac{1}{\epsilon} \ln \frac{1}{\gamma}$:<br>  $$<br>    \begin{aligned}<br>        \Pr[M(x) \in S]<br>            &amp;= \Pr[ M(x) \in S \mid M(x) \neq \bot ] \cdot \Pr[ M(x) \neq \bot]<br>    \end{aligned}<br>  $$</p>
<p>We consider two cases. </p>
<ul>
<li><p>Case 1. When $\delta(x) &gt; \beta$, then $d(x, { y \in \mathcal{X} : \delta(y) &gt; \beta }) = 0$. It follows that<br>$$<br>  \Pr[M(x) \in S] \le \Pr[M(x) \neq \bot ]  \le \frac{\gamma}{2}<br>$$</p>
</li>
<li><p>Case 2. When $\delta(x) \le \beta$, it holds that $|q(x) - q(x’) |\le \beta$ and<br>  $$</p>
<pre><code>  \Pr[ M(x) \in S \mid M(x) \neq \bot ] \le e^\epsilon \cdot \Pr[ M(x&#39;) \in S \mid M(x&#39;) \neq \bot ]
</code></pre>
<p>  $$<br>  Therefore,<br>  $$<br>  \begin{aligned}</p>
<pre><code>  \Pr[M(x) \in S] 
      &amp;= \Pr[ M(x) \in S \mid M(x) \neq \bot ] \cdot \Pr[ M(x) \neq \bot] \\
      &amp;\le \Pr[ M(x) \in S \mid M(x) \neq \bot ] \cdot e^\epsilon \cdot \Pr[ M(x&#39;) \neq \bot] \\
      &amp;\le e^\epsilon \cdot \Pr[ M(x&#39;) \in S \mid M(x&#39;) \neq \bot ] \cdot e^\epsilon \cdot \Pr[ M(x&#39;) \neq \bot] \\
      &amp;= e^&#123;2\epsilon&#125; \Pr[ M(x&#39;) \in S].
</code></pre>
<p>  \end{aligned}<br>  $$</p>
</li>
</ul>
<p>$\blacksquare$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] S. Vadhan, “The Complexity of Differential Privacy,” in Tutorials on the Foundations of Cryptography, Y. Lindell, Ed. Cham: Springer International Publishing, 2017, pp. 347–450.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/19/PAC-Learning-From-Countable-Hypothesis-Family/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/19/PAC-Learning-From-Countable-Hypothesis-Family/" class="post-title-link" itemprop="url">PAC Learning From Countable Hypothesis Family</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-19 15:17:27" itemprop="dateCreated datePublished" datetime="2020-11-19T15:17:27+11:00">2020-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-21 20:31:33" itemprop="dateModified" datetime="2020-11-21T20:31:33+11:00">2020-11-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We generalize the PAC learning problem of finding an hypothesis from a finite hypothesis family to the one from a countable hypothesis family.</p>
<h2 id="Problem-Setting"><a href="#Problem-Setting" class="headerlink" title="Problem Setting"></a>Problem Setting</h2><ol>
<li><p>$\mathcal{X}:$ the input space.  </p>
</li>
<li><p>$\mathcal{Y} \doteq {-1, 1}:$ the output space.  </p>
</li>
<li><p>$\mathcal{Z} \doteq \mathcal{X} \times \mathcal{Y}:$ the product space of $\mathcal{X}$ and $\mathcal{Y}$.  </p>
</li>
<li><p>$\mathcal{D}:$ an unknown distribution defined on $\mathcal{X} \times \mathcal{Y}$. </p>
</li>
<li><p>$\mathcal{H}:$ an countable family of hypothesis, s.t., each $h \in \mathcal{H}$ is a function from $\mathcal{X}$ to $\mathcal{Y}$.   </p>
</li>
<li><p>$\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow [0, 1]$, a loss function that takes two points in $\mathcal{Y}$ and outputs a value between $0$ and $1$.  </p>
</li>
<li><p>$L_\mathcal{D} (h):$ the expected loss of a hypothesis $h$ is defined as<br> $$<br> L_\mathcal{D} (h) = \mathbb{E}_{ (x, y) \sim \mathcal{D} } [ \ell( h(x), y) ]<br> $$</p>
<p> where $(x, y) \sim \mathcal{D}$ implies that the pair $(x, y) \in \mathcal{Z}$ is sampled according to the distribution $\mathcal{D}$. </p>
</li>
<li><p>$h^* \doteq \arg\min_{h \in \mathcal{H} } L_\mathcal{D} (h):$ the hypothesis that minimize the expected loss.</p>
</li>
<li><p>$\mu_h:$ an alias for $L_\mathcal{D} (h)$, when the distribution $\mathcal{D}$ discussed in the context is unique. </p>
</li>
<li><p>$B(\mu_h, \epsilon) \doteq { r \in \mathbb{R} : |r - \mu_h | &lt; \epsilon }:$ an open ball, which is an open interval in $\mathbb{R}$ centered at $\mu_h$ with length $2 \epsilon$, where $\epsilon &gt; 0$. </p>
</li>
<li><p>$S \sim \mathcal{D}^n:$ a set of $n$ i.i.d samples drawn from $\mathcal{D}$, where $n \in \mathbb{N}^+$ is some positive integer. In particular, when $S \sim \mathcal{D}^n$, it can be represented as<br> $$<br> S = { (x_1, y_1), (x_2, y_2), …, (x_n, y_n) : (x_i, y_i) \sim \mathcal  {D}, \forall i \in [n] }.<br> $$<br> We also view $S$ as a point in $\mathcal{Z}^n$.</p>
</li>
<li><p>$L_{S}(h) \doteq \frac{1}{ |S| } \sum_{ (x, y) \in S } \ell( h(x) , y ) :$ the empirical loss of a hypothesis $h$ on a sample set $S$.  </p>
</li>
<li><p>Given a $h \in \mathcal{H}$, its restriction on $\mathcal{C} \subset \mathcal{X}$ is a function $h_S$ defined on $S$, such that<br>$$<br>h_\mathcal{C} (x) = h(x), \forall x \in \mathcal{C}<br>$$</p>
</li>
<li><p>The restriction of $\mathcal{H}$ on $\mathcal{C}$ is the set of possible restriction of a function in $\mathcal{H}$ to $\mathcal{C}$<br>$$</p>
<pre><code>    \mathcal&#123;H&#125;_\mathcal&#123;C&#125;  = \&#123; h_\mathcal&#123;C&#125;  : \mathcal&#123;C&#125;  \rightarrow \mathcal&#123;Y&#125; : h \in \mathcal&#123;H&#125; \&#125;
</code></pre>
<p>$$</p>
<p>As $\mathcal{Y} = { -1, +1 }$, the set of possible functions defined on $\mathcal{C}$ is bounded by $2^{n}$. Hence,<br>$$</p>
<pre><code>|\mathcal&#123;H&#125;_\mathcal&#123;C&#125; | \le 2^&#123;n&#125;.
</code></pre>
<p>$$</p>
</li>
<li><p>The growth function $\Pi_{\mathcal{H} } (n): \mathbb{N}^+ \rightarrow \mathbb{N}^+$ of $\mathcal{H}$ is defined as<br>$$<br>\Pi_{\mathcal{H} } (n) = \max_{\mathcal{C} \subset \mathcal{X}, |C| = n} | \mathcal{H}_\mathcal{C} |<br>$$</p>
</li>
</ol>
<p>Ideally, we would like to find  $h^*$. The problem is difficult, as </p>
<ul>
<li>The space $\mathcal{X} \times \mathcal{Y}$ could be infinite. </li>
<li>The distribution $\mathcal{D}$ is unknown.  </li>
</ul>
<p>To deal with the possibly infinite space $\mathcal{X} \times \mathcal{Y}$ and the unknown distribution $\mathcal{D}$, we investigate $\mathcal{H}$ on a finite sample set $S$. Due to the randomness inherited in sampling $S$, we allow our solution to be approximate and to make error. For a given pair of parameters of $\epsilon &gt; 0$ and $\delta &gt; 0$, we relax the goal to designing an $(\epsilon, \delta)$-learning algorithm $A$ that returns an $h’$, that is   </p>
<blockquote>
<ul>
<li>$\epsilon$-approximate: $\mu_{h’} \le \mu_{h^*} + \epsilon$,  </li>
<li>probably correct: $A$ return an $h’$ that does not satisfies the above condition with probability at most $\delta$.</li>
</ul>
</blockquote>
<p>Combined, $A$ should return an $\epsilon$-approximate solution $h’$ with probability at least $1 - \delta$.</p>
<h2 id="The-Algorithm"><a href="#The-Algorithm" class="headerlink" title="The Algorithm"></a>The Algorithm</h2><blockquote>
<p>An $(\epsilon, \delta)$ approximate algorithm $A$  </p>
<ol>
<li>Draw a set $S$ of $n =$ samples independently from $\mathcal{D}$.  </li>
<li>Return an $h’$ such that<br>$$<br>h’ = \arg\min_{h \in \mathcal{H} } L_{S} (h)<br>$$</li>
</ol>
</blockquote>
<p>A key result of the algorithm states that $L_S(h)$ is a good approximation of $\mu_h$ for all $h \in \mathcal{H}$ simultaneously. </p>
<blockquote>
<p>Theorem. If $n \ge$, then<br>$$<br>    \Pr_{S \sim \mathcal{D}^n } [ \exists h \in \mathcal{H} : L_S(h) \notin B(\mu_h, \epsilon / 2 )  ] \le \delta<br>$$</p>
</blockquote>
<p>An immediate corollary is that $h’$ is $\epsilon$-approximate with probability at least $1 - \delta$:<br>$$<br>\mu_{h’} \le L_S(h’) + \frac{\epsilon}{2} \le L_S(h^*) + \frac{\epsilon}{2} \le \mu_{h^*} + \epsilon.<br>$$</p>
<h2 id="Proof-of-The-Theorem"><a href="#Proof-of-The-Theorem" class="headerlink" title="Proof of The Theorem"></a>Proof of The Theorem</h2><p>The proof relies on a technique called double sampling. Alongside with $S$, we create another sample set $S’ \sim \mathcal{D}^n$ independently, termed the “ghost sample”. Let<br>$$<br>S = { (x_1, y_1), (x_2, y_2), …, (x_n, y_n) }<br>$$<br>and<br>$$<br>S’ = { (x_1’, y_1’), (x_2’, y_2’), …, (x_n’, y_n’) }.<br>$$ </p>
<p>Then, we define</p>
<ol start="16">
<li>$\sigma:$ a random swap that exchanges the $i$-th ($i \in [n]$) element of $S$ and $S’$ independently with probability 0.5. Call the resulting sample sets $\sigma S$ and $\sigma S’$. Let $\sigma S[i]$ ($\sigma S’[i]$) be the $i$-th element of $\sigma S[i]$ ($\sigma S’[i]$). So<br>$$<br>\Pr \left[ <pre><code>\sigma S [i] = (x_i,  y_i) \wedge
\sigma S&#39;[i] = (x_i&#39;, y_i&#39;)
</code></pre>
 \right] = 0.5 \<br> \Pr \left[ <pre><code>\sigma S [i] = (x_i&#39;, y_i&#39;) \wedge
\sigma S&#39;[i] = (x_i,  y_i)
</code></pre>
 \right] = 0.5<br>$$</li>
</ol>
<p>We use the gap of $|L_{\sigma S} (h) - L_{\sigma S’} (h)|$ as a proxy of $|L_S(h) - \mu_h|$. This enables us to focus on finite sets $S$ and $S’$ without worrying about the infinite size of $\mathcal{H}$.  </p>
<p>For convenience, we use $\underset{S}{\Pr}[\cdot]$ ($\underset{S’}{\Pr}[\cdot]$) as shorthand for $\underset{S \sim \mathcal{D}^n}{\Pr}[\cdot]$ ($\underset{S’ \sim \mathcal{D}^n}{\Pr}[\cdot]$). The road map of our proof is as follows. </p>
<blockquote>
<p>Lemma 1.<br>  $$<br>     \Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \le 2 \max_{S, S’ \in \mathcal{Z}^n } \Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S’ } (h) | \ge \frac{\epsilon}{2} ]<br>    $$</p>
</blockquote>
<p>On the left hand side of the inequality, the probability measures the  event of a random set $S$ sampling from $\mathcal{D}$. On the right hand side,  we can pick a fixed pair of $S$ and $S’$ that maximize<br>$$<br>\Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S’ } (h) | \ge \frac{\epsilon}{2} ],<br>$$</p>
<p>and the probability measures the event of the random swap $\sigma$. Fixing $S$ and $S’$ significantly simplifies the structure of $\mathcal{H}$. By Hoeffding inequality and union bound, we will prove that </p>
<blockquote>
<p>Lemma 2. For any fixed pair of $S, S’ \in \mathcal{Z}^n$,<br>  $$<br>        \Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S’ } (h) | \ge \frac{\epsilon}{2} ] \le 2 \Pi_\mathcal{H} (2n) \exp( - \frac{n^2 \epsilon^2 }{2} )<br>    $$</p>
</blockquote>
<h3 id="Proof-of-Lemma-1"><a href="#Proof-of-Lemma-1" class="headerlink" title="Proof of Lemma 1."></a><strong><em>Proof of Lemma 1.</em></strong></h3><p>The proof of Lemma 1 consists of three steps. </p>
<h4 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a><strong>S<em>tep 1</em></strong></h4><p>We will prove that<br>$$<br>    \Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \le 2 \Pr_{S , S’} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S’} (h) \in B(\mu_h, \epsilon / 2)  ].<br>$$</p>
<p><em>Proof of Step 1.</em><br>By Hoeffding inequality, for a fixed $h \in \mathcal{H}$, when $n \ge \frac{2}{\epsilon^2} \ln 4$<br>$$<br>    \Pr_{S’} [ L_{S’} (h) \notin B(\mu_h, \epsilon / 2) ] \le 2 \exp(- 2 n (\frac{\epsilon }{ 2 })^2 ) = 2 \exp( - \frac{n \epsilon^2}{2} ) \le \frac{1}{2}<br>$$</p>
<p>It follows that<br>$$<br>    \Pr_{S , S’} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S’} (h) \in B(\mu_h, \epsilon / 2)  ]<br>        \ge<br>    \frac{1}{2} \Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ]<br>$$</p>
<p>$\square$</p>
<h4 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a><strong><em>Step 2</em></strong></h4><p>Observe that<br>$$<br>    {  \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S’} (h) \in B(\mu_h, \epsilon / 2)  }<br>        \subset<br>    {  \exists h : |L_S (h) - L_{S’} (h) | \ge \frac{\epsilon}{2} }<br>$$</p>
<p>By monotonicity of probability, we get<br>$$<br>    \Pr_{S} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S’} (h) \in B(\mu_h, \epsilon / 2) ]<br>        \le<br>    \Pr_{S, S’} [ \exists h : |L_S (h) - L_{S’} (h) | \ge \frac{\epsilon}{2} ]<br>$$</p>
<h4 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a><strong><em>Step 3</em></strong></h4><p>Finally,<br>$$<br>    \Pr_{S, S’ } {  \exists h : |L_S (h) - L_{S’} (h) | \ge \frac{\epsilon}{2} }<br>        \le<br>    \max_{S, S’ \in \mathcal{Z}^n } \Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S’ } (h) | \ge \frac{\epsilon}{2} ]<br>$$</p>
<p><em>Proof of Step 3.</em><br>We claim that $\sigma S$ and $\sigma S’$ have the same joint distribution as $S$ and $S’$ (if $S, S’ \sim \mathcal{D}^n$). For points $\forall Z, Z’ \in \mathcal{Z}^n$, by symmetry,  it holds that<br>$$<br>\Pr_{S, S’} [ S = Z, S’ = Z’] = \Pr_{S, S’, \sigma} [ \sigma S = Z, \sigma S’ = Z’]<br>$$</p>
<p>The right hand side can be viewed as the successful probability of the following experiment:</p>
<ul>
<li>Sample independently $S \sim \mathcal{D}^n$ and $S’ \sim \mathcal{D}^n$.   </li>
<li>For each $i \in [n]$, exchange the $i$-th elements of $S$ and $S’$ independently with probability $0.5$. </li>
<li>After the random swap, $\sigma S = Z$ and $\sigma S’ = Z’$. </li>
</ul>
<p>Now, $\forall \mathcal{E} \subset \mathcal{Z}^{2n}$, it holds that<br>$$<br>\begin{aligned}<br>    \Pr_{S, S’} [ (S, S’) \in \mathcal{E} ]<br>        &amp;= \Pr_{S, S’, \sigma} [ (\sigma S, \sigma S’) \in \mathcal{E} ] \<br>        &amp;= \mathbb{E}_{S, S’} [ \Pr_\sigma [(\sigma S, \sigma S’) \in \mathcal{E} ] \mid S, S’ ] \<br>        &amp;\le \max_{S, S’ \in \mathcal{Z}^n } \Pr_\sigma [(\sigma S, \sigma S’) \in \mathcal{E} ]<br>\end{aligned}<br>$$</p>
<p>Replacing $\mathcal{E}$ with the set ${Z, Z’ \in \mathcal{Z}^n :  \exists h : |L_Z (h) - L_{Z’} (h) | \ge \frac{\epsilon}{2} }$ finishes the proof. </p>
<p>$\square$</p>
<p><em>Remark of the proof of step 1.</em> If we define<br>$$<br>\mathcal{H}(S, \epsilon) \doteq { h \in \mathcal{H} :L_S(h) \notin B(\mu_h, \epsilon ) }.<br>$$<br><em>which is the set of hypothesis whose empirical loss on $S$ is $\epsilon$ more than its expectation. Then,</em><br>$$<br>\begin{aligned}<br>    \Pr_{S} [ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \cdot \frac{1}{2}<br>    &amp;= \Pr_{S } [ \mathcal{H}(S, \epsilon) \neq \emptyset ] \cdot \frac{1}{2} \<br>    &amp;\le \Pr_{S } [ \mathcal{H}(S, \epsilon) \neq \emptyset] \<br>    &amp; \ \ \cdot \Pr_{S, S’ } [ \exists h \in \mathcal{H}(S, \epsilon) : L_{S’} (h) \in B(\mu_h, \epsilon / 2 ) \mid \mathcal{H}(S, \epsilon) \neq \emptyset ] \<br>    &amp;= \Pr_{S, S’ } [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge L_{S’} (h) \in B(\mu_h, \epsilon / 2 )  ]<br>\end{aligned}<br>$$</p>
<p><em>Note that the event $\mathcal{H}(S, \epsilon) \neq \emptyset$ is equivalent to the one</em><br>$$<br>\cup_{h \in \mathcal{H} } { L_S(h) \notin B(\mu_h, \epsilon ) }<br>$$</p>
<p><em>For a fixed $h \in \mathcal{H}$, the event ${ L_S(h) \notin B(\mu_h, \epsilon ) }$ is measurable when $S$ consists of a finite number of i.i.d samples from $\mathcal{D}$. If $\mathcal{H}$ consists of countable number of $h$, then $\cup_{h \in \mathcal{H} } { L_S(h) \notin B(\mu_h, \epsilon ) }$ is a countable union and should be measurable.</em> </p>
<!-- *Definition*.

 1. $\Gamma_n:$ the set of permutations on $\{1, 2, ..., 2n \}$ that swaps only $i$ and $i + n$ for $\forall i \in [n]$,  s.t., $\forall \sigma \in \Gamma_n$,  $\forall i \in [n]$, 
    $$
    \begin{aligned}
        \text{ either }
        \begin{cases}
            \sigma(i) = i  \\
            \sigma(i + n) = i + n
        \end{cases}
        \text{ or }
        \begin{cases}
            \sigma(i) = i + n \\
            \sigma(i) = i 
        \end{cases}.
    \end{aligned}   
    $$

As $\sigma$ is an one-to-one mapping, its inversion $\sigma^{-1}$ exists. Further, $\sigma^{-1}$ is also a permutation in $\Gamma_n$. 

Let $S \sim \mathcal{D}^{2n}$ and denote it as 
$$
S \doteq \{ (x_1, y_1), ..., (x_i, y_i), ..., (x_{2n}, y_{2n} ) \}.
$$

Applying a permutation $\sigma$ on $S$, we obtain
$$
\sigma S \doteq \{ (x_{ \sigma^{-1}(1) }, y_{ \sigma^{-1}(1) } ), ..., (x_{ \sigma^{-1}(i) }, y_{ \sigma^{-1}(i) } ), ...., (x_{ \sigma^{-1}(2n) }, y_{ \sigma^{-1}(2n) } )\}
$$

Observe that $\forall i \in [n]$, $(x_{ \sigma^{-1}(i) }, y_{ \sigma^{-1}(i) } )$ is sampled independently from $\mathcal{D}$. It follows that $\sigma S$ has the same distribution as $S$. 

***Definition.***

1. $\forall \mathcal{E} \subset \mathcal{Z}^{2n}, \forall S \in \mathcal{Z}^{2n}$, define $\mathbb{1}_\mathcal{E} (S)$ the indicator function of whether $S$ belongs to $\mathcal{E}$. 

> Corollary. $\forall \sigma \in \Gamma_n, \forall \mathcal{E} \subset \mathcal{Z}^{2n}$, it holds that 
> $$
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ] = \Pr_{S \sim \mathcal{D}^{2n}} [ \sigma S \in \mathcal{E} ] = \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ]
> $$

If we choose an $\sigma$ uniformly from $\Gamma_n$, then 
$$
\begin{aligned}
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ]  
        &= \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &= \frac{1}{ |\Gamma_n| } \sum_{\sigma \in \Gamma_n } \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &=  \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \frac{1}{ |\Gamma_n| } \sum_{\sigma \in \Gamma_n } \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &=  \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \Pr_\sigma [\sigma S \in \mathcal{E} ] ]. \\
\end{aligned}
$$

> Corollary 2. $\forall \mathcal{E} \subset \mathcal{Z}^{2n}$, it holds that 
> $$
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ] \le \max_{S \in \mathcal{Z}^{2n} } \Pr_\sigma [\sigma S \in \mathcal{E} ]
> $$

***Definition.*** For $S \in \mathcal{Z}^{2n}$, define 
$$
S_1 \doteq \{ (x_1, y_1), ..., (x_n, y_n) \}
$$
as the first half of $S$ and 
$$
S_2 \doteq \{ (x_{n + 1}, y_{n + 1}), ..., (x_{ 2n } , y_{ 2n }) \}
$$
as the second half. In a similar manner we define $\sigma S_1$ and $\sigma S_2$ as the first and second half of $\sigma S$ respectively. 

It follows from Corollary 2 that 

The corollary allows us to upper bond the failure probability on a fixed $S$ that maximizes $\Pr_\sigma [ \exists h : |L_{ \sigma S_1} (h) - L_{ \sigma S_2 } (h) | \ge \frac{\epsilon}{2} ]$, and now the randomness comes only from the uniform choice of $\sigma$. This is important as there are finitely many of functions defined on finite set $S$, on which we could apply union bound.  -->

<h3 id="Proof-of-Lemma-2"><a href="#Proof-of-Lemma-2" class="headerlink" title="Proof of Lemma 2."></a><strong><em>Proof of Lemma 2.</em></strong></h3><p>Let<br>$$<br>S = { (x_1, y_1), (x_2, y_2), …, (x_n, y_n) }<br>$$<br>and<br>$$<br>S’ = { (x_1’, y_1’), (x_2’, y_2’), …, (x_n’, y_n’) }.<br>$$<br>be a pair of fixed sample sets. Define the set $\mathcal{C} = { x_1, x_2, …, x_n, x_1’, x_2’, …, x_n’ } \subset \mathcal{X}$ (with duplicates removed). The size of $\mathcal{C}$ is bounded by $2n$ and therefore the size of restriction of $\mathcal{H}$ on $\mathcal{C}$ is bounded by $\Pi_\mathcal{H} (2n)$.</p>
<p>First, fix a hypothesis $h$. For all $i \in [n]$, define $a_i = |\ell( h( x_i ), y_i) - \ell( h( x_i’ ), y_i’ ) | \le 1$ and let $V_i \in {-1, 1}$ be a random variable with equal probability:<br>$$<br>\Pr[ V_i = -1 ] = \Pr[ V_i = 1] = 0.5<br>$$</p>
<p>If we apply a random swap $\sigma$ to $(S, S’)$, then $L_{ \sigma S} (h_S) - L_{ \sigma  S’ } (h_S)$ has the same distribution as $\sum_{i \in [n] } \frac{1}{n} a_i V_i$. Since<br>$$<br>\mathbb{E} [\sum_{i \in [n] } \frac{1}{n} a_i V_i] = \sum_{i \in [n] } \frac{1}{n} a_i \mathbb{E}[ V_i ] = 0,<br>$$</p>
<p>by Hoeffding inequality,<br>$$<br>\begin{aligned}<br>    \Pr_\sigma [ |L_{ \sigma S} (h) - L_{ \sigma  S’ } (h) | \ge \frac{\epsilon}{2} ]<br>        &amp;= \Pr[ | \sum_{i \in [n] } \frac{1}{n} a_i V_i - 0 | \ge \frac{\epsilon}{2} ] \<br>        &amp;\le 2 \exp( - \frac{ 2 }{  \sum_{i \in [n] } a_i^2 } (\frac{n \epsilon}{2})^2 )  \<br>        &amp;\le 2 \exp( - \frac{n^2 \epsilon^2 }{2} )<br>\end{aligned}<br>$$</p>
<p>The last inequality follows from that $\sum_{i \in [n] } a_i^2 \le n$. </p>
<p>Since for each $h \in \mathcal{H}$, it has the same behavior on $\mathcal{C}$ as some function in $\mathcal{H}<em>\mathcal{C}$. We can apply union bound on $\mathcal{H}_\mathcal{C}$.<br>$$<br>\Pr_\sigma [ \exists h : |L</em>{ \sigma S} (h) - L_{ \sigma  S’ } (h) | \ge \frac{\epsilon}{2} ] \le 2 |\mathcal{H}_\mathcal{C} | \exp( - \frac{n^2 \epsilon^2 }{2} )\le 2 \Pi_\mathcal{H} (2n) \exp( - \frac{n^2 \epsilon^2 }{2} )<br>$$</p>
<p>$\square$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Ethan Fetaya, “Lecture 02 - Introduction to Statistical Learning Theory”, Weizmann Institute of Science,  2016<br>[2].R. Schapire and D. Bieber, “Lecture 05 - COS 511: Theoretical Machine Learning,”, Princeton University, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/13/Advanced-Composition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/13/Advanced-Composition/" class="post-title-link" itemprop="url">Advanced Composition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-13 22:26:07" itemprop="dateCreated datePublished" datetime="2020-11-13T22:26:07+11:00">2020-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-15 22:41:53" itemprop="dateModified" datetime="2020-11-15T22:41:53+11:00">2020-11-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Once we have designed some basic differentially private algorithms, it is a  natural idea to combine them and analysis the privacy loss. We begin with an illustrative example that sets up the mathematical model step by step. </p>
<p>Image yourself in front of the door of a safe vault protected by a password lock. To open the door, you need the correct password $P$. If tried with the wrong password, the lock would destroy itself automatically. </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/door.png?raw=true" width="400" height="340" />
</div>

<p>Luckily, you know two candidate passwords $P_1$ and $P_2$, with one of them being the correct one. Further, you notice that the designer of the lock left a collection of boxes near the door, which contain information on how they decide the correct passwords. Obtaining complete information of anyone of them gives you the correct password. </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box1.png?raw=true" width="400" height="340" />
</div>

<p>But the boxes are also protected and you don’t have legally access to them. However, you can hack into the boxes. Hacking into the box won’t give you all its information, but a random message. In particular, each box $B$ is associated with a set $\mathcal{R}_B$, which is a finite collection of messages (in English). When $B$ is hacked, it returns a message $Y_B$  generated randomly from $\mathcal{R}_B$, whose distribution depends on the correct password $P$. </p>
<p>Let $\mathcal{D}<em>{B, P_1}$ be the distribution of $Y_B$ if the correct password is $P_1$, and $\mathcal{D}_{B, P_2}$ be the one if the correct password is $P_2$. If there is a huge difference between $\mathcal{D}</em>{B, P_1}$ and $\mathcal{D}_{B, P_2}$, then you might be able to guess the correct password.</p>
<p>E.g., suppose $\mathcal{R}_B =$ { “Dog bites.”, “Cat scratches.” } and the distributions are given as </p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">$\mathcal{D}_{B, P_1}$</th>
<th align="center">$\mathcal{D}_{B, P_2}$</th>
</tr>
</thead>
<tbody><tr>
<td align="center">“Dog bites.”</td>
<td align="center">0.99</td>
<td align="center">0.01</td>
</tr>
<tr>
<td align="center">“Cat scratches.”</td>
<td align="center">0.01</td>
<td align="center">0.99</td>
</tr>
</tbody></table>
<p>Hence, when you get $Y_B=$ “Dog bites.”, you prefer $P_1$ over $P_2$ and vice versa. </p>
<p>Anticipating such potential information leakage, the designer of the lock equips the boxes with a defense mechanism, called $(\epsilon, 0)$-mechanism. It guarantees the distributions $\mathcal{D}_{B, P_1}$ and $\mathcal{D}_{B, P_2}$ are similar so that it is hard for you to infer the correct password from the message. In particular, $\forall S \subset \mathcal{R}_B$, if $S$ is measurable, it holds that<br>$$<br>\Pr[ Y_B \in S \ | \ P = P_1 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_2 ] \<br>\Pr[ Y_B \in S \ | \ P = P_2 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_1 ] \<br>$$</p>
<p>When these inequalities are satisfied, we say that $\mathcal{D}<em>{B, P_1}$ and $\mathcal{D}_{B, P_2}$ are $(\epsilon, 0)$ close. The inequalities require $\mathcal{D}_{B, P_1}$ and $\mathcal{D}_{B, P_2}$ to have the same support on $\mathcal{R}_B$. Therefore, throughout our discussion below, we assume that both $\mathcal{D}</em>{B, P_1}$ and $\mathcal{D}_{B, P_2}$ assign positive probability to each element in $\mathcal{R}_B$. Otherwise, we can just replace $\mathcal{R}<em>B$ with the support of $\mathcal{D}</em>{B, P_1}$ (which is also the support of $\mathcal{D}_{B, P_2}$). </p>
<p>Now, hacking one box is unlikely to help you to guess the correct password. You want to hack more boxes, with the hope that the information combined will assist you. Due to resource limit, you can’t hack all boxes but only a finite number of them. You choose the first box randomly. Then you choose every new box based on the information obtained from the hacked boxes. The figure below shows an example of hacking five boxes. </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box2.png?raw=true" width="400" height="340" />
</div>

<p>Suppose that your resource enables you to hack $n$ boxes and let $\vec Y_n = (Y_1, Y_2, …, Y_n)$ be the output messages you obtained. Similar to the situation of hacking one box, if the distribution of $\vec Y_n$, conditioned on $P = P_1$, is utterly distant from that conditioned on $P = P_2$, then there could be some cases when you can confidently infer the true password. Conversely, to prevent severe information leakage, the designer needs to ensure there isn’t such case, i.e., the two distributions should be similar. </p>
<p>What makes things even more complicated is that, the distribution of $\vec Y_n$ depends not only on the output distributions of the boxes, but also on your strategy of choosing the boxes to hack. Let’s use symbol $A$ to denote your strategy. Whatever $A$ is, the designer need to guarantee that the distribution of $\vec Y_n$ conditioned $P = P_1$ should be similar to that conditioned on $P = P_2$. </p>
<p>Surprisingly, this is in some sense achievable, as long as for each box, its output distribution conditioned on $P = P_1$ is $(\epsilon, 0)$ close to that conditioned on $P = P_2$. </p>
<p>Let $\mathcal{R}$ be the set of possible possible messages of all boxes. </p>
<blockquote>
<p><strong><em>Theorem.</em></strong> $\forall A$, $\forall S \subset \mathcal{R}^n$, it holds that $\forall \delta’ \in (0, 1)$,<br>$$<br>    \Pr[ \vec Y_n \in S \ | \ P = P_1, A] \le e^{\epsilon’} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta’ \<br>    \Pr[ \vec Y_n \in S \ | \ P = P_2, A] \le e^{\epsilon’} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_1, A] + \delta’<br>$$<br>where $\epsilon’ = k\epsilon(e^\epsilon - 1) + \epsilon \sqrt{2 n \log \frac{1}{\delta’} }$.</p>
</blockquote>
<p>If we view $\epsilon$ as the privacy loss of a single box, then the theorem states that the privacy loss grows to $O(\sqrt {n} \epsilon )$ is $n$ boxes are hacked. </p>
<p>To prove the theorem, we need a rigorous model for the problem. </p>
<p><strong><em>Definitions.</em></strong></p>
<ol>
<li><p>$\mathcal{I}$: the index set.</p>
</li>
<li><p>${ B_\alpha : \alpha \in \mathcal{I} }$: the collection of boxes. </p>
</li>
<li><p>${ \mathcal{R}_{\alpha} : \alpha \in \mathcal{I} }$: the ranges of the outputs of the boxes. </p>
</li>
<li><p>$\mathcal{R} \doteq \cup_{\alpha \in \mathcal{I} } R_\alpha$: the range of any possible output by any box. </p>
</li>
<li><p>${ \mathcal{D}<em>{\alpha, P_1} : \alpha \in \mathcal{I} }$ (${ \mathcal{D}_{\alpha, P_2} : \alpha \in \mathcal{I} }$): the set of output distribution when the correct password is $P_1$ ($P_2$). Without loss of generality, we assume that for a fixed $\alpha \in \mathcal{I}$, the distribution $\mathcal{D}_{\alpha, P_1}$ ($\mathcal{D}_{\alpha, P_2}$) assigns positive probability to each element in $\mathcal{R}_\alpha$. Moreover, $\mathcal{D}</em>{\alpha, P_1}$ and $\mathcal{D}_{\alpha, P_2}$ are $(\epsilon, 0)$ close. </p>
</li>
<li><p>$n$: the number of boxes you can hack.</p>
</li>
<li><p>$\vec i_k \doteq (i_1, i_2, …, i_k)$: the index sequence of boxes you have chosen to hack up to time $k$, where $k \in [0, n]$. When $k = 0$, $\vec i_0$ corresponds to a empty sequence. </p>
</li>
<li><p>$\vec Y_k = (Y_1, Y_2, …, Y_k):$ the random variables that represent messages outputted by the chosen boxes up to time $k \in [1, n]$, where $Y_t \in \mathcal{R}_{i_t} \subset \mathcal{R}$ for $t \in [1, k]$. Therefore, $\vec Y_k$ can be view as random vector in $\mathcal{R}^k$. </p>
</li>
<li><p>$\vec x_k = (x_1, x_2, …, x_k) \in \mathcal{R}^k:$ a point in $\mathcal{R}^k$, where $k \in [0, n]$. When $k = 0$, we define $\vec x_0 = \emptyset$. </p>
</li>
<li><p>$\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt; = \left&lt; (i_1, i_2, i_3, …, i_k), (x_1, x_2, …, x_k) \right&gt;:$ the history up to $k \in [0, n]$, which consists of the chosen indexes and observations up to time $k$. We use $\vec h_0$ to denote the empty history.</p>
</li>
</ol>
<ol start="11">
<li>$A:$ your strategy (policy) for choosing boxes. It works as follows: for any fixed history $\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt;$, A is associated with a fixed distribution $\mathcal{D}<em>{A, \vec h_k}$ over $\mathcal{I} \setminus \vec i_k$, the set of indexes of the unchosen boxes. When inputted with $\vec h_k$, $A$ returns a random variable $A(\vec h_k)$ that follows the distribution $\mathcal{D}</em>{A, \vec h_k}$. </li>
</ol>
<p>We will show that, no matter what strategy you use, it is unlikely that you distinguish via the output $\vec Y_n$ whether the correct password is $P_1$ or $P_2$. This is because whether $P = P_1$ or not, the output distributions of $\vec Y_n$ are similar.  </p>
<p><strong><em>Proof of the theorem.</em></strong> We will just prove the first inequality, and the second one follows from symmetry. We consider a bad set in $\mathcal{R}^n$:<br>$$<br>\mathcal{W} \doteq { \vec x_n \in \mathcal{R}^n : \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] \ge e^{\epsilon’} \cdot \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] }<br>$$</p>
<p>and will use the following lemma. </p>
<blockquote>
<p><strong>Lemma.</strong> $\Pr[ \vec Y_n \in \mathcal{W}\ | \ P = P_1, A] \le \delta’$. </p>
</blockquote>
<p>Hence,<br>$$<br>\begin{aligned}<br>    \Pr[ \vec Y_n \in S \ | \ P = P_1, A]<br>        &amp;=  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in S \cap \mathcal{W} \ | \ P = P_1, A] \<br>        &amp;\le  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in \mathcal{W} \ | \ P = P_1, A] \<br>        &amp;\le \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \delta’ \<br>        &amp;= \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] + \delta’ \<br>        &amp;&lt; \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } e^{\epsilon’} \cdot  \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] + \delta’ \<br>        &amp;=  e^{\epsilon’} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta’<br>\end{aligned}<br>$$</p>
<p>The first inequality follows from monotonicity of probability, the second one from the lemma, and the final one from the definition of $\mathcal{W}$. </p>
<p><em>Proof of the lemma.</em> To prove the lemma, we need only to consider those point $\vec x_n$ with<br>$$<br>\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0.<br>$$<br>Otherwise, $\vec x_n$ contributes to 0 probability to the set $\mathcal{W}$ (whether it belongs to $\mathcal{W}$ or not). </p>
<p>Now, we expand the probability $\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A]$. To obtain the $k$-th output, there are two steps</p>
<ol>
<li>$A$ generates a index $i_k$ of a box based on the known history $\vec h_{k - 1}$.  </li>
<li>The box $B_{i_k}$ is hacked, and output a random message $x_k$ sampled from its distribution $D_{i_k, P_1}$. </li>
</ol>
<p>Let $\vec H_k$ be a random variable that represents the history up to $k$. By chain rule, we have<br>$$<br>\begin{aligned}<br>    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A]<br>    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}<em>{i_k, P_1} } [ Y_k = x_k \ | A(\vec h</em>{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ] \<br>    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}<em>{A, \vec h</em>{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A  ]<br>\end{aligned}<br>$$</p>
<p>By $\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0$,  each term in the expansion are positive. Similarly,<br>$$<br>\begin{aligned}<br>    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A]<br>    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}<em>{i_k, P_2} } [ Y_k = x_k \ | A(\vec h</em>{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] \<br>    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}<em>{A, \vec h</em>{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A  ]<br>\end{aligned}<br>$$</p>
<p>As both $\mathcal{D}<em>{i_k, P_1}$ and $\mathcal{D}</em>{i_k, P_2}$ assign positive probability to each point in $\mathcal{R}_{i_k}$, we know $\Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] &gt; 0$.</p>
<p>We are ready to consider the ratio:<br>$$<br>\begin{aligned}<br>    \ln \frac{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] }{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] }<br>    = \sum_{k = 1}^n \ln \frac{ \Pr_{Y_k \sim \mathcal{D}<em>{i_k, P_1} } [ Y_k = x_k \ | A(\vec h</em>{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}<em>{i_k, P_2} } [ Y_k = x_k \ | A(\vec h</em>{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }.<br>\end{aligned}<br>$$</p>
<p>If we replace $x_k$ by a random variable $X_k \sim \mathcal{D}<em>{i_k, P_1}$, then<br>$$<br>C_k \doteq \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = X_k \ | A(\vec h</em>{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}<em>{i_k, P_2} } [ Y_k = X_k \ | A(\vec h</em>{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }<br>$$</p>
<p>is a random variable. As $\mathcal{D}<em>{i_k, P_1}$ and $\mathcal{D}</em>{i_k, P_2}$ are $(\epsilon, 0)$ close, we have<br>$$<br>C_k \le \epsilon.<br>$$</p>
<p>Further, we have </p>
<blockquote>
<p><strong>Fact 1.</strong> For any $\alpha \in \mathcal{I}$, it holds that<br>$$<br>    \begin{aligned}<br>        \mathbb{E}<em>{ X \sim \mathcal{D}</em>{\alpha, P_1} } \left[ \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = X]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = X]  }  \right]<br>        &amp;= \sum_{x \in \mathcal{R}<em>\alpha } \underset{ X \sim \mathcal{D}</em>{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \<br>        &amp;\le \sum_{x \in \mathcal{R}<em>\alpha } \underset{ X \sim \mathcal{D}</em>{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \<br>        &amp;+<br>        \sum_{x \in \mathcal{R}<em>\alpha } \underset{ X \sim \mathcal{D}</em>{\alpha, P_2} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } \<br>        &amp;= \sum_{x \in \mathcal{R}<em>\alpha } \left[ \underset{ X \sim \mathcal{D}</em>{\alpha, P_1} }{ \Pr } [ X = x]  - \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x] \right] \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \<br>        &amp;\le (e^\epsilon - 1) \epsilon<br>    \end{aligned}<br>$$</p>
</blockquote>
<p>Therefore, $\mathbb{E} [C_k] \le (e^\epsilon - 1) \epsilon$. </p>
<blockquote>
<p><strong>Fact 2.</strong> (<strong>Azuma Inequality</strong>). Let $C_1, …., C_n$ be random variables such that $\forall k \in [n]$, $\Pr[ |C_k| \le \epsilon ] = 1$, and for every $(c_1, …, c_{k -1} ) \in \text{Supp} (C_1, …, C_{k - 1} )$, we have<br>$$<br>\mathbb{E}[ C_i \mid C_1 = c_1, …, C_{k -1} = c_{k - 1} ] \le \beta,<br>$$<br>Then for every $z &gt; 0$, we have<br>$$<br>\Pr[ \sum_{k = 1}^n C_i -  n \beta &gt;  z] \le \exp(- \frac{z^2}{ 2 n\epsilon^2 } )<br>$$</p>
</blockquote>
<p>Finally, applying <em>Azuma Inequality</em> with $z = \sqrt{ 2n \log \frac{1}{\delta’} }$ and $\beta = (e^\epsilon - 1) \epsilon$, we get the desired result.</p>
<p>$\blacksquare$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference."></a>Reference.</h3><p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/12/PAC-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/12/PAC-learning/" class="post-title-link" itemprop="url">PAC Learning Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-12 16:06:55" itemprop="dateCreated datePublished" datetime="2020-11-12T16:06:55+11:00">2020-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-19 15:16:12" itemprop="dateModified" datetime="2020-11-19T15:16:12+11:00">2020-11-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss a toy model of probably approximately correct (PAC) learning [1]. The setting focuses on a $d$-dimension Boolean hypercube ${0, 1}^d$ and a set of functions $\mathcal{H} = { h : {0, 1}^d \rightarrow {0, 1 } }$. A function $h \in \mathcal{H}$ is called a hypothesis or a concept, which assigns each vertex in the hypercube a label 0 or 1. </p>
<p>The goal of PAC learning is to learn an unknown hypothesis, denoted as $h^* \in \mathcal{H}$, from a dataset, which consists of $n$ points $X_1, X_2, …, X_n \in {0, 1}^n$ sampled independently from an unknown distribution $D$ (over the hypercube), as well as their labels $Y_1 = h^*(X_1), Y_2 = h^*(X_2), …, Y_n = h^*(X_n)$.  </p>
<p>We can state the algorithm for PAC learning in just one sentence. </p>
<blockquote>
<p>Output any hypothesis $h’ \in \mathcal{H}$ that is consistent with the sampled dataset, i.e.,<br>$$<br>    h’(X_i) = Y_i, \qquad \forall i \in [n]<br>$$</p>
</blockquote>
<p>Such an hypothesis always exists, as $h^*$ satisfies this constraint. We have the classic PAC learning theorem. </p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let $X \in {0, 1}^n$ be a random point sampled from the unknown distribution, then for a given $\delta \in (0, 1)$,<br>$$<br>        \Pr[ h’(X) \neq h^*(X)] \ge \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}<br>$$<br>with probability at most $\delta$. </p>
</blockquote>
<p>The PAC learning theorem states that the probability that $h’$ mislabels a point is bounded by $\sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}$. </p>
<p><em>Proof.</em> Consider some fixed $h \in \mathcal{H}$. For convenience, we rewrite<br>$$<br>\Pr[ h(X) \neq h^*(X)] = \mu_h.<br>$$</p>
<p>Denote $Z_i$ the indicator variable of whether $h$ mislabels $X_i$, i.e.,<br>$$<br>Z_i = \begin{cases}<br>    0, \qquad \text{ if } h(X_i) = Y_i \<br>    1, \qquad \text{ if } h(X_i) \neq Y_i<br>\end{cases}<br>$$</p>
<p>By definition, $\mathbb{E}[Z_i]$ is an unbiased estimator of $\mu_h$. Further, denote $\hat \mu_h$ the empirical mean of $Z_i$’s<br>$$<br>\hat \mu_h \doteq \frac{1}{n} \sum_{i \in [n] } Z_i<br>$$</p>
<p>Note that for the outputted $h’$, $\hat \mu_{h’} = 0$. </p>
<p>By Hoeffding inequality, for any $\delta’ &gt; 0$,<br>$$<br>\Pr[ \hat \mu_h \le \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta’} }{2n} } ] \le \delta’.<br>$$</p>
<p>By union bound, the probability that<br>$$<br>\exists h \in \mathcal{H}, \hat \mu_h \ge \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta’} }{2n} }<br>$$</p>
<p>is bounded by $| \mathcal{H} | \delta’$. By setting $\delta’ = \frac{\delta}{ | \mathcal{H} | }$, it is guaranteed that<br>$$<br>0 = \hat \mu_{h’ } \le \mu_{h’} -  \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}<br>$$</p>
<p>with probability at most $\delta$.</p>
<p>$\blacksquare$</p>
<p>The previous theorem can be improved with the $\sqrt{ \cdot }$ removed. The key here is that the selected $h’$ is error-free on the samples. </p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let $X \in {0, 1}^n$ be a random point sampled from the unknown distribution, then for a given $\delta \in (0, 1)$,<br>$$<br>        \Pr[ h’(X) \neq h^*(X)] \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}<br>$$<br>with probability at most $\delta$. </p>
</blockquote>
<p><em>Proof.</em> Consider some fixed $h \in \mathcal{H}$. For convenience, we rewrite<br>$$<br>\Pr[ h(X) \neq h^*(X)] = \mu_h.<br>$$</p>
<p>We will prove that if $\mu_h \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}$, then $h$’s probability of being selected is very low. </p>
<p>Denote $Z_i$ the indicator variable of whether $h$ mislabels $X_i$, i.e.,<br>$$<br>Z_i = \begin{cases}<br>    0, \qquad \text{ if } h(X_i) = Y_i \<br>    1, \qquad \text{ if } h(X_i) \neq Y_i<br>\end{cases}<br>$$</p>
<p>By definition, $\Pr[Z_i = 1] = \mu_h$. Therefore,<br>$$<br>    \begin{aligned}<br>        \Pr[ Z_1 = 0 \wedge Z_2 = 0 \wedge … \wedge Z_n = 0] = (1 - \mu_h)^n \le \exp(- \mu_h n) = \frac{\delta}{ |\mathcal{H} | }<br>    \end{aligned}<br>$$</p>
<p>Taking union bound over all possible $h$ gives the desired result. </p>
<p>$\blacksquare$</p>
<p><strong><em>Caveat.</em></strong> <em>In both theorem, we see a $\log \mathcal{H}$ term in the failure probability, which is due to the use of union bound and can be roughly considered as the complexity of the hypothesis family. It is tempting to think that this could be avoid as follows.</em> </p>
<blockquote>
<p>For the outputted $h’$, $Z_1, …, Z_n$ are i.i.d samples. Therefore, the probability of all of them being $0$ is give by<br>  $$<br>        (1 - \mu_{h’} )^n \le \exp( - \mu_{h’} n).<br>  $$<br>Bounding this probability by $\delta$ gives $\mu_{h’} \le \frac{ \log \frac{1}{\delta} } {n}$. </p>
</blockquote>
<p><em>The argument is wrong. For the outputted $h’$, the $Z_1, …, Z_n$ are not i.i.d samples, as $h’$ is selected according to $Z_1, …, Z_n$ and depends on them.</em> </p>
<p><em>To make it more concrete, suppose that there is an $h$, such that $\mu_h = \frac{ \log \frac{1}{\delta} } {n}$. Consider the following experiment</em></p>
<blockquote>
<ol>
<li>Sample $n$ points independently the distribution $D$.  </li>
<li>Pick an $h’$ that makes no prediction error on the samples. </li>
</ol>
</blockquote>
<p><em>If we repeat the experiment $N$ times for some positive integer $N$, then the fixed $h$ makes no mistake in roughly $(1 - \delta) N$ of the experiments and constitutes an candidate of $h’$. However, on the other roughly $\delta N$ experiments, where $h$ makes prediction error, it is no longer considered as the candidate of $h’$. The selection procedure of $h’$ ignores bad event of $h$ automatically.</em></p>
<p><em>We could also investigate the following example. Let $h^</em> = h_0$. There are also other two hypothesis $h_1$ and $h_2$. Let $S$ be the sample space. It holds that*<br>$$<br>|h_1(x) - h^*(x) | = \begin{cases}<br>    2 \epsilon, \forall x \in S \setminus S_{h_1} \<br>    0,\ \       \forall x \in S_{h_1}<br>\end{cases}<br>$$<br>$$<br>|h_2(x) - h^*(x) | = \begin{cases}<br>    2 \epsilon, \forall x \in S \setminus S_{h_2} \<br>    0,\ \       \forall x \in S_{h_2}<br>\end{cases}<br>$$<br><em>Moreover, $\Pr[S_{h_1}] = \Pr[S_{h_2} ] = \delta$. How, if we get a sample in $S_{h_1}$, then we can output $h’$ as $h_0$ or $h_1$, since both of them make no error when $x \in S_{h_1}$. Similarly, if $x \in S_{h_2}$, we can output either $h_0$ or $h_2$. We call $S_{h_1}$ and $S_{h_2}$ the bad areas. When $x$ is in the bad area of any hypothesis, we can’t distinguish this hypothesis from the true hypothesis. That is why we need to use union bound to bound the total probabilities of these bad areas.</em> </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/PAC-Learning.png?raw=true" width="400" height="340" />
</div>


<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. 6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/Gaussian-Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/06/Gaussian-Mechanism/" class="post-title-link" itemprop="url">Gaussian Mechanism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-06 23:00:19" itemprop="dateCreated datePublished" datetime="2020-11-06T23:00:19+11:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-10 20:56:26" itemprop="dateModified" datetime="2020-11-10T20:56:26+11:00">2020-11-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h2><p>We illustrate how Gaussian distribution (a.k.a. normal distribution) is constructed and some of its basic properties. </p>
<p>Suppose we would like to have a distribution in $\mathbb{R}$, such that </p>
<ol>
<li>It is centered at the origin.     </li>
<li>Its density function decreases exponentially with respect to the square distance to the origin.</li>
</ol>
<p><em>Remark: if we replace 2 with “a density function decreases exponentially with the distance to the origin”, we obtain Laplace distribution.</em></p>
<p>By definition, the density function $p(x)$ should be inversely and exponentially proportional to $x^2$: </p>
<p>$$<br>p(x) \propto \exp(-x^2 )<br>$$</p>
<p>If we define $M$ to be the value of<br>$$<br>M \doteq \int_{-\infty}^\infty \exp(-x^2) \ dx,<br>$$</p>
<p>then we can give the precise formula of $p(x)$:<br>$$<br>p(x)  = \frac{1}{M} \exp(- x^2)<br>$$</p>
<p>It is easy to see that $p(x)$ is a density function such that $\int_{-\infty}^\infty p(x) \ dx = 1$.   </p>
<h3 id="Closed-Form"><a href="#Closed-Form" class="headerlink" title="Closed Form"></a>Closed Form</h3><p>In general, it is not hard to construct a distribution. Let $h(x)$ be any integrable function on a domain $\mathcal{D}$ such that<br>$$<br>M = \int_\mathcal{D} h(x) \ dx &lt; \infty<br>$$</p>
<p>Then we can view $h(x)$ as almost an density function of some distribution. The only obstacle is that $M$ might not be $1$. However this is easy to overcome. We just scale the value of $h(x)$ by a constant factor at each point on $\mathbb{R}$ and set<br>$$<br>p(x) = \frac{1}{M} h(x)<br>$$</p>
<p>then we have immediately a density function. We don’t even need to know the exact value of $M$. What we need to guarantee is just that </p>
<blockquote>
<p>The value of $M$ exists and is finite.  </p>
</blockquote>
<p>And we can safely use $M$ to denote this value. For example, we know that<br>$$<br>\int_0^\infty x^{0.2} e^{-x} \ dx \le  \int_0^1 e^{-x} \ dx  + \int_1^\infty x e^{-x} \ dx &lt; \infty.<br>$$</p>
<p>We can construct a density function as above. It turns out that this is a special case of <em>Gamma distribution</em>. </p>
<p>For Gaussian distribution, we are lucky as we can compute a closed form of $M$. This is done by a trick of computing its square:<br>$$<br>\begin{aligned}<br>    M^2 &amp;= \big( \int_{-\infty}^\infty \exp( -x^2 ) \  dx \big)^2 \<br>        &amp;= \int_{-\infty}^\infty \exp( -x^2 ) \  dx \ \cdot \int_{-\infty}^\infty \exp( -y^2 ) \  dy \<br>        &amp;= \int_{-\infty}^\infty \int_{-\infty}^\infty \exp( -(x + y) ^2 ) \  dx  dy.<br>\end{aligned}<br>$$</p>
<p>Denote $r = \sqrt{ x^2 + y^2}$ the distance of $(x, y)$ to $(0, 0)$. We are integrating the function $\exp( - r^2)$ over the plane $\mathbb{R}^2$. It is natural to switch to polar coordinate system $(r, \theta)$, where $\theta$ is the angle of a point. Here is an informal but intuitive explanation. As the picture demonstrated below, if $r$ increases by $dr$ and $\theta$ increases by $d\theta$, the new area spanned by $dr$ and $d \theta$ is roughly $r dr d \theta$. Or we can calculate it algebraically<br>$$<br>\frac{1}{2} [(r + dr)^2 - r^2] d\theta = r dr d \theta + \frac{1}{2} (dr)^2 d \theta \approx r dr d \theta.<br>$$</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration1.png?raw=true" width="400" height="340" />
</div>

<p>Therefore,<br>$$<br>\begin{aligned}<br>    M^2 &amp;= \int_{0}^{ 2 \pi} \int_{0}^\infty \exp( -r^2 ) r d r d \theta \<br>        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2}  \int_{0}^\infty \exp( -r^2 ) d r^2 d \theta \<br>        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2} d \theta \<br>        &amp;= \pi.<br>\end{aligned}<br>$$</p>
<p>Hence,<br>$$<br>p(x) = \frac{1}{ \sqrt \pi} \exp( - x^2 ).<br>$$</p>
<h3 id="Basic-Properties"><a href="#Basic-Properties" class="headerlink" title="Basic Properties"></a>Basic Properties</h3><ol>
<li><blockquote>
<p><em>Expectation.</em> As the distribution is symmetric to $0$, it has expectation 0.   </p>
</blockquote>
</li>
<li><blockquote>
<p><em>Variance.</em> As its expectation is 0, the variance is given by </p>
</blockquote>
<p> $$<br> \begin{aligned}</p>
<pre><code> \int_&#123;-\infty&#125;^\infty  \frac&#123; x^2 &#125;&#123; \sqrt \pi &#125; \exp( -x^2 ) \ dx 
     &amp;= -\frac&#123;1&#125;&#123;2 \sqrt \pi &#125; \int_&#123;-\infty&#125;^\infty x \ d \exp( -x^2 ) \\
     &amp;= \frac&#123;1&#125;&#123;2 \sqrt \pi &#125; \int_&#123;-\infty&#125;^\infty \exp( -x^2 ) d x\\
     &amp;= \frac&#123;1&#125;&#123;2 \sqrt \pi &#125; \sqrt \pi \\
     &amp;= \frac&#123;1&#125;&#123;2&#125;
</code></pre>
<p> \end{aligned}<br> $$</p>
</li>
</ol>
<h3 id="General-Gaussian-Distribution"><a href="#General-Gaussian-Distribution" class="headerlink" title="General Gaussian Distribution"></a>General Gaussian Distribution</h3><p>We have proved that if $X$ is a random variable with density function $p(x) = \frac{1}{\sqrt \pi} \exp( -x^2)$, then<br>$$<br>\mathbb{Var}[X] = \frac{1}{2}<br>$$ </p>
<p>Consider another random variable $Y = \sqrt{2} X$. It has variance<br>$$<br>\mathbb{Var}[X] = 2 \mathbb{Var}[X] = 1<br>$$</p>
<p>To obtain its density function of $Y$, we first scale the function $p(x)$ horizontally by a factor of $\sqrt 2$ to get<br>$$<br>\frac{1}{ \sqrt{ \pi } } \exp( -\frac{x^2}{2} ).<br>$$</p>
<p>Now the area under the curve is $\sqrt 2$. We normalize this area to 1 and obtain $Y$’s density function as<br>$$<br>p(y) = \frac{1}{ \sqrt{2 \pi } } \exp( -\frac{y^2}{2} )<br>$$</p>
<p>From now on, we use $N(0, 1)$ to denote a Gaussian distribution with mean 0 and variance $1$. We can also scale $X$ by a factor of $\sqrt 2 \sigma$ for any $\sigma &gt; 0$, to get $Y = \sqrt 2 \sigma X$. It have variance $\sigma^2$ and density function<br>$$<br>p(y) = \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( - (\frac{y}{ \sqrt{2} \sigma })^2 ) = \frac{1}{ \sqrt{2 \pi \sigma^2 } } \exp( -\frac{y^2}{ 2 \sigma^2 } )<br>$$ </p>
<p>Finally, we can shift the center of $0$ to any real number $\mu \in \mathbb{R}$, and obtain a density function<br>$$<br>p(y) = \frac{1}{ \sqrt{ \pi\cdot 2  \sigma^2 } } \exp( -\frac{ (y - \mu)^2 }{ 2 \sigma^2 } )<br>$$</p>
<p>The corresponding distribution is denoted as $N(\mu, \sigma^2)$.  </p>
<p><em>The picture below shows a few Gaussian distributions.</em> </p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution.png?raw=true" width="700" height="430" />
</div>


<h3 id="Further-Properties"><a href="#Further-Properties" class="headerlink" title="Further Properties"></a>Further Properties</h3><ol>
<li><blockquote>
<p>If $X \sim N(0, a^2)$ and $Y \sim N(0, b^2)$, then $X + Y \sim N(0, a^2 + b^2)$. </p>
</blockquote>
<p> The demonstration here follows from [1]. The key observation is that the joint distribution of $X$ and $Y$ is rotation invariant with the origin. </p>
<p> As an example, we plot below the joint distribution of $X, Y \sim N(0, 1)$. </p>
 <div style="text-align:center">
 <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-0.png?raw=true" width="500" height="330" />
 </div>

<p> The figure below shows the same distribution. Observe again that the probability mass concentrates at a small region centered at origin. </p>
 <div style="text-align:center">
 <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-1.png?raw=true" width="500" height="330" />
 </div>
  
 Therefore, for any $t \in \mathbb{R}$, the set 
 $$
 \{ (x, y) \in \mathbb{R}^2 : ax + by \le t \}
 $$
 has the same probability measure as the one 
 $$
 \{ (x, y) \in \mathbb{R}^2 : x \le \frac{ t}{ \sqrt{a^2 + b^2} } \}.
 $$

<p> Observe that the latter can be obtained by rotating the former with respect to the origin (see the figure below).</p>
 <div style="text-align:center">
 <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration2.png?raw=true" width="800" height="350" />
 </div>    
     
 Now, the probability of the latter set is given by 
 $$
 \Pr[ X \le \frac{ t}{ \sqrt{a^2 + b^2} } ] = \Pr[  \sqrt{a^2 + b^2} X \le t]
 $$

<p> It concludes that $X + Y$ has the same distribution as $\sqrt{a^2 + b^2} X \sim N(0, a^2 + b^2)$. </p>
<p> <em>Remark:if we want, we can verify this algebraically:</em><br> $$<br> \Pr[X + Y \le t] = \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} }\exp(-\frac{x^2}{2} ) \ dx<br> $$</p>
<p> hence<br> $$<br> \begin{aligned}</p>
<pre><code> \frac&#123; \partial &#125;&#123; \partial t&#125; \Pr[X + Y \le t]
     &amp;= \frac&#123; \partial &#125;&#123; \partial t&#125; \int_&#123;-\infty&#125;^&#123; \frac&#123; t&#125;&#123; \sqrt&#123;a^2 + b^2&#125; &#125;  &#125; \frac&#123;1&#125;&#123; \sqrt&#123;2 \pi&#125; &#125;\exp(-\frac&#123;x^2&#125;&#123;2&#125; ) \ dx \\
     &amp;= \frac&#123;1&#125;&#123; \sqrt&#123;2 \pi (a^2 + b^2) &#125; &#125;\exp(-\frac&#123;x^2&#125;&#123;2 (a^2 + b^2) &#125; )
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> $\blacksquare$</p>
</li>
<li><blockquote>
<p>If $X \sim N(0, \sigma^2)$, then $\forall t &gt; 0$,<br>$$</p>
<pre><code> \Pr[ |X| \ge t] \le \exp( -\frac&#123;t^2&#125;&#123; 2\sigma^2&#125; )
</code></pre>
<p>$$</p>
</blockquote>
<p> <em>Remark: it is possible to get a tighter bound by using more advanced techniques.</em></p>
<p> In previous figures, it seems Gaussian distributions have a shape bump around its mean. This is kind of mis-leading, because the x-axis and y-axis are scaled. In the figure below, we plot $N(0, 1)$,  with ratio between $x$ and $y$ axes being approximate $1:1$. We see only a small bump around its mean. Although the distribution span the entire range of $\mathbb{R}$, the probability mass is centered tightly around the origin.</p>
 <div style="text-align:center">
 <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Concentration.png?raw=true" width="800" height="550" />
 </div>

<p> Let $T = \Pr[ |X| \ge t]$. Then<br> $$<br> T^2 = \int_{ |x| \ge t, |y| \ge t} \frac{1}{ 2 \pi \sigma^2 } \exp( - \frac{ x^2 + y^2}{2 \sigma^2} )<br> $$</p>
<p> As<br> $$<br> { (x, y) \in \mathbb{R}^2: |x| \ge t \wedge |y| \ge t } \subset<br> { (x, y) \in \mathbb{R}^2: x^2 + y^2 \ge 2 t^2 }<br> $$</p>
 <div style="text-align:center">
 <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration3.png?raw=true" width="600" height="600" />
 </div>

<p> It follows that<br> $$<br> \begin{aligned}</p>
<pre><code> T^2 
     &amp;\le \int_&#123; \sqrt 2 t&#125;^\infty \int_&#123;0&#125;^&#123;2 \pi &#125; \frac&#123;1&#125;&#123; 2 \pi \sigma^2 &#125; \exp( - \frac&#123; r^2 &#125;&#123;2 \sigma^2&#125; ) r \ d\theta dr\\
     &amp;\le \int_&#123; \sqrt 2 t &#125;^\infty \exp( - \frac&#123; r^2 &#125;&#123;2 \sigma^2&#125; ) \ d \frac&#123;r^2&#125;&#123; 2\sigma^2&#125; \\
     &amp;\le \int_&#123; \frac&#123; t^2 &#125;&#123;  \sigma^2 &#125; &#125;^\infty \exp( - z ) \ d z \\
     &amp;= \exp( -\frac&#123;t^2&#125;&#123; \sigma^2&#125; )
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> Hence $T \le \exp( -\frac{t^2}{ 2\sigma^2} )$. </p>
</li>
</ol>
<pre><code>*Remark: we briefly discuss the algebraic approach here.* 
$$
\begin&#123;aligned&#125;
    \Pr[X &gt; t] = \int_&#123;t&#125;^\infty \frac&#123;1&#125;&#123; \sqrt&#123;2 \pi \sigma^2&#125; &#125; \exp( - \frac&#123; x^2 &#125;&#123; 2 \sigma^2 &#125; ) \ dx
\end&#123;aligned&#125;
$$

*When $x \ge t$, $\frac&#123;x&#125;&#123;t&#125; \ge 1$, therefore,*
$$
\begin&#123;aligned&#125;
    \Pr[X &gt; t] 
        &amp;\le \int_&#123;t&#125;^\infty \frac&#123;1&#125;&#123; \sqrt&#123;2 \pi \sigma^2&#125; &#125; \frac&#123;x&#125;&#123;t&#125; \exp( - \frac&#123; x^2 &#125;&#123; 2  \sigma^2 &#125; ) \ dx \\
        &amp;\le  \frac&#123; \sigma &#125;&#123; \sqrt&#123;2 \pi &#125;  t &#125;  \int_&#123;t&#125;^\infty  \exp( - \frac&#123; x^2 &#125;&#123; 2  \sigma^2 &#125; ) \ d \frac&#123; x^2 &#125;&#123; 2 \sigma^2 &#125; \\
        &amp;=  - \frac&#123; \sigma &#125;&#123; \sqrt&#123;2 \pi &#125;  t &#125;  \exp( - \frac&#123; x^2 &#125;&#123; 2  \sigma^2 &#125; ) \mid_&#123;t&#125;^\infty \\
        &amp;=  \frac&#123; \sigma &#125;&#123; \sqrt&#123;2 \pi &#125;  t &#125;  \exp( - \frac&#123; t^2 &#125;&#123; 2 \sigma^2 &#125; )  \\
\end&#123;aligned&#125;
$$

*This bound is more useful only when we know that $t \ge \frac&#123; \sigma &#125;&#123; \sqrt&#123;2 \pi &#125; &#125;$*.
&lt;!-- ![](https://www.mathworks.com/help/examples/stats/win64/ComputeTheMultivariateNormalPdfExample_01.png) --&gt;


$\blacksquare$
</code></pre>
<h2 id="Gaussian-Mechanism"><a href="#Gaussian-Mechanism" class="headerlink" title="Gaussian Mechanism"></a>Gaussian Mechanism</h2><p>Our discussion in this section focus on a metric space $(\mathcal{X}, d)$ and a function $f: \mathcal{X} \rightarrow \mathbb{R}^n$. The Gaussian mechanism adds Gaussian noise to the output of $f$, such that for any neighboring pairs $x, y \in \mathcal{X}$ with $d(x, y) = 1$, it is hard to distinguish the outputs, i.e., their outputs have similar distribution after adding noises. </p>
<blockquote>
<p><strong>Definition.</strong> The sensitivity $\Delta$ of $f$ is defined as<br>$$<br>    \Delta = \max_{x, y \in \mathcal{X}, d(x, y) = 1 } ||f(x) - f(y) ||<br>$$<br>where $|| \cdot ||$ is the $\ell_2$-norm of a vector in $\mathbb{R}^n$.  </p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Given $f$, and a variance parameter $\sigma^2$, the Gaussian mechanism is a random variable defines as<br>  $$<br>        g(x) = f(x) + X<br>  $$<br>where $X = (X_1, X_2, …, X_n) \sim N(0, \sigma^2 I)$.</p>
</blockquote>
<p>We are ready to state the main theorem of this section. </p>
<blockquote>
<p><strong>Theorem.</strong> For $\delta, \epsilon \in (0, 1)$, if $\sigma = \frac{ \Delta \log \frac{1}{\delta} }{ \epsilon }$, then $\forall x, y \in \mathcal{X}$, s.t., $d(x, y) = 1$, and for all measurable subset $S \subset \mathbb{R}^n$,  it holds </p>
<p>  $$<br>    \Pr[ g(x) \in S] \le e^\epsilon \cdot \Pr[ g(y) \in S] + \delta<br>    $$</p>
</blockquote>
<p>It is obvious that if we set $\sigma \rightarrow \infty$, then the Gaussian distribution tends to be a uniform one over $\mathbb{R}^n$. So we are interested in how small $\sigma$ can be.  </p>
<p>We claim that $\sigma = O(\frac{\Delta}{\epsilon } )$ suffices. This is based on three observations: 1) Most of the probability mass of a Gaussian distribution concentrates on a ball centered at its mean with radius $O(\sigma)$; 2) the density ratio of any two points within the ball is bounded by a constant; 3) the distance between the distribution center of $g(x)$ and $g(y)$ is bounded by $\Delta$.</p>
<p><strong><em>Proof.</em></strong> For convenience of discussion, we write $g(y) = f(y) + Y$ to distinguish it from $g(x)$. By definition, $g(x)$ is a random variable that follows $N(f(x), \sigma^2 I)$. Similarly, $g(y) \sim N( f(y), \sigma^2 I)$. </p>
<p>Further, we define $p_x$ and $p_y$ the density function of $g(x)$ and $g(y)$ respectively. The sketch of the proof is as follows: we partition $\mathbb{R}^n$ into two parts:<br>$$<br>\mathcal{Y}_1 = { t \in \mathcal{R}^n : p_x(t) \le e^\epsilon \cdot p_y(t) } \<br>\mathcal{Y}_2 = { t \in \mathcal{R}^n : p_x(t) &gt; e^\epsilon \cdot p_y(t) }.<br>$$</p>
<p>If $\Pr[ g(x) \in \mathcal{Y}<em>2 ] \le \delta$, then<br>$$<br>\begin{aligned}<br>    \Pr[ g(x) \in S]<br>        &amp;= \Pr[ g(x) \in S \cap \mathcal{Y}<em>1] + \Pr[ g(x) \in S \cap \mathcal{Y}_2] \<br>        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[  g(x) \in \mathcal{Y}_2] \<br>        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \delta \<br>        &amp;= \int</em>{S \cap \mathcal{Y}_1 } p_x(t) \ d t + \delta \<br>        &amp;\le \int</em>{S \cap \mathcal{Y}_1 } e^\epsilon \cdot p_y(t) \ d t + \delta \<br>        &amp;= e^\epsilon \cdot \Pr[ g(y) \in S \cap \mathcal{Y}_1] + \delta. \<br>\end{aligned}<br>$$</p>
<p>As $p_y(t) &gt; 0$ for all $t \in \mathbb{R}^n$, we can rewrite<br>$$<br>\mathcal{Y}_1 = { t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } \le \epsilon  } \<br>\mathcal{Y}_2 = { t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } &gt; \epsilon  }.<br>$$</p>
<p>In literature, the ratio $\ln \frac{p_x(t) }{ p_y(t) }$ is know as <em>privacy loss</em>. Substituting $p_x(t)$ and $p_y(t)$ with their definitions, we get<br>$$<br>\begin{aligned}<br>    \ln \frac{ p_x( t)  }{  p_y(t)}<br>        &amp;= \ln \frac{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( -\frac{ ||t - f(x)||^2 }{ 2 \sigma^2 } ) }{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( -\frac{ ||t - f(y) ||^2 }{ 2 \sigma^2 } )  } \<br>        &amp;=  -\frac{ || t - f(x) ||^2 - || t - f(x) + f(x) - f(y) ||^2 }{ 2 \sigma^2 }<br>\end{aligned}<br>$$</p>
<p>As $t \sim N( f(x), \sigma^2 I)$, it holds that $t - f(x) \sim N( 0, \sigma^2 I)$. Let $t’ = t - f(x)$ and $v = f(x) - f(y)$, then<br>$$<br>\begin{aligned}<br>    \ln \frac{ p_x( t)  }{  p_y(t)}<br>        &amp;=  -\frac{ || t’ ||^2 - || t’ + v ||^2 }{ 2 \sigma^2 }    \<br>        &amp;= \frac{ 2 v^T t’ + || v ||^2 }{ 2 \sigma^2 }    \<br>\end{aligned}<br>$$</p>
<p>But $v^T t’ \sim N(0, || v ||^2 \sigma^2)$. Let $Z \sim N(0, 1)$, then $\ln \frac{ p_x( t)  }{  p_y(t)}$ has the same distribution as<br>$$<br>\frac{ 2 ||v|| \sigma Z + ||v||^2 }{ 2 \sigma^2  } = \frac{ ||v||^2 }{ 2 \sigma^2  } + \frac{ ||v|| }{  \sigma  } Z \sim N(\frac{ ||v||^2 }{ 2 \sigma^2  },  \frac{ ||v||^2 }{  \sigma^2  } ).   \<br>$$</p>
<p>Now,<br>$$<br>\frac{ ||v||^2 }{ 2 \sigma^2  } + \frac{ ||v|| }{  \sigma  } Z  \ge \epsilon \<br>\longleftrightarrow  Z  \ge \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  }<br>$$</p>
<p>As $Z \sim N(0, 1)$, as shown in the previous section, $\forall z \in \mathbb{R}$,<br>$$<br>\Pr[ Z \ge z] \le \exp( -\frac{ z^2 }{2} )<br>$$</p>
<p>Replacing $z$ with $\frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  }$,<br>$$<br>\Pr[ z \ge t ] \le  \exp( -\frac{1}{2} (\frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } )^2 ).<br>$$</p>
<p>We would like to bound this probability by some $\delta \in (0, 1)$, then<br>$$<br>\begin{aligned}<br>    &amp;\exp( -\frac{1}{2} ( \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } )^2 ) \le \delta \<br>    &amp;\Longrightarrow \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } \ge \sqrt{2 \ln \frac{1}{\delta} } \<br>    &amp;\Longrightarrow \frac{\epsilon }{||v|| } \sigma^2 - \sqrt{2 \ln \frac{1}{\delta} } \sigma - \frac{ ||v|| }{2} \ge 0<br>\end{aligned}<br>$$</p>
<p>Finally, we get<br>$$<br>\sigma \ge \frac{ \sqrt{ 2 \ln \frac{1}{\delta} } + \sqrt{ 2 \ln \frac{1}{\delta} + 2 \epsilon  }  }{ 2 \frac{\epsilon }{||v|| } }<br>$$</p>
<p>By concavity of the square root function, it suffices to take<br>$$<br>\sigma = \frac{ ||v|| }{ \epsilon } \sqrt{ 0.5 * 2 \ln \frac{1}{\delta} + 0.5 * (2 \ln \frac{1}{\delta} + 2\epsilon) } = \frac{ ||v|| }{ \epsilon } \sqrt{  2 \ln \frac{1}{\delta} + \epsilon },<br>$$</p>
<p>i.e.,<br>$$<br>\sigma^2 = \frac{ ||v||^2 }{ \epsilon^2 } (2 \ln \frac{1}{\delta} + \epsilon)<br>$$</p>
<p>$\blacksquare$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] B. Eisenberg and R. Sullivan, “Why Is the Sum of Independent Normal Random Variables Normal?,” Mathematics Magazine, vol. 81, no. 5, pp. 362–366, Dec. 2008<br>[2] G. Kamath, “Lecture 5 — Approximate Diﬀerential Privacy”, CS 860 - Algorithms for Private Data Analysis.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/03/Laplace-Distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/03/Laplace-Distribution/" class="post-title-link" itemprop="url">Laplace Distribution</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-03 10:22:41" itemprop="dateCreated datePublished" datetime="2020-11-03T10:22:41+11:00">2020-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-05 16:03:07" itemprop="dateModified" datetime="2020-11-05T16:03:07+11:00">2020-11-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The density function of a Laplace distribution (denoted as Laplace($\mu, b$) ) with location and scale parameters $\mu$ and $b$ is given by<br>$$<br>p(x) = \frac{1}{2 b} \exp( -\frac{ | x - \mu |  }{ b } )<br>$$</p>
<p>The density function of the distribution is symmetric with respect to $\mu$, where it achieves peak value $\frac{1}{2b}$. We plot below a few Laplace distributions with different parameters of $\mu$ and $b$. </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Laplace-Distribution.png?raw=true"></p>
<!-- <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Laplace-Distribution.png?raw=true" style="zoom: 67%;" /> -->


<h3 id="Properties"><a href="#Properties" class="headerlink" title="Properties."></a>Properties.</h3><p>We list and prove a few properties of the distribution. Let $X$ be a random variable that follows Laplace($\mu, b$).   </p>
<ol>
<li><p>$\mathbb{E}[X] = \mu$.</p>
<p><em>Proof:</em><br> $$<br> \begin{aligned}</p>
<pre><code> \frac&#123;1&#125;&#123;2&#125; \int_&#123;\mu &#125;^\infty \frac&#123;1&#125;&#123;b&#125; \exp( -\frac&#123;x - \mu&#125;&#123;b&#125; ) \ dx
 = \frac&#123;1&#125;&#123;2&#125; \int_&#123;0 &#125;^\infty \frac&#123;1&#125;&#123;b&#125; \exp( -\frac&#123;x&#125;&#123;b&#125; ) \ dx = \frac&#123;1&#125;&#123;2&#125;
</code></pre>
<p> \end{aligned}<br> $$<br> The proof follows from symmetry of Laplace($\mu, b$) at $\mu$.  </p>
</li>
<li><p>$\forall t \ge 0, \Pr[X - \mu \ge t] \le \frac{1}{2} \exp(-t)$. </p>
<p> <em>Proof:</em><br> $$<br> \begin{aligned}</p>
<pre><code> \Pr[X - \mu \ge t] = \frac&#123;1&#125;&#123;2&#125; \int_&#123;\mu + t &#125;^\infty \frac&#123;1&#125;&#123;b&#125; \exp( -\frac&#123;x - \mu&#125;&#123;b&#125; ) \ dx
 = \frac&#123;1&#125;&#123;2&#125; \int_&#123;t &#125;^\infty \exp( -x ) \ dx = \frac&#123;1&#125;&#123;2&#125; \exp(-t)
</code></pre>
<p> \end{aligned}<br> $$<br> $\square$</p>
</li>
<li><p>$\forall t \ge 0, \Pr[|X - \mu| \ge t] \le \exp(-t)$. </p>
<p> $\square$</p>
</li>
<li><p>$\mathbb{Var}[X] = 2b^2$.</p>
<p> <em>Proof:</em> First, for an integer $n \ge 0$, we define<br> $$<br> \Gamma(n) = \int_{0 }^\infty x^n \exp( -x ) \ dx<br> $$</p>
<p> Therefore, $\Gamma(0) = 1$. Suppose $n \ge 1$, we have<br> $$<br> \begin{aligned}</p>
<pre><code> \Gamma(n) 
     &amp;= -\int_&#123;0 &#125;^\infty x^n \ d \exp( -x ) \\
     &amp;= - x^n \exp(-x) \mid_0^\infty + \int_&#123;0 &#125;^\infty \exp( -x ) \ d x^n \\
     &amp;= n\int_&#123;0 &#125;^\infty x^&#123;n - 1&#125; \exp( -x ) \ dx \\
     &amp;= n \Gamma(n - 1)
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> By induction, we conclude $\Gamma(n) = n!$. Now, consider</p>
<p> $$<br> \begin{aligned}</p>
<pre><code> \frac&#123;1&#125;&#123;2&#125; \int_&#123;\mu &#125;^\infty \frac&#123;1&#125;&#123;b&#125; (x - \mu)^2 \exp( -\frac&#123;x - \mu&#125;&#123;b&#125; ) \ dx
 &amp;= \frac&#123;1&#125;&#123;2&#125; \int_&#123;0 &#125;^\infty \frac&#123;1&#125;&#123;b&#125; x^2 \exp( -\frac&#123;x&#125;&#123;b&#125; ) \ dx \\
 &amp;= \frac&#123;b^2&#125;&#123;2&#125; \int_&#123;0 &#125;^\infty x^2 \exp( -x ) \ dx \\
 &amp;= \frac&#123;b^2&#125;&#123;2&#125; \Gamma(2) \\
 &amp;= b^2
</code></pre>
<p> \end{aligned}<br> $$<br> The proof follows from symmetry of Laplace($\mu, b$) at $\mu$.  </p>
</li>
<li><p>Let $X’ \sim Laplace(\mu, b)$ and define another random variable $Y = X’ + b$, then the density function of $Y$ shares the same shape with that of $X’$, with its center shifted to the right by distance $b$. To distinguish their density function, denote $p_{X’} (\cdot)$ the one for random variable $X’$ and $p_Y(\cdot)$ the one for $Y$.  For any $t \in \mathbb{R}$, </p>
<p> $$<br> \frac{ p_{X’} (t) }{ p_Y(t) } = \exp( - \frac{ |t - \mu | - | t - ( \mu + b ) | }{ b  }) \in [ \exp(-1), \exp(1) \ ]<br> $$</p>
</li>
<li><p>If $X, X’ \sim Laplace(\mu, \frac{b}{\epsilon} )$ be a pair of independent random variables, and $Y = X’ + b$, then </p>
<p> $$<br> \frac{ p_X(t) }{ p_Y(t) } = \exp( - \epsilon \frac{ |t - \mu | - | t - ( \mu + b ) | }{ b  }) \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]<br> $$</p>
</li>
<li><p>Continued. Let $S$ be any interval in $\mathbb{R}$. </p>
<p> $$<br> \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } = \frac{ \int_{t \in S} p_X(t) \ dt }{ \int_{t \in S} p_Y(t) \ dt } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]<br> $$</p>
</li>
<li><p>Continued. Denote $P_X$ and $P_Y$ the probability functions of $X$ and $Y$ respectively. Let $S$ be any Borel set in $\mathbb{R}$. </p>
<p> $$<br> \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } = \frac{ \int_S 1 \ d P_X }{ \int_S 1 \ d P_Y } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]<br> $$</p>
</li>
<li><p>Let $X = (X_1, X_2, …, X_n), X’ = (X_1’, X_2’, …, X_n’)$ be independent random vectors with dimension $n$, where $X_i, X_i’ \sim Laplace(0, \frac{\Delta}{\epsilon} )$ (for $1 \le i \le n$) are independent random variables. Let $Y \doteq X’ + \mu = (Y_1, Y_2, …, Y_n)$ be another random vector, such that $Y_i = X_i’ + \mu_i$ for $1 \le i \le n$ and $|\mu| = \sum_{i \in [n] } \mu_i \le \Delta$. For any $t \in \mathbb{R}^n$, </p>
<p> $$<br> \begin{aligned}</p>
<pre><code> \frac&#123; p_X(t) &#125;&#123; p_Y(t) &#125; &amp;= \frac&#123; \exp( - \frac&#123;\epsilon&#125;&#123;\Delta&#125; \sum_&#123;i \in [n] &#125; |t_i| ) &#125;&#123; \exp( - \frac&#123;\epsilon&#125;&#123;\Delta&#125; \sum_&#123;i \in [n] &#125; |t_i - \mu_i| ) &#125; \\
 &amp;= \exp( - \epsilon \frac&#123; |t | - | t - \mu| &#125;&#123; \Delta  &#125; ) \\
 &amp;\in [\exp( - \epsilon \frac&#123; |\mu| &#125;&#123; \Delta  &#125; ), \exp( \epsilon \frac&#123; |\mu| &#125;&#123; \Delta  &#125; )] \\
 &amp;\in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
</code></pre>
<p> \end{aligned}<br> $$</p>
</li>
<li><p>Continued. Let $S \subset \mathbb{R}^n$ be any Borel set. Then </p>
<p>$$</p>
<pre><code>\frac&#123; \Pr[X \in S] &#125;&#123; \Pr[Y \in S] &#125; \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
</code></pre>
<p>$$</p>
</li>
</ol>
<h3 id="Laplacian-Mechanism"><a href="#Laplacian-Mechanism" class="headerlink" title="Laplacian Mechanism."></a>Laplacian Mechanism.</h3><p>Let $(\mathcal{X}, d)$ be a metric space and $f: \mathcal{X} \rightarrow \mathbb{R}^n$ a function defined on $\mathcal{X}$. </p>
<p><strong><em>Definition.</em></strong> $x, x’ \in \mathcal{X}$ are called neighboring points if $d(x, x’) = 1$.   </p>
<p><strong><em>Definition.</em></strong> The sensitivity $\Delta$ of $f$ is defined as<br>$$<br>\Delta \doteq \max_{x, x’ \in \mathcal{X}, d(x, x’) = 1} | f(x) - f(x’) |<br>$$</p>
<p>where $| \cdot |$ is the $\ell_1$ distance. In other words, $\Delta$ is the maximum $\ell_1$ distance between the images of two neighboring points. </p>
<p>We now construct a new randomized function, called <em>Laplacian mechanism</em> from $f$, as follows. </p>
<p><strong><em>Definition.</em></strong> The Laplacian mechanism of $f$ is defined as<br>$$<br>g(x) = f(x) + Y<br>$$</p>
<p>where $Y = (Y_1, Y_2, …, Y_n)$ is a random vector with independent random variables $Y_i \sim Laplace(\Delta / \epsilon)$. </p>
<p>$\square$</p>
<h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><p><em>$g$ is $(\epsilon, 0)$-differentially private.</em></p>
<p><em>Proof.</em> Let $S \subset \mathbb{R}^n$ be an arbitrary Borel set. We need to prove for any pair of neighboring $x, x’ \in \mathcal{X}$,<br>$$<br>    \frac{ \Pr[ g(x) \in S]}{ \Pr[ g(x’) \in S] } \le \exp(\epsilon)<br>$$</p>
<p>Denote $p_x$ and $p_{x’}$ the density functions for $g(x)$ and $g(x’)$ respectively. It reduces to prove the following property of the density function<br>$$<br>    \frac{ p_x(t) }{ p_{x’} (t)  } \le \exp(\epsilon), \qquad \forall t \in \mathbb{R}<br>$$</p>
<p>But<br>$$<br>\begin{aligned}<br>    \frac{ p_x(t) }{ p_{x’} (t)  }<br>        &amp;= \frac{ \exp( - \frac{\epsilon |t - f(x)| }{\Delta} ) }{ \exp( - \frac{\epsilon |t - f(x’)| }{\Delta} )  } \<br>        &amp;= \exp\big( - \frac{\epsilon }{\Delta} (|t - f(x)| - |t - f(x’) | )  \big) \<br>        &amp;\le \exp \big( \frac{\epsilon }{\Delta}  |f(x) - f(x’) |  \big) \<br>        &amp;= \exp( \epsilon )<br>\end{aligned}<br>$$</p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/02/Randomized-Response/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/02/Randomized-Response/" class="post-title-link" itemprop="url">Randomized Response</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-02 10:41:39" itemprop="dateCreated datePublished" datetime="2020-11-02T10:41:39+11:00">2020-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-09 17:26:31" itemprop="dateModified" datetime="2020-11-09T17:26:31+11:00">2020-11-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the setting of randomized response, there is a coordinator and a set of $n$ players each having a secrete bit $x_i \in {0, 1}$.  Instead of sending $x_i$ to the coordinator directly, the player sends a randomly perturbed version $X_i$, such that<br>$$<br>X_i =<br>\begin{cases}<br>    x_i, \qquad \text{with probability } 0.5 + \epsilon \<br>    \bar{ x}_i, \qquad \text{with probability } 0.5 - \epsilon \<br>\end{cases}<br>$$</p>
<p>where $\epsilon \in [0, 0.5]$ and $\bar{x}_i = 1 - x_i$. If $\epsilon = 0$, then the response from play $i$ is totally random. On the other hand, if $\epsilon = 0.5$, there isn’t noise in $X_i$. </p>
<p>The coordinator would like to estimate the ratio of $1$’s among the $x_i$’s. Let<br>$$<br>\bar{X} = \frac{1}{n} \sum_{i \in [n] } X_i.<br>$$</p>
<p>Denote $\mu = \frac{1}{n} \sum_{i \in [n] } x_i$ the true ratio. Now<br>$$<br>\begin{aligned}<br>    \mathbb{E} [ \bar{X} ] &amp;= \frac{1}{n} \sum_{i \in [n] } \mathbb{E}[ X_i ] \<br>                            &amp;= \frac{1}{n} \sum_{i \in [n] } [ (0.5 + \epsilon) x_i + (0.5 - \epsilon )\bar{x}<em>i ] \<br>                            &amp;= \frac{1}{n} \sum</em>{i \in [n] } [ 2 \epsilon x_i  + (0.5 - \epsilon) (x_i + \bar{x}<em>i) ] \<br>                            &amp;= \frac{1}{n} \sum</em>{i \in [n] } [ 2 \epsilon x_i  + (0.5 - \epsilon)  ] \<br>                            &amp;= \frac{1 - 2\epsilon }{2n}  + 2\epsilon \mu<br>\end{aligned}<br>$$</p>
<p>and<br>$$<br>\begin{aligned}<br>    \mathbb{Var} [ \bar{X} ] = \frac{1}{n} \sum_{i \in [n] } \mathbb{Var}[ X_i ]<br>                            \le \frac{1}{4n}<br>\end{aligned}<br>$$</p>
<p>Hence,<br>$$<br>\mu = \mathbb{E}[ \frac{1}{2\epsilon} (\bar X + \frac{2\epsilon - 1}{2n}) ]<br>$$</p>
<p>and $\hat \mu \doteq \frac{1}{2\epsilon} (\bar X + \frac{2\epsilon - 1}{2n})$ is an unbiased estimator of $\mu$. As<br>$$<br>\mathbb{Var}[ \hat \mu ] = \frac{1}{4 \epsilon^2 } \mathbb{Var}[\bar{X} ] \le \frac{1}{16 n \epsilon^2}<br>$$</p>
<p>By Chebyshev’s inequality,<br>$$<br>\Pr[ |\hat \mu - \mu | \ge \sqrt{2} \cdot \frac{1 }{4\epsilon \sqrt{n} } ] \le \frac{ \mathbb{Var}[ \hat \mu ] }{ ( \sqrt{2} \cdot \frac{1 }{4 \epsilon \sqrt{n} } )^2 } = \frac{1}{2}<br>$$</p>
<p>Or we can apply Hoeffding inequality to show that, with probability at least $1 - \delta$,<br>$$<br>|\bar{X} - \mathbb{E} [ \bar{X} ] | \le \sqrt{ \frac{\log \frac{2}{\delta} }{2n} }<br>$$</p>
<p>i.e.,<br>$$<br>|\hat \mu - \mu | = \frac{1}{2 \epsilon} |\bar{X} - \mathbb{E} [ \bar{X} ] | \le \frac{1}{ 2 \epsilon } \sqrt{ \frac{\log \frac{2}{\delta} }{2n} }<br>$$</p>
<p>which is much tighter than Chebyshev inequality. </p>
<p>Finally, observe that to get a meaningful estimate $\hat \mu$, we require that $\frac{ 1 }{\epsilon \sqrt{n} } \le 1$, i.e., $n \in \Omega(\frac{1}{\epsilon^2})$ or $\epsilon \in \Omega( \frac{1}{ \sqrt{n} } )$.</p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/27/Twin-Drive-or-Not/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/27/Twin-Drive-or-Not/" class="post-title-link" itemprop="url">Twin Drive or Not?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-27 16:15:03" itemprop="dateCreated datePublished" datetime="2020-10-27T16:15:03+11:00">2020-10-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-06 11:08:17" itemprop="dateModified" datetime="2020-11-06T11:08:17+11:00">2020-11-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this article, we introduce differential privacy. We start with a story of information leakage. </p>
<h3 id="Twin-Drive-or-Not"><a href="#Twin-Drive-or-Not" class="headerlink" title="Twin Drive or Not"></a>Twin Drive or Not</h3><p>Celestial Being is a private military organization with superior technology. It has four advanced machines. The most powerful one is the Gundam 00.</p>
<p>The Gundam 00 is about to engage the enemy. </p>
<div style="text-align:center">
<img src="https://lh5.googleusercontent.com/-9AOc-O5bdRk/S_mZxjf1DWI/AAAAAAAACvs/uXI0SvAMqWc/s640/5.jpg" width="500" height="250" />
</div>

<p>Due to maintenance, Gundam 00 is not always equipped with two engines (known as  <strong><em>twin drive system</em></strong>). With probability <strong><em>0.5</em></strong>, it uses one drive. We use $T =  (1, 1)$ to denote the status of equipping with the twin drive system, and $S = (1, 0)$ (or $(0, 1)$) for status of single drive. Let $X$ be a random variable that indicates drive status. Therefore,<br>$$<br>\Pr[ X = T ] = 0.5, \<br>\Pr[ X = S ] = 0.5.<br>$$</p>
<div style="text-align:center">
<img src="https://knolly.files.wordpress.com/2009/03/00gundamdrive1.jpg" width="500" height="250" />
</div>

<p>Raiser sword is one of Gundam 00 most powerful weapon. The energy level of the Raiser sword, denote as $Y$, is a random variable in $[0, 100]$, whose distribution, denoted as $\Pr[\cdot \mid X]$, depends on the drive status. Thus, $\Pr[Y = y \mid X = T]$ (or $\Pr[Y = y \mid X = S]$) is the probability that $Y$ equals to $y$ conditioned on $X = T$ ($X = S$). Specifically,<br>$$<br>\Pr[\cdot \mid T] \sim B(100, 0.81) \<br>\Pr[\cdot \mid S] \sim B(100, 0.09)<br>$$</p>
<p>where $B(n, p)$ denotes a binomial distribution. The expected energy level  is $81$ with <em>twin drive system</em>, compared to only $9$ with single drive. This is called “<strong><em>squaring</em></strong>“ phenomenon of <em>twin drive</em>. </p>
<div style="text-align:center">
<img src="https://vignette.wikia.nocookie.net/gundam/images/8/84/Raiser_sword.png/revision/latest?cb=20101124151429" width="500" height="250" />
</div>

<p>Now suppose that you’re the enemy pilot of a mobile suit with just average performance. Before you start dog fighting with Gundam 00, you will be attacked by the 00’s long-range raiser sword (you have not seen 00 yet). Luckily, you survive the raiser sword attack. Now you need to make a decision. If 00 is equipped with twin drive, there is zero chance that you can win the dog fight. The best choice is to leave the battle. Otherwise, you can outperform 00 and you would like to engage it. </p>
<div style="text-align:center">
<img src="https://blogimg.goo.ne.jp/user_image/79/da/c198b8cf829b5f7f7dd507d40bb39ba5.jpg" width="500" height="250" />
</div>


<p>Since you have just been attacked by the raiser sword, you have an observation of its energy level $y$. By Bayes’ theorem,<br>$$<br>\begin{aligned}<br>\Pr[ X = T \mid Y = y] = \frac{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] }{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] + \Pr[Y = y \mid X = S] \cdot \Pr[X = S] }<br>\<br>\<br>\Pr[ X = S \mid Y = y] = \frac{ \Pr[Y = y \mid X = S] \cdot \Pr[X = S] }{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] + \Pr[Y = y \mid X = S] \cdot \Pr[X = S] }<br>\end{aligned}<br>$$</p>
<p>Substituting with $\Pr[X = S] = \Pr[X = T] = 0.5$, we get<br>$$<br>\begin{aligned}<br>\Pr[ X = T \mid Y = y] = \frac{ \Pr[Y = y \mid X = T]  }{ \Pr[Y = y \mid X = T]  + \Pr[Y = y \mid X = S] }<br>\<br>\<br>\Pr[ X = S \mid Y = y] = \frac{ \Pr[Y = y \mid X = S]  }{ \Pr[Y = y \mid X = T]  + \Pr[Y = y \mid X = S] }<br>\end{aligned}<br>$$</p>
<p>The distributions $B(100, 0.09)$ and $B(100, 0.81)$ are plotted together below. </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/B-100-0.09-vs-B-100-0.81.png?raw=true"></p>
<p>Immediately, you can draw some conclusions based on single value of $y$, such as</p>
<ol>
<li>If $y = 10$, it is likely 00 has only one drive.   </li>
<li>If $y = 75$, it is likely 00 has twin drive.</li>
</ol>
<p>Or you can conclude based on the range of $y$, such as </p>
<ol>
<li>If $y \in [0, 20]$, it is likely 00 has single drive. </li>
<li>If $y \in [70, 90]$, it is likely 00 has twin drive. </li>
</ol>
<p>Before the observation of $y$, as $\Pr[X = S] = \Pr[X = T] = 0.5$, you have only random guess over the engine status of 00. After the observation, you might be much more confident about your guess, even though the energy level of the raiser sword is a random variable. </p>
<p>Why does this happen? Because the two distributions are well-separated. They are far from each other. Further, the prior probabilities $\Pr[X = S]$ and $\Pr[X = T]$ play important roles. If you know $\Pr[X = S] = 10^{-10}$, even if you observe an energy level of $y = 10$, you had better not engage Gundam 00. You know it could be just a trap!  </p>
<h3 id="Formal-Definition"><a href="#Formal-Definition" class="headerlink" title="Formal Definition"></a>Formal Definition</h3><p>Database managers face similar scenarios in privacy protection. We know define the setting for differential privacy. We view a dataset $D$ as a table of $n$ rows, each of which comes from a domain $\mathcal{X}$. Hence $D \in \mathcal{X}^n$. Instead of releasing the dataset directly, the manager runs a randomized algorithm $A: \mathcal{X}^n \rightarrow \mathcal{Y}$ on $D$, and outputs $Y = A(D) \in \mathcal{Y}$. Here $\mathcal{Y}$ is called the co-domain of $A$ and does not necessarily equal to $\mathcal{X}^n$. Note that the output distribution of $A$ may depend on the input $D$. Further, for simplicity of discussion, we believe a uniform prior distribution over datasets in $\mathcal{X}^n$. </p>
<p>In the Gundam 00’s story, $\mathcal{X} = {0, 1}^2$, and $\mathcal{Y} = [0, 100]$.</p>
<p>Suppose there is another dataset $D’$ that differs only one row from $D$. The algorithm $A$ is said to be differentially private if a malicious user is unlikely to distinguish the input to $A$ between $D$ and $D’$, by merely observing $A$’s output. Simply put, the conditional distributions of $\Pr[\cdot \mid X = D]$ and $\Pr[\cdot \mid X = D’]$ should be similar, where $X$ is a variable that denotes the input dataset. </p>
<p>We observe in the previous section that, if the distributions are well separated, then we can infer the underlying input with high confidence when the output value takes specific values or lies in certain ranges. </p>
<p>There are many ways to characterize the closeness of two distributions. We introduce the one proposed in [1]. </p>
<blockquote>
<p>Algorithm $A$ is called $(\epsilon, \delta)$ differentially private, if for any pair of neighboring datasets $D$ and $D’$ (the ones that differ in only one row), and for any (measurable) subset $\mathcal{R} \subset \mathcal{Y}$, it holds that<br>$$<br>    \Pr[Y \in \mathcal{R} \mid X = D ] \le \exp(\epsilon) \cdot \Pr[Y \in \mathcal{R} \mid X = D’ ]+ \delta<br>$$</p>
</blockquote>
<p>In other words, we can’t find a subset $\mathcal{R}$, with which we can distinguish $D$ and $D’$ with high confidence. In the previous example, such $\mathcal{R}$ exists. E.g., $R = [70, 90]$. If $Y$ lies in $\mathcal{R}$, we can infer that Gundam 00 is likely to have twin drive.</p>
<h3 id="Hypothesis-Testing"><a href="#Hypothesis-Testing" class="headerlink" title="Hypothesis Testing"></a>Hypothesis Testing</h3><p>There is alternative view of $(\epsilon, 0)$ differential privacy as hypothesis testing. Suppose that we know the underlying dataset is either $D$ or $D’$ with equal probability. After observing the output of $A$, we need to decide which hypothesis of the following holds:<br>$$<br>H_0: \text{ the dataset if } D \<br>H_1: \text{ the dataset if } D’<br>$$</p>
<p>Assume that we adopt a fixed strategy: </p>
<ol>
<li>First we choose a fixed subset $\mathcal{R} \subset \mathcal{Y}$,</li>
<li>If $Y \in \mathcal{R}$, we choose to accept $H_0$, </li>
<li>Otherwise, we accept $H_1$. </li>
</ol>
<p>There are two kinds of errors we can make. The type I error is the one when the true hypothesis is $H_0$ and we accept $H_1$. Conversely, type II error is the one when the true hypothesis is $H_1$ and we accept $H_0$. Let $p$ and $q$ be the probabilities we make type I and II errors respectively. The following theorem holds </p>
<blockquote>
<p>Algorithm A is $(\epsilon, 0)$ differentially private then</p>
<ol>
<li>$\frac{1}{1 + \exp(\epsilon) } \le q \le \frac{1}{1 + \exp(-\epsilon) }$  </li>
<li>$\frac{1}{1 + \exp(\epsilon) } \le p \le \frac{1}{1 + \exp(-\epsilon) }$  </li>
</ol>
</blockquote>
<p><em>Proof.</em> By definition,<br>$$<br>\begin{aligned}<br>    q   &amp;= \Pr[ X = D \mid  Y \in \bar{\mathcal{R}} ] \<br>        &amp;= \frac{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \Pr[X = D ] }{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \Pr[X = D ] + \Pr[  Y \in \bar{\mathcal{R}} \mid X = D’ ] \Pr[X = D’ ] } \<br>        &amp;= \frac{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]  }{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]  + \Pr[  Y \in \bar{\mathcal{R}} \mid X = D’ ]  }<br>\end{aligned}<br>$$</p>
<p>As $A$ is $(\epsilon, 0)$ differentially private, it holds that<br>$$<br>\Pr[  Y \in \bar{\mathcal{R}} \mid X = D’ ] \le \exp(\epsilon) \cdot \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]<br>$$<br>and<br>$$<br>\Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \le \exp(\epsilon) \cdot \Pr[  Y \in \bar{\mathcal{R}} \mid X = D’ ]<br>$$</p>
<p>therefore,<br>$$<br>\frac{1}{1 + \exp(\epsilon) } \le q \le \frac{1}{1 + \exp(-\epsilon) }<br>$$</p>
<p>By symmetry, we also have<br>$$<br>\frac{1}{1 + \exp(\epsilon) } \le p \le \frac{1}{1 + \exp(-\epsilon) }<br>$$</p>
<p>$\blacksquare$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013<br>[2] L. Wasserman and S. Zhou, “A statistical framework for differential privacy,” arXiv:0811.2501 [math, stat], Oct. 2009, Accessed: Oct. 27, 2020. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/26/Norm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/26/Norm/" class="post-title-link" itemprop="url">Norm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-26 22:17:18" itemprop="dateCreated datePublished" datetime="2020-10-26T22:17:18+11:00">2020-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-01 00:49:31" itemprop="dateModified" datetime="2020-11-01T00:49:31+11:00">2020-11-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>A norm $| \cdot |:\mathbb{R}^n \rightarrow \mathbb{R}$ is a function that satisfies the following properties: </p>
<ol>
<li>Positive definite: $|x| = 0 \rightarrow x = \vec 0$,</li>
<li>Nonnegative:$|x| \ge 0$ for any $x \in \mathbb{R}^n$, </li>
<li>Absolutely homogeneous: $|k x| = |k| |x|$, for any $x \in \mathbb{R}^n, k \in \mathbb{R}$,</li>
<li>Subadditive (triangle inequality): $|x + y| \le |x| + |y|$, for any $x \in \mathbb{R}^n, y \in \mathbb{R}^n$. </li>
</ol>
<p>Given properties 1-3, we claim that property 4 is equivalent to </p>
<ol start="5">
<li>The region ${ x \in \mathbb{R}^n: |x| \le 1}$ is convex. </li>
</ol>
<p><em>Proof:</em> The proof is straightforward.   </p>
<p>$5 \rightarrow 4:$ If $x = \vec 0$ or $\vec y = 0$, then 4 holds trivially. Otherwise, suppose that $x, y \neq \vec 0$. Then<br>$$<br>\begin{aligned}<br>    &amp;|x + y| \le |x| + |y| \<br>    \leftrightarrow<br>    &amp;| \frac{ |x| }{ |x| + |y| } \frac{ x }{ |x| } + \frac{ |x| }{ |x| + |y| } \frac{ y }{ |y| } | \le 1<br>\end{aligned}<br>$$</p>
<p>As $|\frac{ x }{ |x| } | =1$, $|\frac{ y }{ |y| }| = 1$ and $\frac{ |x| }{ |x| + |y| } + \frac{ |y| }{ |x| + |y| } = 1$, the second inequality follows exactly from condition 5.    </p>
<p>$4 \rightarrow 5:$ Let $x, y \in \mathbb{R}^n, |x| \le 1, |y| \le 1$ and $a, b \ge 0$, $a + b = 1$. Then<br>$$<br>|ax + by| \le |ax| + |by| = a|x| + b|y| = 1<br>$$</p>
<p>$\blacksquare$</p>
<h3 id="Induced-Norm"><a href="#Induced-Norm" class="headerlink" title="Induced Norm"></a><strong>Induced Norm</strong></h3><p>To illustrate a deeper connection between property 4 and 5, we first show how a norm can be induced by a convex and symmetric region centered at the origin $O$.  Let the boundary of the region as $E$. We are going to induce a norm by $E$, denoted as $|\cdot |_E$.</p>
<p>First, we define </p>
<ol>
<li>$| x |_E = 1, \forall x \in E$.  </li>
</ol>
<p>For any other vector $x \notin E$, consider the ray initiated from the origin and with the same direction as $x$. Denote its intersection point with $E$ as $x_E$. Suppose that $x = a \cdot x_E$ for some $a \in \mathbb{R}_+$. Then we define </p>
<ol start="2">
<li>$| x |_E = a |x_E|_E = a$.</li>
</ol>
<p>Indeed, if we write the  $|\cdot|_2$ as the $\ell_2$ norm, then the value of $a$ is given by $\frac{ |x|_2 } { |x_E|_2 }$. Clearly, by definition, $|\cdot|_2$ is positive definite and non-negative. To prove that it is absolutely homogeneous, $\forall k \in \mathbb{R}$,<br>$$<br>|k x|_E = |k ax_E|_E<br>$$</p>
<p>If $k \ge 0$, we have $|k ax_E|_E = ka |x_E|_E = ka$. Otherwise, if $k &lt; 0$,<br>$$<br>|k x|_E = |k ax_E|_E = |-k a (-x_E)|_E = -k a |(-x_E)|_E<br>$$</p>
<p>By symmetry of $E$ (with respect to the origin $O$), $|(-x_E)|_E = |x_E|_E = 1$. Therefore,<br>$$<br>|k x|_E = |k|a<br>$$</p>
<p>Finally, we have a graphical verification of triangle inequality. Let $v, u \in \mathbb{R}^n$, which intersect with $E$ at $v_E$ and $u_E$ respectively. If $|v|_E &lt; 1$ ($|u|_E &lt; 1$), then we can extend it along its direction to get its intersection with $E$. Let $c = |v|_E$ and $d = |u|_E$, then </p>
<ol>
<li>$v = c \cdot v_E$, </li>
<li>$u = d \cdot u_E$. </li>
</ol>
<p>Let $w = v + u = c \cdot v_E + d \cdot u_E$. Let $p$ be the intersection between $w$ and the line segment between $v$ and $u$. It is easy to verify that<br>$$<br>p = \frac{c}{c + d} v_E + \frac{d}{ c + d} u_E<br>$$</p>
<p>By convexity of the region bounded by $E$,<br>$$<br>|p|_E \le 1<br>$$</p>
<p>It concludes that $|w|_E = |c v_E + d u_E |_E \le c + d = |v|_E + |u|_E$. </p>
<!-- <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Induced-Norm.png?raw=true" style="zoom: 67%;" /> -->

<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Induced-Norm.png?raw=true"></p>
<h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a><strong>Application</strong></h3><p>The idea discussed above significantly simplifies the proof of Minkowski inequality. </p>
<p>The $p$-norm ($p \ge 1$) on $\mathbb{R}^n$ is given as<br>$$<br>| x |<em>p = \big( \sum</em>{i = 1}^n |x_i|^p \big)^\frac{1}{p}<br>$$</p>
<p>Clearly, $p$-norm is positive definite, nonnegative, absolutely homogeneous. It is left to verify triangle inequality, which is known as Minkowski inequality for the case of $p$-norm. This is equivalent to show that<br>$$<br>{ x \in \mathbb{R}^n : |x |_p \le 1 }<br>$$</p>
<p>is convex. The trick here is that $|x|<em>p \le 1$ is equivalent to $(|x|_p)^p \le 1$. Hence,<br>$$<br>{ x \in \mathbb{R}^n : |x |_p \le 1 } = { x \in \mathbb{R}^n : \sum</em>{i = 1}^n |x_i|^p  \le 1 }<br>$$</p>
<p>The convexity of this set follows directly from the point-wise convexity of the function $|\cdot |^p$. </p>
<p>$\square$</p>
<p><strong><em>Remark:</em></strong> To appreciate how concise the above proof is, we also give one traditional proof here, which consists of three steps. </p>
<h4 id="Young’s-Inequality"><a href="#Young’s-Inequality" class="headerlink" title="Young’s Inequality"></a>Young’s Inequality</h4><p>For $a, b &gt; 0$, $p, q &gt; 0$, s.t., $\frac{1}{p} + \frac{1}{q} = 1$, it holds that<br>$$<br>ab \le \frac{a^p}{p} + \frac{b^q}{q}<br>$$</p>
<p><em>Proof:</em> The inequality is nothing more than convexity of the function $x \rightarrow e^x$:<br>$$<br>ab = \exp(\frac{ p \ln a }{p} + \frac{ q \ln b }{q}) \le \frac{ \exp( \ln a^p ) }{p} + \frac{ \exp( \ln b^q ) }{q} = \frac{a^p}{p} + \frac{b^q}{q}<br>$$ </p>
<p>$\square$</p>
<h4 id="Holder’s-Inequality"><a href="#Holder’s-Inequality" class="headerlink" title="Holder’s Inequality"></a>Holder’s Inequality</h4><p>For $x, y \in \mathbb{R}^n$, $p, q &gt; 0$, s.t., $\frac{1}{p} + \frac{1}{q} = 1$, it holds that<br>$$<br>\sum_{i = 1}^n |x_i| |y_i| \le |x|_p |y|_q<br>$$</p>
<p><em>Proof:</em> The inequality is trivial if $x = \vec 0$ or $y = \vec 0$. Otherwise, let $u = \frac{x}{ |x|_p }$ and $v = \frac{ y }{ |y|<em>q }$. It remains to prove that<br>$$<br>\sum</em>{i = 1}^n |u_i| |v_i| \le 1<br>$$</p>
<p>for $|u|_p = 1$ and $|v|<em>q = 1$. WLOG, we assume $u &gt; 0$ and $v &gt; 0$. Applying Young’s Inequality, we have<br>$$<br>u_i v_i \le \frac{ u_i^p }{p}  + \frac{v_i^q}{q}.<br>$$<br>Hence,<br>$$<br>u \cdot v = \sum</em>{i = 1}^n \big( \frac{ u_i^p }{p}  + \frac{v_i^q}{q} \big) = \frac{1}{p} + \frac{1}{q} = 1 .<br>$$</p>
<p>$\square$</p>
<h4 id="Minkowski-Inequality"><a href="#Minkowski-Inequality" class="headerlink" title="Minkowski Inequality"></a>Minkowski Inequality</h4><p>For $x, y \in \mathbb{R}^n$, $p \ge 1$, it holds that<br>$$<br>|x + y|_p \le |x|_p + |y|_p<br>$$</p>
<p><em>Proof:</em> The inequality is trivial if $x = \vec 0$ or $y = \vec 0$. Otherwise, assume that $x &gt; 0$ and $y &gt; 0$. Now </p>
<p>$$<br>|x + y|<em>p^p = \sum</em>{i = 1}^n |x_i + y_i|^p = \sum_{i = 1}^n [ x_i (x_i + y_i)^{p - 1} + y_i (x_i + y_i)^{p - 1} ]<br>$$</p>
<p>Let $q = \frac{p}{p - 1}$. Then $\frac{1}{p} + \frac{1}{q} = 1$. By Holder’s inequality,<br>$$<br>\begin{aligned}<br>    \sum_{i = 1}^n x_i (x_i + y_i)^{p - 1}<br>    &amp;\le |x|<em>p ( \sum</em>{i = 1}^n (x_i + y_i)^{(p - 1)q} )^{ \frac{1}{q} } \<br>    &amp;= |x|<em>p ( \sum</em>{i = 1}^n (x_i + y_i)^{ p } )^{ \frac{p - 1}{ p } } \<br>    &amp;= |x|_p (|x + y|_p)^{p - 1}<br>\end{aligned}<br>$$</p>
<p>Similarly,  we have<br>$$<br>\sum_{i = 1}^n y_i (x_i + y_i)^{p - 1} \le |y|_p (|x + y|_p)^{p - 1}<br>$$</p>
<p>Putting the inequalities together,<br>$$<br>|x + y|_p^p \le |x|_p (|x + y|_p)^{p - 1} + |y|_p (|x + y|_p)^{p - 1}<br>$$</p>
<p>We finish the proof by dividing both side with $(|x + y|_p)^{p - 1}$. </p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/25/Bit-Guessing-Game/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/25/Bit-Guessing-Game/" class="post-title-link" itemprop="url">Bit Guessing Game</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-25 15:33:16" itemprop="dateCreated datePublished" datetime="2020-10-25T15:33:16+11:00">2020-10-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-03 10:23:55" itemprop="dateModified" datetime="2020-11-03T10:23:55+11:00">2020-11-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The bit guessing game models two players, namely Alice and Bob, where Alice has a secret vector $\vec x \in {0, 1 }^n$ which Bob tries to learn. Bob is only allowed to ask Alice the value of<br>$$<br>\vec c \cdot \vec x<br>$$</p>
<p>for bit vector $\vec c \in {0, 1}^n$ he comes up with. Under this setting, it is trivial for Bob to learn each bit of $x$: he needs to just query Alice for $\vec e_i \cdot \vec x$, where $\vec e_i$ is the indicator vector with its $i$-th dimension equal to 1 and other dimensions equal to 0. </p>
<p>One way for Alice to prevent information leakage is to add some bounded noise $Z$ to the query result and returns<br>$$<br>r(\vec c) \doteq \vec c \cdot \vec x + Z<br>$$</p>
<p>For example, $Z$ could be a bounded random variable. Is is still a chance for Bob to learn the vector $\vec x$?</p>
<p><strong><em>Theorem 1.</em></strong> If Bob is allowed to query $r(\vec c)$ for all $\vec c \in {0, 1}^n$, and suppose that $Z$ is bounded by some real number $R$, then Bob can have an estimation $\vec y$ such that<br>$$<br>|\vec x - \vec y| = \sum_{i = 1}^n | x_i - y_i | \le 4R<br>$$</p>
<p><strong>Remark:</strong> <em>In this case Bob queries for $2^n$ vectors. This result implies that is Bob is able to ask exponential number of questions, then it is hard to prevent information leakage. E.g., suppose $R = n / 400$, which is a relatively large number when $n$ is large, the theorem says that Bob can recover $99%$ of bits in $\vec x$.</em>  </p>
<p><em>Proof:</em> We say a vector $\vec y$ is consistent with a query result $r(\vec c)$ if<br>$$<br>| r(\vec c) - \vec c \cdot \vec y| \le R.<br>$$</p>
<p>The strategy of Bob is to choose any $\vec y$ that is consistent with all $\vec c \in {0, 1}^n$. Such $\vec y$ exits, a special of which is given by $\vec y = \vec x$. It remains to prove that such an $\vec y$ satisfies the constraint. </p>
<p>Consider the special case of $\vec c = \vec x$,<br>$$<br>|r(\vec x) - \vec x \cdot \vec y| \le R \<br>|r(\vec x) - \vec x \cdot \vec x| \le R \<br>$$</p>
<p>By triangle inequality, it holds<br>$$<br>|\vec x \cdot \vec y - \vec x \cdot \vec x| \le 2R.<br>$$  </p>
<p>Observe that $\vec x \cdot \vec x \ge \vec x \cdot \vec y$, which implies<br>$$<br>|\vec x \cdot \vec y - \vec x \cdot \vec x| = \vec x \cdot \vec x - \vec x \cdot \vec y \le 2R<br>$$</p>
<p>Hence, $\vec y$ and $\vec x$ differ by at most $2R$ bits, on the non-zero dimensions of $\vec x$. </p>
<p>Let $\bar x$ be the bit-wise complement of $\vec x$. By similar arguments, we get<br>$$<br>|\bar x \cdot \vec y - \bar x \cdot \vec x|\le 2R.<br>$$  </p>
<p>and $\vec x$ and $\vec y$ differ by at most $2R$ bits, on the zero dimensions of $\vec x$. It concludes that $|\vec x - \vec y | \le 4R$.  </p>
<!-- Define $S$ to be the set of non-zero dimensions of $\vec x$: 
$$
S \doteq \{ i \in [n]: x_i > 0 \}
$$

Let $\vec e_S \in \{0, 1\}^n$ ($\vec e_{\bar S} \in \{0, 1\}^n$) be the indicator vector that takes $1$ on dimensions belonging to $S$ ($\bar S$).  It follows that 
$$
|r(\vec e_S) - \vec e_S \cdot \vec y| \le R \\
|r(\vec e_S) - \vec e_S \cdot \vec x| \le R \\
$$ -->

<!-- By triangle inequality, it holds 
$$
|\vec e_S \cdot \vec y - \vec e_S \cdot \vec x|\le 2R.
$$   -->

<!-- Similarly, $|\vec e_{\bar S} \cdot \vec y - \vec e_{\bar S} \cdot \vec x| \le 2R$. As $S$ and $\bar S$ is a partition of $[n]$, 
$$
| \vec x - \vec y| = |\vec e_S \cdot \vec y - \vec e_S \cdot \vec x| + |\vec e_{\bar S} \cdot \vec y - \vec e_{\bar S} \cdot \vec x| \le 4R
$$ -->

<p>$\blacksquare$</p>
<p>What if Bob asks $O(n)$ questions? The following theorem states that if $Z$ is bounded by $O(\sqrt n)$, then Bob is able to have a good estimation of $\vec x$.</p>
<p><strong><em>Theorem 2.</em></strong> If Bob is able to ask $\frac{n \log 2 + \ln n}{\log \frac{3}{2} - \log \lambda }$ queries, and $Z$ is bounded by $\alpha \sqrt n$, then Bob can obtains an estimation $\vec y$, such that<br>$$<br>|\vec x - \vec y| \le (3e \frac{\alpha}{\lambda } )^2 \cdot n<br>$$</p>
<p><em>Proof:</em> As before, a vector $\vec y$ is said to be consistent with a query result $r(\vec c)$ if<br>$$<br>| r(\vec c) - \vec c \cdot \vec y| \le \alpha \sqrt n.<br>$$</p>
<p>It suffices to find a sequence of vectors $\vec c_1, \vec c_2, …, \vec c_k$ ($k$ is a value to be determined), such that </p>
<ol>
<li>$k = O(n)$,</li>
<li>For any $\vec y$ with $|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n$, there exists at least one $\vec c_i$, such that $\vec y$ is not consistent with $r(\vec c_i)$, with high probability.</li>
</ol>
<p>If Bob can find a sequence of query vectors with the specified properties, he can rule out any $\vec y$ such that $|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n$. Note that not every vector in ${0, 1}^n$ will be ruled out. At least $\vec x$ will remain. Bob can output any $\vec y$ that is not ruled out as his guess of $\vec x$. </p>
<p>It turns out that fining $\vec c_i$’s is simple: each $\vec c_i$ is sampled independently and uniformly at random from ${0, 1}^n$. It remains to prove that any $\vec y$ with $|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n$ will be ruled out with high probability. </p>
<p>For a fix $i$, consider the probability of<br>$$<br>\Pr[ | r(\vec c_i) - \vec c_i \cdot \vec y| &gt; \alpha \sqrt n ] = \Pr[ |\vec c_i \cdot (\vec x - \vec y) | &gt; \alpha \sqrt n ].<br>$$</p>
<p>Lower bounding this probability is equivalent to upper bounding the probability<br>$$<br>\Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ].<br>$$</p>
<p><em>Lemma.</em> <strong>For a value $l \in \mathbb{N}$</strong>,<br>$$<br>\Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] \le \frac{e }{\sqrt {|\vec x - \vec y| } }<br>$$</p>
<p>We leave the proof of the lemma at the end of our discussion. It following by union bound that<br>$$<br>\begin{aligned}<br>    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ]<br>        &amp;= \sum_{l = -\alpha \sqrt n}^{\alpha \sqrt n} \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] \<br>        &amp;\le \frac{2 e \cdot \alpha \sqrt n}{\sqrt {|\vec x - \vec y| } }<br>\end{aligned}<br>$$</p>
<p>Conditioning on that $|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n$, this probability is at most<br>$$<br>\begin{aligned}<br>    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ]<br>        &amp;\le \frac{2}{3} \lambda<br>\end{aligned}<br>$$</p>
<p>The probability that this holds simultaneously for all $\vec c_i$ is therefore $(\frac{2\lambda }{3})^k$. Applying union bound over all $\vec y$ such that $|\vec x - \vec y| &gt; (3e\alpha)^2 \cdot n$, we have an upper bound of failure probability<br>$$<br>(\frac{2 \lambda }{3})^k 2^n = \exp( k \log \frac{2 \lambda }{3} + n \log 2)<br>$$</p>
<p>If we take $k = \frac{n \log 2 + \ln n}{\log \frac{3}{2} - \log \lambda }$, this probability becomes $\frac{1}{n}$. </p>
<p><em>Proof of the lemma:</em> Without lose of generality, suppose that $l \ge 0$. Observe that $\vec x - \vec y$ is a vector in ${-1, 0, 1 }^n$. Denote $v = |\vec x - \vec y|$ be the number of non-zero dimensions in $\vec x - \vec y$. Further, let $v_+$ ($v_-$) be the number of positive (negative) dimensions in $\vec x - \vec y$. Hence, $v = v_+ + v_-$. As $\vec c_i$ is sampled uniformly from ${0, 1}^n$, we have<br>$$<br>\begin{aligned}<br>    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ]<br>    &amp;= \frac{1}{2^{v} } \sum_{j = 0}^{ \min { v_+ - l, v_- } } \binom{v_+}{l + j} \binom{v_-}{j} \<br>    &amp;= \frac{1}{2^{v} } \sum_{j = 0}^{ \min { v_+ - l, v_- } } \binom{v_+}{l + j} \binom{v_-}{v_- - j} \<br>    &amp;\le \frac{1}{2^{v} } \binom{v_+ + v_- }{l + v_-} \<br>    &amp;\le \frac{1}{2^{v} } \binom{v }{ \lfloor v / 2 \rfloor } \<br>\end{aligned}<br>$$</p>
<p>By Stirling’s approximation,<br>$$<br>v! \le e \sqrt v (\frac{v}{e} )^v  \<br>\lfloor v / 2 \rfloor! \ge \sqrt{2 \pi v} (\frac{ \lfloor v / 2 \rfloor }{e} )^{\lfloor v / 2 \rfloor }  \<br>\lceil v / 2 \rceil ! \ge \sqrt{2 \pi v} ( \frac{ \lceil v / 2 \rceil }{ e } )^{\lceil v / 2 \rceil }   \<br>$$</p>
<p>Assuming that $\lfloor v / 2\rfloor \ge 1$, we get<br>$$<br>\begin{aligned}<br>    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ]<br>    &amp;\le \frac{1}{2^{v} } \binom{v }{ \lfloor v / 2 \rfloor } \<br>    &amp;\le \frac{1}{2^{v} } \frac{e}{2 \pi} \frac{v^{ v + 0.5 } }{ (v / 2)^{ v + 1 } } (\frac{ v / 2 }{ \lfloor v / 2 \rfloor })^{ \lfloor v / 2 \rfloor + 0.5 } (\frac{ v / 2 }{ \lceil v / 2 \rceil })^{ \lceil v / 2 \rceil + 0.5 } \<br>    &amp;\le \frac{ 1 }{ \sqrt v } ( \frac{ v / 2 }{ \lfloor v / 2 \rfloor })^{ \lfloor v / 2 \rfloor + 0.5 } \<br>    &amp;\le \frac{ 1 }{ \sqrt v } \exp( \frac{ 1 }{ 2 \lfloor v / 2 \rfloor } ( \lfloor v / 2 \rfloor + 0.5 )  ) \<br>    &amp;\le \frac{ e }{ \sqrt v }  \<br>\end{aligned}<br>$$</p>
<p>$\square$</p>
<p>The idea behind Theorem 2 is more obvious if we study the scenario where $\vec  x \in {-1, 1}^n$. For any estimation $\vec y \in {-1, 1}^n$, if $\vec c$ is sampled uniformly at random from ${-1, 1}^n$, then<br>$$<br>\mathbb{E}[ \vec c (\vec x - \vec y)] = 0<br>$$</p>
<p>If $|\vec x - \vec y| = O(\alpha^2 n)$, by anti-concentration property, the probability<br>$$<br>\Pr[ |\vec c (\vec x - \vec y) | \ge \alpha \sqrt n]<br>$$</p>
<p>equals to some constant. Equivalently, $\Pr[ |\vec c (\vec x - \vec y) | &lt; \alpha \sqrt n]$ is some constant. By sampling $\vec c$ repeatedly, the probability that $|\vec c (\vec x - \vec y) | &lt; \alpha \sqrt n$ holds for all sampled $\vec c$ decreases exponentially. That is why we can rule out any $\vec y$ such that $|\vec x - \vec y| = O(\alpha^2 n)$ with high probability.  </p>
<!-- First, observe that 
$$
\mathbb{E}[ \vec c_i \cdot ( \vec x - \vec y) ] = \mathbb{E}[ \vec c_i \cdot ( \vec x - \vec y) ] \ge 0.
$$ -->

<!-- We re-write the event of $\vec c_i$ not consistent with $\vec y$ as 
$$
\begin{array}{ll}
\{ | r(\vec c_i) - \vec c_i \cdot \vec y| > \alpha \sqrt n \} \\
= \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] + \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n \} \\
= \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | + |\mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n \} \\
\supset \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n  \} 
\end{array}
$$

and 
$$
\Pr[ \vec c \cdot (\vec x - \vec y) - \mathbb{E}[ \vec c \cdot ( \vec x - \vec y) ] \ge \sqrt n] ] \le \binom{\alpha n}{ \alpha n / 2} \frac{1}{2^{\alpha n} } 
$$ -->

<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013    </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/24/Littlewood%E2%80%93Offord-Problem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/24/Littlewood%E2%80%93Offord-Problem/" class="post-title-link" itemprop="url">Littlewood–Offord Problem</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-24 23:25:49" itemprop="dateCreated datePublished" datetime="2020-10-24T23:25:49+11:00">2020-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-31 23:07:05" itemprop="dateModified" datetime="2020-10-31T23:07:05+11:00">2020-10-31</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $a = (a_1, a_2, …, a_n) \in \mathbb{R}^n$ with $|a_i| \ge 1$. Let $p(a)$ be the number of vectors $c = (c_1, c_2, …, c_n) \in \mathbb{R}^n$ where $c_i = \pm 1,  \forall i \in [n]$, such that<br>$$<br>-1 \le \left&lt; a , c \right&gt; = \sum_{i = 1}^n c_i a_i &lt; 1<br>$$</p>
<ol>
<li><p>Prove that $p(a) \le \binom{n}{\lfloor n / 2 \rfloor }$.   </p>
</li>
<li><p>Find an $a \in \mathbb{R}^n$ with $p(a) = \binom{n}{\lfloor n / 2 \rfloor }$. </p>
</li>
</ol>
<p><em>Proof.</em> By symmetry, we can assume that $a &gt; 0$ without lose of generality. Define<br>$$<br>C = { c = (c_1, c_2, …, c_n) \in \mathbb{R}^n \mid c_i = \pm 1, i \in [n] }.<br>$$</p>
<p>Let $[n]$ be the set of integers from $1$ to $n$,  and $\mathscr{P}([n])$ be the power set of $[n]$, i.e., the set of all subsets of $[n]$. For any subset $s \subset [n]$, we can associate to $s$ a vector in $C$, which is denoted as<br>$$<br>c(s) \doteq (c(s)_1, c(s)_2, …, c(s)_n),<br>$$</p>
<p>where<br>$$<br>\begin{cases}<br>    c(s)_i = 1, &amp; \forall i \in s \<br>    c(s)_i = -1,&amp; \forall i \notin s<br>\end{cases}<br>$$</p>
<p>It remains to show the set<br>$$<br>S(a) \doteq { s \subset [n] : -1 &lt; \left&lt; a, c(s) \right&gt; &lt; 1 }<br>$$</p>
<p>has size at most $\binom{n}{\lfloor n / 2 \rfloor }$. The key observation is that<br>$$<br>\forall s, s’ \in S(a) \Rightarrow s \nsubseteq s’.<br>$$</p>
<p>We call $S(a)$ an antichain. To show this, suppose thta  $s’$ contains at least one element that is not in $s$. Denote this element as $j$. It follows that  $c(s’)_j = 1$ and $c(s’)_j = -1$ and that<br>$$<br>\left&lt; a, c(s’) \right&gt; - \left&lt; a, c(s) \right&gt; \ge c(s’)_j a_j - c(s)_j a_j = 2 a_j \ge 2<br>$$</p>
<p>We conclude that $\left&lt; a, c(s’) \right&gt; \ge \left&lt; a, c(s) \right&gt; + 2 \ge 1$, a contradiction. </p>
<p>Now, consider an increasing sequence of $n$ subsets of $[n]$ of the form:<br>$$<br>\emptyset \subset { x_1 } \subset { x_1, x_2 } \subset { x_1, x_2, x_3 } \subset … \subset { x_1, x_2, …, x_n }<br>$$</p>
<p>where $x_1, …, x_n$ are different integers in $[n]$. Each such sequence induces a permutation of integers in $[n]$. As a result, the total number of such sequence equals $n!$. The relation between $S(a)$ and such sequence is established as follows: </p>
<ol>
<li><p>For each such sequence, there can be at most one set in $S(a)$, because $S(a)$ is a antichain. </p>
</li>
<li><p>For each $s = {x_1, x_2, …, x_k } \in S(a)$, where $k = |s| \ge 1$ is the size of $s$, the number of sequences it appears in is given by</p>
<p> $$<br> |s| ! (n - |s|)! \ge \lfloor n / 2 \rfloor ! (n - \lfloor n / 2 \rfloor)!<br> $$</p>
</li>
</ol>
<p>We obtain<br>$$<br>|S(a) | \cdot \lfloor n / 2 \rfloor ! (n - \lfloor n / 2 \rfloor)! \le\sum_{s \in S(a) } |s| ! (n - |s|) ! \le n!<br>$$</p>
<p>and<br>$$<br>p(a) = |S(a) | = \frac{n!}{ \lfloor n / 2 \rfloor ! (n - \lfloor n / 2 \rfloor)! } = \binom{n}{\lfloor n / 2 \rfloor }<br>$$</p>
<p>This finishes the proof for problem 1. </p>
<p>Finally, $a_i = 1, \forall i \in [n]$ gives a tight example to problem 2.   </p>
<p>$\blacksquare$  </p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] J. Matoušek and J. Nešetřil, Invitation to discrete mathematics, 2nd ed. Oxford ; New York: Oxford University Press, 2009.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description">你生之前悠悠千載已逝，未來還會有千年沉寂的期待</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
