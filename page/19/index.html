<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/19/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/19/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/13/Advanced-Composition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/13/Advanced-Composition/" class="post-title-link" itemprop="url">Advanced Composition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-13 22:26:07" itemprop="dateCreated datePublished" datetime="2020-11-13T22:26:07+11:00">2020-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-15 22:41:53" itemprop="dateModified" datetime="2020-11-15T22:41:53+11:00">2020-11-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Once we have designed some basic differentially private algorithms, it is a natural idea to combine them and analysis the privacy loss. We begin with an illustrative example that sets up the mathematical model step by step.</p>
<p>Image yourself in front of the door of a safe vault protected by a password lock. To open the door, you need the correct password <span class="math inline">\(P\)</span>. If tried with the wrong password, the lock would destroy itself automatically.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/door.png?raw=true" width="400" height="340" /></p>
</div>
<p>Luckily, you know two candidate passwords <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span>, with one of them being the correct one. Further, you notice that the designer of the lock left a collection of boxes near the door, which contain information on how they decide the correct passwords. Obtaining complete information of anyone of them gives you the correct password.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box1.png?raw=true" width="400" height="340" /></p>
</div>
<p>But the boxes are also protected and you don't have legally access to them. However, you can hack into the boxes. Hacking into the box won't give you all its information, but a random message. In particular, each box <span class="math inline">\(B\)</span> is associated with a set <span class="math inline">\(\mathcal{R}_B\)</span>, which is a finite collection of messages (in English). When <span class="math inline">\(B\)</span> is hacked, it returns a message <span class="math inline">\(Y_B\)</span> generated randomly from <span class="math inline">\(\mathcal{R}_B\)</span>, whose distribution depends on the correct password <span class="math inline">\(P\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> be the distribution of <span class="math inline">\(Y_B\)</span> if the correct password is <span class="math inline">\(P_1\)</span>, and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> be the one if the correct password is <span class="math inline">\(P_2\)</span>. If there is a huge difference between <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>, then you might be able to guess the correct password.</p>
<p>E.g., suppose <span class="math inline">\(\mathcal{R}_B =\)</span> { "Dog bites.", "Cat scratches." } and the distributions are given as</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_1}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">"Dog bites."</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="even">
<td style="text-align: center;">"Cat scratches."</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.99</td>
</tr>
</tbody>
</table>
<p>Hence, when you get <span class="math inline">\(Y_B=\)</span> "Dog bites.", you prefer <span class="math inline">\(P_1\)</span> over <span class="math inline">\(P_2\)</span> and vice versa.</p>
<p>Anticipating such potential information leakage, the designer of the lock equips the boxes with a defense mechanism, called <span class="math inline">\((\epsilon, 0)\)</span>-mechanism. It guarantees the distributions <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are similar so that it is hard for you to infer the correct password from the message. In particular, <span class="math inline">\(\forall S \subset \mathcal{R}_B\)</span>, if <span class="math inline">\(S\)</span> is measurable, it holds that <span class="math display">\[
\Pr[ Y_B \in S \ | \ P = P_1 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_2 ] \\
\Pr[ Y_B \in S \ | \ P = P_2 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_1 ] \\
\]</span></p>
<p>When these inequalities are satisfied, we say that <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close. The inequalities require <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> to have the same support on <span class="math inline">\(\mathcal{R}_B\)</span>. Therefore, throughout our discussion below, we assume that both <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> assign positive probability to each element in <span class="math inline">\(\mathcal{R}_B\)</span>. Otherwise, we can just replace <span class="math inline">\(\mathcal{R}_B\)</span> with the support of <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> (which is also the support of <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>).</p>
<p>Now, hacking one box is unlikely to help you to guess the correct password. You want to hack more boxes, with the hope that the information combined will assist you. Due to resource limit, you can't hack all boxes but only a finite number of them. You choose the first box randomly. Then you choose every new box based on the information obtained from the hacked boxes. The figure below shows an example of hacking five boxes.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box2.png?raw=true" width="400" height="340" /></p>
</div>
<p>Suppose that your resource enables you to hack <span class="math inline">\(n\)</span> boxes and let <span class="math inline">\(\vec Y_n = (Y_1, Y_2, ..., Y_n)\)</span> be the output messages you obtained. Similar to the situation of hacking one box, if the distribution of <span class="math inline">\(\vec Y_n\)</span>, conditioned on <span class="math inline">\(P = P_1\)</span>, is utterly distant from that conditioned on <span class="math inline">\(P = P_2\)</span>, then there could be some cases when you can confidently infer the true password. Conversely, to prevent severe information leakage, the designer needs to ensure there isn't such case, i.e., the two distributions should be similar.</p>
<p>What makes things even more complicated is that, the distribution of <span class="math inline">\(\vec Y_n\)</span> depends not only on the output distributions of the boxes, but also on your strategy of choosing the boxes to hack. Let's use symbol <span class="math inline">\(A\)</span> to denote your strategy. Whatever <span class="math inline">\(A\)</span> is, the designer need to guarantee that the distribution of <span class="math inline">\(\vec Y_n\)</span> conditioned <span class="math inline">\(P = P_1\)</span> should be similar to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Surprisingly, this is in some sense achievable, as long as for each box, its output distribution conditioned on <span class="math inline">\(P = P_1\)</span> is <span class="math inline">\((\epsilon, 0)\)</span> close to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{R}\)</span> be the set of possible possible messages of all boxes.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> <span class="math inline">\(\forall A\)</span>, <span class="math inline">\(\forall S \subset \mathcal{R}^n\)</span>, it holds that <span class="math inline">\(\forall \delta&#39; \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ \vec Y_n \in S \ | \ P = P_1, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; \\
\Pr[ \vec Y_n \in S \ | \ P = P_2, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_1, A] + \delta&#39;
\]</span> where <span class="math inline">\(\epsilon&#39; = k\epsilon(e^\epsilon - 1) + \epsilon \sqrt{2 n \log \frac{1}{\delta&#39;} }\)</span>.</p>
</blockquote>
<p>If we view <span class="math inline">\(\epsilon\)</span> as the privacy loss of a single box, then the theorem states that the privacy loss grows to <span class="math inline">\(O(\sqrt {n} \epsilon )\)</span> is <span class="math inline">\(n\)</span> boxes are hacked.</p>
<p>To prove the theorem, we need a rigorous model for the problem.</p>
<p><strong><em>Definitions.</em></strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathcal{I}\)</span>: the index set.</p></li>
<li><p><span class="math inline">\(\{ B_\alpha : \alpha \in \mathcal{I} \}\)</span>: the collection of boxes.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{R}_{\alpha} : \alpha \in \mathcal{I} \}\)</span>: the ranges of the outputs of the boxes.</p></li>
<li><p><span class="math inline">\(\mathcal{R} \doteq \cup_{\alpha \in \mathcal{I} } R_\alpha\)</span>: the range of any possible output by any box.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{D}_{\alpha, P_1} : \alpha \in \mathcal{I} \}\)</span> (<span class="math inline">\(\{ \mathcal{D}_{\alpha, P_2} : \alpha \in \mathcal{I} \}\)</span>): the set of output distribution when the correct password is <span class="math inline">\(P_1\)</span> (<span class="math inline">\(P_2\)</span>). Without loss of generality, we assume that for a fixed <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, the distribution <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> (<span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span>) assigns positive probability to each element in <span class="math inline">\(\mathcal{R}_\alpha\)</span>. Moreover, <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close.</p></li>
<li><p><span class="math inline">\(n\)</span>: the number of boxes you can hack.</p></li>
<li><p><span class="math inline">\(\vec i_k \doteq (i_1, i_2, ..., i_k)\)</span>: the index sequence of boxes you have chosen to hack up to time <span class="math inline">\(k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, <span class="math inline">\(\vec i_0\)</span> corresponds to a empty sequence.</p></li>
<li><p><span class="math inline">\(\vec Y_k = (Y_1, Y_2, ..., Y_k):\)</span> the random variables that represent messages outputted by the chosen boxes up to time <span class="math inline">\(k \in [1, n]\)</span>, where <span class="math inline">\(Y_t \in \mathcal{R}_{i_t} \subset \mathcal{R}\)</span> for <span class="math inline">\(t \in [1, k]\)</span>. Therefore, <span class="math inline">\(\vec Y_k\)</span> can be view as random vector in <span class="math inline">\(\mathcal{R}^k\)</span>.</p></li>
<li><p><span class="math inline">\(\vec x_k = (x_1, x_2, ..., x_k) \in \mathcal{R}^k:\)</span> a point in <span class="math inline">\(\mathcal{R}^k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, we define <span class="math inline">\(\vec x_0 = \emptyset\)</span>.</p></li>
<li><p><span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt; = \left&lt; (i_1, i_2, i_3, ..., i_k), (x_1, x_2, ..., x_k) \right&gt;:\)</span> the history up to <span class="math inline">\(k \in [0, n]\)</span>, which consists of the chosen indexes and observations up to time <span class="math inline">\(k\)</span>. We use <span class="math inline">\(\vec h_0\)</span> to denote the empty history.</p></li>
<li><p><span class="math inline">\(A:\)</span> your strategy (policy) for choosing boxes. It works as follows: for any fixed history <span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt;\)</span>, A is associated with a fixed distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span> over <span class="math inline">\(\mathcal{I} \setminus \vec i_k\)</span>, the set of indexes of the unchosen boxes. When inputted with <span class="math inline">\(\vec h_k\)</span>, <span class="math inline">\(A\)</span> returns a random variable <span class="math inline">\(A(\vec h_k)\)</span> that follows the distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span>.</p></li>
</ol>
<p>We will show that, no matter what strategy you use, it is unlikely that you distinguish via the output <span class="math inline">\(\vec Y_n\)</span> whether the correct password is <span class="math inline">\(P_1\)</span> or <span class="math inline">\(P_2\)</span>. This is because whether <span class="math inline">\(P = P_1\)</span> or not, the output distributions of <span class="math inline">\(\vec Y_n\)</span> are similar.</p>
<p><strong><em>Proof of the theorem.</em></strong> We will just prove the first inequality, and the second one follows from symmetry. We consider a bad set in <span class="math inline">\(\mathcal{R}^n\)</span>: <span class="math display">\[
\mathcal{W} \doteq \{ \vec x_n \in \mathcal{R}^n : \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] \ge e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] \}
\]</span></p>
<p>and will use the following lemma.</p>
<blockquote>
<p><strong>Lemma.</strong> <span class="math inline">\(\Pr[ \vec Y_n \in \mathcal{W}\ | \ P = P_1, A] \le \delta&#39;\)</span>.</p>
</blockquote>
<p>Hence, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n \in S \ | \ P = P_1, A] 
        &amp;=  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in S \cap \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \delta&#39; \\
        &amp;= \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] + \delta&#39; \\
        &amp;&lt; \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } e^{\epsilon&#39;} \cdot  \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] + \delta&#39; \\
        &amp;=  e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; 
\end{aligned}
\]</span></p>
<p>The first inequality follows from monotonicity of probability, the second one from the lemma, and the final one from the definition of <span class="math inline">\(\mathcal{W}\)</span>.</p>
<p><em>Proof of the lemma.</em> To prove the lemma, we need only to consider those point <span class="math inline">\(\vec x_n\)</span> with <span class="math display">\[
\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0. 
\]</span> Otherwise, <span class="math inline">\(\vec x_n\)</span> contributes to 0 probability to the set <span class="math inline">\(\mathcal{W}\)</span> (whether it belongs to <span class="math inline">\(\mathcal{W}\)</span> or not).</p>
<p>Now, we expand the probability <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A]\)</span>. To obtain the <span class="math inline">\(k\)</span>-th output, there are two steps</p>
<ol type="1">
<li><span class="math inline">\(A\)</span> generates a index <span class="math inline">\(i_k\)</span> of a box based on the known history <span class="math inline">\(\vec h_{k - 1}\)</span>.<br />
</li>
<li>The box <span class="math inline">\(B_{i_k}\)</span> is hacked, and output a random message <span class="math inline">\(x_k\)</span> sampled from its distribution <span class="math inline">\(D_{i_k, P_1}\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(\vec H_k\)</span> be a random variable that represents the history up to <span class="math inline">\(k\)</span>. By chain rule, we have <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A  ]
\end{aligned}
\]</span></p>
<p>By <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0\)</span>, each term in the expansion are positive. Similarly, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A  ]
\end{aligned}
\]</span></p>
<p>As both <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> assign positive probability to each point in <span class="math inline">\(\mathcal{R}_{i_k}\)</span>, we know <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] &gt; 0\)</span>.</p>
<p>We are ready to consider the ratio: <span class="math display">\[
\begin{aligned}
    \ln \frac{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] }{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] } 
    = \sum_{k = 1}^n \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }. 
\end{aligned}
\]</span></p>
<p>If we replace <span class="math inline">\(x_k\)</span> by a random variable <span class="math inline">\(X_k \sim \mathcal{D}_{i_k, P_1}\)</span>, then <span class="math display">\[
C_k \doteq \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }
\]</span></p>
<p>is a random variable. As <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close, we have <span class="math display">\[
C_k \le \epsilon.
\]</span></p>
<p>Further, we have</p>
<blockquote>
<p><strong>Fact 1.</strong> For any <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, it holds that <span class="math display">\[
\begin{aligned}
\mathbb{E}_{ X \sim \mathcal{D}_{\alpha, P_1} } \left[ \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = X]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = X]  }  \right] 
&amp;= \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;+ 
\sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } \\
&amp;= \sum_{x \in \mathcal{R}_\alpha } \left[ \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  - \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x] \right] \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le (e^\epsilon - 1) \epsilon
\end{aligned}
\]</span></p>
</blockquote>
<p>Therefore, <span class="math inline">\(\mathbb{E} [C_k] \le (e^\epsilon - 1) \epsilon\)</span>.</p>
<blockquote>
<p><strong>Fact 2.</strong> (<strong>Azuma Inequality</strong>). Let <span class="math inline">\(C_1, ...., C_n\)</span> be random variables such that <span class="math inline">\(\forall k \in [n]\)</span>, <span class="math inline">\(\Pr[ |C_k| \le \epsilon ] = 1\)</span>, and for every <span class="math inline">\((c_1, ..., c_{k -1} ) \in \text{Supp} (C_1, ..., C_{k - 1} )\)</span>, we have <span class="math display">\[
\mathbb{E}[ C_i \mid C_1 = c_1, ..., C_{k -1} = c_{k - 1} ] \le \beta,
\]</span> Then for every <span class="math inline">\(z &gt; 0\)</span>, we have <span class="math display">\[
\Pr[ \sum_{k = 1}^n C_i -  n \beta &gt;  z] \le \exp(- \frac{z^2}{ 2 n\epsilon^2 } )
\]</span></p>
</blockquote>
<p>Finally, applying <em>Azuma Inequality</em> with <span class="math inline">\(z = \sqrt{ 2n \log \frac{1}{\delta&#39;} }\)</span> and <span class="math inline">\(\beta = (e^\epsilon - 1) \epsilon\)</span>, we get the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference.">Reference.</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/12/PAC-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/12/PAC-learning/" class="post-title-link" itemprop="url">PAC Learning Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-12 16:06:55" itemprop="dateCreated datePublished" datetime="2020-11-12T16:06:55+11:00">2020-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-09 21:28:44" itemprop="dateModified" datetime="2021-11-09T21:28:44+11:00">2021-11-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss a toy model of probably approximately correct (PAC) learning [1]. The setting focuses on a <span class="math inline">\(d\)</span>-dimension Boolean hypercube <span class="math inline">\(\{0, 1\}^d\)</span> and a set of functions <span class="math inline">\(\mathcal{H} = \{ h : \{0, 1\}^d \rightarrow \{0, 1 \} \}\)</span>. A function <span class="math inline">\(h \in \mathcal{H}\)</span> is called a hypothesis or a concept, which assigns each vertex in the hypercube a label 0 or 1.</p>
<p>The goal of PAC learning is to learn an unknown hypothesis, denoted as <span class="math inline">\(h^* \in \mathcal{H}\)</span>, from a dataset, which consists of <span class="math inline">\(n\)</span> points <span class="math inline">\(X_1, X_2, \ldots, X_n \in \{0, 1\}^n\)</span> sampled independently from an unknown distribution <span class="math inline">\(D\)</span> (over the hypercube), as well as their labels <span class="math inline">\(Y_1 = h^*(X_1), Y_2 = h^*(X_2), \ldots, Y_n = h^*(X_n)\)</span>.</p>
<p>We can state the algorithm for PAC learning in just one sentence.</p>
<blockquote>
<p>Output any hypothesis <span class="math inline">\(h&#39; \in \mathcal{H}\)</span> that is consistent with the sampled dataset, i.e., <span class="math display">\[
h&#39;(X_i) = Y_i, \qquad \forall i \in [n]
\]</span></p>
</blockquote>
<p>Such an hypothesis always exists, as <span class="math inline">\(h^*\)</span> satisfies this constraint. We have the classic PAC learning theorem.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The PAC learning theorem states that the probability that <span class="math inline">\(h&#39;\)</span> mislabels a point is bounded by <span class="math inline">\(\sqrt{ \log |\mathcal{H}| + \log ( 1 / \delta ) \over 2n}\)</span>.</p>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
    \Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
    Z_i = \begin{cases}
        0, \qquad \text{ if } h(X_i) = Y_i \\
        1, \qquad \text{ if } h(X_i) \neq Y_i 
    \end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\mathbb{E}[Z_i]\)</span> is an unbiased estimator of <span class="math inline">\(\mu_h\)</span>. Further, denote <span class="math inline">\(\hat \mu_h\)</span> the empirical mean of <span class="math inline">\(Z_i\)</span>'s <span class="math display">\[
    \hat \mu_h \doteq \frac{1}{n} \sum_{i \in [n] } Z_i. 
\]</span></p>
<p>Note that for the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(\hat \mu_{h&#39;} = 0\)</span>.</p>
<p>By Hoeffding inequality, for any <span class="math inline">\(\delta&#39; &gt; 0\)</span>, <span class="math display">\[
    \Pr \left[ \hat \mu_h \le \mu_h - \sqrt{ \frac{ \log ( 1 / \delta&#39;) }{2n} } \right] \le \delta&#39;. 
\]</span></p>
<p>By union bound, the probability that <span class="math display">\[
    \exists h \in \mathcal{H}, \hat \mu_h \ge \mu_h - \sqrt{ \frac{ \log ( 1 / \delta&#39;) }{2n} }
\]</span></p>
<p>is bounded by <span class="math inline">\(| \mathcal{H} | \delta&#39;\)</span>. By setting <span class="math inline">\(\delta&#39; = \frac{\delta}{ | \mathcal{H} | }\)</span>, it is guaranteed that with probability at most <span class="math inline">\(\delta\)</span>, <span class="math display">\[
    0 = \hat \mu_{h&#39; } \le \mu_{h&#39;} -  \sqrt{ \log |\mathcal{H}| + \log ( 1 / \delta) \over 2n}.
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>The previous theorem can be improved with the <span class="math inline">\(\sqrt{ \cdot }\)</span> removed. The key here is that the selected <span class="math inline">\(h&#39;\)</span> is error-free on the samples.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
    \Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>We will prove that if <span class="math inline">\(\mu_h \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}\)</span>, then <span class="math inline">\(h\)</span>'s probability of being selected is very low.</p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
    Z_i = \begin{cases}
        0, \qquad \text{ if } h(X_i) = Y_i \\
        1, \qquad \text{ if } h(X_i) \neq Y_i 
    \end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\Pr[Z_i = 1] = \mu_h\)</span>. Therefore, <span class="math display">\[
    \begin{aligned}
        \Pr[ Z_1 = 0 \wedge Z_2 = 0 \wedge \ldots \wedge Z_n = 0] = (1 - \mu_h)^n \le \exp(- \mu_h n) = \frac{\delta}{ |\mathcal{H} | }
    \end{aligned}
\]</span></p>
<p>Taking union bound over all possible <span class="math inline">\(h\)</span> gives the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Caveat.</em></strong> <em>In both theorem, we see a <span class="math inline">\(\log \mathcal{H}\)</span> term in the failure probability, which is due to the use of union bound and can be roughly considered as the complexity of the hypothesis family. It is tempting to think that this could be avoid as follows.</em></p>
<blockquote>
<p>For the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(Z_1, \ldots, Z_n\)</span> are i.i.d samples. Therefore, the probability of all of them being <span class="math inline">\(0\)</span> is give by<br />
<span class="math display">\[
(1 - \mu_{h&#39;} )^n \le \exp( - \mu_{h&#39;} n). 
  \]</span><br />
Bounding this probability by <span class="math inline">\(\delta\)</span> gives <span class="math inline">\(\mu_{h&#39;} \le \frac{ \log ( 1 / \delta ) } {n}\)</span>.</p>
</blockquote>
<p><em>The argument is wrong. For the outputted <span class="math inline">\(h&#39;\)</span>, the <span class="math inline">\(Z_1, \ldots, Z_n\)</span> are not i.i.d samples, as <span class="math inline">\(h&#39;\)</span> is selected according to <span class="math inline">\(Z_1, \ldots, Z_n\)</span> and depends on them.</em></p>
<p><em>To make it more concrete, suppose that there is an <span class="math inline">\(h\)</span>, such that <span class="math inline">\(\mu_h = \frac{ \log ( 1 / \delta ) } {n}\)</span>. Consider the following experiment</em></p>
<blockquote>
<ol type="1">
<li>Sample <span class="math inline">\(n\)</span> points independently the distribution <span class="math inline">\(D\)</span>.<br />
</li>
<li>Pick an <span class="math inline">\(h&#39;\)</span> that makes no prediction error on the samples.</li>
</ol>
</blockquote>
<p><em>If we repeat the experiment <span class="math inline">\(N\)</span> times for some positive integer <span class="math inline">\(N\)</span>, then the fixed <span class="math inline">\(h\)</span> makes no mistake in roughly <span class="math inline">\((1 - \delta) N\)</span> of the experiments and constitutes an candidate of <span class="math inline">\(h&#39;\)</span>. However, on the other roughly <span class="math inline">\(\delta N\)</span> experiments, where <span class="math inline">\(h\)</span> makes prediction error, it is no longer considered as the candidate of <span class="math inline">\(h&#39;\)</span>. The selection procedure of <span class="math inline">\(h&#39;\)</span> ignores bad event of <span class="math inline">\(h\)</span> automatically.</em></p>
<p><em>We could also investigate the following example. Let <span class="math inline">\(h^* = h_0\)</span>. There are also other two hypothesis <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span>. Let <span class="math inline">\(S\)</span> be the sample space. It holds that</em> <span class="math display">\[
    |h_1(x) - h^*(x) | = \begin{cases}
        2 \epsilon, \forall x \in S \setminus S_{h_1} \\
        0,\ \       \forall x \in S_{h_1}
    \end{cases}
\]</span> <span class="math display">\[
    |h_2(x) - h^*(x) | = \begin{cases}
        2 \epsilon, \forall x \in S \setminus S_{h_2} \\
        0,\ \       \forall x \in S_{h_2}
    \end{cases}
\]</span> <em>Moreover, <span class="math inline">\(\Pr[S_{h_1}] = \Pr[S_{h_2} ] = \delta\)</span>. How, if we get a sample in <span class="math inline">\(S_{h_1}\)</span>, then we can output <span class="math inline">\(h&#39;\)</span> as <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_1\)</span>, since both of them make no error when <span class="math inline">\(x \in S_{h_1}\)</span>. Similarly, if <span class="math inline">\(x \in S_{h_2}\)</span>, we can output either <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_2\)</span>. We call <span class="math inline">\(S_{h_1}\)</span> and <span class="math inline">\(S_{h_2}\)</span> the bad areas. When <span class="math inline">\(x\)</span> is in the bad area of any hypothesis, we can't distinguish this hypothesis from the true hypothesis. That is why we need to use union bound to bound the total probabilities of these bad areas.</em></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/PAC-Learning.png?raw=true" width="400" height="340" /></p>
</div>
<h3 id="reference">Reference</h3>
<p>[1] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. 6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/Gaussian-Distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/06/Gaussian-Distribution/" class="post-title-link" itemprop="url">Gaussian Distribution</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-06 23:00:19" itemprop="dateCreated datePublished" datetime="2020-11-06T23:00:19+11:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-09 22:00:48" itemprop="dateModified" datetime="2021-06-09T22:00:48+10:00">2021-06-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="gaussian-distribution">Gaussian Distribution</h1>
<p>We show how Gaussian distribution (a.k.a. normal distribution) is constructed and list some of its basic properties.</p>
<p>Suppose we would like to have a distribution in <span class="math inline">\(\mathbb{R}\)</span>, such that</p>
<ol type="1">
<li>It is centered at the origin.<br />
</li>
<li>Its density function decreases exponentially with respect to the square distance to the origin.</li>
</ol>
<blockquote>
<p><em>Remark: if we replace 2 with "a density function decreases exponentially with the distance to the origin", we obtain Laplace distribution.</em></p>
</blockquote>
<p>The density function <span class="math inline">\(p(x)\)</span> should be inversely and exponentially proportional to <span class="math inline">\(x^2\)</span>: <span class="math display">\[
    p(x) \propto \exp(-x^2 )
\]</span></p>
<p>Denote <span class="math inline">\(M\)</span> the value of<br />
<span class="math display">\[
    M \doteq \int_{-\infty}^\infty \exp(-x^2) \ dx, 
\]</span></p>
<p>then the close form of <span class="math inline">\(p(x)\)</span> is <span class="math display">\[
    p(x)  = \frac{1}{M} \exp(- x^2). 
\]</span></p>
<p>It is easy to verity that <span class="math inline">\(p(x)\)</span> is a valid density function: <span class="math inline">\(\int_{-\infty}^\infty p(x) \ dx = 1\)</span>.</p>
<h1 id="closed-form">Closed Form</h1>
<blockquote>
<p><strong>Theorem.</strong> $ p(x) =  ( - x^2 ) $ is a density function.</p>
</blockquote>
<p>In general, it is not hard to construct a distribution. Let <span class="math inline">\(h(x)\)</span> be any integrable function on a domain <span class="math inline">\(\mathcal{D}\)</span> such that <span class="math display">\[
    M = \int_\mathcal{D} h(x) \ dx &lt; \infty
\]</span></p>
<p>Then we can view <span class="math inline">\(h(x)\)</span> as almost an density function of some distribution. The only obstacle is that <span class="math inline">\(M\)</span> might not be <span class="math inline">\(1\)</span>. However this is easy to fix. We scale the value of <span class="math inline">\(h(x)\)</span> by a constant factor at each point by setting <span class="math display">\[
    p(x) = \frac{1}{M} h(x).
\]</span></p>
<p>In this case, we don't even need to know the exact value of <span class="math inline">\(M\)</span>. We only need to guarantee that</p>
<blockquote>
<p>The value of <span class="math inline">\(M\)</span> exists and is finite.</p>
</blockquote>
<p>Then we can safely rely on <span class="math inline">\(M\)</span> as a normalized. For example, we know that <span class="math display">\[
    \int_0^\infty x^{0.2} e^{-x} \ dx \le  \int_0^1 e^{-x} \ dx  + \int_1^\infty x e^{-x} \ dx &lt; \infty.
\]</span></p>
<p>We can construct a density function as discussed above. It turns out that this is a special case of <em>Gamma distribution</em>.</p>
<p>For Gaussian distribution, we are lucky as the closed form of <span class="math inline">\(M\)</span> does exists, due to the following "squaring trick": <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \Big( \int_{-\infty}^\infty \exp( -x^2 ) \  dx \Big)^2 \\
        &amp;= \int_{-\infty}^\infty \exp( -x^2 ) \  dx \ \cdot \int_{-\infty}^\infty \exp( -y^2 ) \  dy \\
        &amp;= \int_{-\infty}^\infty \int_{-\infty}^\infty \exp( -(x + y) ^2 ) \  dx  dy. 
\end{aligned}
\]</span></p>
<p>Denote <span class="math inline">\(r \doteq \sqrt{ x^2 + y^2}\)</span> the distance of <span class="math inline">\((x, y)\)</span> to <span class="math inline">\((0, 0)\)</span>. We are integrating the function <span class="math inline">\(\exp( - r^2)\)</span> over the plane <span class="math inline">\(\mathbb{R}^2\)</span>. It is natural to switch to polar coordinate system <span class="math inline">\((r, \theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the angle of a point. Here is an informal but intuitive explanation. As the picture below demonstrates, if <span class="math inline">\(r\)</span> increases by <span class="math inline">\(dr\)</span> and <span class="math inline">\(\theta\)</span> increases by <span class="math inline">\(d\theta\)</span>, the new area spanned by <span class="math inline">\(dr\)</span> and <span class="math inline">\(d \theta\)</span> is roughly <span class="math inline">\(r dr d \theta\)</span>.<br />
We can also calculate this algebraically<br />
<span class="math display">\[
\frac{1}{2} [(r + dr)^2 - r^2] d\theta = r dr d \theta + \frac{1}{2} (dr)^2 d \theta \approx r dr d \theta.
\]</span></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration1.png?raw=true" width="400" height="340" /></p>
</div>
<p>Finally, <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \int_{0}^{ 2 \pi} \int_{0}^\infty \exp( -r^2 ) r d r d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2}  \int_{0}^\infty \exp( -r^2 ) d r^2 d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2} d \theta \\
        &amp;= \pi.
\end{aligned}
\]</span></p>
<h1 id="basic-properties">Basic Properties</h1>
<h2 id="expectation"><strong>Expectation</strong></h2>
<p>As the distribution is symmetric to <span class="math inline">\(0\)</span>, it has expectation 0.</p>
<h2 id="variance"><strong>Variance</strong></h2>
<p>As its expectation is 0, the variance is given by</p>
<p><span class="math display">\[
\begin{aligned}
    \int_{-\infty}^\infty  \frac{ x^2 }{ \sqrt \pi } \exp( -x^2 ) \ dx 
        &amp;= -\frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty x \ d \exp( -x^2 ) \\
        &amp;= \frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty \exp( -x^2 ) d x\\
        &amp;= \frac{1}{2 \sqrt \pi } \sqrt \pi \\
        &amp;= \frac{1}{2}
\end{aligned}
\]</span></p>
<h2 id="general-gaussian-distribution">General Gaussian Distribution</h2>
<p>We have proved that if <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(p(x) = \frac{1}{\sqrt \pi} \exp( -x^2)\)</span>, then <span class="math display">\[
    \mathbb{Var}[X] = \frac{1}{2}.
\]</span></p>
<p>Define another random variable <span class="math inline">\(Y \doteq \sqrt{2} X\)</span>, whose variance is <span class="math display">\[
    \mathbb{Var}[X] = 2 \mathbb{Var}[X] = 1.
\]</span></p>
<p>To obtain its density function of <span class="math inline">\(Y\)</span>, we first scale the function <span class="math inline">\(p(x)\)</span> horizontally by a factor of <span class="math inline">\(\sqrt 2\)</span> to get <span class="math display">\[
    \frac{1}{ \sqrt{ \pi } } \exp \Big( -\frac{x^2}{2} \Big).
\]</span></p>
<p>Now the area under the curve <span class="math inline">\(f(x) = \frac{1}{ \sqrt{ \pi } } \exp \big( - x^2 / {2} \big)\)</span> is <span class="math inline">\(\sqrt 2\)</span>. We normalize this area to <span class="math inline">\(1\)</span> and obtain <span class="math inline">\(Y\)</span>'s density function as <span class="math display">\[
    p(y) = \frac{1}{ \sqrt{2 \pi } } \exp \Big( -\frac{y^2}{2} \Big)
\]</span></p>
<p>In what follows, we use <span class="math inline">\(N(0, 1)\)</span> to denote a Gaussian distribution with mean 0 and variance <span class="math inline">\(1\)</span>. We can also scale <span class="math inline">\(X\)</span> by a factor of <span class="math inline">\(\sqrt 2 \sigma\)</span> for any <span class="math inline">\(\sigma &gt; 0\)</span>, to get <span class="math inline">\(Y = \sqrt 2 \sigma X\)</span>. It have variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math display">\[
    p(y) = \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( - \Big(\frac{y}{ \sqrt{2} \sigma } \Big)^2 \Big) = \frac{1}{ \sqrt{2 \pi \sigma^2 } } \exp \Big( -\frac{y^2}{ 2 \sigma^2 } \Big).
\]</span></p>
<p>Finally, we can shift the center of <span class="math inline">\(0\)</span> by any real number <span class="math inline">\(\mu \in \mathbb{R}\)</span> to obtain a new density function <span class="math display">\[
    p(y) = \frac{1}{ \sqrt{ \pi \cdot 2  \sigma^2 } } \exp\Big( -\frac{ (y - \mu)^2 }{ 2 \sigma^2 } \Big).
\]</span></p>
<p>The corresponding distribution is denoted as <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<em>The picture below shows a few Gaussian distributions.</em>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution.png?raw=true" width="700" height="430" /></p>
</div>
<h1 id="advanced-properties">Advanced Properties</h1>
<h2 id="sum-of-normal-random-variables">Sum of Normal Random Variables</h2>
<blockquote>
<p>If <span class="math inline">\(X \sim N(0, a^2)\)</span> and <span class="math inline">\(Y \sim N(0, b^2)\)</span>, then <span class="math inline">\(X + Y \sim N(0, a^2 + b^2)\)</span>.</p>
</blockquote>
<p>The demonstration here follows from [1]. The key observation is that the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is rotation invariant with the origin.</p>
<p>As an example, we plot below the joint distribution of <span class="math inline">\(X, Y \sim N(0, 1)\)</span>.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-0.png?raw=true" width="500" height="330" /></p>
</div>
<p>The figure below shows the same distribution. Observe again that the probability mass concentrates at a small region centered at origin.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-1.png?raw=true" width="500" height="330" /></p>
</div>
<p>Therefore, for any <span class="math inline">\(t \in \mathbb{R}\)</span>, the set <span class="math display">\[
    \{ (x, y) \in \mathbb{R}^2 : ax + by \le t \}
\]</span> has the same probability measure as the one <span class="math display">\[
    \Big\{ (x, y) \in \mathbb{R}^2 : x \le \frac{ t}{ \sqrt{a^2 + b^2} } \Big\}.
\]</span></p>
<p>Observe that the latter can be obtained by rotating the former with respect to the origin (see the figure below).</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration2.png?raw=true" width="500" height="250" /></p>
</div>
<p>Now, the probability of the latter set is given by <span class="math display">\[
    \Pr \Big[ X \le \frac{ t}{ \sqrt{a^2 + b^2} } \Big] = \Pr\Big[  \sqrt{a^2 + b^2} X \le t \Big]
\]</span></p>
<p>It concludes that <span class="math inline">\(X + Y\)</span> has the same distribution as <span class="math inline">\(\sqrt{a^2 + b^2} X \sim N(0, a^2 + b^2)\)</span>.</p>
<!-- **Remark**: we can also verify this algebraically:
$$
    \begin{aligned}
        \Pr [ X + Y \le t ] 
            &= \int_{-\infty}^t \int_{-\infty }^{ t - x } \frac{1}{ 2 \pi } \exp \Big(-\frac{x^2 + y^2 }{2} \Big) \ dx 
    \end{aligned}
$$ -->
<!-- $$
    \Pr [X + Y \le t] = \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} } \exp \Big(-\frac{x^2}{2} \Big) \ dx 
$$

Hence
$$
\begin{aligned}
    \frac{ \partial }{ \partial t} \Pr[X + Y \le t]
        &= \frac{ \partial }{ \partial t} \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} }\exp \Big(-\frac{x^2}{2} \Big) \ dx \\
        &= \frac{1}{ \sqrt{2 \pi (a^2 + b^2) } } \exp \Big( -\frac{x^2}{2 (a^2 + b^2) } \Big).
\end{aligned}
$$ -->
<h2 id="concentration-inequalities">Concentration Inequalities</h2>
<h3 id="bound-1"><strong>Bound 1</strong></h3>
<blockquote>
<p>If <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>, then <span class="math inline">\(\forall t &gt; 0\)</span>, <span class="math display">\[
\Pr[ |X| \ge t] \le \exp \Big( -\frac{t^2}{ 2\sigma^2} \Big)
\]</span></p>
</blockquote>
<p><strong>Remark</strong>: it is possible to get a tighter bound by using more advanced techniques.</p>
<p><strong>Intuition:</strong> in previous figures, it seems Gaussian distributions have a shape bump around its mean. This is kind of mis-leading, because the x-axis and y-axis are scaled. In the figure below, we plot <span class="math inline">\(N(0, 1)\)</span>, with ratio between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes being approximate <span class="math inline">\(1:1\)</span>. We see only a small bump around its mean. Although the distribution span the entire range of <span class="math inline">\(\mathbb{R}\)</span>, the probability mass is centered tightly around the origin.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Concentration.png?raw=true" width="600" height="400" /></p>
</div>
<h4 id="proof-by-moment-generating-function">Proof by Moment Generating Function</h4>
<p>We prove a weaker version by a factor of <span class="math inline">\(2\)</span>. For all <span class="math inline">\(\lambda \in \mathbb{R}\)</span>, <span class="math display">\[
    \begin{aligned}
        \mathbb{E}[ e^{\lambda X} ] 
            &amp;= \frac{1}{\sqrt{2 \pi \sigma} }\int e^{\lambda x} e^{ - \frac{x^2}{2 \sigma^2 } } \ d x \\
            &amp;= \frac{ e^{\lambda^2 \sigma^2 / 2} }{\sqrt{2 \pi \sigma} }\int e^{ - \frac{ (x - \lambda \sigma^2)^2}{2 \sigma^2 } } \ d x \\
            &amp;= e^{\lambda^2 \sigma^2 / 2}.
    \end{aligned}
\]</span></p>
<p>Via Markov's inequality, for all <span class="math inline">\(\lambda \ge 0\)</span>, we have <span class="math display">\[
    \begin{aligned}
        \Pr[ X \ge t] 
            &amp;\le \frac{ \mathbb{E}[ e^{\lambda X} ]  }{ e^{\lambda t} } 
            &amp;\le e^{\lambda^2 \sigma^2 / 2 - \lambda t}.
    \end{aligned}
\]</span></p>
<p>Setting <span class="math inline">\(\lambda = \frac{t}{\sigma^2}\)</span>, we get <span class="math display">\[
    \Pr[ X \ge t ] \le e^{ - \frac{t^2}{2 \sigma^2} }.
\]</span></p>
<p>It concludes that <span class="math inline">\(\Pr[ |X| \ge t ] \le 2 e^{ - \frac{t^2}{2 \sigma^2} }.\)</span></p>
<h4 id="proof-by-geometry">Proof by Geometry</h4>
<p>Let <span class="math inline">\(T = \Pr[ |X| \ge t]\)</span>. Then <span class="math display">\[
    T^2 = \int_{ |x| \ge t, |y| \ge t} \frac{1}{ 2 \pi \sigma^2 } \exp \Big( - \frac{ x^2 + y^2}{2 \sigma^2} \Big). 
\]</span></p>
<p>As <span class="math display">\[
    \{ (x, y) \in \mathbb{R}^2: |x| \ge t \wedge |y| \ge t \} \subset
    \{ (x, y) \in \mathbb{R}^2: x^2 + y^2 \ge 2 t^2 \} 
\]</span></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration3.png?raw=true" width="400" height="400" /></p>
</div>
<p>It follows that <span class="math display">\[
\begin{aligned}
    T^2 
        &amp;\le \int_{ \sqrt 2 t}^\infty \int_{0}^{2 \pi } \frac{1}{ 2 \pi \sigma^2 } \exp \Big( - \frac{ r^2 }{2 \sigma^2} \Big) r \ d\theta dr\\
        &amp;\le \int_{ \sqrt 2 t }^\infty \exp \Big( - \frac{ r^2 }{2 \sigma^2} \Big) \ d \frac{r^2}{ 2\sigma^2} \\
        &amp;\le \int_{ \frac{ t^2 }{  \sigma^2 } }^\infty \exp ( - z ) \ d z \\
        &amp;= \exp \Big( -\frac{t^2}{ \sigma^2} \Big).
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math display">\[
    T \le \exp \Big( - \frac{t^2}{ 2\sigma^2} \Big).
\]</span></p>
<h3 id="bound-2"><strong>Bound 2</strong></h3>
<blockquote>
<p>If <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>, then <span class="math inline">\(\forall t &gt; 0\)</span>, <span class="math display">\[
\Pr[ X \ge t] \le \frac{ \sigma }{ \sqrt{2 \pi }  t } \exp \Big( -\frac{t^2}{ 2\sigma^2} \Big).
\]</span> By symmetry we also have <span class="math display">\[
\Pr[ X \le -t] \le \frac{ \sigma }{ \sqrt{2 \pi }  t } \exp \Big( -\frac{t^2}{ 2\sigma^2} \Big).
\]</span></p>
</blockquote>
<p><strong>Proof.</strong> <span class="math display">\[
    \begin{aligned}
        \Pr[X &gt; t] 
            &amp;= \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( - \frac{ x^2 }{ 2 \sigma^2 } \Big) \ dx \\
            &amp;\le \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \frac{x}{t} \exp \Big( - \frac{ x^2 }{ 2  \sigma^2 } \Big) \ dx \\
            &amp;\le  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \int_{t}^\infty  \exp \Big( - \frac{ x^2 }{ 2  \sigma^2 } \Big) \ d \frac{ x^2 }{ 2 \sigma^2 } \\
            &amp;=  - \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp \Big( - \frac{ x^2 }{ 2  \sigma^2 } \Big) \mid_{t}^\infty \\
            &amp;=  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp \Big( - \frac{ t^2 }{ 2 \sigma^2 } \Big).  
    \end{aligned}
\]</span></p>
<p>This bound is more useful only when we know that <span class="math inline">\(t \ge \frac{ \sigma }{ \sqrt{2 \pi } }\)</span>.</p>
<h1 id="reference">Reference</h1>
<p>[1] B. Eisenberg and R. Sullivan, “Why Is the Sum of Independent Normal Random Variables Normal?,” Mathematics Magazine, vol. 81, no. 5, pp. 362–366, Dec. 2008</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/18/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><span class="page-number current">19</span><a class="page-number" href="/page/20/">20</a><span class="space">&hellip;</span><a class="page-number" href="/page/57/">57</a><a class="extend next" rel="next" href="/page/20/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">170</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
