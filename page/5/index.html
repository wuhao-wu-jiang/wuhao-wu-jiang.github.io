<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/5/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/19/PAC-Learning-From-Countable-Hypothesis-Family/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/19/PAC-Learning-From-Countable-Hypothesis-Family/" class="post-title-link" itemprop="url">PAC Learning From Countable Hypothesis Family</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-19 15:17:27" itemprop="dateCreated datePublished" datetime="2020-11-19T15:17:27+11:00">2020-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-21 20:31:33" itemprop="dateModified" datetime="2020-11-21T20:31:33+11:00">2020-11-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We generalize the PAC learning problem of finding an hypothesis from a finite hypothesis family to the one from a countable hypothesis family.</p>
<h2 id="problem-setting">Problem Setting</h2>
<ol type="1">
<li><p><span class="math inline">\(\mathcal{X}:\)</span> the input space.</p></li>
<li><p><span class="math inline">\(\mathcal{Y} \doteq \{-1, 1\}:\)</span> the output space.</p></li>
<li><p><span class="math inline">\(\mathcal{Z} \doteq \mathcal{X} \times \mathcal{Y}:\)</span> the product space of <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal{D}:\)</span> an unknown distribution defined on <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal{H}:\)</span> an countable family of hypothesis, s.t., each <span class="math inline">\(h \in \mathcal{H}\)</span> is a function from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathcal{Y}\)</span>.</p></li>
<li><p><span class="math inline">\(\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow [0, 1]\)</span>, a loss function that takes two points in <span class="math inline">\(\mathcal{Y}\)</span> and outputs a value between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p></li>
<li><p><span class="math inline">\(L_\mathcal{D} (h):\)</span> the expected loss of a hypothesis <span class="math inline">\(h\)</span> is defined as <span class="math display">\[
 L_\mathcal{D} (h) = \mathbb{E}_{ (x, y) \sim \mathcal{D} } [ \ell( h(x), y) ]
 \]</span></p>
<p>where <span class="math inline">\((x, y) \sim \mathcal{D}\)</span> implies that the pair <span class="math inline">\((x, y) \in \mathcal{Z}\)</span> is sampled according to the distribution <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p><span class="math inline">\(h^* \doteq \arg\min_{h \in \mathcal{H} } L_\mathcal{D} (h):\)</span> the hypothesis that minimize the expected loss.</p></li>
<li><p><span class="math inline">\(\mu_h:\)</span> an alias for <span class="math inline">\(L_\mathcal{D} (h)\)</span>, when the distribution <span class="math inline">\(\mathcal{D}\)</span> discussed in the context is unique.</p></li>
<li><p><span class="math inline">\(B(\mu_h, \epsilon) \doteq \{ r \in \mathbb{R} : |r - \mu_h | &lt; \epsilon \}:\)</span> an open ball, which is an open interval in <span class="math inline">\(\mathbb{R}\)</span> centered at <span class="math inline">\(\mu_h\)</span> with length <span class="math inline">\(2 \epsilon\)</span>, where <span class="math inline">\(\epsilon &gt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(S \sim \mathcal{D}^n:\)</span> a set of <span class="math inline">\(n\)</span> i.i.d samples drawn from <span class="math inline">\(\mathcal{D}\)</span>, where <span class="math inline">\(n \in \mathbb{N}^+\)</span> is some positive integer. In particular, when <span class="math inline">\(S \sim \mathcal{D}^n\)</span>, it can be represented as <span class="math display">\[
 S = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) : (x_i, y_i) \sim \mathcal  {D}, \forall i \in [n] \}.
 \]</span> We also view <span class="math inline">\(S\)</span> as a point in <span class="math inline">\(\mathcal{Z}^n\)</span>.</p></li>
<li><p><span class="math inline">\(L_{S}(h) \doteq \frac{1}{ |S| } \sum_{ (x, y) \in S } \ell( h(x) , y ) :\)</span> the empirical loss of a hypothesis <span class="math inline">\(h\)</span> on a sample set <span class="math inline">\(S\)</span>.</p></li>
<li><p>Given a <span class="math inline">\(h \in \mathcal{H}\)</span>, its restriction on <span class="math inline">\(\mathcal{C} \subset \mathcal{X}\)</span> is a function <span class="math inline">\(h_S\)</span> defined on <span class="math inline">\(S\)</span>, such that <span class="math display">\[
h_\mathcal{C} (x) = h(x), \forall x \in \mathcal{C} 
\]</span></p></li>
<li><p>The restriction of <span class="math inline">\(\mathcal{H}\)</span> on <span class="math inline">\(\mathcal{C}\)</span> is the set of possible restriction of a function in <span class="math inline">\(\mathcal{H}\)</span> to <span class="math inline">\(\mathcal{C}\)</span> <span class="math display">\[
        \mathcal{H}_\mathcal{C}  = \{ h_\mathcal{C}  : \mathcal{C}  \rightarrow \mathcal{Y} : h \in \mathcal{H} \}
\]</span></p>
<p>As <span class="math inline">\(\mathcal{Y} = \{ -1, +1 \}\)</span>, the set of possible functions defined on <span class="math inline">\(\mathcal{C}\)</span> is bounded by <span class="math inline">\(2^{n}\)</span>. Hence, <span class="math display">\[
    |\mathcal{H}_\mathcal{C} | \le 2^{n}.
\]</span></p></li>
<li><p>The growth function <span class="math inline">\(\Pi_{\mathcal{H} } (n): \mathbb{N}^+ \rightarrow \mathbb{N}^+\)</span> of <span class="math inline">\(\mathcal{H}\)</span> is defined as <span class="math display">\[
   \Pi_{\mathcal{H} } (n) = \max_{\mathcal{C} \subset \mathcal{X}, |C| = n} | \mathcal{H}_\mathcal{C} |
   \]</span></p></li>
</ol>
<p>Ideally, we would like to find <span class="math inline">\(h^*\)</span>. The problem is difficult, as</p>
<ul>
<li>The space <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span> could be infinite.</li>
<li>The distribution <span class="math inline">\(\mathcal{D}\)</span> is unknown.</li>
</ul>
<p>To deal with the possibly infinite space <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span> and the unknown distribution <span class="math inline">\(\mathcal{D}\)</span>, we investigate <span class="math inline">\(\mathcal{H}\)</span> on a finite sample set <span class="math inline">\(S\)</span>. Due to the randomness inherited in sampling <span class="math inline">\(S\)</span>, we allow our solution to be approximate and to make error. For a given pair of parameters of <span class="math inline">\(\epsilon &gt; 0\)</span> and <span class="math inline">\(\delta &gt; 0\)</span>, we relax the goal to designing an <span class="math inline">\((\epsilon, \delta)\)</span>-learning algorithm <span class="math inline">\(A\)</span> that returns an <span class="math inline">\(h&#39;\)</span>, that is</p>
<blockquote>
<ul>
<li><span class="math inline">\(\epsilon\)</span>-approximate: <span class="math inline">\(\mu_{h&#39;} \le \mu_{h^*} + \epsilon\)</span>,<br />
</li>
<li>probably correct: <span class="math inline">\(A\)</span> return an <span class="math inline">\(h&#39;\)</span> that does not satisfies the above condition with probability at most <span class="math inline">\(\delta\)</span>.</li>
</ul>
</blockquote>
<p>Combined, <span class="math inline">\(A\)</span> should return an <span class="math inline">\(\epsilon\)</span>-approximate solution <span class="math inline">\(h&#39;\)</span> with probability at least <span class="math inline">\(1 - \delta\)</span>.</p>
<h2 id="the-algorithm">The Algorithm</h2>
<blockquote>
<p>An <span class="math inline">\((\epsilon, \delta)\)</span> approximate algorithm <span class="math inline">\(A\)</span><br />
1. Draw a set <span class="math inline">\(S\)</span> of <span class="math inline">\(n =\)</span> samples independently from <span class="math inline">\(\mathcal{D}\)</span>.<br />
2. Return an <span class="math inline">\(h&#39;\)</span> such that <span class="math display">\[
h&#39; = \arg\min_{h \in \mathcal{H} } L_{S} (h)
\]</span></p>
</blockquote>
<p>A key result of the algorithm states that <span class="math inline">\(L_S(h)\)</span> is a good approximation of <span class="math inline">\(\mu_h\)</span> for all <span class="math inline">\(h \in \mathcal{H}\)</span> simultaneously.</p>
<blockquote>
<p>Theorem. If <span class="math inline">\(n \ge\)</span>, then <span class="math display">\[
\Pr_{S \sim \mathcal{D}^n } [ \exists h \in \mathcal{H} : L_S(h) \notin B(\mu_h, \epsilon / 2 )  ] \le \delta
\]</span></p>
</blockquote>
<p>An immediate corollary is that <span class="math inline">\(h&#39;\)</span> is <span class="math inline">\(\epsilon\)</span>-approximate with probability at least <span class="math inline">\(1 - \delta\)</span>: <span class="math display">\[
\mu_{h&#39;} \le L_S(h&#39;) + \frac{\epsilon}{2} \le L_S(h^*) + \frac{\epsilon}{2} \le \mu_{h^*} + \epsilon.
\]</span></p>
<h2 id="proof-of-the-theorem">Proof of The Theorem</h2>
<p>The proof relies on a technique called double sampling. Alongside with <span class="math inline">\(S\)</span>, we create another sample set <span class="math inline">\(S&#39; \sim \mathcal{D}^n\)</span> independently, termed the "ghost sample". Let <span class="math display">\[
S = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \}
\]</span> and <span class="math display">\[
S&#39; = \{ (x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;), ..., (x_n&#39;, y_n&#39;) \}.
\]</span></p>
<p>Then, we define</p>
<ol start="16" type="1">
<li><span class="math inline">\(\sigma:\)</span> a random swap that exchanges the <span class="math inline">\(i\)</span>-th (<span class="math inline">\(i \in [n]\)</span>) element of <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> independently with probability 0.5. Call the resulting sample sets <span class="math inline">\(\sigma S\)</span> and <span class="math inline">\(\sigma S&#39;\)</span>. Let <span class="math inline">\(\sigma S[i]\)</span> (<span class="math inline">\(\sigma S&#39;[i]\)</span>) be the <span class="math inline">\(i\)</span>-th element of <span class="math inline">\(\sigma S[i]\)</span> (<span class="math inline">\(\sigma S&#39;[i]\)</span>). So <span class="math display">\[
\Pr \left[ 
    \sigma S [i] = (x_i,  y_i) \wedge
    \sigma S&#39;[i] = (x_i&#39;, y_i&#39;)
 \right] = 0.5 \\
 \Pr \left[ 
    \sigma S [i] = (x_i&#39;, y_i&#39;) \wedge
    \sigma S&#39;[i] = (x_i,  y_i)
 \right] = 0.5
\]</span></li>
</ol>
<p>We use the gap of <span class="math inline">\(|L_{\sigma S} (h) - L_{\sigma S&#39;} (h)|\)</span> as a proxy of <span class="math inline">\(|L_S(h) - \mu_h|\)</span>. This enables us to focus on finite sets <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> without worrying about the infinite size of <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>For convenience, we use <span class="math inline">\(\underset{S}{\Pr}[\cdot]\)</span> (<span class="math inline">\(\underset{S&#39;}{\Pr}[\cdot]\)</span>) as shorthand for <span class="math inline">\(\underset{S \sim \mathcal{D}^n}{\Pr}[\cdot]\)</span> (<span class="math inline">\(\underset{S&#39; \sim \mathcal{D}^n}{\Pr}[\cdot]\)</span>). The road map of our proof is as follows.</p>
<blockquote>
<p>Lemma 1.<br />
<span class="math display">\[
\Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \le 2 \max_{S, S&#39; \in \mathcal{Z}^n } \Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ]
\]</span></p>
</blockquote>
<p>On the left hand side of the inequality, the probability measures the event of a random set <span class="math inline">\(S\)</span> sampling from <span class="math inline">\(\mathcal{D}\)</span>. On the right hand side, we can pick a fixed pair of <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> that maximize <span class="math display">\[
\Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ],
\]</span></p>
<p>and the probability measures the event of the random swap <span class="math inline">\(\sigma\)</span>. Fixing <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> significantly simplifies the structure of <span class="math inline">\(\mathcal{H}\)</span>. By Hoeffding inequality and union bound, we will prove that</p>
<blockquote>
<p>Lemma 2. For any fixed pair of <span class="math inline">\(S, S&#39; \in \mathcal{Z}^n\)</span>, <span class="math display">\[
\Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ] \le 2 \Pi_\mathcal{H} (2n) \exp( - \frac{n^2 \epsilon^2 }{2} )
\]</span></p>
</blockquote>
<h3 id="proof-of-lemma-1."><strong><em>Proof of Lemma 1.</em></strong></h3>
<p>The proof of Lemma 1 consists of three steps.</p>
<h4 id="step-1"><strong>S<em>tep 1</em></strong></h4>
<p>We will prove that <span class="math display">\[
    \Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \le 2 \Pr_{S , S&#39;} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2)  ].
\]</span></p>
<p><em>Proof of Step 1.</em><br />
By Hoeffding inequality, for a fixed <span class="math inline">\(h \in \mathcal{H}\)</span>, when <span class="math inline">\(n \ge \frac{2}{\epsilon^2} \ln 4\)</span> <span class="math display">\[
    \Pr_{S&#39;} [ L_{S&#39;} (h) \notin B(\mu_h, \epsilon / 2) ] \le 2 \exp(- 2 n (\frac{\epsilon }{ 2 })^2 ) = 2 \exp( - \frac{n \epsilon^2}{2} ) \le \frac{1}{2}
\]</span></p>
<p>It follows that <span class="math display">\[
    \Pr_{S , S&#39;} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2)  ] 
        \ge 
    \frac{1}{2} \Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ]
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="step-2"><strong><em>Step 2</em></strong></h4>
<p>Observe that <span class="math display">\[
    \{  \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2)  \} 
        \subset 
    \{  \exists h : |L_S (h) - L_{S&#39;} (h) | \ge \frac{\epsilon}{2} \} 
\]</span></p>
<p>By monotonicity of probability, we get <span class="math display">\[
    \Pr_{S} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2) ] 
        \le 
    \Pr_{S, S&#39;} [ \exists h : |L_S (h) - L_{S&#39;} (h) | \ge \frac{\epsilon}{2} ]
\]</span></p>
<h4 id="step-3"><strong><em>Step 3</em></strong></h4>
<p>Finally, <span class="math display">\[
    \Pr_{S, S&#39; } \{  \exists h : |L_S (h) - L_{S&#39;} (h) | \ge \frac{\epsilon}{2} \} 
        \le 
    \max_{S, S&#39; \in \mathcal{Z}^n } \Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ]
\]</span></p>
<p><em>Proof of Step 3.</em><br />
We claim that <span class="math inline">\(\sigma S\)</span> and <span class="math inline">\(\sigma S&#39;\)</span> have the same joint distribution as <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> (if <span class="math inline">\(S, S&#39; \sim \mathcal{D}^n\)</span>). For points <span class="math inline">\(\forall Z, Z&#39; \in \mathcal{Z}^n\)</span>, by symmetry, it holds that <span class="math display">\[
\Pr_{S, S&#39;} [ S = Z, S&#39; = Z&#39;] = \Pr_{S, S&#39;, \sigma} [ \sigma S = Z, \sigma S&#39; = Z&#39;]
\]</span></p>
<p>The right hand side can be viewed as the successful probability of the following experiment:</p>
<ul>
<li>Sample independently <span class="math inline">\(S \sim \mathcal{D}^n\)</span> and <span class="math inline">\(S&#39; \sim \mathcal{D}^n\)</span>.<br />
</li>
<li>For each <span class="math inline">\(i \in [n]\)</span>, exchange the <span class="math inline">\(i\)</span>-th elements of <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> independently with probability <span class="math inline">\(0.5\)</span>.</li>
<li>After the random swap, <span class="math inline">\(\sigma S = Z\)</span> and <span class="math inline">\(\sigma S&#39; = Z&#39;\)</span>.</li>
</ul>
<p>Now, <span class="math inline">\(\forall \mathcal{E} \subset \mathcal{Z}^{2n}\)</span>, it holds that <span class="math display">\[
\begin{aligned}
    \Pr_{S, S&#39;} [ (S, S&#39;) \in \mathcal{E} ] 
        &amp;= \Pr_{S, S&#39;, \sigma} [ (\sigma S, \sigma S&#39;) \in \mathcal{E} ] \\
        &amp;= \mathbb{E}_{S, S&#39;} [ \Pr_\sigma [(\sigma S, \sigma S&#39;) \in \mathcal{E} ] \mid S, S&#39; ] \\
        &amp;\le \max_{S, S&#39; \in \mathcal{Z}^n } \Pr_\sigma [(\sigma S, \sigma S&#39;) \in \mathcal{E} ]
\end{aligned}
\]</span></p>
<p>Replacing <span class="math inline">\(\mathcal{E}\)</span> with the set <span class="math inline">\(\{Z, Z&#39; \in \mathcal{Z}^n : \exists h : |L_Z (h) - L_{Z&#39;} (h) | \ge \frac{\epsilon}{2} \}\)</span> finishes the proof.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><em>Remark of the proof of step 1.</em> If we define <span class="math display">\[
\mathcal{H}(S, \epsilon) \doteq \{ h \in \mathcal{H} :L_S(h) \notin B(\mu_h, \epsilon ) \}.
\]</span> <em>which is the set of hypothesis whose empirical loss on <span class="math inline">\(S\)</span> is <span class="math inline">\(\epsilon\)</span> more than its expectation. Then,</em> <span class="math display">\[
\begin{aligned}
    \Pr_{S} [ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \cdot \frac{1}{2} 
    &amp;= \Pr_{S } [ \mathcal{H}(S, \epsilon) \neq \emptyset ] \cdot \frac{1}{2} \\
    &amp;\le \Pr_{S } [ \mathcal{H}(S, \epsilon) \neq \emptyset] \\
    &amp; \ \ \cdot \Pr_{S, S&#39; } [ \exists h \in \mathcal{H}(S, \epsilon) : L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2 ) \mid \mathcal{H}(S, \epsilon) \neq \emptyset ] \\
    &amp;= \Pr_{S, S&#39; } [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2 )  ]
\end{aligned}
\]</span></p>
<p><em>Note that the event <span class="math inline">\(\mathcal{H}(S, \epsilon) \neq \emptyset\)</span> is equivalent to the one</em> <span class="math display">\[
\cup_{h \in \mathcal{H} } \{ L_S(h) \notin B(\mu_h, \epsilon ) \} 
\]</span></p>
<p><em>For a fixed <span class="math inline">\(h \in \mathcal{H}\)</span>, the event <span class="math inline">\(\{ L_S(h) \notin B(\mu_h, \epsilon ) \}\)</span> is measurable when <span class="math inline">\(S\)</span> consists of a finite number of i.i.d samples from <span class="math inline">\(\mathcal{D}\)</span>. If <span class="math inline">\(\mathcal{H}\)</span> consists of countable number of <span class="math inline">\(h\)</span>, then <span class="math inline">\(\cup_{h \in \mathcal{H} } \{ L_S(h) \notin B(\mu_h, \epsilon ) \}\)</span> is a countable union and should be measurable.</em></p>
<!-- *Definition*.

 1. $\Gamma_n:$ the set of permutations on $\{1, 2, ..., 2n \}$ that swaps only $i$ and $i + n$ for $\forall i \in [n]$,  s.t., $\forall \sigma \in \Gamma_n$,  $\forall i \in [n]$, 
    $$
    \begin{aligned}
        \text{ either }
        \begin{cases}
            \sigma(i) = i  \\
            \sigma(i + n) = i + n
        \end{cases}
        \text{ or }
        \begin{cases}
            \sigma(i) = i + n \\
            \sigma(i) = i 
        \end{cases}.
    \end{aligned}   
    $$

As $\sigma$ is an one-to-one mapping, its inversion $\sigma^{-1}$ exists. Further, $\sigma^{-1}$ is also a permutation in $\Gamma_n$. 

Let $S \sim \mathcal{D}^{2n}$ and denote it as 
$$
S \doteq \{ (x_1, y_1), ..., (x_i, y_i), ..., (x_{2n}, y_{2n} ) \}.
$$

Applying a permutation $\sigma$ on $S$, we obtain
$$
\sigma S \doteq \{ (x_{ \sigma^{-1}(1) }, y_{ \sigma^{-1}(1) } ), ..., (x_{ \sigma^{-1}(i) }, y_{ \sigma^{-1}(i) } ), ...., (x_{ \sigma^{-1}(2n) }, y_{ \sigma^{-1}(2n) } )\}
$$

Observe that $\forall i \in [n]$, $(x_{ \sigma^{-1}(i) }, y_{ \sigma^{-1}(i) } )$ is sampled independently from $\mathcal{D}$. It follows that $\sigma S$ has the same distribution as $S$. 

***Definition.***

1. $\forall \mathcal{E} \subset \mathcal{Z}^{2n}, \forall S \in \mathcal{Z}^{2n}$, define $\mathbb{1}_\mathcal{E} (S)$ the indicator function of whether $S$ belongs to $\mathcal{E}$. 

> Corollary. $\forall \sigma \in \Gamma_n, \forall \mathcal{E} \subset \mathcal{Z}^{2n}$, it holds that 
> $$
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ] = \Pr_{S \sim \mathcal{D}^{2n}} [ \sigma S \in \mathcal{E} ] = \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ]
> $$

If we choose an $\sigma$ uniformly from $\Gamma_n$, then 
$$
\begin{aligned}
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ]  
        &= \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &= \frac{1}{ |\Gamma_n| } \sum_{\sigma \in \Gamma_n } \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &=  \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \frac{1}{ |\Gamma_n| } \sum_{\sigma \in \Gamma_n } \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &=  \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \Pr_\sigma [\sigma S \in \mathcal{E} ] ]. \\
\end{aligned}
$$

> Corollary 2. $\forall \mathcal{E} \subset \mathcal{Z}^{2n}$, it holds that 
> $$
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ] \le \max_{S \in \mathcal{Z}^{2n} } \Pr_\sigma [\sigma S \in \mathcal{E} ]
> $$

***Definition.*** For $S \in \mathcal{Z}^{2n}$, define 
$$
S_1 \doteq \{ (x_1, y_1), ..., (x_n, y_n) \}
$$
as the first half of $S$ and 
$$
S_2 \doteq \{ (x_{n + 1}, y_{n + 1}), ..., (x_{ 2n } , y_{ 2n }) \}
$$
as the second half. In a similar manner we define $\sigma S_1$ and $\sigma S_2$ as the first and second half of $\sigma S$ respectively. 

It follows from Corollary 2 that 

The corollary allows us to upper bond the failure probability on a fixed $S$ that maximizes $\Pr_\sigma [ \exists h : |L_{ \sigma S_1} (h) - L_{ \sigma S_2 } (h) | \ge \frac{\epsilon}{2} ]$, and now the randomness comes only from the uniform choice of $\sigma$. This is important as there are finitely many of functions defined on finite set $S$, on which we could apply union bound.  -->
<h3 id="proof-of-lemma-2."><strong><em>Proof of Lemma 2.</em></strong></h3>
<p>Let <span class="math display">\[
S = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \}
\]</span> and <span class="math display">\[
S&#39; = \{ (x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;), ..., (x_n&#39;, y_n&#39;) \}.
\]</span> be a pair of fixed sample sets. Define the set <span class="math inline">\(\mathcal{C} = \{ x_1, x_2, ..., x_n, x_1&#39;, x_2&#39;, ..., x_n&#39; \} \subset \mathcal{X}\)</span> (with duplicates removed). The size of <span class="math inline">\(\mathcal{C}\)</span> is bounded by <span class="math inline">\(2n\)</span> and therefore the size of restriction of <span class="math inline">\(\mathcal{H}\)</span> on <span class="math inline">\(\mathcal{C}\)</span> is bounded by <span class="math inline">\(\Pi_\mathcal{H} (2n)\)</span>.</p>
<p>First, fix a hypothesis <span class="math inline">\(h\)</span>. For all <span class="math inline">\(i \in [n]\)</span>, define <span class="math inline">\(a_i = |\ell( h( x_i ), y_i) - \ell( h( x_i&#39; ), y_i&#39; ) | \le 1\)</span> and let <span class="math inline">\(V_i \in \{-1, 1\}\)</span> be a random variable with equal probability: <span class="math display">\[
\Pr[ V_i = -1 ] = \Pr[ V_i = 1] = 0.5
\]</span></p>
<p>If we apply a random swap <span class="math inline">\(\sigma\)</span> to <span class="math inline">\((S, S&#39;)\)</span>, then <span class="math inline">\(L_{ \sigma S} (h_S) - L_{ \sigma S&#39; } (h_S)\)</span> has the same distribution as <span class="math inline">\(\sum_{i \in [n] } \frac{1}{n} a_i V_i\)</span>. Since <span class="math display">\[
\mathbb{E} [\sum_{i \in [n] } \frac{1}{n} a_i V_i] = \sum_{i \in [n] } \frac{1}{n} a_i \mathbb{E}[ V_i ] = 0,
\]</span></p>
<p>by Hoeffding inequality, <span class="math display">\[
\begin{aligned}
    \Pr_\sigma [ |L_{ \sigma S} (h) - L_{ \sigma  S&#39; } (h) | \ge \frac{\epsilon}{2} ] 
        &amp;= \Pr[ | \sum_{i \in [n] } \frac{1}{n} a_i V_i - 0 | \ge \frac{\epsilon}{2} ] \\
        &amp;\le 2 \exp( - \frac{ 2 }{  \sum_{i \in [n] } a_i^2 } (\frac{n \epsilon}{2})^2 )  \\
        &amp;\le 2 \exp( - \frac{n^2 \epsilon^2 }{2} )
\end{aligned}
\]</span></p>
<p>The last inequality follows from that <span class="math inline">\(\sum_{i \in [n] } a_i^2 \le n\)</span>.</p>
<p>Since for each <span class="math inline">\(h \in \mathcal{H}\)</span>, it has the same behavior on <span class="math inline">\(\mathcal{C}\)</span> as some function in <span class="math inline">\(\mathcal{H}_\mathcal{C}\)</span>. We can apply union bound on <span class="math inline">\(\mathcal{H}_\mathcal{C}\)</span>. <span class="math display">\[
\Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma  S&#39; } (h) | \ge \frac{\epsilon}{2} ] \le 2 |\mathcal{H}_\mathcal{C} | \exp( - \frac{n^2 \epsilon^2 }{2} )\le 2 \Pi_\mathcal{H} (2n) \exp( - \frac{n^2 \epsilon^2 }{2} )
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] Ethan Fetaya, "Lecture 02 - Introduction to Statistical Learning Theory", Weizmann Institute of Science, 2016<br />
[2].R. Schapire and D. Bieber, “Lecture 05 - COS 511: Theoretical Machine Learning,”, Princeton University, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/13/Advanced-Composition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/13/Advanced-Composition/" class="post-title-link" itemprop="url">Advanced Composition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-13 22:26:07" itemprop="dateCreated datePublished" datetime="2020-11-13T22:26:07+11:00">2020-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-15 22:41:53" itemprop="dateModified" datetime="2020-11-15T22:41:53+11:00">2020-11-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Once we have designed some basic differentially private algorithms, it is a natural idea to combine them and analysis the privacy loss. We begin with an illustrative example that sets up the mathematical model step by step.</p>
<p>Image yourself in front of the door of a safe vault protected by a password lock. To open the door, you need the correct password <span class="math inline">\(P\)</span>. If tried with the wrong password, the lock would destroy itself automatically.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/door.png?raw=true" width="400" height="340" /></p>
</div>
<p>Luckily, you know two candidate passwords <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span>, with one of them being the correct one. Further, you notice that the designer of the lock left a collection of boxes near the door, which contain information on how they decide the correct passwords. Obtaining complete information of anyone of them gives you the correct password.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box1.png?raw=true" width="400" height="340" /></p>
</div>
<p>But the boxes are also protected and you don't have legally access to them. However, you can hack into the boxes. Hacking into the box won't give you all its information, but a random message. In particular, each box <span class="math inline">\(B\)</span> is associated with a set <span class="math inline">\(\mathcal{R}_B\)</span>, which is a finite collection of messages (in English). When <span class="math inline">\(B\)</span> is hacked, it returns a message <span class="math inline">\(Y_B\)</span> generated randomly from <span class="math inline">\(\mathcal{R}_B\)</span>, whose distribution depends on the correct password <span class="math inline">\(P\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> be the distribution of <span class="math inline">\(Y_B\)</span> if the correct password is <span class="math inline">\(P_1\)</span>, and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> be the one if the correct password is <span class="math inline">\(P_2\)</span>. If there is a huge difference between <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>, then you might be able to guess the correct password.</p>
<p>E.g., suppose <span class="math inline">\(\mathcal{R}_B =\)</span> { "Dog bites.", "Cat scratches." } and the distributions are given as</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_1}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">"Dog bites."</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="even">
<td style="text-align: center;">"Cat scratches."</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.99</td>
</tr>
</tbody>
</table>
<p>Hence, when you get <span class="math inline">\(Y_B=\)</span> "Dog bites.", you prefer <span class="math inline">\(P_1\)</span> over <span class="math inline">\(P_2\)</span> and vice versa.</p>
<p>Anticipating such potential information leakage, the designer of the lock equips the boxes with a defense mechanism, called <span class="math inline">\((\epsilon, 0)\)</span>-mechanism. It guarantees the distributions <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are similar so that it is hard for you to infer the correct password from the message. In particular, <span class="math inline">\(\forall S \subset \mathcal{R}_B\)</span>, if <span class="math inline">\(S\)</span> is measurable, it holds that <span class="math display">\[
\Pr[ Y_B \in S \ | \ P = P_1 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_2 ] \\
\Pr[ Y_B \in S \ | \ P = P_2 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_1 ] \\
\]</span></p>
<p>When these inequalities are satisfied, we say that <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close. The inequalities require <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> to have the same support on <span class="math inline">\(\mathcal{R}_B\)</span>. Therefore, throughout our discussion below, we assume that both <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> assign positive probability to each element in <span class="math inline">\(\mathcal{R}_B\)</span>. Otherwise, we can just replace <span class="math inline">\(\mathcal{R}_B\)</span> with the support of <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> (which is also the support of <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>).</p>
<p>Now, hacking one box is unlikely to help you to guess the correct password. You want to hack more boxes, with the hope that the information combined will assist you. Due to resource limit, you can't hack all boxes but only a finite number of them. You choose the first box randomly. Then you choose every new box based on the information obtained from the hacked boxes. The figure below shows an example of hacking five boxes.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box2.png?raw=true" width="400" height="340" /></p>
</div>
<p>Suppose that your resource enables you to hack <span class="math inline">\(n\)</span> boxes and let <span class="math inline">\(\vec Y_n = (Y_1, Y_2, ..., Y_n)\)</span> be the output messages you obtained. Similar to the situation of hacking one box, if the distribution of <span class="math inline">\(\vec Y_n\)</span>, conditioned on <span class="math inline">\(P = P_1\)</span>, is utterly distant from that conditioned on <span class="math inline">\(P = P_2\)</span>, then there could be some cases when you can confidently infer the true password. Conversely, to prevent severe information leakage, the designer needs to ensure there isn't such case, i.e., the two distributions should be similar.</p>
<p>What makes things even more complicated is that, the distribution of <span class="math inline">\(\vec Y_n\)</span> depends not only on the output distributions of the boxes, but also on your strategy of choosing the boxes to hack. Let's use symbol <span class="math inline">\(A\)</span> to denote your strategy. Whatever <span class="math inline">\(A\)</span> is, the designer need to guarantee that the distribution of <span class="math inline">\(\vec Y_n\)</span> conditioned <span class="math inline">\(P = P_1\)</span> should be similar to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Surprisingly, this is in some sense achievable, as long as for each box, its output distribution conditioned on <span class="math inline">\(P = P_1\)</span> is <span class="math inline">\((\epsilon, 0)\)</span> close to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{R}\)</span> be the set of possible possible messages of all boxes.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> <span class="math inline">\(\forall A\)</span>, <span class="math inline">\(\forall S \subset \mathcal{R}^n\)</span>, it holds that <span class="math inline">\(\forall \delta&#39; \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ \vec Y_n \in S \ | \ P = P_1, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; \\
\Pr[ \vec Y_n \in S \ | \ P = P_2, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_1, A] + \delta&#39;
\]</span> where <span class="math inline">\(\epsilon&#39; = k\epsilon(e^\epsilon - 1) + \epsilon \sqrt{2 n \log \frac{1}{\delta&#39;} }\)</span>.</p>
</blockquote>
<p>If we view <span class="math inline">\(\epsilon\)</span> as the privacy loss of a single box, then the theorem states that the privacy loss grows to <span class="math inline">\(O(\sqrt {n} \epsilon )\)</span> is <span class="math inline">\(n\)</span> boxes are hacked.</p>
<p>To prove the theorem, we need a rigorous model for the problem.</p>
<p><strong><em>Definitions.</em></strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathcal{I}\)</span>: the index set.</p></li>
<li><p><span class="math inline">\(\{ B_\alpha : \alpha \in \mathcal{I} \}\)</span>: the collection of boxes.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{R}_{\alpha} : \alpha \in \mathcal{I} \}\)</span>: the ranges of the outputs of the boxes.</p></li>
<li><p><span class="math inline">\(\mathcal{R} \doteq \cup_{\alpha \in \mathcal{I} } R_\alpha\)</span>: the range of any possible output by any box.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{D}_{\alpha, P_1} : \alpha \in \mathcal{I} \}\)</span> (<span class="math inline">\(\{ \mathcal{D}_{\alpha, P_2} : \alpha \in \mathcal{I} \}\)</span>): the set of output distribution when the correct password is <span class="math inline">\(P_1\)</span> (<span class="math inline">\(P_2\)</span>). Without loss of generality, we assume that for a fixed <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, the distribution <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> (<span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span>) assigns positive probability to each element in <span class="math inline">\(\mathcal{R}_\alpha\)</span>. Moreover, <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close.</p></li>
<li><p><span class="math inline">\(n\)</span>: the number of boxes you can hack.</p></li>
<li><p><span class="math inline">\(\vec i_k \doteq (i_1, i_2, ..., i_k)\)</span>: the index sequence of boxes you have chosen to hack up to time <span class="math inline">\(k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, <span class="math inline">\(\vec i_0\)</span> corresponds to a empty sequence.</p></li>
<li><p><span class="math inline">\(\vec Y_k = (Y_1, Y_2, ..., Y_k):\)</span> the random variables that represent messages outputted by the chosen boxes up to time <span class="math inline">\(k \in [1, n]\)</span>, where <span class="math inline">\(Y_t \in \mathcal{R}_{i_t} \subset \mathcal{R}\)</span> for <span class="math inline">\(t \in [1, k]\)</span>. Therefore, <span class="math inline">\(\vec Y_k\)</span> can be view as random vector in <span class="math inline">\(\mathcal{R}^k\)</span>.</p></li>
<li><p><span class="math inline">\(\vec x_k = (x_1, x_2, ..., x_k) \in \mathcal{R}^k:\)</span> a point in <span class="math inline">\(\mathcal{R}^k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, we define <span class="math inline">\(\vec x_0 = \emptyset\)</span>.</p></li>
<li><p><span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt; = \left&lt; (i_1, i_2, i_3, ..., i_k), (x_1, x_2, ..., x_k) \right&gt;:\)</span> the history up to <span class="math inline">\(k \in [0, n]\)</span>, which consists of the chosen indexes and observations up to time <span class="math inline">\(k\)</span>. We use <span class="math inline">\(\vec h_0\)</span> to denote the empty history.</p></li>
<li><p><span class="math inline">\(A:\)</span> your strategy (policy) for choosing boxes. It works as follows: for any fixed history <span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt;\)</span>, A is associated with a fixed distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span> over <span class="math inline">\(\mathcal{I} \setminus \vec i_k\)</span>, the set of indexes of the unchosen boxes. When inputted with <span class="math inline">\(\vec h_k\)</span>, <span class="math inline">\(A\)</span> returns a random variable <span class="math inline">\(A(\vec h_k)\)</span> that follows the distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span>.</p></li>
</ol>
<p>We will show that, no matter what strategy you use, it is unlikely that you distinguish via the output <span class="math inline">\(\vec Y_n\)</span> whether the correct password is <span class="math inline">\(P_1\)</span> or <span class="math inline">\(P_2\)</span>. This is because whether <span class="math inline">\(P = P_1\)</span> or not, the output distributions of <span class="math inline">\(\vec Y_n\)</span> are similar.</p>
<p><strong><em>Proof of the theorem.</em></strong> We will just prove the first inequality, and the second one follows from symmetry. We consider a bad set in <span class="math inline">\(\mathcal{R}^n\)</span>: <span class="math display">\[
\mathcal{W} \doteq \{ \vec x_n \in \mathcal{R}^n : \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] \ge e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] \}
\]</span></p>
<p>and will use the following lemma.</p>
<blockquote>
<p><strong>Lemma.</strong> <span class="math inline">\(\Pr[ \vec Y_n \in \mathcal{W}\ | \ P = P_1, A] \le \delta&#39;\)</span>.</p>
</blockquote>
<p>Hence, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n \in S \ | \ P = P_1, A] 
        &amp;=  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in S \cap \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \delta&#39; \\
        &amp;= \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] + \delta&#39; \\
        &amp;&lt; \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } e^{\epsilon&#39;} \cdot  \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] + \delta&#39; \\
        &amp;=  e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; 
\end{aligned}
\]</span></p>
<p>The first inequality follows from monotonicity of probability, the second one from the lemma, and the final one from the definition of <span class="math inline">\(\mathcal{W}\)</span>.</p>
<p><em>Proof of the lemma.</em> To prove the lemma, we need only to consider those point <span class="math inline">\(\vec x_n\)</span> with <span class="math display">\[
\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0. 
\]</span> Otherwise, <span class="math inline">\(\vec x_n\)</span> contributes to 0 probability to the set <span class="math inline">\(\mathcal{W}\)</span> (whether it belongs to <span class="math inline">\(\mathcal{W}\)</span> or not).</p>
<p>Now, we expand the probability <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A]\)</span>. To obtain the <span class="math inline">\(k\)</span>-th output, there are two steps</p>
<ol type="1">
<li><span class="math inline">\(A\)</span> generates a index <span class="math inline">\(i_k\)</span> of a box based on the known history <span class="math inline">\(\vec h_{k - 1}\)</span>.<br />
</li>
<li>The box <span class="math inline">\(B_{i_k}\)</span> is hacked, and output a random message <span class="math inline">\(x_k\)</span> sampled from its distribution <span class="math inline">\(D_{i_k, P_1}\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(\vec H_k\)</span> be a random variable that represents the history up to <span class="math inline">\(k\)</span>. By chain rule, we have <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A  ]
\end{aligned}
\]</span></p>
<p>By <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0\)</span>, each term in the expansion are positive. Similarly, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A  ]
\end{aligned}
\]</span></p>
<p>As both <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> assign positive probability to each point in <span class="math inline">\(\mathcal{R}_{i_k}\)</span>, we know <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] &gt; 0\)</span>.</p>
<p>We are ready to consider the ratio: <span class="math display">\[
\begin{aligned}
    \ln \frac{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] }{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] } 
    = \sum_{k = 1}^n \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }. 
\end{aligned}
\]</span></p>
<p>If we replace <span class="math inline">\(x_k\)</span> by a random variable <span class="math inline">\(X_k \sim \mathcal{D}_{i_k, P_1}\)</span>, then <span class="math display">\[
C_k \doteq \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }
\]</span></p>
<p>is a random variable. As <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close, we have <span class="math display">\[
C_k \le \epsilon.
\]</span></p>
<p>Further, we have</p>
<blockquote>
<p><strong>Fact 1.</strong> For any <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, it holds that <span class="math display">\[
\begin{aligned}
\mathbb{E}_{ X \sim \mathcal{D}_{\alpha, P_1} } \left[ \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = X]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = X]  }  \right] 
&amp;= \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;+ 
\sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } \\
&amp;= \sum_{x \in \mathcal{R}_\alpha } \left[ \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  - \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x] \right] \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le (e^\epsilon - 1) \epsilon
\end{aligned}
\]</span></p>
</blockquote>
<p>Therefore, <span class="math inline">\(\mathbb{E} [C_k] \le (e^\epsilon - 1) \epsilon\)</span>.</p>
<blockquote>
<p><strong>Fact 2.</strong> (<strong>Azuma Inequality</strong>). Let <span class="math inline">\(C_1, ...., C_n\)</span> be random variables such that <span class="math inline">\(\forall k \in [n]\)</span>, <span class="math inline">\(\Pr[ |C_k| \le \epsilon ] = 1\)</span>, and for every <span class="math inline">\((c_1, ..., c_{k -1} ) \in \text{Supp} (C_1, ..., C_{k - 1} )\)</span>, we have <span class="math display">\[
\mathbb{E}[ C_i \mid C_1 = c_1, ..., C_{k -1} = c_{k - 1} ] \le \beta,
\]</span> Then for every <span class="math inline">\(z &gt; 0\)</span>, we have <span class="math display">\[
\Pr[ \sum_{k = 1}^n C_i -  n \beta &gt;  z] \le \exp(- \frac{z^2}{ 2 n\epsilon^2 } )
\]</span></p>
</blockquote>
<p>Finally, applying <em>Azuma Inequality</em> with <span class="math inline">\(z = \sqrt{ 2n \log \frac{1}{\delta&#39;} }\)</span> and <span class="math inline">\(\beta = (e^\epsilon - 1) \epsilon\)</span>, we get the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference.">Reference.</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/12/PAC-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/12/PAC-learning/" class="post-title-link" itemprop="url">PAC Learning Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-12 16:06:55" itemprop="dateCreated datePublished" datetime="2020-11-12T16:06:55+11:00">2020-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-19 15:16:12" itemprop="dateModified" datetime="2020-11-19T15:16:12+11:00">2020-11-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss a toy model of probably approximately correct (PAC) learning [1]. The setting focuses on a <span class="math inline">\(d\)</span>-dimension Boolean hypercube <span class="math inline">\(\{0, 1\}^d\)</span> and a set of functions <span class="math inline">\(\mathcal{H} = \{ h : \{0, 1\}^d \rightarrow \{0, 1 \} \}\)</span>. A function <span class="math inline">\(h \in \mathcal{H}\)</span> is called a hypothesis or a concept, which assigns each vertex in the hypercube a label 0 or 1.</p>
<p>The goal of PAC learning is to learn an unknown hypothesis, denoted as <span class="math inline">\(h^* \in \mathcal{H}\)</span>, from a dataset, which consists of <span class="math inline">\(n\)</span> points <span class="math inline">\(X_1, X_2, ..., X_n \in \{0, 1\}^n\)</span> sampled independently from an unknown distribution <span class="math inline">\(D\)</span> (over the hypercube), as well as their labels <span class="math inline">\(Y_1 = h^*(X_1), Y_2 = h^*(X_2), ..., Y_n = h^*(X_n)\)</span>.</p>
<p>We can state the algorithm for PAC learning in just one sentence.</p>
<blockquote>
<p>Output any hypothesis <span class="math inline">\(h&#39; \in \mathcal{H}\)</span> that is consistent with the sampled dataset, i.e., <span class="math display">\[
h&#39;(X_i) = Y_i, \qquad \forall i \in [n]
\]</span></p>
</blockquote>
<p>Such an hypothesis always exists, as <span class="math inline">\(h^*\)</span> satisfies this constraint. We have the classic PAC learning theorem.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The PAC learning theorem states that the probability that <span class="math inline">\(h&#39;\)</span> mislabels a point is bounded by <span class="math inline">\(\sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}\)</span>.</p>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\mathbb{E}[Z_i]\)</span> is an unbiased estimator of <span class="math inline">\(\mu_h\)</span>. Further, denote <span class="math inline">\(\hat \mu_h\)</span> the empirical mean of <span class="math inline">\(Z_i\)</span>'s <span class="math display">\[
\hat \mu_h \doteq \frac{1}{n} \sum_{i \in [n] } Z_i
\]</span></p>
<p>Note that for the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(\hat \mu_{h&#39;} = 0\)</span>.</p>
<p>By Hoeffding inequality, for any <span class="math inline">\(\delta&#39; &gt; 0\)</span>, <span class="math display">\[
\Pr[ \hat \mu_h \le \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} } ] \le \delta&#39;. 
\]</span></p>
<p>By union bound, the probability that <span class="math display">\[
\exists h \in \mathcal{H}, \hat \mu_h \ge \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} }
\]</span></p>
<p>is bounded by <span class="math inline">\(| \mathcal{H} | \delta&#39;\)</span>. By setting <span class="math inline">\(\delta&#39; = \frac{\delta}{ | \mathcal{H} | }\)</span>, it is guaranteed that <span class="math display">\[
0 = \hat \mu_{h&#39; } \le \mu_{h&#39;} -  \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span></p>
<p>with probability at most <span class="math inline">\(\delta\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>The previous theorem can be improved with the <span class="math inline">\(\sqrt{ \cdot }\)</span> removed. The key here is that the selected <span class="math inline">\(h&#39;\)</span> is error-free on the samples.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>We will prove that if <span class="math inline">\(\mu_h \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}\)</span>, then <span class="math inline">\(h\)</span>'s probability of being selected is very low.</p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\Pr[Z_i = 1] = \mu_h\)</span>. Therefore, <span class="math display">\[
    \begin{aligned}
        \Pr[ Z_1 = 0 \wedge Z_2 = 0 \wedge ... \wedge Z_n = 0] = (1 - \mu_h)^n \le \exp(- \mu_h n) = \frac{\delta}{ |\mathcal{H} | }
    \end{aligned}
\]</span></p>
<p>Taking union bound over all possible <span class="math inline">\(h\)</span> gives the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Caveat.</em></strong> <em>In both theorem, we see a <span class="math inline">\(\log \mathcal{H}\)</span> term in the failure probability, which is due to the use of union bound and can be roughly considered as the complexity of the hypothesis family. It is tempting to think that this could be avoid as follows.</em></p>
<blockquote>
<p>For the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(Z_1, ..., Z_n\)</span> are i.i.d samples. Therefore, the probability of all of them being <span class="math inline">\(0\)</span> is give by<br />
<span class="math display">\[
(1 - \mu_{h&#39;} )^n \le \exp( - \mu_{h&#39;} n). 
  \]</span><br />
Bounding this probability by <span class="math inline">\(\delta\)</span> gives <span class="math inline">\(\mu_{h&#39;} \le \frac{ \log \frac{1}{\delta} } {n}\)</span>.</p>
</blockquote>
<p><em>The argument is wrong. For the outputted <span class="math inline">\(h&#39;\)</span>, the <span class="math inline">\(Z_1, ..., Z_n\)</span> are not i.i.d samples, as <span class="math inline">\(h&#39;\)</span> is selected according to <span class="math inline">\(Z_1, ..., Z_n\)</span> and depends on them.</em></p>
<p><em>To make it more concrete, suppose that there is an <span class="math inline">\(h\)</span>, such that <span class="math inline">\(\mu_h = \frac{ \log \frac{1}{\delta} } {n}\)</span>. Consider the following experiment</em></p>
<blockquote>
<ol type="1">
<li>Sample <span class="math inline">\(n\)</span> points independently the distribution <span class="math inline">\(D\)</span>.<br />
</li>
<li>Pick an <span class="math inline">\(h&#39;\)</span> that makes no prediction error on the samples.</li>
</ol>
</blockquote>
<p><em>If we repeat the experiment <span class="math inline">\(N\)</span> times for some positive integer <span class="math inline">\(N\)</span>, then the fixed <span class="math inline">\(h\)</span> makes no mistake in roughly <span class="math inline">\((1 - \delta) N\)</span> of the experiments and constitutes an candidate of <span class="math inline">\(h&#39;\)</span>. However, on the other roughly <span class="math inline">\(\delta N\)</span> experiments, where <span class="math inline">\(h\)</span> makes prediction error, it is no longer considered as the candidate of <span class="math inline">\(h&#39;\)</span>. The selection procedure of <span class="math inline">\(h&#39;\)</span> ignores bad event of <span class="math inline">\(h\)</span> automatically.</em></p>
<p><em>We could also investigate the following example. Let <span class="math inline">\(h^* = h_0\)</span>. There are also other two hypothesis <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span>. Let <span class="math inline">\(S\)</span> be the sample space. It holds that</em> <span class="math display">\[
|h_1(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_1} \\
    0,\ \       \forall x \in S_{h_1}
\end{cases}
\]</span> <span class="math display">\[
|h_2(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_2} \\
    0,\ \       \forall x \in S_{h_2}
\end{cases}
\]</span> <em>Moreover, <span class="math inline">\(\Pr[S_{h_1}] = \Pr[S_{h_2} ] = \delta\)</span>. How, if we get a sample in <span class="math inline">\(S_{h_1}\)</span>, then we can output <span class="math inline">\(h&#39;\)</span> as <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_1\)</span>, since both of them make no error when <span class="math inline">\(x \in S_{h_1}\)</span>. Similarly, if <span class="math inline">\(x \in S_{h_2}\)</span>, we can output either <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_2\)</span>. We call <span class="math inline">\(S_{h_1}\)</span> and <span class="math inline">\(S_{h_2}\)</span> the bad areas. When <span class="math inline">\(x\)</span> is in the bad area of any hypothesis, we can't distinguish this hypothesis from the true hypothesis. That is why we need to use union bound to bound the total probabilities of these bad areas.</em></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/PAC-Learning.png?raw=true" width="400" height="340" /></p>
</div>
<h3 id="reference">Reference</h3>
<p>[1] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. 6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/43/">43</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">127</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
