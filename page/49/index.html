<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/49/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/49/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/09/10/Nonuniform%20Sampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/10/Nonuniform%20Sampling/" class="post-title-link" itemprop="url">Nonuniform Sampling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-10 20:38:54" itemprop="dateCreated datePublished" datetime="2016-09-10T20:38:54+10:00">2016-09-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2016-09-11 19:59:33" itemprop="dateModified" datetime="2016-09-11T19:59:33+10:00">2016-09-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose <span class="math inline">\(S = \{ s_1, s_2, ..., s_m\}\)</span> is a set of samples, with weight <span class="math inline">\(W = \{ w_1, w_2, ..., w_m\}\)</span> and probability <span class="math inline">\(P = \{ p_1, p_2, ..., p_m\}\)</span>.</p>
<p>Now we randomly select an element <span class="math inline">\(X\)</span> from <span class="math inline">\(S\)</span> according to probability <span class="math inline">\(P\)</span>. Define <span class="math inline">\(Y\)</span> the index <span class="math inline">\(X\)</span>. For example, if <span class="math inline">\(X = s_i\)</span>, then <span class="math inline">\(Y = i\)</span>. The expected weight of <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
E[w_Y] = \sum_{i \in [1,m]} w_i P( Y = i) = \sum_{i \in [1,m]} w_i p_i
\]</span></p>
<p>If <span class="math inline">\(P\)</span> is a uniform distribution, i.e., <span class="math inline">\(p_i = 1/m, \ \forall i \in [1,m]\)</span>, then <span class="math display">\[
E[w_Y] = 1 / m * \sum_{i \in [1,m]} w_i
\]</span></p>
<p>Then <span class="math inline">\(m E[w_Y]\)</span> equals just the sum of <span class="math inline">\(W\)</span>. We say that it is an <strong>unbiased estimator</strong> of the sum of <span class="math inline">\(W\)</span>. But what if <span class="math inline">\(P\)</span> is not uniform, i.e., <span class="math inline">\(\exists i,j \ s.t., p_i \neq p_j\)</span>.</p>
<p>One technique to remedy this is to construct a new variable <span class="math inline">\(Z_Y = w_Y / p_Y\)</span>.</p>
<p><span class="math display">\[
E[Z_Y] =  \sum_{i \in [1,m]} w_i / p_i * P(Y = i) = \sum_{i \in [1,m]} w_i
\]</span></p>
<p>The strange thing is that how we could know <span class="math inline">\(P = \{ p_1, p_2, ..., p_m\}\)</span>. If we know it in advance, we need not to do sampling to estimate $ _{i } w_i$.</p>
<p>But in some circumstance, it is possible. To get a random sample <span class="math inline">\(X\)</span> from <span class="math inline">\(S\)</span>, we need to go through some sampling process. Once <span class="math inline">\(X\)</span> is returned, we can calculate the probability of <span class="math inline">\(X\)</span> by examining the sampling process.</p>
<p>[FBKZ16] gives such an example. Consider two relations <span class="math inline">\(R_1 = (A_1, F)\)</span> and <span class="math inline">\(R_2 = (F, A_2)\)</span>, where <span class="math inline">\(R_1.F\)</span> and <span class="math inline">\(R_2.F\)</span> shares the same domain. Let <span class="math inline">\(R = R_1 \bowtie_{R_1.F = R_2.F} R_2\)</span> be the natural join result based on <span class="math inline">\(R_1.F\)</span> and <span class="math inline">\(R_2.F\)</span>. The object is to calculate the sum of attribute <span class="math inline">\(A_2\)</span> on <span class="math inline">\(R = (A_1, F, A_2)\)</span>.</p>
<p>The natural join can be expensive. Is there any way to estimate the join result? Indeed we have.</p>
<p>First sample uniformly at random a tuple <span class="math inline">\((a_1, f)\)</span> from <span class="math inline">\(R_1\)</span>. The probability of a particular tuple chosen is <span class="math inline">\(1/|R_1|\)</span>, where <span class="math inline">\(|R_1|\)</span> is the number of tuples in <span class="math inline">\(R_1\)</span>.</p>
<p>We further assume that there is an index on <span class="math inline">\(R_2.F\)</span>. So we immediately know the size of the set <span class="math inline">\(R_2[f] \doteq \{ (f, a_2) | (f, a_2) \in R_2 \}\)</span>. We choose uniformly at random a tuple <span class="math inline">\((f, a_2)\)</span> from <span class="math inline">\(R_2[f]\)</span> and get the join result <span class="math inline">\((a_1, f, a_2)\)</span>. The probability corresponding to <span class="math inline">\((a_1, f, a_2)\)</span> is <span class="math inline">\(1/ |R_1| * 1 / R_2[f]\)</span>. So <span class="math inline">\(\frac{a_2}{|R_1| |R_2[f]|}\)</span> is an unbiased estimator of <span class="math inline">\(sum(R.A2)\)</span></p>
<p><strong>Reference</strong><br />
[FBKZ16] Feifei. Li, Bin. Wu, Ke. Yi, Zhuoyue. Zhao, Wander Join: Online Aggregation via Random Walks. <em>SIGMOD'16</em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/08/29/SimRank/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/29/SimRank/" class="post-title-link" itemprop="url">SimRank</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-29 20:38:54" itemprop="dateCreated datePublished" datetime="2016-08-29T20:38:54+10:00">2016-08-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-19 21:34:01" itemprop="dateModified" datetime="2020-05-19T21:34:01+10:00">2020-05-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>SimRank</strong><br />
Given a directed graph <span class="math inline">\(G = \left&lt; V, E \right&gt;\)</span>, SimRank score is defined as follows <span class="math display">\[
s(i,j) = 
    \begin{cases}
        \begin{array}{ll}
        1, &amp;\text{ if } i = j \\
        \frac{c}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a,b), &amp;\text{otherwise}
        \end{array}
    \end{cases}
\]</span></p>
<p>where <span class="math inline">\(I(i)\)</span> is the set of in-neighbors of <span class="math inline">\(i \in V\)</span>, and <span class="math inline">\(c \in (0, 1)\)</span>.</p>
<p>By the definition of <em>SimRank</em>, <span class="math inline">\(s(i,j)\)</span> can be interpreted as the value of <span class="math display">\[
E[c^{T_{i,j} } ]
\]</span></p>
<p>where <span class="math inline">\(T_{i,j}\)</span> is the random variable of the first time that a reverse random walk (a random walk that travel along the incoming edges of a vertex randomly at each step) from <span class="math inline">\(i\)</span> and a reverse random walk from <span class="math inline">\(j\)</span> meets. Note that <span class="math display">\[
T_{i,j} = 
    \begin{cases}
        \begin{array}{ll}
        0, &amp;\text{ if } i = j \\
        \frac{1}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} T_{a,b}, &amp;\text{otherwise}
        \end{array}
    \end{cases}
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
E[c^{T_{i,j} } ] 
    &amp;= c^0 \cdot \Pr[T_{i,j} = 0] + \frac{1}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} E[c^{1 + T_{a, b}} ] \\
    &amp;= \mathfrak{1}_{i = j} +  \frac{c}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} E[c^{ T_{a, b}} ] \\
    &amp;= \mathfrak{1}_{i = j} +  \frac{c}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a, b) \\
\end{aligned}
\]</span></p>
<p>However, there is another random walk interpretation as follows:</p>
<ol type="1">
<li><p>First, notice that when <span class="math inline">\(i \neq j\)</span>, [TX16] rewrites the SimRank as <span class="math display">\[
s(i,j) = \frac{ \sqrt c \sqrt c} {|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a,b)
\]</span></p></li>
<li><p>Second, [TX16] defines the <span class="math inline">\(\sqrt c\)</span>-random walk as</p>
<ul>
<li>In any step, the walk stops with probability <span class="math inline">\(1 - \sqrt c\)</span>.</li>
<li>With the other <span class="math inline">\(\sqrt c\)</span> probability, the walk chose the in-neighbors of the current vertex uniformly at random.</li>
</ul></li>
<li><p>Finally <span class="math inline">\(s(i,j)\)</span> is interpreted as</p>
<blockquote>
<p>The probability of two <span class="math inline">\(\sqrt c\)</span> random walks ever meet, such that one of the random walks starts at vertex <span class="math inline">\(i\)</span> and the other starts at <span class="math inline">\(j\)</span>.</p>
</blockquote></li>
</ol>
<p><strong>Monte Carlo</strong><br />
A straightforward solution is to perform Monte Carlo simulation to calculate <span class="math inline">\(s(i,j)\)</span> -- we initialize two random walks from <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> respectively and simulate the two walks until they meet or either walk dies.</p>
<p>We need not to worry about that this process continues forever. Since the expected number of steps of a <span class="math inline">\(\sqrt c\)</span> random walk is given by <span class="math display">\[
\sqrt c + \sqrt c^2 + \sqrt c^3 + ...... = \frac{ \sqrt c}{ 1 - \sqrt c}
\]</span> which is a constant number.</p>
<p><strong>Chernoff Bound</strong><br />
We repeat this process <span class="math inline">\(n_w\)</span> times and use <span class="math inline">\(X_k\)</span> to indicate whether the two walks meet in the k-th trial, i.e., <span class="math inline">\(X_k = 1\)</span> if the two walks meet and <span class="math inline">\(0\)</span> otherwise. It is easy to see that <span class="math inline">\(\{ X_k \}\)</span> is a set of i.i.d random variables with <span class="math inline">\(E[ X_k ] = s(i, j)\)</span>.</p>
<p>Let <span class="math inline">\(\bar{X} \doteq \frac{1}{n_w} \sum_{k = 1}^{n_w} X_k\)</span> and <span class="math inline">\(\mu \doteq s(i,j)\)</span>, So by <em>Chernoff Bound</em>, <span class="math display">\[
P({| \bar{X} - \mu | &gt; (1 + \lambda) \mu }) \le 2 \exp(- \frac{n_w \mu \lambda^2}{3}) = \delta
\]</span> where <span class="math inline">\(\lambda\)</span> is relative error ratio and <span class="math inline">\(\delta\)</span> the error probability we can tolerate.</p>
<p><strong>Time Complexity</strong><br />
The previous section implies <span class="math inline">\(n_w = \frac{3 }{ \mu \lambda ^2}\log \frac{ 2 }{ \delta }\)</span>. By setting <span class="math inline">\(\epsilon = (1 + \lambda) \mu \Leftrightarrow \lambda = \frac{\epsilon}{\mu} - 1\)</span>, we get</p>
<p><span class="math display">\[
n_w = \frac{ 3 }{ \mu ( \frac{\epsilon}{\mu}^2 -  2\frac{\epsilon}{\mu}  + 1 ) }\log \frac{ 2 }{ \delta } 
= O( \frac{ \mu }{ \epsilon^2 }\log \frac{ 1 }{ \delta })
\]</span></p>
<p>Notice that when <span class="math inline">\(\epsilon\)</span> is small, say <span class="math inline">\(0.01\)</span>, <span class="math inline">\(\frac{1}{\epsilon^2}\)</span> is large thus can not be neglected. The problem remains whether we could reduce the number of random walks.</p>
<p><strong>Hitting probability</strong> Another acute observation from [TX16] is that if two walks reach the same vertex, their following behavior follows the same probability distribution (since the walks are memoryless).</p>
<p>Motivated by this, we define <span class="math inline">\(h_i^l(k)\)</span> as the probability such that a walk begins at vertex <span class="math inline">\(i\)</span> and reaches vertex <span class="math inline">\(j\)</span> at the l-th step. <span class="math inline">\(h_i^l(k)\)</span> can be updated iteratively using <span class="math display">\[
h_i^l(k) = \frac{ \sqrt c }{ |I(i) |  } \sum_{j \in I(i)} h_j^{l -1}(k)
\]</span></p>
<p><strong>SimRank and Hitting Probability</strong><br />
Now we set about bridging the gap between SimRank score and hitting probability.</p>
<p>Define <span class="math inline">\(E(i,j)\)</span> as the event that two random walks starting from i and j respectively ever meet. <span class="math inline">\(P(E(i,j)) = s(i,j)\)</span>.</p>
<p><span class="math inline">\(E(i, j, k, l)\)</span> denotes the event that two random walks, starting from <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> respectively, meet at <span class="math inline">\(l\)</span>-th step at <span class="math inline">\(k\)</span> and this is the last time they meet. In other word, these two walk never meet again after <span class="math inline">\(l\)</span>-th step at <span class="math inline">\(k\)</span>. It is easy to see that for different <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>, the <span class="math inline">\(E(i, j, k, l)\)</span>'s are mutually exclusive and constitute a partition of <span class="math inline">\(E(i,j)\)</span>,</p>
<p><span class="math display">\[
E(i,j) = \bigcup_l \bigcup_l E(i, j, k, l)
\]</span></p>
<p>Let <span class="math inline">\(d_k\)</span> be the probability that two walks from k do not meet each other after <span class="math inline">\(0\)</span>-th step. Then <span class="math inline">\(P(E(i, j, k, l)) = h_i^l(k) h_j^l(k) d_k\)</span>. Thus</p>
<p><span class="math display">\[
\begin{aligned}
P( E(i,j) ) &amp;= 
P(\bigcup_l \bigcup_l E(i, j, k, l))  \\
&amp;= \sum_{l = 0}^{\infty}  \sum_{k = 1}^{n} P(E(i, j, k, l)) \\
&amp;= \sum_{l = 0}^{\infty}  \sum_{k = 1}^{n} h_i^l(k) h_j^l(k) d_k
\end{aligned}
\]</span></p>
<p><strong>Computation</strong><br />
Notice that for a given <span class="math inline">\(i\)</span>, there infinity many <span class="math inline">\(h_i^l(k)\)</span>'s we need to kept, which is unfeasible. To get around this, we only keep the <span class="math inline">\(h_i^l(k)\)</span>'s greater than <span class="math inline">\(\epsilon_h\)</span>, where <span class="math inline">\(\epsilon_h = O(\epsilon)\)</span>.</p>
<p>The probability that the <span class="math inline">\(\sqrt c\)</span>-random walk does not to stop before <span class="math inline">\(l\)</span>-th step is <span class="math inline">\(\sqrt c ^l\)</span>, i.e., <span class="math display">\[
\sum_{k = 1}^n h_i^l(k) = \sqrt c^l
\]</span></p>
<p>For a fixed <span class="math inline">\(l\)</span>, at most <span class="math inline">\(\sqrt c ^l / \epsilon_h\)</span> number of <span class="math inline">\(h_i^l(k)\)</span>'s are kept. So the total number of <span class="math inline">\(h_i^l(k)\)</span>'s kept is bounded by <span class="math display">\[
\frac{1}{\epsilon_h} (\sqrt c + \sqrt c^2 + \sqrt c^3 + ......  )= O(\frac{1}{\epsilon_h}) = O(\frac{1}{\epsilon})
\]</span></p>
<p>On the other hand, estimation of <span class="math inline">\(d_k\)</span> could be done by monte carlo simulation. Similar to analysis in <strong>Chernoff Bound</strong> and <strong>Time Complexity</strong> section, we can show that with <span class="math inline">\(O \left( \frac{1}{\epsilon_d^2} \log \frac{1}{\delta_d} \right)\)</span> expected time, <span class="math inline">\(d_k\)</span> could be estimated by <span class="math inline">\(\bar{d_k}\)</span> such that <span class="math inline">\(| \bar{d_k} - d_k| &lt; \epsilon_d\)</span> holds with at least <span class="math inline">\(1 - \delta_d\)</span> probability, where <span class="math inline">\(\epsilon_d = O(\epsilon)\)</span>.</p>
<p><strong>Reference</strong><br />
[TX16] Boyu. Tian and Xiaokui. Xiao SLING: A Near-Optimal Index Structure for SimRank. <em>SIGMOD'16</em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/12/01/Random%20Sampling%20and%20Cuts%20II/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/12/01/Random%20Sampling%20and%20Cuts%20II/" class="post-title-link" itemprop="url">Random Sampling and Cuts II</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-12-01 22:27:54" itemprop="dateCreated datePublished" datetime="2015-12-01T22:27:54+11:00">2015-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-04-07 13:01:58" itemprop="dateModified" datetime="2018-04-07T13:01:58+10:00">2018-04-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>Sampling Theorem</strong></p>
<p>In 1994, Karger gave the following result:</p>
<p><strong>[Sampling Karger]</strong> <em>[Using randomized sparsification to approximate minimum cuts]</em> Given a graph G and a collection of random variables <span class="math inline">\(X_e (X_e \le M)\)</span> associated with all edges e, with probability <span class="math inline">\(1 - O(n^{-d})\)</span> that every cut in the instance <span class="math inline">\(G(X_e)\)</span> generated by putting weight <span class="math inline">\(X_e\)</span> on each e has value with <span class="math inline">\((1 - \epsilon, 1 + \epsilon)\)</span> times its expected value, where <span class="math display">\[
\epsilon = \sqrt{2(d + 2)M\ln n \over c&#39;}
\]</span> and c' is G's minimum expected value.</p>
<p>As an application, if we fix <span class="math inline">\(\epsilon\)</span> and let <span class="math inline">\(p = \Theta({\ln n \over \epsilon^2 c})\)</span>, where c is the minimum cut of G, then G' approximate all its original graph cuts by a factor p with high probability.</p>
<p>The limitation is that either <span class="math inline">\(\epsilon\)</span> or p depends on c. When c is little, we could not reduce many edges.</p>
<p><strong>Improvement:</strong> In 1996, Benczur and Karger improved previous sampling method such that the dependence on c is removed.</p>
<p><strong>New Result</strong>:</p>
<blockquote>
<p>Given a graph G and a parameter <span class="math inline">\(\epsilon\)</span>, there is a new graph G' such that 1. G' has <span class="math inline">\(O({n \ln n \over \epsilon^2})\)</span> edges. 2. every cut in G' has value <span class="math inline">\((1 \pm \epsilon)\)</span> times its original cut value.</p>
</blockquote>
<p>The intuition is that in the dense parts of the graph, each edge can be sample with smaller probability in order to preserve cut value while edge should be sample with higher probability where the graph is sparse.</p>
<p>Before moving on we first formalize the notion of a dense region. We characterize a dense region by <em>Strong Connectivity</em>.</p>
<p><strong>k-Connected</strong></p>
<blockquote>
<p>A graph G is k connected if its minimum cut value is no less than k.</p>
</blockquote>
<p><strong>Vertex-Induced Subgraph</strong></p>
<blockquote>
<p><span class="math inline">\(G&#39;=&lt;V&#39;,E&#39;&gt;\)</span> is a vertex induced subgraph of <span class="math inline">\(G =&lt;V,E&gt;\)</span> if <span class="math inline">\(V&#39;\)</span> is a subset of <span class="math inline">\(V\)</span> and <span class="math inline">\(E&#39;\)</span> is all edges of <span class="math inline">\(E\)</span> whose endpoints are both in <span class="math inline">\(V&#39;\)</span>.</p>
</blockquote>
<p><strong>k-Strong Component</strong></p>
<blockquote>
<p>A k-Strong Component is a maximal k-connected vertex-induced subgraph.</p>
</blockquote>
<p><strong>Strong Connectivity</strong></p>
<blockquote>
<p>The strong connectivity <span class="math inline">\(c_e\)</span> of an edge e is the maximum value of k such that a k-Strong component contains e.</p>
</blockquote>
<h5 id="generating-the-graph">Generating the graph</h5>
<p>Here is how new sampling technique operates</p>
<blockquote>
<p>Given a parameter <span class="math inline">\(\epsilon\)</span>, let <span class="math inline">\(\rho = {16(d + 2)M \ln n \over \epsilon^2}\)</span>, we assign to each edge a random weight <span class="math inline">\(X_e\)</span>, which takes value <span class="math inline">\(1/p\)</span> with probability <span class="math inline">\(p\)</span> and <span class="math inline">\(0\)</span> with probability <span class="math inline">\((1 - p)\)</span>, where <span class="math inline">\(p = \min \{ \rho / c_e, 1 \}\)</span>. When <span class="math inline">\(X_e = 0\)</span>, we eliminate e from the new graph.</p>
</blockquote>
<p>Immediately follows from the definition of <span class="math inline">\(X_e\)</span> we see that <span class="math inline">\(E[X_e] = 1\)</span>.</p>
<p>To check the sampling technique works, we need to verify two things: 1. The number of edge generated is <span class="math inline">\(O(n \ln n / \epsilon^2)\)</span> 2. Each cut does not deviate much from its original value.</p>
<p><strong>Number of edges</strong></p>
<p><strong>Theorem 1</strong> &gt; If an undirected connected graph has k(n - 1) edges, then it must has a k strong component.</p>
<p><em>Proof:</em> We prove it by induction. When k = 1, then the graph itself is a k-strong component. When n = 2, the theorem clearly holds.</p>
<p>Now we consider the general case. Suppose that the graph does not contain a k-strong component. Then there exist a cut <span class="math inline">\(C = &lt;A, B&gt;\)</span> in the graph such that <span class="math inline">\(cut(A,B) &lt; k\)</span>. Now both A and B have less than n nodes. By induction, A has less then k(|A| - 1) edges inside and B has less then k(|B| - 1) edges inside. The total number of edges is less than k(|A| - 1) + k + k(|B| - 1) = k(|A|+|B| - 1) = k(n - 1). A contradiction.</p>
<p><strong>Theorem 2</strong> &gt; <span class="math inline">\(\Sigma {1 \over c_e} &lt; 2(n -1)\)</span></p>
<p><em>Proof:</em> Suppose that <span class="math inline">\(\Sigma {1 \over c_e} \ge 2(n -1)\)</span>, then by theorem 1, there is a 2-strong component. Let e* be the edge with minimum <span class="math inline">\(c_{e^*}\)</span> in that component. Then there is a cut C in the 2-strong component containing e* such that <span class="math inline">\(cut(C) \le c_{e^*}\)</span>. Now</p>
<p><span class="math display">\[
{\underset{e \in C}{\Sigma }} c_e^{-1} \le c_{e^*}^{-1} c_{e^*} = 1 &lt; 2
\]</span></p>
<p>A contradiction.</p>
<p><strong>Error analysis</strong></p>
<p>To analyze the errors, we denote <span class="math inline">\(F_i\)</span> the set of edges whose strong connectivity are between <span class="math inline">\([2^i, 2^{i+1}]\)</span> for <span class="math inline">\(i \ge 0\)</span>. Let <span class="math inline">\(G_i = &lt;V, F_i \cup F_{i + 1} \cup F_{i + 2}...&gt;\)</span>.</p>
<p>In <span class="math inline">\(G_i\)</span>, each edge is sampled in the following way <span class="math display">\[
X_e = \begin{cases}
1, if \ e \notin F_i \\
         \begin{Bmatrix}
           {1 \over p} with \ probability \ p, \\
           0 \  with  \ probability \ 1 - p
    \end{Bmatrix} \ if \ e \in F_i
\end{cases}
\]</span></p>
<p>Then, by sampling theorem, the sampled graph <span class="math inline">\(G_i&#39;\)</span> is within <span class="math inline">\((1 \pm \epsilon)G_i\)</span>.</p>
<p>So <span class="math display">\[
\begin{aligned}
G&#39; &amp; = \Sigma_{i = 0}^{\log m} F_i&#39; \\ 
&amp; = \Sigma G_i&#39; - \Sigma G_{i + 1} \\
&amp; = \Sigma (1 \pm \epsilon)G_i - \Sigma G_{i + 1} \\
&amp; = (1 \pm \epsilon)G_0 \pm \epsilon\Sigma G_{i + 1} \\
&amp; \in G \pm \epsilon \log m \ G
\end{aligned}
\]</span></p>
<p>But there is a log m term in the final result. We can eliminate the log m term with more sophisticated techniques.</p>
<p>The random variable <span class="math inline">\(X_e\)</span> assigned to e in <span class="math inline">\(G_i\)</span> is revised as follows: <span class="math display">\[
X_e = \begin{cases}
2^{i - j}, if \ e \notin F_i \ and \ e \in F_j \\
         \begin{Bmatrix}
           {1 \over p} with \ probability \ p, \\
           0 \  with  \ probability \ 1 - p
    \end{Bmatrix} \ if \ e \in F_i
\end{cases}
\]</span></p>
<p>It can be shown that every cut in <span class="math inline">\(G_i\)</span> is at least <span class="math inline">\(2^i\)</span> heavy.</p>
<p>So <span class="math display">\[
\begin{aligned}
G&#39; &amp; = \Sigma_{i = 0}^{\log m} F_i&#39; \\ 
&amp; = \Sigma G_i&#39; - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = \Sigma (1 \pm \epsilon)G_i - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = \Sigma F_i + (1 \pm \epsilon)\Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = G \pm 2 \epsilon \ \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = G\pm 2 \epsilon \ G
\end{aligned} 
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/48/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/48/">48</a><span class="page-number current">49</span><a class="page-number" href="/page/50/">50</a><a class="page-number" href="/page/51/">51</a><a class="extend next" rel="next" href="/page/50/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">152</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
