<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Helvetica:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.24.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/64/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/64/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/64/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>WOW</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">WOW</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">200</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/15/VC%20Dimension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/02/15/VC%20Dimension/" class="post-title-link" itemprop="url">VC Dimension</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-02-15 16:20:33" itemprop="dateCreated datePublished" datetime="2017-02-15T16:20:33-05:00">2017-02-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-02-07 20:15:27" itemprop="dateModified" datetime="2022-02-07T20:15:27-05:00">2022-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Let <span class="math inline">\((S, R)\)</span> be a set system,
where <span class="math inline">\(S\)</span> is a set and <span
class="math inline">\(R\)</span> is a collection of subsets in <span
class="math inline">\(S\)</span>.</p>
<blockquote>
<p><strong>Definition.</strong> Given <span class="math inline">\((S,
R)\)</span>, <span class="math inline">\(X \subset S\)</span> is
<strong>shattered</strong> by <span class="math inline">\(R\)</span>
if,<br />
<span class="math display">\[
\forall X&#39; \subset X,\, \exists R&#39; \in R,\, \text{ such that }
X&#39; = S \cap R&#39;.
\]</span></p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The <strong><em>VC</em></strong>
dimension of <span class="math inline">\((S, R)\)</span> is the maximum
cardinality of some <span class="math inline">\(X \subset S\)</span>
that is shattered by <span class="math inline">\(R\)</span>.</p>
</blockquote>
<p>Note: If we view <span class="math inline">\((S, R)\)</span> as a
classification problem, i.e., <span class="math inline">\(S\)</span> is
a set of points and <span class="math inline">\(R\)</span> a set of
functions, the <strong>VC</strong> dimension <span
class="math inline">\(d\)</span> is the maximum size of a subset <span
class="math inline">\(X\)</span> in which <span
class="math inline">\(R\)</span> could separate arbitrary set points
from the rest. In some sense, it measures how complex a separation plane
<span class="math inline">\(R\)</span> could form.</p>
<blockquote>
<p><strong>Definition.</strong> Given a nonnegative integer <span
class="math inline">\(n\)</span> and <span
class="math inline">\(d\)</span>, we define <span
class="math inline">\(\binom{n}{\le d} = \sum_{i = 0}^{d}
\binom{n}{i}\)</span></p>
</blockquote>
<blockquote>
<p><strong>Lemma.</strong> If <span class="math inline">\((S,
R)\)</span> has <span class="math inline">\(VC\)</span> dimension <span
class="math inline">\(d\)</span>, then <span class="math inline">\(|R|
\le \binom{n}{\le d}\)</span>, where <span class="math inline">\(|S| =
n\)</span>.</p>
</blockquote>
<p><strong>Proof.</strong> Let <span class="math inline">\(X \subset
S\)</span> be the set that has size <span
class="math inline">\(d\)</span> and is shattered by <span
class="math inline">\(R\)</span>. Define <span
class="math inline">\(R|X\)</span> as <span
class="math inline">\(\{R&#39; \cap X | R&#39; \in R \}\)</span>. It is
obvious that <span class="math inline">\((X, R|X)\)</span> has
<strong>VC</strong> dimension <span class="math inline">\(d\)</span>.
Also, the number of subset of <span class="math inline">\(X\)</span> is
given by <span class="math display">\[
2^d = \binom{d}{\le d} = |\ R|X\ |
\]</span></p>
<p>Now we will show this lemma by induction. Label the elements in <span
class="math inline">\(S - X\)</span> in arbitrary order as <span
class="math inline">\(\{ x_{d + 1}, x_{d + 2}, ..., x_{n} \}\)</span>.
Define <span class="math inline">\(X_i\)</span> = <span
class="math inline">\(X \cup \{x_{d+1}, x_{d + 2}, ..., x_{d+i}
\}\)</span> for <span class="math inline">\(0 \le i \le n - d\)</span>,
where <span class="math inline">\(X_0 = X\)</span>. So <span
class="math inline">\(|X_i| = d + i\)</span>. We will show that <span
class="math inline">\(|R | X_i| \le \binom{d + i} {\le d}\)</span>
holds.</p>
<p>Suppose the assumption holds for <span
class="math inline">\(R|X_i\)</span>, now consider <span
class="math inline">\(R|X_{i + 1}\)</span>. The elements <span
class="math inline">\(r\)</span> in <span
class="math inline">\(R|X_{i}\)</span> can be divided into</p>
<ol type="1">
<li><span class="math inline">\(D_1  = \{ r | r \in R | X_{i + 1}, r
\cup x_{d + 1} \in R|X_{i + 1} \}\)</span>.</li>
<li><span class="math inline">\(D_2 = \{ r | r \notin R | X_{i + 1}, r
\cup x_{d + 1} \in R|X_{i + 1} \}\)</span>.</li>
<li><span class="math inline">\(D_2 = \{ r | r \in R | X_{i + 1}, r \cup
x_{d + 1} \notin R|X_{i + 1} \}\)</span>.</li>
</ol>
<p>We can verify that <span class="math inline">\(|R|X_i | = |D_1| +
|D_2| + |D_3|\)</span>, <span class="math inline">\(|R|X_{i + 1}| =
2|D_1| + |D_2| + |D_3|\)</span>.</p>
<p>We show that <span class="math inline">\(|D_1| \le \binom{d +
i}{\le  d - 1}\)</span>. This is because the <strong>VC</strong>
dimension of <span class="math inline">\((X_i, D_1)\)</span> is at most
<span class="math inline">\(d - 1\)</span>. Otherwise, suppose <span
class="math inline">\(Y \subset X_i\)</span> is shattered by <span
class="math inline">\(D_1\)</span> and <span class="math inline">\(|Y| =
d\)</span>. It is apparent <span class="math inline">\(Y \cup x_{d +
1}\)</span> is shattered by <span class="math inline">\(R|X_{i +
1}\)</span> in <span class="math inline">\(X_{i + 1}\)</span>. A
contradiction.</p>
<p>Therefore, <span class="math display">\[
    \begin{aligned}
        |R | X_{i + 1}|
        &amp; = |D_1| + |R | X_{i}| \\
        &amp; \le \binom{d + i}{\le d - 1} + \binom{d + i}{\le d} \\
        &amp; = \sum_{j = 0}^{d - 1} \binom{d + i}{j} + \sum_{j =
0}^{d}\binom{d +i}{j} \\
        &amp; = \sum_{j = 1}^{d} \binom{d + i}{j - 1} + \sum_{j =
1}^{d}\binom{d +i}{j} + \binom{d + i}{0} \\
        &amp; = \sum_{j = 1}^d \binom{d + i + 1}{j} + \binom{d + i
+1}{0} \\
        &amp; = \sum_{j = 0}^d \binom{d + i + 1}{j} \\
        &amp; = \binom{d + i + 1}{\le d}.
    \end{aligned}
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/02/Johnson-Lindenstrauss%20Lemma/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/02/02/Johnson-Lindenstrauss%20Lemma/" class="post-title-link" itemprop="url">Johnson-Lindenstrauss Lemma</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-02-02 20:38:54" itemprop="dateCreated datePublished" datetime="2017-02-02T20:38:54-05:00">2017-02-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-08 13:00:36" itemprop="dateModified" datetime="2024-03-08T13:00:36-05:00">2024-03-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="problem">Problem</h1>
<p>Let <span class="math inline">\(\mathcal{U} \subseteq
\mathbb{R}^d\)</span> be a set of <span class="math inline">\(n\)</span>
points. We want to find a linear transformation <span
class="math inline">\(A \in \mathbb{R}^{k \times d}\)</span> that embeds
the points in <span class="math inline">\(\mathcal{U}\)</span> to <span
class="math inline">\(\mathbb{R}^k\)</span> while preserving the
pairwise Euclidean distances.</p>
<p>If <span class="math inline">\(k \ge d\)</span>, the problem is
trivial. There is an isomorphic mapping between <span
class="math inline">\(\mathbb{R}^d\)</span> and a <span
class="math inline">\(d\)</span>-dimensional subspace in <span
class="math inline">\(\mathbb{R}^k\)</span>. Let <span
class="math inline">\(u_1, \ldots, u_d\)</span> be a subset of standard
basis vectors in <span class="math inline">\(\mathbb{R}^k\)</span>, and
<span class="math inline">\(e_1, \ldots, e_d\)</span> be standard basis
vectors in <span class="math inline">\(\mathbb{R}^d\)</span>. Then one
possible isomorphic mapping is given by <span class="math display">\[
    A = u_1 e_1^T + \ldots + u_d e_d^T.
\]</span></p>
<p>If <span class="math inline">\(k &lt; d\)</span>, then such an
isomorphic embedding is impossible. However, it is feasible, if we aim
to preserve the pair-wise distance up to a multiplicative error <span
class="math inline">\(\epsilon \in (0, 1)\)</span> with high
probability, s.t., <span class="math inline">\(\forall x, y \in
\mathcal{U}\)</span>, <span class="math display">\[
    \Vert A x - A y \Vert_2 \in (1 \pm \epsilon) \Vert  x - y \Vert_2,
\]</span> where <span class="math inline">\(\Vert \cdot \Vert_2\)</span>
is the <span class="math inline">\(\ell_2\)</span> distance.</p>
<p>The smaller <span class="math inline">\(k\)</span> is, the less space
we need to store the embedded vectors <span class="math display">\[
    A \mathcal{U}
        \doteq \{ Ax : x \in \mathcal{U} \} \subseteq \mathbb{R}^{k
\times n}.
\]</span> It remains to study how small <span
class="math inline">\(k\)</span> can be?</p>
<blockquote>
<p><strong>Theorem.</strong> For each set <span
class="math inline">\(\mathcal{U} \subseteq \mathbb{R}^d\)</span> of
<span class="math inline">\(n\)</span> points, there is a matrix <span
class="math inline">\(A \in \mathbb{R}^{k \times d}\)</span>, such that
<span class="math inline">\(k = O\left( \frac{ \log n }{ \epsilon^2}
\right)\)</span> and <span class="math display">\[
\forall x, y \in \mathcal{U}, \quad \Vert A x - A y \Vert_2 \in (1 \pm
\epsilon) \Vert  x - y \Vert_2,
\]</span> Further, there is an algorithm which returns such a matrix
<span class="math inline">\(A\)</span> with success probability at least
<span class="math inline">\(1 - \frac{1}{n}\)</span></p>
</blockquote>
<p>The value of <span class="math inline">\(k\)</span> depends only on
<span class="math inline">\(n\)</span> but not <span
class="math inline">\(d\)</span>.</p>
<h1 id="column-perspective">Column Perspective</h1>
<p>On the high level, we try to squeeze <span
class="math inline">\(d\)</span> (in expectation) orthonormal vectors
into a smaller space <span class="math inline">\(\mathbb{R}^k\)</span>.
We do this by generating a set of <span class="math inline">\(d\)</span>
random vectors <span class="math inline">\(u_1, u_2, \ldots, u_d \in
\mathbb{R}^k\)</span>, such that:</p>
<ol type="1">
<li>Each coordinate of <span class="math inline">\(u_i\)</span> is
generated independently for <span class="math inline">\(i \in
[d]\)</span>;</li>
</ol>
<p>and in expectation, they are orthonormal:</p>
<ol start="2" type="1">
<li><span class="math inline">\(\mathbb{E} \left[ \Vert u_i \Vert_2^2
\right] = 1\)</span> for <span class="math inline">\(i \in
[d]\)</span>;<br />
</li>
<li><span class="math inline">\(\mathbb{E} \left[ \langle u_i, u_j
\rangle \right] = 0\)</span> for <span class="math inline">\(i, j \in
[d], i \neq j\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(e_1, \ldots, e_d \in
\mathbb{R}^d\)</span> be unit vectors in <span
class="math inline">\(\mathbb{R}^d\)</span> and <span
class="math inline">\(e_i\)</span> (<span class="math inline">\(i \in
[d]\)</span>) has the <span class="math inline">\(i\)</span>-th entry
equal to 1. Define <span class="math display">\[
    A = \begin{bmatrix}
        u_1, u_2, \ldots, u_d
    \end{bmatrix}
    = u_1 e_1^T + \ldots + u_d e_d^T.
\]</span></p>
<p>For a fixed pair <span class="math inline">\(x, y \in
\mathcal{U}\)</span>, define <span class="math inline">\(z = x -
y\)</span>. Then <span class="math display">\[
    \begin{aligned}
        \Vert  A x - A y \Vert_2^2
            &amp;= \Vert  A z \Vert_2^2 \\
            &amp;= \left\Vert  \sum_{ i \in [d] } z[i] \cdot u_i
\right\Vert_2^2 \\
            &amp;= \sum_{i \in [d] } (z[i])^2 \Vert u_i \Vert_2^2
            + \sum_{ i, j \in [d], i \neq j} (z[i] \cdot z[j]) \cdot
\langle u_i, u_j \rangle.
    \end{aligned}
\]</span></p>
<p>Taking expectation of both side, and by assumption of <span
class="math inline">\(v_t, t \in [d]\)</span>, it holds that <span
class="math display">\[
    \mathbb{ E } \left[ \Vert z \Vert_2^2 \right] = \Vert z \Vert_2^2.
\]</span></p>
<h1 id="row-perspective">Row Perspective</h1>
<p>We can also characterize <span class="math inline">\(A\)</span> by
the properties of its rows. Let <span class="math inline">\(\tilde
A\)</span> be a scaled version of <span
class="math inline">\(A\)</span>, such that <span
class="math display">\[
    A = \frac{1}{ \sqrt k} \tilde{A}.
\]</span></p>
<p>Denote <span class="math inline">\(v_1, v_2, \ldots, v_k \in
\mathbb{R}^d\)</span> the row vectors of <span
class="math inline">\(\tilde A\)</span>. They are generated randomly and
independently. Each vector <span class="math inline">\(v_i\)</span>
(<span class="math inline">\(i \in [k]\)</span>) satisfies</p>
<ol type="1">
<li>Each entry of <span class="math inline">\(v_i\)</span> is generated
independently;</li>
<li><span class="math inline">\(\mathbb{E} \left[v_i [j] \right] = 0,
\forall j \in [d]\)</span>, i.e., each entry has mean zero;<br />
</li>
<li><span class="math inline">\(\mathbb{Var} \left[ v_i [j] \right] = 1,
\forall j \in [d]\)</span>, i.e., each entry has variance one.</li>
</ol>
<p>For a fixed pair <span class="math inline">\(x, y \in
\mathcal{U}\)</span>, define <span class="math inline">\(z = x -
y\)</span>. Consider <span class="math display">\[
    \Vert A z \Vert_2^2
    = \frac{1}{k} \Vert \tilde{A } z \Vert_2^2
    = \frac{1}{k} \sum_{i \in [k] } \langle v_i, z \rangle^2.
\]</span></p>
<p>Our assumption on <span class="math inline">\(v_i, i \in [k]\)</span>
implies that the <span class="math inline">\(\langle v_i, z \rangle^2, i
\in [k]\)</span> are independent random variables. We will verify that
each of them has variance <span class="math inline">\(\Vert z
\Vert_2^2\)</span>. For each <span class="math inline">\(i \in
[k]\)</span>, <span class="math display">\[
    \langle v_i, z \rangle = \sum_{j \in [d] } v_i [j] \cdot z[j]
\]</span> can be view as the sum of <span
class="math inline">\(d\)</span> independent random variables. Hence, by
our assumption on <span class="math inline">\(v_i\)</span>, we have
<span class="math display">\[
    \mathbb{Var} \left[ \langle v_i, z \rangle \right]
        = \sum_{j \in [d] } \mathbb{Var}\left[ v_i [j] \cdot z[j]
\right]
        = \sum_{j \in [d] } (z[j])^2 \cdot \mathbb{Var} \left[ v_i [j]
\right] = \Vert z \Vert_2^2.
\]</span></p>
<p>By linearity of expectation, <span class="math display">\[
    \mathbb{E} \left[ \langle v_i, z \rangle \right] = \sum_{j \in [d] }
\mathbb{E}\left[ v_i [j] \cdot z[j] \right] = \sum_{j \in [d] } z[j]
\cdot \mathbb{E} \left[ v_i [j] \right] = 0.
\]</span></p>
<p>It concludes that <span class="math display">\[
    \mathbb{E} \left[ \langle v_i, z \rangle^2 \right] = \mathbb{Var}
\left[ \langle v_i, z \rangle \right] - \big(  \mathbb{E} \left[ \langle
v_i, z \rangle \right] \big)^2 = \Vert z \Vert_2^2.
\]</span></p>
<p><strong>Therefore, <span class="math inline">\(\Vert A z
\Vert_2^2\)</span> can be viewed as the average of <span
class="math inline">\(k\)</span> independent, mean <span
class="math inline">\(\Vert z \Vert_2^2\)</span> random
variables.</strong> By the law of large numbers, the average should
concentrate around <span class="math inline">\(\Vert z
\Vert_2^2\)</span>. The analysis of the concentration phenomenon depends
on the detailed implementation of <span class="math inline">\(\tilde
A\)</span>.</p>
<h1 id="gaussian">Gaussian</h1>
<p>In this section, we discuss a possible implementation of <span
class="math inline">\(A\)</span>.</p>
<p>Let <span class="math inline">\(\tilde A \in \mathbb{R}^{k \times
d}\)</span> be a random matrix, each entry of which is generated
independently from standard normal distribution. Let <span
class="math inline">\(\tilde u_1, \ldots, \tilde u_d\)</span> be the
column vectors, and <span class="math inline">\(\tilde v_1, \ldots,
\tilde v_k\)</span> be the row vectors of <span
class="math inline">\(\tilde A\)</span>. Define <span
class="math inline">\(A \doteq \frac{1}{\sqrt k} \tilde{A}\)</span>.
Hence, for the column and row vectors of <span
class="math inline">\(A\)</span>, it holds that <span
class="math inline">\(u_i = \frac{1}{\sqrt k} \tilde u_i, i \in
[d]\)</span> and <span class="math inline">\(v_i = \frac{1}{\sqrt
k}  \tilde v_i, i \in [k]\)</span>.</p>
<p>It is easy to see that <span class="math inline">\(A\)</span>
satisfies previous discussed column and row properties.</p>
<p>Define <span class="math inline">\(\sigma^2 = \Vert z
\Vert_2^2\)</span>, and <span class="math display">\[
    X_i
        \doteq \langle \tilde{v_i}, z \rangle
        = \sum_{j \in [d]} \tilde{v_i} [j] \cdot z[j],
    \qquad \forall i \in [k].
\]</span> Since each <span class="math inline">\(\tilde{v_i} [j] \sim
N(0, 1)\)</span>, it follows that <span class="math inline">\(X_i \sim
N(0, \sigma^2)\)</span>. Then <span class="math display">\[
    \mathbb{E} \left[ X_i^2 \right] = \sigma^2.
\]</span> <!-- 
Column properties: 

1. $\mathbb{E} \left[  \left\Vert u_i \right\Vert_2^2 \right] = \frac{1}{k} \mathbb{E} \left[  \left\Vert \tilde u_i \right\Vert_2^2 \right] = 1, \forall i \in [d]$;  
   
2. $\mathbb{E} \left[  \left\langle u_i, u_j \right\rangle \right] = \frac{1}{k} \sum_{t \in [k] } \mathbb{E} \left[ \tilde u_i [t] \right] \cdot \mathbb{E} \left[ \tilde u_j[t] \right] = 0, \forall i, j \in [d], i \neq j$.

Clearly, by definition, we have $v_i[t] \sim N(0, 1), \forall i \in [k], t \in [d]$.
--></p>
<p>And <span class="math display">\[
    \mathbb{E} \left[ \Vert A z \Vert_2^2 \right]
        = \frac{1}{k} \mathbb{E} \left[ \Vert \tilde{A } z \Vert_2^2
\right]
        = \frac{1}{k} \sum_{i \in [k] } \mathbb{E} \left[ \langle
\tilde{v_i}, z \rangle^2 \right]
        = \sigma^2.
\]</span></p>
<blockquote>
<p><strong>Theorem</strong>. (Chernoff bound for the <span
class="math inline">\(\chi^2\)</span> distribution). Let <span
class="math inline">\(X_i \sim N(0, \sigma^2), i \in [k]\)</span> be
independent random variables. Then <span class="math display">\[
\begin{aligned}
\Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \ge (1 + \epsilon)
\cdot  \sigma^2 \right]
&amp;\le \exp \left( - \frac{k}{2} ( \epsilon^2 / 2 - \epsilon^3 / 3)
\right),
\\
\Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \le (1 - \epsilon)
\cdot  \sigma^2 \right]
&amp;\le \exp \left( - \frac{k}{4} \epsilon^2 \right).
\end{aligned}
\]</span></p>
</blockquote>
<!-- 
#### Lemma One
Let $t \in R^d$ be a random vector s.t. $t_i \overset{i.i.d}{\thicksim} N(0,1)$ and $x \in R^d$ be any fixed vector. Then 

 1.  $t^T x \thicksim N(0, \Vert x\Vert)$, where $\Vert x\Vert = \sum_{i = 1}^d x_i^2$.
 2.  $\mathbb{E}[(t^Tx)^2] = \mathbb{E}[(\sum_{i = 1}^d t_i x_i)^2]$ = $\sum_{1 \le i, j\le d} x_i x_j \mathbb{E}[t_i t_j]$ =$\sum_{i = 1}^d x_i^2 = \Vert x\Vert$ 
-->
<p>The proof of the theorem is based on the following lemma.</p>
<blockquote>
<p><strong>Lemma.</strong> It holds that <span class="math display">\[
\begin{aligned}
\Pr \left( \Vert A z \Vert_2^2 \ge (1 + \epsilon) \Vert z \Vert_2^2
\right)
&amp;\le \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) )
}{2} \right), \\
\Pr \left( \Vert A z \Vert_2^2 \le (1 - \epsilon) \Vert z \Vert_2^2
\right)
&amp;\le \exp\left( \frac{k \cdot (\epsilon + \ln (1 - \epsilon) ) }{2}
\right).
\end{aligned}
\]</span></p>
</blockquote>
<p><strong>Proof of the Theorem.</strong> As <span
class="math inline">\(\forall \epsilon \in [0, 1)\)</span>, <span
class="math display">\[
    \begin{aligned}
        \ln(1 + \epsilon)
            &amp;= \epsilon - \frac{\epsilon^2}{2} +
\frac{\epsilon^3}{3} - \left( \frac{\epsilon^4}{4} -
\frac{\epsilon^5}{5} \right) - \left( \frac{\epsilon^6}{6} +
\frac{\epsilon^7}{7} \right) - \ldots
            \le \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3},
\\
        \epsilon - \ln(1 + \epsilon)
            &amp;= \int_0^\epsilon \left( 1 - \frac{1}{1 + x} \right) \,
d x
            \ge \int_0^\epsilon \frac{x}{1 + \epsilon} \, d x
            = \frac{\epsilon^2}{2(1 + \epsilon)},
        \\
        \ln(1 - \epsilon)
            &amp;= -\epsilon - \frac{\epsilon^2}{2} -
\frac{\epsilon^3}{3} - \frac{\epsilon^4}{4} - \ldots\le - \epsilon -
\frac{\epsilon^2}{2} - \frac{\epsilon^3}{3},
    \end{aligned}
\]</span></p>
<p>we get <span class="math display">\[
    \Pr \left( \Vert A z \Vert_2^2 \ge (1 + \epsilon) \Vert z \Vert_2^2
\right)
        \le \exp \left( - \frac{k}{2}
            \cdot \left( \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3}
\right)
        \right) .
\]</span></p>
<p>and <span class="math display">\[
    \Pr \left( \Vert A z \Vert_2^2 \le (1 - \epsilon) \Vert z \Vert_2^2
\right) \le \exp\left( \frac{k \cdot (\epsilon - \epsilon -
\frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} ) }{2} \right) \le \exp
\left( - \frac{k }{4} \epsilon^2 \right).
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p>To prove the lemma, we have the following fact.</p>
<blockquote>
<p><strong>Fact</strong>. (Moment Generating Function of Normal
Distribution). Let <span class="math inline">\(X \thicksim
N(0,\sigma^2)\)</span>, then for <span class="math inline">\(\lambda
&lt; \frac{1}{2 \sigma^2}\)</span>, we have <span
class="math display">\[
\mathbb{E} \left[ e^{ \lambda X^2 } \right] = \frac{1}{ \sqrt{ 1 - 2
\sigma^2 \lambda} }.
\]</span></p>
</blockquote>
<p><strong>Proof of the Fact.</strong> <span class="math display">\[
    \begin{aligned}
        \mathbb{E} \left[ e^{ \lambda X^2 } \right]
            &amp;= \frac{1}{ \sqrt{2 \pi \sigma } } \int e^{ \lambda x^2
} e^{ -x^2 / \big( 2 \sigma^2 \big) } \ dx \\
            &amp;=  \frac{1}{ \sqrt{2 \pi \sigma } } \int \exp \left(-
\frac{ (1 - 2 \sigma^2 \lambda ) x^2}{2 \sigma^2 } \right) \ dx  \\
            &amp;= \frac{1}{ \sqrt{1 - 2 \sigma^2 \lambda } }.
    \end{aligned}
\]</span> <span class="math inline">\(\square\)</span></p>
<p><strong>Proof of Lemma.</strong> Let <span
class="math inline">\(\sigma^2 \doteq \Vert z \Vert_2^2\)</span>, and
<span class="math inline">\(X_i = \langle v_i, z \rangle, \forall i \in
[k]\)</span>. Then <span class="math inline">\(X_i, \forall i \in
[k]\)</span> are independent normal random variables with variance <span
class="math inline">\(\sigma^2\)</span>. By the previous fact, for each
<span class="math inline">\(\lambda &lt; \frac{1}{2 \sigma^2}\)</span>,
it holds that <span class="math display">\[
\begin{aligned}
    \Pr \left( \Vert A z \Vert_2^2 \ge (1 + \epsilon) \Vert z \Vert_2^2
\right)  
        &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \langle v_i, z
\rangle^2 \ge (1 + \epsilon) \cdot \sigma^2 \right) \\
        &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \ge k \cdot (1 +
\epsilon) \cdot \sigma^2 \right) \\
        &amp; = \Pr \left( \exp \left( \lambda \sum_{i \in [k] } X_i^2
\right)\ge \exp \left( \lambda \cdot k \cdot (1 + \epsilon) \cdot
\sigma^2 \right) \right) \\
        &amp;\le \frac{\mathbb{E} [ \exp(\lambda X^2 ) ]^k}{ \exp(
\lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 ) } \\
        &amp;= \left( \frac{1}{ \exp( \lambda \cdot (1 + \epsilon) \cdot
\sigma^2 ) \cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \lambda} } \right)^k.
\end{aligned}
\]</span></p>
<p>We want to find a <span class="math inline">\(\lambda\)</span> that
minimizes the RHS. Let <span class="math display">\[
    f = \lambda \cdot (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \ln (1
- 2 \cdot \sigma^2 \cdot  \lambda).
\]</span></p>
<p>Taking the derivative with respect to <span
class="math inline">\(\lambda\)</span>, <span class="math display">\[
\begin{aligned}
    \frac{ \partial f }{ \partial \lambda}
        &amp;= (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \cdot
\frac{1}{1 - 2  \cdot \sigma^2 \cdot \lambda} \cdot (-2  \cdot \sigma^2
) \\
        &amp;= \sigma^2 \cdot \left( (1 + \epsilon) - \frac{1}{ 1 - 2
\cdot \sigma^2 \cdot \lambda } \right).
\end{aligned}
\]</span></p>
<p>Hence <span class="math inline">\(f\)</span> increases when <span
class="math inline">\(\lambda &lt; \frac{ \epsilon }{ 2 \cdot \sigma^2
\cdot (1 + \epsilon) }\)</span> and decreases afterwards. It follows
that <span class="math display">\[
\begin{aligned}
    \Pr \left( \Vert A z \Vert_2^2 \ge (1 + \epsilon) \Vert z \Vert_2^2
\right)
        &amp;\le \left( \frac{1}{ \exp( \frac{ \epsilon }{ 2 \cdot
\sigma^2 \cdot (1 + \epsilon) } \cdot (1 + \epsilon) \cdot \sigma^2 )
\cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot
\sigma^2 \cdot (1 + \epsilon) } } } \right)^k \\
        &amp;= \left( \frac{ \sqrt{1 + \epsilon} }{ \exp( \epsilon / 2)
} \right)^k \\
        &amp;= \exp \left( - \frac{k \cdot (\epsilon - \ln (1 +
\epsilon) ) }{2} \right).
\end{aligned}
\]</span></p>
<p>Similarly, we have <span class="math display">\[
\begin{aligned}
    \Pr \left( \Vert A z \Vert_2^2 \le (1 - \epsilon) \Vert z \Vert_2^2
\right)
        &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \langle v_i, z
\rangle^2 \le (1 - \epsilon) \cdot \sigma^2 \right) \\
        &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \le k \cdot (1 -
\epsilon) \cdot \sigma^2 \right) \\
        &amp; = \Pr \left( \exp \left( - \lambda \sum_{i \in [k] } X_i^2
\right) \ge \exp \left( - \lambda \cdot k \cdot (1 - \epsilon) \cdot
\sigma^2 \right) \right) \\
        &amp;\le \frac{\mathbb{E} [ \exp( -\lambda X^2 ) ]^k}{ \exp( -
\lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2 ) } \\
        &amp;= \left( \frac{1}{ \exp( -\lambda \cdot (1 - \epsilon)
\cdot \sigma^2 ) \cdot \sqrt{ 1 + 2 \cdot \sigma^2 \cdot \lambda} }
\right)^k.
\end{aligned}
\]</span></p>
<p>Taking <span class="math inline">\(\lambda = \frac{ \epsilon }{ 2
\cdot \sigma^2 \cdot (1 - \epsilon) }\)</span>, we get <span
class="math display">\[
\begin{aligned}
    \Pr \left( \Vert A z \Vert_2^2 \le (1 - \epsilon) \sigma^2 \right)
        &amp;\le \left( \frac{1}{ \exp( - \frac{ \epsilon }{ 2 \cdot
\sigma^2 \cdot (1 - \epsilon) } \cdot (1 - \epsilon) \cdot \sigma^2 )
\cdot \sqrt{ 1 + 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot
\sigma^2 \cdot (1 - \epsilon) } } } \right)^k \\
        &amp;= \left( \sqrt{1 - \epsilon} \exp( \epsilon / 2) \right)^k
\\
        &amp;= \exp\left( \frac{k \cdot (\epsilon + \ln (1 - \epsilon) )
}{2} \right).
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<!-- Thus, 
$$
\begin{aligned}
\Pr (\frac{\Vert Ax\Vert}{k} \ge (1 + \epsilon) \Vert x\Vert)  
& \le (\frac{1}{e^{s(1 + \epsilon)} \sqrt{1 - 2s}})^k \\\\
& \le (\frac{1 + \epsilon}{e^{\epsilon}})^{k / 2} \\\\
& \le e^{(\ln (1 + \epsilon) - \epsilon) (k  / 2)}  \\\\
& \le e^{(\epsilon - \epsilon^2 / 2 + \epsilon^3 / 3 - \epsilon) ( k / 2)} \\\\
& \le e^{(- \epsilon^2 / 2 + \epsilon^3 / 3) ( k / 2)}  
\end{aligned}
$$ -->
<!-- When $k$ = $4 \ln n \over (\epsilon^2 / 2- \epsilon^3/ 3)$, we get 
$$
\Pr (\frac{\Vert Ax\Vert}{k} \notin [ (1 - \epsilon) \Vert x\Vert, (1 + \epsilon) \Vert x\Vert])   \le 2 / n^2
$$

Given lemma three, by union bound $\Vert f(x_i) - f(x_j)\Vert \notin (1 \pm \epsilon) \Vert x_i - x_j\Vert$ occurs with less than $\frac{2}{n^2} * \frac{n (n -1)}{2} = 1 - \frac{1}{n}$ probability. Repeating generating $A$ will boost the success probability to a desired level.  -->
<h1 id="rademacher">Rademacher</h1>
<p>In this implementation, <span class="math inline">\(\tilde A \in
\mathbb{R}^{k \times d}\)</span> is a random matrix, each entry of which
is independent Rademacher random variables, which equals <span
class="math inline">\(1\)</span> and <span
class="math inline">\(-1\)</span> with the equal probability.</p>
<p>Let <span class="math inline">\(\tilde u_1, \ldots, \tilde
u_d\)</span> be the column vectors, and <span
class="math inline">\(\tilde v_1, \ldots, \tilde v_k\)</span> be the row
vectors of <span class="math inline">\(\tilde A\)</span>. Define <span
class="math inline">\(A \doteq \frac{1}{\sqrt k} \tilde{A}\)</span>.
Hence, for the column and row vectors of <span
class="math inline">\(A\)</span>, it holds that <span
class="math inline">\(u_i = \frac{1}{\sqrt k} \tilde u_i, i \in
[d]\)</span> and <span class="math inline">\(v_i = \frac{1}{\sqrt
k}  \tilde v_i, i \in [k]\)</span>.</p>
<p>It is easy to see that <span class="math inline">\(A\)</span>
satisfies previous discussed column and row properties.</p>
<p>As before, let <span class="math inline">\(\sigma^2 = \Vert z
\Vert_2^2\)</span>, and <span class="math display">\[
    X_i
        \doteq \langle \tilde{v_i}, z \rangle
        = \sum_{j \in [d]} \tilde{v_i} [j] \cdot z[j],
    \qquad \forall i \in [k].
\]</span></p>
<p>Since <span class="math inline">\(\mathbb{E} \left[ \tilde{v_i} [j]^2
\right] = 1\)</span>, and <span class="math inline">\(\mathbb{E} \left[
\tilde{v_i} [j] \right] = 0\)</span>, we have <span
class="math display">\[
    \begin{aligned}
        \mathbb{E} \left[ X_i^2 \right]
        &amp;= \sum_{j, j&#39; \in [d]} z[j] \cdot z[j&#39;] \cdot
\mathbb{E} \left[ \tilde{v_i} [j] \cdot \tilde{v_i} [j&#39;] \right] \\
        &amp;= \sum_{j \in [d]} z[j]^2 \cdot \mathbb{E} \left[
\tilde{v_i} [j]^2 \right]  
        + \sum_{j, j&#39; \in [d], j \neq j&#39;} z[j] \cdot z[j&#39;]
\cdot \mathbb{E} \left[ \tilde{v_i} [j] \right] \cdot \mathbb{E}
\left[  \tilde{v_i} [j&#39;] \right]  \\
        &amp;= \sigma^2.
    \end{aligned}
\]</span></p>
<p>And <span class="math display">\[
    \mathbb{E} \left[ \Vert A z \Vert_2^2 \right]
        = \frac{1}{k} \mathbb{E} \left[ \Vert \tilde{A } z \Vert_2^2
\right]
        = \frac{1}{k} \sum_{i \in [k] } \mathbb{E} \left[ \langle
\tilde{v_i}, z \rangle^2 \right]
        = \sigma^2.
\]</span></p>
<blockquote>
<p><strong>Theorem</strong>. (Chernoff bound for the <span
class="math inline">\(\chi^2\)</span> distribution). Let <span
class="math inline">\(X_i \sim N(0, \sigma^2), i \in [k]\)</span> be
independent random variables. Then <span class="math display">\[
\begin{aligned}
\Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \ge (1 + \epsilon)
\cdot  \sigma^2 \right]
&amp;\le \exp \left( - \frac{k}{2} ( \epsilon^2 / 2 - \epsilon^3 / 3)
\right), \\
\Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \le (1 - \epsilon)
\cdot  \sigma^2 \right]
&amp;\le \exp \left( - \frac{k}{4} \epsilon^2 \right).
\end{aligned}
\]</span></p>
</blockquote>
<h2 id="upper-bound">Upper Bound</h2>
<p>To prove the upper bound, it suffices to establish the following
theorem. The rest of the proof is the same as the one in the previous
section.</p>
<blockquote>
<p><strong>Lemma</strong>. (Moment Generating Function). Let <span
class="math inline">\(X =\sum_{j \in [d]} \tilde{v} [j] \cdot
z[j]\)</span>, where <span class="math inline">\(\tilde{v} [j], j \in
[d]\)</span> are independent Rademacher random variables. Then for <span
class="math inline">\(0 \le \lambda &lt; \frac{1}{2 \sigma^2}\)</span>,
we have <span class="math display">\[
\mathbb{E} \left[ e^{ \lambda X^2 } \right] \le \frac{1}{ \sqrt{ 1 - 2
\sigma^2 \lambda} }.
\]</span></p>
</blockquote>
<p><strong>Proof.</strong> Let <span class="math inline">\(Y =\sum_{j
\in [d]} w [j] \cdot z[j]\)</span>, where <span class="math inline">\(w
[j], j \in [d]\)</span> are independent standard gaussian random
variables. Then <span class="math inline">\(Y \sim N(0,
\sigma^2)\)</span>, and <span class="math display">\[
    \mathbb{E} \left[ e^{ \lambda Y^2 } \right] = \frac{1}{ \sqrt{ 1 - 2
\sigma^2 \lambda} }.
\]</span></p>
<p>Our goal is to prove that <span class="math inline">\(\mathbb{E}
\left[ e^{ \lambda X^2 } \right] \le \mathbb{E} \left[ e^{ \lambda Y^2 }
\right]\)</span>.</p>
<p>Since <span class="math inline">\(X^{2m} \ge 0, \lambda \ge
0\)</span>, via monotone convergence theorem, <span
class="math display">\[
    \begin{aligned}
        \mathbb{E} \left[ \exp \left( \lambda X^2 \right) \right]
            &amp;= \mathbb{E} \left[ \lim_{t \rightarrow \infty} \sum_{m
= 0}^t \frac{\lambda^m X^{2m}}{m!} \right] \\
            &amp;= \lim_{t \rightarrow \infty} \sum_{m = 0}^t \mathbb{E}
\left[ \frac{\lambda^m X^{2m}}{m!} \right]
            = \sum_{m = 0}^\infty \frac{\lambda^m \mathbb{E} \left[
X^{2m} \right] }{m!} .
    \end{aligned}
\]</span></p>
<p>Following similar vein, we have <span class="math display">\[
    \begin{aligned}
        \mathbb{E} \left[ \exp \left( \lambda Y^2 \right) \right]
            = \sum_{m = 0}^\infty \frac{\lambda^m \mathbb{E} \left[
Y^{2m} \right] }{m!} .
    \end{aligned}
\]</span></p>
<p>It suffices to prove that <span class="math inline">\(\mathbb{E}
\left[ X^{2m} \right] \le \mathbb{E} \left[ Y^{2m} \right]\)</span> for
each <span class="math inline">\(m \in \mathbb{N}\)</span>.</p>
<p>Note, <span class="math display">\[
    \begin{aligned}
        \mathbb{E} \left[ X^{2m} \right]
            &amp;= \sum_{
                \begin{array}{c}
                    m_1, \ldots, m_d \\ m_1 + \ldots + m_d = 2m
                \end{array}
            } \binom{2m}{m_1, \ldots, m_d}  \prod_{j \in [d]} \Big( z[j]
\Big)^{m_j} \mathbb{E} \left[ \Big( \tilde{v} [j] \Big)^{m_j} \right],
    \end{aligned}
    \\
    \begin{aligned}
        \mathbb{E} \left[ Y^{2m} \right]
            &amp;= \sum_{
                \begin{array}{c}
                    m_1, \ldots, m_d \\ m_1 + \ldots + m_d = 2m
                \end{array}
            } \binom{2m}{m_1, \ldots, m_d}  \prod_{j \in [d]} \Big( z[j]
\Big)^{m_j} \mathbb{E} \left[ \Big( w [j] \Big)^{m_j} \right].
    \end{aligned}
\]</span></p>
<p>We compare these expansions:</p>
<ul>
<li><p>If <span class="math inline">\(m_j\)</span> is odd, then <span
class="math inline">\(\mathbb{E} \left[ \Big( \tilde{v} [j] \Big)^{m_j}
\right] = \mathbb{E} \left[ \Big( w [j] \Big)^{m_j} \right] =
0\)</span>.</p></li>
<li><p>If <span class="math inline">\(m_j\)</span> is even, then <span
class="math inline">\(\Big( z[j] \Big)^{m_j} \ge 0\)</span>, and <span
class="math inline">\(1 = \mathbb{E} \left[ \Big( \tilde{v} [j]
\Big)^{m_j} \right] \le \mathbb{E} \left[ \Big( w [j] \Big)^{m_j}
\right] = (m_j - 1)!!\)</span>.</p></li>
</ul>
<p>This proves that <span class="math inline">\(\mathbb{E} \left[ X^{2m}
\right] \le \mathbb{E} \left[ Y^{2m} \right]\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<!-- 
> Lemma 1. (Moment Generating Function of Rademacher random variable). Let $X$ be a random variable such that 
> $$
    X = \begin{cases}
        1,    &\text{w.p. }\, 0.5,    \\
        -1,    &\text{w.p. }\, 0.5,    
    \end{cases}
> $$
> then for $\lambda \in \mathbb{R}$, we have 
> $$
    \mathbb{E} \left[ e^{ \lambda X^2 } \right] = \exp(\lambda ).
> $$

**Proof.**
$$
    \begin{aligned}
    \mathbb{E} [ e^{ \lambda X^2 } ] &= \frac{1}{2} \exp( \lambda ) + \frac{1}{2} \exp(\lambda) = \exp(\lambda)
    \end{aligned}
$$
$\square$

Observe that
$$
    \begin{aligned}
        \frac{1}{2} \exp( - \sigma \lambda ) + \frac{1}{2} \exp( \sigma \lambda) 
        &= \frac{1}{2} \left( \sum_{i = 0}^\infty \frac{ (- \sigma \lambda)^i }{ i! } + \sum_{i = 0}^\infty \frac{ ( \sigma \lambda)^i }{ i! } \right) \\
        &=  \sum_{i = 0}^\infty \frac{ (\sigma \lambda)^{2i}  }{ (2i)! } \\
        &\le \exp( \sigma^2 \lambda^2 )
    \end{aligned}
$$ 
-->
<h2 id="lower-bound">Lower Bound</h2>
<p>We need the following facts.</p>
<blockquote>
<p><strong>Fact.</strong> For <span class="math inline">\(t \ge
0\)</span>, <span class="math display">\[
e^{-t} \le 1 - t + \frac{t^2}{2}.
\]</span></p>
</blockquote>
<p>Further, in the previous section, we prove that</p>
<blockquote>
<p><strong>Fact.</strong> Let <span class="math inline">\(X \doteq
\sum_{j \in [d]} \tilde{v} [j] \cdot z[j]\)</span>, where <span
class="math inline">\(\tilde{v} [j], j \in [d]\)</span> are independent
Rademacher random variables, and <span class="math inline">\(Y \doteq
\sum_{j \in [d]} w [j] \cdot z[j]\)</span>, where <span
class="math inline">\(w [j], j \in [d]\)</span> are independent standard
gaussian random variables. Then <span class="math display">\[
\mathbb{E}[X^4] \le \mathbb{E} [Y^4].
\]</span></p>
</blockquote>
<p>Noting that <span class="math inline">\(Y \sim N(0,
\sigma^2)\)</span>, we can show <span
class="math inline">\(\mathbb{E}[Y^4] = \sigma^2 (4 - 1)!! = 3
\sigma^2\)</span>.</p>
<p><strong>Proof.</strong> For each <span class="math inline">\(\lambda
&gt; 0\)</span>, <span class="math display">\[
\begin{aligned}
    \Pr \left[ \Vert A z \Vert_2^2 \le (1 - \epsilon) \Vert z \Vert_2^2
\right]
        &amp; = \Pr \left[ \sum_{i \in [k] } X_i^2 \le k \cdot (1 -
\epsilon) \cdot \sigma^2 \right] \\
        &amp; = \Pr \left[ \exp \left( - \lambda \sum_{i \in [k] } X_i^2
\right) \ge \exp \left( - \lambda \cdot k \cdot (1 - \epsilon) \cdot
\sigma^2 \right) \right] \\
        &amp;\le \frac{ \mathbb{E} [ \exp( -\lambda X^2 ) ]^k}{ \exp( -
\lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2 ) } \\
        &amp;\stackrel{\lambda X^2 \ge 0}{\le} \left( \mathbb{E} \left[
1 - \lambda X^2  + \frac{\lambda^2 X^4}{2} \right] \right)^k
        \exp \left( \lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2
\right)  \\
        &amp;\le \left( 1 - \lambda \sigma^2  + \frac{\lambda^2
\mathbb{E} \left[ X^4 \right] }{2}  \right)^k
        \exp \left( \lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2
\right)  \\
        &amp;\le \left( 1 - \lambda \sigma^2  + \frac{3 \lambda^2
\sigma^4 }{2}  \right)^k
        \exp \left( \lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2
\right).  
\end{aligned}
\]</span> <!-- 
Choosing $\lambda = \frac{\epsilon}{2 \sigma^2 (1 + \epsilon)}$, we have 
--> <!--  
$$
    \left( 1 - \frac{\epsilon}{2 (1 + \epsilon)}  + \frac{3 \epsilon^2 }{8 (1 + \epsilon)^2}  \right)^k 
        \exp \left( \frac{\epsilon \cdot k \cdot (1 - \epsilon) }{2 (1 + \epsilon)} \right)  \\
    \le \exp \left( 
        - \frac{k \epsilon}{2 (1 + \epsilon)}  + \frac{3 k \epsilon^2 }{8 (1 + \epsilon)^2}
        + \frac{\epsilon \cdot k \cdot (1 - \epsilon) }{2 (1 + \epsilon)} 
    \right)  \\
    = \exp \left( 
        \frac{3 k \epsilon^2 }{8 (1 + \epsilon)^2}
        - \frac{k \epsilon^2 }{2 (1 + \epsilon)} 
    \right)  \\
    = \exp \left( 
        - \frac{k \epsilon^2}{2 (1 + \epsilon) } \left( 
            \frac{1 + 4\epsilon}{4 (1 + \epsilon)}
        \right) 
    \right)  
$$
--> <!-- 
$$
    \begin{aligned}
        &\left( 1 - \frac{\epsilon}{2 (1 + \epsilon)}  + \frac{3 \epsilon^2 }{8 (1 + \epsilon)^2}  \right)^k 
        \exp \left( \frac{\epsilon \cdot k \cdot (1 - \epsilon) }{2 (1 + \epsilon)} \right) \\
        &= \exp \left( 
            \frac{\epsilon \cdot k \cdot (1 - \epsilon) }{2 (1 + \epsilon)}
            + k \ln \left( 
                1 - \frac{\epsilon}{2 (1 + \epsilon)}  + \frac{3 \epsilon^2 }{8 (1 + \epsilon)^2}
            \right)
        \right) \\
        &\le \exp \left( 
            - \frac{k}{2} \left( 
                \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3}
            \right)
        \right)
    \end{aligned}
$$ 
--></p>
<p>Choosing <span class="math inline">\(\lambda = \frac{\epsilon}{2
\sigma^2}\)</span>, we have <span class="math display">\[
    \begin{aligned}
        \left( 1 - \frac{\epsilon}{2}  + \frac{3 \epsilon^2 }{8
}  \right)^k
        \exp \left( \frac{\epsilon \cdot k \cdot (1 - \epsilon) }{2 }
\right)  
        &amp;\le \exp \left(
            - \frac{k \epsilon}{2}  + \frac{3 k \epsilon^2 }{8}
            + \frac{\epsilon \cdot k \cdot (1 - \epsilon) }{2}
        \right)  \\
        &amp;= \exp \left(
            \frac{3 k \epsilon^2 }{8}
            - \frac{k \epsilon^2 }{2}
        \right)  \\
        &amp;= \exp \left(
            - \frac{k \epsilon^2}{4}
        \right).  
    \end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Remark.</strong> For the proof of the lower bound, we only
require the <span class="math inline">\(\tilde{v} [j], j \in
[d]\)</span> to be 4-wise independent.</p>
<h1 id="reference">Reference</h1>
<p>[1] <em>J. Nelson, “Sketching and Streaming High-Dimensional
Vectors,” p. 145.</em></p>
<p>[2]. <em>D. Achlioptas, “Database-friendly random projections,” in
PODS ’01.</em></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/02/TF-IDF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/02/02/TF-IDF/" class="post-title-link" itemprop="url">TF-IDF</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-02-02 20:38:54" itemprop="dateCreated datePublished" datetime="2017-02-02T20:38:54-05:00">2017-02-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2018-07-02 04:29:42" itemprop="dateModified" datetime="2018-07-02T04:29:42-04:00">2018-07-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>TF-IDF stands for term frequency and inverse document frequency. It
evaluates the importance of a word to a document in a corpus, which
increases with the number of times it appears in the document but is
offset by its frequency in the corpus. More precisely, for TF, we define
<span class="math display">\[
tf(term| document) = \frac{\mbox{the number of times
&quot;term&quot; appears in &quot;document&quot;} }{\mbox{total number
of words in &quot;document&quot;} }
\]</span></p>
<p>For IDF, we define <span class="math display">\[
idf(term) = 1 + \log(\frac{\mbox{the total number of documents}
}{\mbox{the number of documents this term appears in} } )
\]</span></p>
<p>Now suppose we have a query. Let <span class="math inline">\(Q =
\{t_1, t_2, ..., t_n \}\)</span> the set of different term in the query.
We convert <span class="math inline">\(Q\)</span> into a vector, <span
class="math display">\[
V(Q) = [tf(t_1 | Q)* idf(t_1), tf(t_2|Q) * idf(t_2),..., tf(t_n|Q) *
idf(t_n)]
\]</span></p>
<p>Similarily, for a document <span class="math inline">\(D\)</span>, we
can get a vector <span class="math inline">\(V(D)\)</span>. Now the
similarity between <span class="math inline">\(Q\)</span> and <span
class="math inline">\(D\)</span> is given by <span
class="math display">\[
\frac{V(Q) \times V(D)}{||V(Q)|| \times ||V(D)||}
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/63/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/63/">63</a><span class="page-number current">64</span><a class="page-number" href="/page/65/">65</a><span class="space">&hellip;</span><a class="page-number" href="/page/67/">67</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/65/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">WOW</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
