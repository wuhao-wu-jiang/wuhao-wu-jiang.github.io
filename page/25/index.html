<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/25/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/25/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/12/PAC-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/12/PAC-learning/" class="post-title-link" itemprop="url">PAC Learning Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-12 16:06:55" itemprop="dateCreated datePublished" datetime="2020-11-12T16:06:55+11:00">2020-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-09 21:28:44" itemprop="dateModified" datetime="2021-11-09T21:28:44+11:00">2021-11-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss a toy model of probably approximately correct (PAC) learning [1]. The setting focuses on a <span class="math inline">\(d\)</span>-dimension Boolean hypercube <span class="math inline">\(\{0, 1\}^d\)</span> and a set of functions <span class="math inline">\(\mathcal{H} = \{ h : \{0, 1\}^d \rightarrow \{0, 1 \} \}\)</span>. A function <span class="math inline">\(h \in \mathcal{H}\)</span> is called a hypothesis or a concept, which assigns each vertex in the hypercube a label 0 or 1.</p>
<p>The goal of PAC learning is to learn an unknown hypothesis, denoted as <span class="math inline">\(h^* \in \mathcal{H}\)</span>, from a dataset, which consists of <span class="math inline">\(n\)</span> points <span class="math inline">\(X_1, X_2, \ldots, X_n \in \{0, 1\}^n\)</span> sampled independently from an unknown distribution <span class="math inline">\(D\)</span> (over the hypercube), as well as their labels <span class="math inline">\(Y_1 = h^*(X_1), Y_2 = h^*(X_2), \ldots, Y_n = h^*(X_n)\)</span>.</p>
<p>We can state the algorithm for PAC learning in just one sentence.</p>
<blockquote>
<p>Output any hypothesis <span class="math inline">\(h&#39; \in \mathcal{H}\)</span> that is consistent with the sampled dataset, i.e., <span class="math display">\[
h&#39;(X_i) = Y_i, \qquad \forall i \in [n]
\]</span></p>
</blockquote>
<p>Such an hypothesis always exists, as <span class="math inline">\(h^*\)</span> satisfies this constraint. We have the classic PAC learning theorem.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The PAC learning theorem states that the probability that <span class="math inline">\(h&#39;\)</span> mislabels a point is bounded by <span class="math inline">\(\sqrt{ \log |\mathcal{H}| + \log ( 1 / \delta ) \over 2n}\)</span>.</p>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
    \Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
    Z_i = \begin{cases}
        0, \qquad \text{ if } h(X_i) = Y_i \\
        1, \qquad \text{ if } h(X_i) \neq Y_i 
    \end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\mathbb{E}[Z_i]\)</span> is an unbiased estimator of <span class="math inline">\(\mu_h\)</span>. Further, denote <span class="math inline">\(\hat \mu_h\)</span> the empirical mean of <span class="math inline">\(Z_i\)</span>'s <span class="math display">\[
    \hat \mu_h \doteq \frac{1}{n} \sum_{i \in [n] } Z_i. 
\]</span></p>
<p>Note that for the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(\hat \mu_{h&#39;} = 0\)</span>.</p>
<p>By Hoeffding inequality, for any <span class="math inline">\(\delta&#39; &gt; 0\)</span>, <span class="math display">\[
    \Pr \left[ \hat \mu_h \le \mu_h - \sqrt{ \frac{ \log ( 1 / \delta&#39;) }{2n} } \right] \le \delta&#39;. 
\]</span></p>
<p>By union bound, the probability that <span class="math display">\[
    \exists h \in \mathcal{H}, \hat \mu_h \ge \mu_h - \sqrt{ \frac{ \log ( 1 / \delta&#39;) }{2n} }
\]</span></p>
<p>is bounded by <span class="math inline">\(| \mathcal{H} | \delta&#39;\)</span>. By setting <span class="math inline">\(\delta&#39; = \frac{\delta}{ | \mathcal{H} | }\)</span>, it is guaranteed that with probability at most <span class="math inline">\(\delta\)</span>, <span class="math display">\[
    0 = \hat \mu_{h&#39; } \le \mu_{h&#39;} -  \sqrt{ \log |\mathcal{H}| + \log ( 1 / \delta) \over 2n}.
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>The previous theorem can be improved with the <span class="math inline">\(\sqrt{ \cdot }\)</span> removed. The key here is that the selected <span class="math inline">\(h&#39;\)</span> is error-free on the samples.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
    \Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>We will prove that if <span class="math inline">\(\mu_h \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}\)</span>, then <span class="math inline">\(h\)</span>'s probability of being selected is very low.</p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
    Z_i = \begin{cases}
        0, \qquad \text{ if } h(X_i) = Y_i \\
        1, \qquad \text{ if } h(X_i) \neq Y_i 
    \end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\Pr[Z_i = 1] = \mu_h\)</span>. Therefore, <span class="math display">\[
    \begin{aligned}
        \Pr[ Z_1 = 0 \wedge Z_2 = 0 \wedge \ldots \wedge Z_n = 0] = (1 - \mu_h)^n \le \exp(- \mu_h n) = \frac{\delta}{ |\mathcal{H} | }
    \end{aligned}
\]</span></p>
<p>Taking union bound over all possible <span class="math inline">\(h\)</span> gives the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Caveat.</em></strong> <em>In both theorem, we see a <span class="math inline">\(\log \mathcal{H}\)</span> term in the failure probability, which is due to the use of union bound and can be roughly considered as the complexity of the hypothesis family. It is tempting to think that this could be avoid as follows.</em></p>
<blockquote>
<p>For the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(Z_1, \ldots, Z_n\)</span> are i.i.d samples. Therefore, the probability of all of them being <span class="math inline">\(0\)</span> is give by<br />
<span class="math display">\[
(1 - \mu_{h&#39;} )^n \le \exp( - \mu_{h&#39;} n). 
  \]</span><br />
Bounding this probability by <span class="math inline">\(\delta\)</span> gives <span class="math inline">\(\mu_{h&#39;} \le \frac{ \log ( 1 / \delta ) } {n}\)</span>.</p>
</blockquote>
<p><em>The argument is wrong. For the outputted <span class="math inline">\(h&#39;\)</span>, the <span class="math inline">\(Z_1, \ldots, Z_n\)</span> are not i.i.d samples, as <span class="math inline">\(h&#39;\)</span> is selected according to <span class="math inline">\(Z_1, \ldots, Z_n\)</span> and depends on them.</em></p>
<p><em>To make it more concrete, suppose that there is an <span class="math inline">\(h\)</span>, such that <span class="math inline">\(\mu_h = \frac{ \log ( 1 / \delta ) } {n}\)</span>. Consider the following experiment</em></p>
<blockquote>
<ol type="1">
<li>Sample <span class="math inline">\(n\)</span> points independently the distribution <span class="math inline">\(D\)</span>.<br />
</li>
<li>Pick an <span class="math inline">\(h&#39;\)</span> that makes no prediction error on the samples.</li>
</ol>
</blockquote>
<p><em>If we repeat the experiment <span class="math inline">\(N\)</span> times for some positive integer <span class="math inline">\(N\)</span>, then the fixed <span class="math inline">\(h\)</span> makes no mistake in roughly <span class="math inline">\((1 - \delta) N\)</span> of the experiments and constitutes an candidate of <span class="math inline">\(h&#39;\)</span>. However, on the other roughly <span class="math inline">\(\delta N\)</span> experiments, where <span class="math inline">\(h\)</span> makes prediction error, it is no longer considered as the candidate of <span class="math inline">\(h&#39;\)</span>. The selection procedure of <span class="math inline">\(h&#39;\)</span> ignores bad event of <span class="math inline">\(h\)</span> automatically.</em></p>
<p><em>We could also investigate the following example. Let <span class="math inline">\(h^* = h_0\)</span>. There are also other two hypothesis <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span>. Let <span class="math inline">\(S\)</span> be the sample space. It holds that</em> <span class="math display">\[
    |h_1(x) - h^*(x) | = \begin{cases}
        2 \epsilon, \forall x \in S \setminus S_{h_1} \\
        0,\ \       \forall x \in S_{h_1}
    \end{cases}
\]</span> <span class="math display">\[
    |h_2(x) - h^*(x) | = \begin{cases}
        2 \epsilon, \forall x \in S \setminus S_{h_2} \\
        0,\ \       \forall x \in S_{h_2}
    \end{cases}
\]</span> <em>Moreover, <span class="math inline">\(\Pr[S_{h_1}] = \Pr[S_{h_2} ] = \delta\)</span>. How, if we get a sample in <span class="math inline">\(S_{h_1}\)</span>, then we can output <span class="math inline">\(h&#39;\)</span> as <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_1\)</span>, since both of them make no error when <span class="math inline">\(x \in S_{h_1}\)</span>. Similarly, if <span class="math inline">\(x \in S_{h_2}\)</span>, we can output either <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_2\)</span>. We call <span class="math inline">\(S_{h_1}\)</span> and <span class="math inline">\(S_{h_2}\)</span> the bad areas. When <span class="math inline">\(x\)</span> is in the bad area of any hypothesis, we can't distinguish this hypothesis from the true hypothesis. That is why we need to use union bound to bound the total probabilities of these bad areas.</em></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/PAC-Learning.png?raw=true" width="400" height="340" /></p>
</div>
<h3 id="reference">Reference</h3>
<p>[1] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. 6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/Gaussian-Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/06/Gaussian-Mechanism/" class="post-title-link" itemprop="url">Gaussian Mechanism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-06 23:00:19" itemprop="dateCreated datePublished" datetime="2020-11-06T23:00:19+11:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-09 20:59:47" itemprop="dateModified" datetime="2021-06-09T20:59:47+10:00">2021-06-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="background">Background</h1>
<p>Let <span class="math inline">\((\mathcal{X}, d)\)</span> be a metric space, and <span class="math inline">\(f\)</span> be a function <span class="math inline">\(\mathcal{X} \rightarrow \mathbb{R}^n\)</span>. A Gaussian mechanism adds Gaussian noise to the output of <span class="math inline">\(f\)</span>, such that for all neighboring pairs <span class="math inline">\(x, y \in \mathcal{X}\)</span> with <span class="math inline">\(d(x, y) = 1\)</span>, it is hard to distinguish the outputs, i.e., their outputs distributions are similar.</p>
<blockquote>
<p><strong>Definition.</strong> The sensitivity <span class="math inline">\(\Delta\)</span> of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
\Delta = \max_{x, y \in \mathcal{X}, d(x, y) = 1 } \Vert f(x) - f(y) \Vert_2, 
\]</span> where <span class="math inline">\(\Vert \cdot \Vert_2\)</span> is the <span class="math inline">\(\ell_2\)</span>-norm of a vector in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Given <span class="math inline">\(f\)</span> and a variance parameter <span class="math inline">\(\sigma^2\)</span>, the output of the Gaussian mechanism is a random variable defines as <span class="math display">\[
g(x) = f(x) + X,
  \]</span> where <span class="math inline">\(X = (X_1, X_2, ..., X_n) \sim N(0, \sigma^2 I)\)</span>.</p>
</blockquote>
<h1 id="privacy-guarantee">Privacy Guarantee</h1>
<blockquote>
<p><strong>Theorem.</strong> For <span class="math inline">\(\delta, \epsilon \in (0, 1)\)</span>, if <span class="math inline">\(\sigma^2 = \frac{ \Delta^2 }{ \epsilon^2 } \Big( 2 \ln \frac{1}{\delta} + \epsilon \Big)\)</span>, then <span class="math inline">\(\forall x, y \in \mathcal{X}\)</span>, s.t., <span class="math inline">\(d(x, y) = 1\)</span>, and for all measurable subset <span class="math inline">\(S \subset \mathbb{R}^n\)</span>, it holds <span class="math display">\[
\Pr[ g(x) \in S] \le e^\epsilon \cdot \Pr[ g(y) \in S] + \delta 
\]</span></p>
</blockquote>
<p>It is obvious that if we set <span class="math inline">\(\sigma \rightarrow \infty\)</span>, then the Gaussian distribution converge to a uniform one and it holds that <span class="math inline">\(\Pr[ g(x) \in S] = \Pr[ g(y) \in S]\)</span>. Therefore, we are interested in how small <span class="math inline">\(\sigma\)</span> can be.</p>
<p>We claim that <span class="math inline">\(\sigma = O(\frac{\Delta}{\epsilon } )\)</span> suffices. This is based on three observations: 1. Most of the probability mass of a Gaussian distribution concentrates on a ball centered at its mean with radius <span class="math inline">\(O(\sigma)\)</span>. 2. The density ratio of any two points within the ball is bounded by a constant. 3. The distance between the distribution center of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> is bounded by <span class="math inline">\(\Delta\)</span>.</p>
<p><strong>Proof.</strong> To ease the notations, we write <span class="math inline">\(g(y) = f(y) + Y\)</span> to distinguish it from <span class="math inline">\(g(x)\)</span>. By definition, <span class="math display">\[g(x) \sim N(f(x), \sigma^2 I), \quad g(y) \sim N( f(y), \sigma^2 I).
\]</span></p>
<p>Let <span class="math inline">\(p_x\)</span> and <span class="math inline">\(p_y\)</span> the density functions of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> respectively. The sketch of the proof is as follows: we partition <span class="math inline">\(\mathbb{R}^n\)</span> into two parts: <span class="math display">\[
    \mathcal{Y}_1 = \{ t \in \mathcal{R}^n : p_x(t) \le e^\epsilon \cdot p_y(t) \} \\
    \mathcal{Y}_2 = \{ t \in \mathcal{R}^n : p_x(t) &gt; e^\epsilon \cdot p_y(t) \}.
\]</span></p>
<p>If <span class="math inline">\(\Pr[ g(x) \in \mathcal{Y}_2 ] \le \delta\)</span> <span class="math inline">\(\big(\)</span> here we measure the size of <span class="math inline">\(\mathcal{Y}_2\)</span> by the probability measure induced by <span class="math inline">\(g(x)\)</span> <span class="math inline">\(\big)\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[ g(x) \in S] 
        &amp;= \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[ g(x) \in S \cap \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[  g(x) \in \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \delta \\
        &amp;= \int_{S \cap \mathcal{Y}_1 } p_x(t) \ d t + \delta \\
        &amp;\le \int_{S \cap \mathcal{Y}_1 } e^\epsilon \cdot p_y(t) \ d t + \delta \\
        &amp;= e^\epsilon \cdot \Pr[ g(y) \in S \cap \mathcal{Y}_1] + \delta. \\
        &amp;\le e^\epsilon \cdot \Pr[ g(y) \in S ] + \delta. \\
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(p_y(t) &gt; 0\)</span> for all <span class="math inline">\(t \in \mathbb{R}^n\)</span>, we can rewrite <span class="math display">\[
    \mathcal{Y}_1 = \Big\{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } \le \epsilon \Big\} \\
    \mathcal{Y}_2 = \Big\{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } &gt; \epsilon \Big\}.
\]</span></p>
<p>In literature, the ratio <span class="math inline">\(\ln \frac{p_x(t) }{ p_y(t) }\)</span> is know as <em>privacy loss</em>. Substituting <span class="math inline">\(p_x(t)\)</span> and <span class="math inline">\(p_y(t)\)</span> with their definitions, and letting <span class="math inline">\(t&#39; = t - f(x)\)</span> and <span class="math inline">\(v = f(x) - f(y)\)</span>, we get <span class="math display">\[
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &amp;= \ln \frac{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( -\frac{ \Vert t - f(x)\Vert ^2 }{ 2 \sigma^2 } \Big) }{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( -\frac{ \Vert t - f(y) \Vert ^2 }{ 2 \sigma^2 } \Big)  } \\
        &amp;=  -\frac{ \Vert t&#39; \Vert ^2 - \Vert  t&#39; + v \Vert ^2 }{ 2 \sigma^2 } \\
        &amp;= \frac{ 2 v^T t&#39; + \Vert  v \Vert ^2 }{ 2 \sigma^2 }  . 
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(t \sim N( f(x), \sigma^2 I)\)</span>, it holds that <span class="math inline">\(t&#39; \sim N( 0, \sigma^2 I)\)</span>.</p>
<!-- $$
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &=  -\frac{ \Vert  t' \Vert ^2 - \Vert  t' + v \Vert ^2 }{ 2 \sigma^2 }    \\
        &= \frac{ 2 v^T t' + \Vert  v \Vert ^2 }{ 2 \sigma^2 }    \\
\end{aligned}
$$ -->
<p>Then <span class="math inline">\(v^T t&#39; \sim N(0, \Vert v \Vert ^2 \sigma^2)\)</span>. Let <span class="math inline">\(Z \sim N(0, 1)\)</span>, then <span class="math inline">\(\ln \frac{ p_x( t) }{ p_y(t)}\)</span> has the same distribution as <span class="math display">\[
    \frac{ 2 \Vert v\Vert  \sigma Z + \Vert v\Vert ^2 }{ 2 \sigma^2  } = \frac{ \Vert v\Vert ^2 }{ 2 \sigma^2  } + \frac{ \Vert v\Vert  }{  \sigma  } Z \sim N \Big( \frac{ \Vert v\Vert ^2 }{ 2 \sigma^2  },  \frac{ \Vert v\Vert ^2 }{  \sigma^2  } \Big).   \\
\]</span></p>
<p>Now, <span class="math display">\[
    \frac{ \Vert v\Vert ^2 }{ 2 \sigma^2  } + \frac{ \Vert v\Vert  }{  \sigma  } Z  \ge \epsilon \\
    \longleftrightarrow  Z  \ge \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  }
\]</span></p>
<p>As <span class="math inline">\(Z \sim N(0, 1)\)</span>, <span class="math inline">\(\forall z \in \mathbb{R}_{\ge 0}\)</span>, <span class="math display">\[
    \Pr[ Z \ge z] \le \exp \Big( -\frac{ z^2 }{2} \Big).
\]</span></p>
<p>Assuming that <span class="math inline">\(\sigma^2 \ge \frac{\Vert v \Vert^2}{ 2 \epsilon }\)</span>. Then <span class="math inline">\(\frac{ \sigma }{ \Vert v\Vert } \epsilon - \frac{ \Vert v\Vert }{ 2 \sigma } \ge 0\)</span>. Replacing <span class="math inline">\(z\)</span> with <span class="math inline">\(\frac{ \sigma }{ \Vert v\Vert } \epsilon - \frac{ \Vert v\Vert }{ 2 \sigma }\)</span>, <span class="math display">\[
    \Pr[ z \ge t ] \le  \exp \Big( -\frac{1}{2} \Big( \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  } \Big)^2 \Big).
\]</span></p>
<p>We would like to bound this probability by some <span class="math inline">\(\delta \in (0, 1)\)</span>, then <span class="math display">\[
\begin{aligned}
    &amp;\exp \Big( -\frac{1}{2} \Big( \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  } \Big)^2 \Big) \le \delta \\
    &amp;\Longrightarrow \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  } \ge \sqrt{2 \ln \frac{1}{\delta} } \\
    &amp;\Longrightarrow \frac{\epsilon }{\Vert v\Vert  } \sigma^2 - \sqrt{2 \ln \frac{1}{\delta} } \sigma - \frac{ \Vert v\Vert  }{2} \ge 0
\end{aligned}
\]</span></p>
<p>Finally, we get <span class="math display">\[
    \sigma \ge \frac{ \sqrt{ 2 \ln \frac{1}{\delta} } + \sqrt{ 2 \ln \frac{1}{\delta} + 2 \epsilon  }  }{ 2 \frac{\epsilon }{\Vert v\Vert  } }
\]</span></p>
<p>By concavity of the square root function, it suffices to take <span class="math display">\[
    \sigma = \frac{ \Vert v\Vert  }{ \epsilon } \sqrt{ 0.5 \cdot 2 \ln \frac{1}{\delta} + 0.5 \cdot \Big( 2 \ln \frac{1}{\delta} + 2 \epsilon \Big) } = \frac{ \Vert v\Vert  }{ \epsilon } \sqrt{  2 \ln \frac{1}{\delta} + \epsilon },
\]</span></p>
<p>i.e., <span class="math display">\[
    \sigma^2 = \frac{ \Vert v\Vert ^2 }{ \epsilon^2 } \Big( 2 \ln \frac{1}{\delta} + \epsilon \Big). 
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h1 id="reference">Reference</h1>
<p>[1] G. Kamath, “Lecture 5 — Approximate Diﬀerential Privacy”, CS 860 - Algorithms for Private Data Analy sis.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/Gaussian-Distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/06/Gaussian-Distribution/" class="post-title-link" itemprop="url">Gaussian Distribution</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-06 23:00:19" itemprop="dateCreated datePublished" datetime="2020-11-06T23:00:19+11:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-09 22:00:48" itemprop="dateModified" datetime="2021-06-09T22:00:48+10:00">2021-06-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="gaussian-distribution">Gaussian Distribution</h1>
<p>We show how Gaussian distribution (a.k.a. normal distribution) is constructed and list some of its basic properties.</p>
<p>Suppose we would like to have a distribution in <span class="math inline">\(\mathbb{R}\)</span>, such that</p>
<ol type="1">
<li>It is centered at the origin.<br />
</li>
<li>Its density function decreases exponentially with respect to the square distance to the origin.</li>
</ol>
<blockquote>
<p><em>Remark: if we replace 2 with "a density function decreases exponentially with the distance to the origin", we obtain Laplace distribution.</em></p>
</blockquote>
<p>The density function <span class="math inline">\(p(x)\)</span> should be inversely and exponentially proportional to <span class="math inline">\(x^2\)</span>: <span class="math display">\[
    p(x) \propto \exp(-x^2 )
\]</span></p>
<p>Denote <span class="math inline">\(M\)</span> the value of<br />
<span class="math display">\[
    M \doteq \int_{-\infty}^\infty \exp(-x^2) \ dx, 
\]</span></p>
<p>then the close form of <span class="math inline">\(p(x)\)</span> is <span class="math display">\[
    p(x)  = \frac{1}{M} \exp(- x^2). 
\]</span></p>
<p>It is easy to verity that <span class="math inline">\(p(x)\)</span> is a valid density function: <span class="math inline">\(\int_{-\infty}^\infty p(x) \ dx = 1\)</span>.</p>
<h1 id="closed-form">Closed Form</h1>
<blockquote>
<p><strong>Theorem.</strong> $ p(x) =  ( - x^2 ) $ is a density function.</p>
</blockquote>
<p>In general, it is not hard to construct a distribution. Let <span class="math inline">\(h(x)\)</span> be any integrable function on a domain <span class="math inline">\(\mathcal{D}\)</span> such that <span class="math display">\[
    M = \int_\mathcal{D} h(x) \ dx &lt; \infty
\]</span></p>
<p>Then we can view <span class="math inline">\(h(x)\)</span> as almost an density function of some distribution. The only obstacle is that <span class="math inline">\(M\)</span> might not be <span class="math inline">\(1\)</span>. However this is easy to fix. We scale the value of <span class="math inline">\(h(x)\)</span> by a constant factor at each point by setting <span class="math display">\[
    p(x) = \frac{1}{M} h(x).
\]</span></p>
<p>In this case, we don't even need to know the exact value of <span class="math inline">\(M\)</span>. We only need to guarantee that</p>
<blockquote>
<p>The value of <span class="math inline">\(M\)</span> exists and is finite.</p>
</blockquote>
<p>Then we can safely rely on <span class="math inline">\(M\)</span> as a normalized. For example, we know that <span class="math display">\[
    \int_0^\infty x^{0.2} e^{-x} \ dx \le  \int_0^1 e^{-x} \ dx  + \int_1^\infty x e^{-x} \ dx &lt; \infty.
\]</span></p>
<p>We can construct a density function as discussed above. It turns out that this is a special case of <em>Gamma distribution</em>.</p>
<p>For Gaussian distribution, we are lucky as the closed form of <span class="math inline">\(M\)</span> does exists, due to the following "squaring trick": <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \Big( \int_{-\infty}^\infty \exp( -x^2 ) \  dx \Big)^2 \\
        &amp;= \int_{-\infty}^\infty \exp( -x^2 ) \  dx \ \cdot \int_{-\infty}^\infty \exp( -y^2 ) \  dy \\
        &amp;= \int_{-\infty}^\infty \int_{-\infty}^\infty \exp( -(x + y) ^2 ) \  dx  dy. 
\end{aligned}
\]</span></p>
<p>Denote <span class="math inline">\(r \doteq \sqrt{ x^2 + y^2}\)</span> the distance of <span class="math inline">\((x, y)\)</span> to <span class="math inline">\((0, 0)\)</span>. We are integrating the function <span class="math inline">\(\exp( - r^2)\)</span> over the plane <span class="math inline">\(\mathbb{R}^2\)</span>. It is natural to switch to polar coordinate system <span class="math inline">\((r, \theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the angle of a point. Here is an informal but intuitive explanation. As the picture below demonstrates, if <span class="math inline">\(r\)</span> increases by <span class="math inline">\(dr\)</span> and <span class="math inline">\(\theta\)</span> increases by <span class="math inline">\(d\theta\)</span>, the new area spanned by <span class="math inline">\(dr\)</span> and <span class="math inline">\(d \theta\)</span> is roughly <span class="math inline">\(r dr d \theta\)</span>.<br />
We can also calculate this algebraically<br />
<span class="math display">\[
\frac{1}{2} [(r + dr)^2 - r^2] d\theta = r dr d \theta + \frac{1}{2} (dr)^2 d \theta \approx r dr d \theta.
\]</span></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration1.png?raw=true" width="400" height="340" /></p>
</div>
<p>Finally, <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \int_{0}^{ 2 \pi} \int_{0}^\infty \exp( -r^2 ) r d r d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2}  \int_{0}^\infty \exp( -r^2 ) d r^2 d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2} d \theta \\
        &amp;= \pi.
\end{aligned}
\]</span></p>
<h1 id="basic-properties">Basic Properties</h1>
<h2 id="expectation"><strong>Expectation</strong></h2>
<p>As the distribution is symmetric to <span class="math inline">\(0\)</span>, it has expectation 0.</p>
<h2 id="variance"><strong>Variance</strong></h2>
<p>As its expectation is 0, the variance is given by</p>
<p><span class="math display">\[
\begin{aligned}
    \int_{-\infty}^\infty  \frac{ x^2 }{ \sqrt \pi } \exp( -x^2 ) \ dx 
        &amp;= -\frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty x \ d \exp( -x^2 ) \\
        &amp;= \frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty \exp( -x^2 ) d x\\
        &amp;= \frac{1}{2 \sqrt \pi } \sqrt \pi \\
        &amp;= \frac{1}{2}
\end{aligned}
\]</span></p>
<h2 id="general-gaussian-distribution">General Gaussian Distribution</h2>
<p>We have proved that if <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(p(x) = \frac{1}{\sqrt \pi} \exp( -x^2)\)</span>, then <span class="math display">\[
    \mathbb{Var}[X] = \frac{1}{2}.
\]</span></p>
<p>Define another random variable <span class="math inline">\(Y \doteq \sqrt{2} X\)</span>, whose variance is <span class="math display">\[
    \mathbb{Var}[X] = 2 \mathbb{Var}[X] = 1.
\]</span></p>
<p>To obtain its density function of <span class="math inline">\(Y\)</span>, we first scale the function <span class="math inline">\(p(x)\)</span> horizontally by a factor of <span class="math inline">\(\sqrt 2\)</span> to get <span class="math display">\[
    \frac{1}{ \sqrt{ \pi } } \exp \Big( -\frac{x^2}{2} \Big).
\]</span></p>
<p>Now the area under the curve <span class="math inline">\(f(x) = \frac{1}{ \sqrt{ \pi } } \exp \big( - x^2 / {2} \big)\)</span> is <span class="math inline">\(\sqrt 2\)</span>. We normalize this area to <span class="math inline">\(1\)</span> and obtain <span class="math inline">\(Y\)</span>'s density function as <span class="math display">\[
    p(y) = \frac{1}{ \sqrt{2 \pi } } \exp \Big( -\frac{y^2}{2} \Big)
\]</span></p>
<p>In what follows, we use <span class="math inline">\(N(0, 1)\)</span> to denote a Gaussian distribution with mean 0 and variance <span class="math inline">\(1\)</span>. We can also scale <span class="math inline">\(X\)</span> by a factor of <span class="math inline">\(\sqrt 2 \sigma\)</span> for any <span class="math inline">\(\sigma &gt; 0\)</span>, to get <span class="math inline">\(Y = \sqrt 2 \sigma X\)</span>. It have variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math display">\[
    p(y) = \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( - \Big(\frac{y}{ \sqrt{2} \sigma } \Big)^2 \Big) = \frac{1}{ \sqrt{2 \pi \sigma^2 } } \exp \Big( -\frac{y^2}{ 2 \sigma^2 } \Big).
\]</span></p>
<p>Finally, we can shift the center of <span class="math inline">\(0\)</span> by any real number <span class="math inline">\(\mu \in \mathbb{R}\)</span> to obtain a new density function <span class="math display">\[
    p(y) = \frac{1}{ \sqrt{ \pi \cdot 2  \sigma^2 } } \exp\Big( -\frac{ (y - \mu)^2 }{ 2 \sigma^2 } \Big).
\]</span></p>
<p>The corresponding distribution is denoted as <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<em>The picture below shows a few Gaussian distributions.</em>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution.png?raw=true" width="700" height="430" /></p>
</div>
<h1 id="advanced-properties">Advanced Properties</h1>
<h2 id="sum-of-normal-random-variables">Sum of Normal Random Variables</h2>
<blockquote>
<p>If <span class="math inline">\(X \sim N(0, a^2)\)</span> and <span class="math inline">\(Y \sim N(0, b^2)\)</span>, then <span class="math inline">\(X + Y \sim N(0, a^2 + b^2)\)</span>.</p>
</blockquote>
<p>The demonstration here follows from [1]. The key observation is that the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is rotation invariant with the origin.</p>
<p>As an example, we plot below the joint distribution of <span class="math inline">\(X, Y \sim N(0, 1)\)</span>.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-0.png?raw=true" width="500" height="330" /></p>
</div>
<p>The figure below shows the same distribution. Observe again that the probability mass concentrates at a small region centered at origin.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-1.png?raw=true" width="500" height="330" /></p>
</div>
<p>Therefore, for any <span class="math inline">\(t \in \mathbb{R}\)</span>, the set <span class="math display">\[
    \{ (x, y) \in \mathbb{R}^2 : ax + by \le t \}
\]</span> has the same probability measure as the one <span class="math display">\[
    \Big\{ (x, y) \in \mathbb{R}^2 : x \le \frac{ t}{ \sqrt{a^2 + b^2} } \Big\}.
\]</span></p>
<p>Observe that the latter can be obtained by rotating the former with respect to the origin (see the figure below).</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration2.png?raw=true" width="500" height="250" /></p>
</div>
<p>Now, the probability of the latter set is given by <span class="math display">\[
    \Pr \Big[ X \le \frac{ t}{ \sqrt{a^2 + b^2} } \Big] = \Pr\Big[  \sqrt{a^2 + b^2} X \le t \Big]
\]</span></p>
<p>It concludes that <span class="math inline">\(X + Y\)</span> has the same distribution as <span class="math inline">\(\sqrt{a^2 + b^2} X \sim N(0, a^2 + b^2)\)</span>.</p>
<!-- **Remark**: we can also verify this algebraically:
$$
    \begin{aligned}
        \Pr [ X + Y \le t ] 
            &= \int_{-\infty}^t \int_{-\infty }^{ t - x } \frac{1}{ 2 \pi } \exp \Big(-\frac{x^2 + y^2 }{2} \Big) \ dx 
    \end{aligned}
$$ -->
<!-- $$
    \Pr [X + Y \le t] = \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} } \exp \Big(-\frac{x^2}{2} \Big) \ dx 
$$

Hence
$$
\begin{aligned}
    \frac{ \partial }{ \partial t} \Pr[X + Y \le t]
        &= \frac{ \partial }{ \partial t} \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} }\exp \Big(-\frac{x^2}{2} \Big) \ dx \\
        &= \frac{1}{ \sqrt{2 \pi (a^2 + b^2) } } \exp \Big( -\frac{x^2}{2 (a^2 + b^2) } \Big).
\end{aligned}
$$ -->
<h2 id="concentration-inequalities">Concentration Inequalities</h2>
<h3 id="bound-1"><strong>Bound 1</strong></h3>
<blockquote>
<p>If <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>, then <span class="math inline">\(\forall t &gt; 0\)</span>, <span class="math display">\[
\Pr[ |X| \ge t] \le \exp \Big( -\frac{t^2}{ 2\sigma^2} \Big)
\]</span></p>
</blockquote>
<p><strong>Remark</strong>: it is possible to get a tighter bound by using more advanced techniques.</p>
<p><strong>Intuition:</strong> in previous figures, it seems Gaussian distributions have a shape bump around its mean. This is kind of mis-leading, because the x-axis and y-axis are scaled. In the figure below, we plot <span class="math inline">\(N(0, 1)\)</span>, with ratio between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes being approximate <span class="math inline">\(1:1\)</span>. We see only a small bump around its mean. Although the distribution span the entire range of <span class="math inline">\(\mathbb{R}\)</span>, the probability mass is centered tightly around the origin.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Concentration.png?raw=true" width="600" height="400" /></p>
</div>
<h4 id="proof-by-moment-generating-function">Proof by Moment Generating Function</h4>
<p>We prove a weaker version by a factor of <span class="math inline">\(2\)</span>. For all <span class="math inline">\(\lambda \in \mathbb{R}\)</span>, <span class="math display">\[
    \begin{aligned}
        \mathbb{E}[ e^{\lambda X} ] 
            &amp;= \frac{1}{\sqrt{2 \pi \sigma} }\int e^{\lambda x} e^{ - \frac{x^2}{2 \sigma^2 } } \ d x \\
            &amp;= \frac{ e^{\lambda^2 \sigma^2 / 2} }{\sqrt{2 \pi \sigma} }\int e^{ - \frac{ (x - \lambda \sigma^2)^2}{2 \sigma^2 } } \ d x \\
            &amp;= e^{\lambda^2 \sigma^2 / 2}.
    \end{aligned}
\]</span></p>
<p>Via Markov's inequality, for all <span class="math inline">\(\lambda \ge 0\)</span>, we have <span class="math display">\[
    \begin{aligned}
        \Pr[ X \ge t] 
            &amp;\le \frac{ \mathbb{E}[ e^{\lambda X} ]  }{ e^{\lambda t} } 
            &amp;\le e^{\lambda^2 \sigma^2 / 2 - \lambda t}.
    \end{aligned}
\]</span></p>
<p>Setting <span class="math inline">\(\lambda = \frac{t}{\sigma^2}\)</span>, we get <span class="math display">\[
    \Pr[ X \ge t ] \le e^{ - \frac{t^2}{2 \sigma^2} }.
\]</span></p>
<p>It concludes that <span class="math inline">\(\Pr[ |X| \ge t ] \le 2 e^{ - \frac{t^2}{2 \sigma^2} }.\)</span></p>
<h4 id="proof-by-geometry">Proof by Geometry</h4>
<p>Let <span class="math inline">\(T = \Pr[ |X| \ge t]\)</span>. Then <span class="math display">\[
    T^2 = \int_{ |x| \ge t, |y| \ge t} \frac{1}{ 2 \pi \sigma^2 } \exp \Big( - \frac{ x^2 + y^2}{2 \sigma^2} \Big). 
\]</span></p>
<p>As <span class="math display">\[
    \{ (x, y) \in \mathbb{R}^2: |x| \ge t \wedge |y| \ge t \} \subset
    \{ (x, y) \in \mathbb{R}^2: x^2 + y^2 \ge 2 t^2 \} 
\]</span></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration3.png?raw=true" width="400" height="400" /></p>
</div>
<p>It follows that <span class="math display">\[
\begin{aligned}
    T^2 
        &amp;\le \int_{ \sqrt 2 t}^\infty \int_{0}^{2 \pi } \frac{1}{ 2 \pi \sigma^2 } \exp \Big( - \frac{ r^2 }{2 \sigma^2} \Big) r \ d\theta dr\\
        &amp;\le \int_{ \sqrt 2 t }^\infty \exp \Big( - \frac{ r^2 }{2 \sigma^2} \Big) \ d \frac{r^2}{ 2\sigma^2} \\
        &amp;\le \int_{ \frac{ t^2 }{  \sigma^2 } }^\infty \exp ( - z ) \ d z \\
        &amp;= \exp \Big( -\frac{t^2}{ \sigma^2} \Big).
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math display">\[
    T \le \exp \Big( - \frac{t^2}{ 2\sigma^2} \Big).
\]</span></p>
<h3 id="bound-2"><strong>Bound 2</strong></h3>
<blockquote>
<p>If <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>, then <span class="math inline">\(\forall t &gt; 0\)</span>, <span class="math display">\[
\Pr[ X \ge t] \le \frac{ \sigma }{ \sqrt{2 \pi }  t } \exp \Big( -\frac{t^2}{ 2\sigma^2} \Big).
\]</span> By symmetry we also have <span class="math display">\[
\Pr[ X \le -t] \le \frac{ \sigma }{ \sqrt{2 \pi }  t } \exp \Big( -\frac{t^2}{ 2\sigma^2} \Big).
\]</span></p>
</blockquote>
<p><strong>Proof.</strong> <span class="math display">\[
    \begin{aligned}
        \Pr[X &gt; t] 
            &amp;= \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( - \frac{ x^2 }{ 2 \sigma^2 } \Big) \ dx \\
            &amp;\le \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \frac{x}{t} \exp \Big( - \frac{ x^2 }{ 2  \sigma^2 } \Big) \ dx \\
            &amp;\le  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \int_{t}^\infty  \exp \Big( - \frac{ x^2 }{ 2  \sigma^2 } \Big) \ d \frac{ x^2 }{ 2 \sigma^2 } \\
            &amp;=  - \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp \Big( - \frac{ x^2 }{ 2  \sigma^2 } \Big) \mid_{t}^\infty \\
            &amp;=  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp \Big( - \frac{ t^2 }{ 2 \sigma^2 } \Big).  
    \end{aligned}
\]</span></p>
<p>This bound is more useful only when we know that <span class="math inline">\(t \ge \frac{ \sigma }{ \sqrt{2 \pi } }\)</span>.</p>
<h1 id="reference">Reference</h1>
<p>[1] B. Eisenberg and R. Sullivan, “Why Is the Sum of Independent Normal Random Variables Normal?,” Mathematics Magazine, vol. 81, no. 5, pp. 362–366, Dec. 2008</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/24/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/63/">63</a><a class="extend next" rel="next" href="/page/26/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">187</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
