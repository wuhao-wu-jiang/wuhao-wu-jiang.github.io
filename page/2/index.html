<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Helvetica:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.24.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/2/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>WOW</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">WOW</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">201</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/10/22/Conditional-Probability/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/22/Conditional-Probability/" class="post-title-link" itemprop="url">Conditional Probability</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-22 22:45:24" itemprop="dateCreated datePublished" datetime="2024-10-22T22:45:24-04:00">2024-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-10-25 09:50:52" itemprop="dateModified" datetime="2024-10-25T09:50:52-04:00">2024-10-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Let <span class="math inline">\((\Omega, \mathcal{F},
\mathbb{P})\)</span> be a probability space.</p>
<blockquote>
<p><strong>Definition (Conditional Probability of A Set).</strong> Given
<span class="math inline">\(A, B \in \mathcal{F}\)</span>, if <span
class="math inline">\(\mathbb{P}(B) \neq 0\)</span>, then the
conditional probability of <span class="math inline">\(A\)</span> on
<span class="math inline">\(B\)</span> is defined by <span
class="math display">\[
\mathbb{P}[ A \mid B ] \doteq \frac{\mathbb{P}[A \cap
B]}{\mathbb{P}[B]}.
\]</span></p>
</blockquote>
<p>In general, the conditional probability is defined as a random
variable measurable over a sub <span
class="math inline">\(\sigma\)</span>-field over <span
class="math inline">\(\mathcal{F}\)</span>. We present first the
definition, and then use examples to illustrate it.</p>
<blockquote>
<p><strong>Definition (Conditional Probability).</strong> Let <span
class="math inline">\(\mathcal{G}\)</span> be a <span
class="math inline">\(\sigma\)</span>-field in <span
class="math inline">\(\mathcal{F}\)</span>, and <span
class="math inline">\(A \in \mathcal{F}\)</span>. The conditional
probability <span class="math inline">\(\mathbb{P}[A \mid
\mathcal{G}]\)</span> is a random variable with two properties:<br />
1. <span class="math inline">\(\mathbb{P}[ A \mid \mathcal{G} ]\)</span>
is measurable <span class="math inline">\(\mathcal{G}\)</span> and
integrable.<br />
2. <span class="math inline">\(\mathbb{P}[A \mid \mathcal{G}]\)</span>
satisfies the functional equation <span class="math display">\[
\int_G \mathbb{P}[A \mid \mathcal{G}] \, d \mathbb{P}
= \mathbb{P}[ A \cap G ],
\quad
\forall G \in \mathcal{G}.
\]</span></p>
</blockquote>
<p>The following provides the definition of conditional probability with
respect to a random variable.</p>
<blockquote>
<p><strong>Definition.</strong> The conditional probability of <span
class="math inline">\(A \in \mathcal{F}\)</span> given a random variable
<span class="math inline">\(X\)</span> is defined as <span
class="math inline">\(\mathbb{P}[ A \mid \sigma(X) ]\)</span> and is
denoted <span class="math inline">\(\mathbb{P}[A \mid X]\)</span>, where
<span class="math inline">\(\sigma(X)\)</span> is the <span
class="math inline">\(\sigma\)</span>-field generated by <span
class="math inline">\(X\)</span>.</p>
</blockquote>
<p><strong>Remark.</strong> The <span
class="math inline">\(\sigma\)</span>-field <span
class="math inline">\(\sigma(X)\)</span> consists of the sets <span
class="math inline">\([\omega: X(\omega) \in H]\)</span> for <span
class="math inline">\(H \in \mathcal{R}^1\)</span>.</p>
<p><strong>Remark.</strong> <em>From the experiment corresponding to the
<span class="math inline">\(\sigma\)</span>-field <span
class="math inline">\(\sigma(X)\)</span>, one learns which of the set
<span class="math inline">\([\omega&#39; : X(\omega&#39;) = x]\)</span>
contains <span class="math inline">\(\omega\)</span> and hence learns
the value of <span class="math inline">\(X(\omega)\)</span>.</em></p>
<h2 id="existence">Existence</h2>
<p>It is easy to observe that if such a random variable exists and we
alter its value on a set in <span
class="math inline">\(\mathcal{G}\)</span> of measure 0, the conditions
will still hold. The key challenge, however, is to prove the existence
of such a random variable.</p>
<blockquote>
<p><strong>Theorem.</strong> <span class="math inline">\(\mathbb{P}[A
\mid \mathcal{G}]\)</span> exists.</p>
</blockquote>
<p><strong>Proof.</strong> Define a measure <span
class="math inline">\(\mu_A\)</span> in <span
class="math inline">\(\mathcal{G}\)</span> by <span
class="math display">\[
    \mu_A (G) \doteq \mathbb{P}[ A \cap G ],
    \quad
    \forall G \in \mathcal{G}.
\]</span> Then <span class="math inline">\(\mu_A\)</span> is absolutely
continuous w.r.t. <span class="math inline">\(\mu\)</span>. The
Randon-Nikodym theorem implies that there exists a function <span
class="math inline">\(f: \Omega \rightarrow \mathbb{R}\)</span>,
measurable <span class="math inline">\(\mathcal{G}\)</span> and
integrable w.r.t. <span class="math inline">\(\mathbb{P}\)</span>, s.t.,
<span class="math display">\[
    \mu_A(G)
        = \int_G f \, d \mathbb{P},
    \quad
    \forall G \in \mathcal{G}.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="examples">Examples</h2>
<p><strong>Example.</strong> Let <span class="math inline">\(A \in
\mathcal{F}\)</span>. Let <span class="math inline">\(\mathcal{G} =
\lbrace \varnothing, \Omega \rbrace\)</span>. Define the random variable
<span class="math inline">\(X\)</span> by <span class="math display">\[
    X(\omega) \equiv \mathbb{P}[ A ].
\]</span> It is clear that <span
class="math inline">\(X(\omega)\)</span> satisfies the two conditions
required for the definition of conditional probability.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Example.</strong> Let <span class="math inline">\(A, B \in
\mathcal{F}\)</span> satisfies <span class="math inline">\(\mathbb{P}[B]
\neq 0\)</span> and <span class="math inline">\(\mathbb{P}[\bar{B}] \neq
0\)</span>. Let <span class="math inline">\(\mathcal{G} = \lbrace
\varnothing, B, \bar{B}, \Omega \rbrace\)</span>. Define the random
variable <span class="math inline">\(X\)</span> by <span
class="math display">\[
    X(\omega) = \begin{cases}
        \mathbb{P}[ A \mid B ], &amp; \text{ if } \omega \in B, \\
        \mathbb{P}[ A \mid \bar{B} ], &amp; \text{ if } \omega \in
\bar{B}.
    \end{cases}
\]</span> Clearly, <span class="math inline">\(X(\omega)\)</span>
satisfies the two conditions required for the definition of conditional
probability.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Example.</strong> Let <span class="math inline">\(A \in
\mathcal{F}\)</span>, and <span class="math inline">\(B_n \in
\mathcal{F}, n \in \mathbb{N}^+\)</span> be a partition of <span
class="math inline">\(\Omega\)</span> satisfying <span
class="math inline">\(\mathbb{P}[B_n] \neq 0\)</span>. Further, let
<span class="math inline">\(\mathcal{G}\)</span> be the <span
class="math inline">\(\sigma\)</span>-field generated by the <span
class="math inline">\(B_n\)</span>. Define the random variable <span
class="math inline">\(X\)</span> by <span class="math display">\[
    X(\omega) =
        \mathbb{P}[ A \mid B_n ], \, \text{ if } \omega \in B_n,
\]</span> Once again, <span class="math inline">\(X(\omega)\)</span>
satisfies the two conditions required for the definition of
<em>conditional probability.</em></p>
<p><span class="math inline">\(\square\)</span></p>
<p>In the examples above, when interpreting the conditional probability
as a function, we don't need to know the exact value of <span
class="math inline">\(\omega\)</span> to determine the function output;
knowing which set in the <span
class="math inline">\(\sigma\)</span>-field contains <span
class="math inline">\(\omega\)</span> is sufficient. Requiring that
<span class="math inline">\(X(\omega) = \mathbb{P}[A \mid
\mathcal{G}]_{\omega}\)</span> to be measurable <span
class="math inline">\(\mathcal{G}\)</span> essentially means that, if we
can determine whether <span class="math inline">\(\omega\)</span>
belongs to each set <span class="math inline">\(H \in
\mathcal{G}\)</span>, we can determine the function's output.
Specifically, knowing which set <span class="math inline">\(X^{-1}(a),
\forall a \in \mathbb{R}\)</span> contains <span
class="math inline">\(\omega\)</span> allows us to identify the value of
<span class="math inline">\(X\)</span>.</p>
<p><strong>Example.</strong> Assume that <span
class="math inline">\(\Omega = \mathbb{R}^2\)</span>, <span
class="math inline">\(\mathcal{F} = \mathcal{R}^2\)</span> be the class
of Borel sets, and <span class="math inline">\(\mathbb{P}\)</span> be a
probability measure on with density <span class="math inline">\(f(x,
y)\)</span> w.r.t the Lebesgue measure. Let <span
class="math inline">\(X(x, y) = x\)</span>. Further, <span
class="math display">\[
    \sigma(X) = \mathcal{G}
\]</span> is the <span class="math inline">\(\sigma\)</span>-field
generated by the vertical strips : <span class="math inline">\(\lbrace E
\times \mathbb{R}
= [ (x, y): x \in E ] : E \in \mathcal{R}^1 \rbrace\)</span>. Let <span
class="math inline">\(A  = \mathbb{R} \times F = [(x, y ): y \in
F]\)</span>, where <span class="math inline">\(F \in
\mathcal{R}^1\)</span> be a horizontal strip. Then we claim that <span
class="math display">\[
    \mathbb{P}\big[ A \mid \mathcal{G} \big]_{(x, y)}
        \doteq
        \varphi(x, y)
        =
        \begin{cases}
            \begin{aligned}
                \frac{
                    \int_F f(x, t) \, dt
                }{
                    \int_\mathbb{R} f(x, t) \, dt
                },
            \end{aligned}
            &amp; \text{ if } \int_\mathbb{R} f(x, t) \, dt \neq 0, \\
            0,
            &amp; \text{ if } \int_\mathbb{R} f(x, t) \, dt = 0.
        \end{cases}
\]</span></p>
<p>We need to verify that</p>
<ol type="1">
<li><p><span class="math inline">\(\varphi(x, y)\)</span> is measurable
<span class="math inline">\(\mathcal{G}\)</span>: first, note that both
<span class="math inline">\(\int_F f(x, t) \, dt\)</span> and <span
class="math inline">\(\int_\mathbb{R} f(x, t) \, dt\)</span> are
measurable <span class="math inline">\(\mathcal{R}\)</span>. So is their
ratio. It follows that, for each <span class="math inline">\(H \in
\mathcal{R}\)</span>, <span class="math inline">\(\varphi^{-1}(H) = E
\times \mathbb{R}\)</span> <span class="math inline">\(E \in
\mathcal{R}\)</span>.</p></li>
<li><p>For each <span class="math inline">\(G = E \times \mathbb{R} \in
\mathcal{G}\)</span>, where <span class="math inline">\(E \in
\mathcal{R}\)</span>, it holds that <span class="math display">\[
    \begin{aligned}
        \int_{E \times \mathbb{R}} \varphi(x, y) \, d \mathbb{P}
            &amp;= \int_{E \times \mathbb{R}} \varphi(x, y) \cdot f(x,
y) \, d (x \times y) \\
            &amp;= \int_E \int_\mathbb{R} \frac{
                \int_F f(x, t) \, dt
            }{
                \int_\mathbb{R} f(x, t) \, dt
            } \cdot f(x, y) \, dy \, d x \\
            &amp;= \int_E
                \int_F f(x, t) \, dt
            \, d x \\
            &amp;= \mathbb{P}[E \times F] = \mathbb{P}[A \times G].
    \end{aligned}
\]</span></p></li>
</ol>
<p><strong>Example.</strong> Let <span class="math inline">\(X : \Omega
\rightarrow \mathbb{R}^j\)</span> and <span class="math inline">\(Y :
\Omega \rightarrow \mathbb{R}^k\)</span> be random vectors, with
distributions <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> respectively. Suppose that <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are independent, then by Fubini's
theorem, for each <span class="math inline">\(H \in \mathcal{R}^j, J \in
\mathcal{R}^{j + k}\)</span>, <span class="math display">\[
\begin{aligned}
    \mathbb{P}[ X \in H, (X, Y) \in J ]
        &amp;= \int_{\mathbb{R}^2} \mathbf{1}_{[X \in H, (X, Y) \in J]}
\, d \mathbb{P} \\
        &amp;= \int_{\mathbb{R}} \int_{\mathbb{R}} \mathbf{1}_{[x \in H,
(x, y) \in J]} \, \nu(dy) \, \mu(dx) \\
        &amp;= \int_{\mathbb{R}} \mathbb{P}[ x \in H, (x, Y) \in J] \,
\mu(dx)
        = \int_{H} \mathbb{P}[ (x, Y) \in J] \, \mu(dx).
\end{aligned}
\]</span> We can construct the conditional probability <span
class="math inline">\(\mathbb{P}[(X, Y) \in J]_\omega\)</span> as
follows. Let <span class="math inline">\(f(x) \doteq \mathbb{P}[ (x, Y)
\in J] = \mathbb{P}[ \lbrace \omega : (x, Y(\omega)) \in J \rbrace
]\)</span>. Then we claim that <span
class="math inline">\(f(X(\omega))\)</span> is a version of <span
class="math inline">\(\mathbb{P}[(X, Y) \in J \mid X
]_\omega\)</span>.</p>
<ol type="1">
<li>First, by Fubini's theorem, <span class="math inline">\(f\)</span>
is measurable <span class="math inline">\(\mathcal{R}\)</span>,
therefore <span class="math inline">\(f \circ X\)</span> is measurable
<span class="math inline">\(\sigma(X)\)</span>.<br />
</li>
<li>Second, each <span class="math inline">\(G \in \sigma(X)\)</span>
has the form <span class="math inline">\(G = \left\lbrace \omega :
X(\omega) \in H \right\rbrace\)</span> for some <span
class="math inline">\(H \in \mathcal{R}\)</span>. Therefore, <span
class="math display">\[
\begin{aligned}
     \mathbb{P}\left[ \left\lbrace \omega : X(\omega) \in H
\right\rbrace \bigcap \left\lbrace \omega : (X(\omega), Y(\omega)) \in J
\right\rbrace \right]
         &amp;= \mathbb{P}[ X \in H, (X, Y) \in J ] \\
         &amp;= \int_{H} \mathbb{P}[ (x, Y) \in J] \, \mu(dx) \\
         &amp;= \int_{H} f(X) \, d \mathbb{P}.
\end{aligned}
\]</span></li>
</ol>
<h2 id="properties-of-conditional-probability">Properties of Conditional
Probability</h2>
<ol type="1">
<li><p>With probability <span class="math inline">\(1\)</span>, <span
class="math inline">\(\mathbb{P}[\varnothing \mid \mathcal{G}] =
0\)</span>, and <span class="math inline">\(\mathbb{P}[\Omega \mid
\mathcal{G}] = 1\)</span>; and <span class="math display">\[
    0 \le
    \mathbb{P}[A \mid \mathcal{G}]
    \le 1,
\]</span> for each <span class="math inline">\(A \in
\mathcal{G}\)</span>.</p>
<p><strong>Proof.</strong> Since <span
class="math inline">\(\mathbb{P}[\varnothing \mid \mathcal{G}]\)</span>
is measurable <span class="math inline">\(\mathcal{G}\)</span>, <span
class="math inline">\(G_k \doteq \lbrace \omega : \mathbb{P}[\varnothing
\mid \mathcal{G}]_\omega &gt; \frac{1}{k} \rbrace \in
\mathcal{G}\)</span> for each <span class="math inline">\(k \in
\mathbb{N}^+\)</span>. It follows that <span class="math display">\[
\begin{aligned}
    0 = \mathbb{P}[\varnothing \cap G_k]
    = \int_{G_k} \mathbb{P}[\varnothing \mid \mathcal{G}] \, d
\mathbb{P}   
    \ge \frac{1}{k} \cdot \mathbb{P}[G_k]  = 0.
\end{aligned}
\]</span><br />
Therefore, the set <span class="math inline">\(\{ \omega :
\mathbb{P}[\varnothing \mid \mathcal{G}]_\omega &gt; 0\} = \bigcup_{k
\ge 1} G_k\)</span> has measure <span class="math inline">\(0\)</span>.
By analogous argument, we can prove that for the random variable <span
class="math inline">\(- \mathbb{P}[\varnothing \mid
\mathcal{G}]\)</span> that the set <span class="math inline">\(\{ \omega
: - \mathbb{P}[\varnothing \mid \mathcal{G}]_\omega &gt; 0\}\)</span>
has measure <span class="math inline">\(0\)</span>.</p>
<p>Similarly, we can show that <span class="math inline">\(1 -
\mathbb{P}[\Omega \mid \mathcal{G}] = 0\)</span> with probability <span
class="math inline">\(1\)</span>.</p>
<p>To prove the last claim, let <span class="math inline">\(G_k \doteq
\lbrace \omega : \mathbb{P}[\varnothing \mid \mathcal{G}]_\omega &lt;
-\frac{1}{k} \rbrace \in \mathcal{G}\)</span> for each <span
class="math inline">\(k \in \mathbb{N}^+\)</span>. Then: <span
class="math display">\[
\begin{aligned}
    0 \le \mathbb{P}[A \cap G_k]
    = \int_{G_k} \mathbb{P}[A \mid \mathcal{G}] \, d \mathbb{P}   
    &lt; -\frac{1}{k} \cdot \mathbb{P}[G_k]  = 0.
\end{aligned}
\]</span><br />
Thus, <span class="math inline">\(\mathbb{P}[G_k]  = 0\)</span>.
Consequently, the set <span class="math inline">\(\{ \omega :
\mathbb{P}[A \mid \mathcal{G}]_\omega &lt; 0\} = \bigcup_{k \ge 1}
G_k\)</span> has measure <span class="math inline">\(0\)</span>.
Applying the same reasoning to <span class="math inline">\(1 -
\mathbb{P}[A \mid \mathcal{G}]\)</span> proves that <span
class="math inline">\(\mathbb{P}[A \mid \mathcal{G}] \le 1\)</span> with
probability <span class="math inline">\(1\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p></li>
<li><p>If <span class="math inline">\(A_1, A_2, \ldots\)</span> is a
finite or countable sequence of disjoint sets, then <span
class="math display">\[
     \mathbb{P}[ \bigcup_{n \ge 1} A_n \mid \mathcal{G} ]
         = \sum_{n \ge 1} \mathbb{P}[A_n \mid \mathcal{G}].
\]</span></p>
<p><strong>Proof.</strong> Without loss of generality, assume that <span
class="math inline">\(\mathbb{P}[A_n \mid \mathcal{G}] \ge 0\)</span>.
For each <span class="math inline">\(G \in \mathcal{G}\)</span>, it
holds that <span class="math display">\[
     \begin{aligned}
         \int_G \left( \mathbb{P}\left[ \bigcup_{n \ge 1} A_n \mid
\mathcal{G} \right]
         - \sum_{n \ge 1} \mathbb{P}\left[A_n \mid \mathcal{G}\right]
\right) \, d \mathbb{P}
             &amp;= \int_G \mathbb{P}\left[ \bigcup_{n \ge 1} A_n \mid
\mathcal{G} \right] \, d \mathbb{P}
             - \int_G \sum_{n \ge 1} \mathbb{P}\left[A_n \mid
\mathcal{G}\right] \, d \mathbb{P} \\
             &amp;= \int_G \mathbb{P}\left[ \bigcup_{n \ge 1} A_n \mid
\mathcal{G} \right] \, d \mathbb{P}
             - \sum_{n \ge 1} \int_G \mathbb{P}\left[A_n \mid
\mathcal{G}\right] \, d \mathbb{P} \\
             &amp;= \mathbb{P}\left[ \bigcup_{n \ge 1} A_n \cap G
\right] - \sum_{n \ge 1} \mathbb{P}[ A_n \cap G ]
             = 0,
     \end{aligned}
\]</span> where the second equality follows from monotone convergence
theorem. By the previous property, we conclude that <span
class="math inline">\(\mathbb{P}[ \bigcup_{n \ge 1} A_n \mid \mathcal{G}
] - \sum_{n \ge 1} \mathbb{P}[A_n \mid \mathcal{G}] = 0\)</span> with
probability <span class="math inline">\(1\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p></li>
<li><p>If <span class="math inline">\(A \subseteq B\)</span>, then <span
class="math display">\[
    \mathbb{P}[B - A \mid \mathcal{G}]
    = \mathbb{P}[B \mid \mathcal{G}]
    - \mathbb{P}[A \mid \mathcal{G}],
    \quad
    \mathbb{P}[A \mid \mathcal{G}]
    \le
    \mathbb{P}[B \mid \mathcal{G}].
\]</span></p>
<p><strong>Proof.</strong> Based on the previous property, <span
class="math display">\[
    \mathbb{P}[B - A \mid \mathcal{G}] + \mathbb{P}[A \mid \mathcal{G}]
    = \mathbb{P}[B \mid \mathcal{G}],
\]</span> which finishes the proof.</p>
<p><span class="math inline">\(\square\)</span></p></li>
<li><p>If <span class="math inline">\(A_n \uparrow A\)</span>, then
<span class="math display">\[
    \mathbb{P}[A_n \mid \mathcal{G}] \uparrow \mathbb{P}[A \mid
\mathcal{G}].
\]</span></p>
<p><strong>Proof.</strong> Define <span class="math inline">\(B_1 =
A_1\)</span>, and for each <span class="math inline">\(n \ge 2\)</span>,
let <span class="math inline">\(B_n = A_n - A_{n - 1}\)</span>. Then the
<span class="math inline">\(B_n\)</span> are mutually disjoint, and
<span class="math inline">\(\bigcup_{n \ge 1} B_n = A\)</span>. Based on
property 3, it holds with probability <span
class="math inline">\(1\)</span> that <span class="math display">\[
    \mathbb{P}[A_1 \mid \mathcal{G}] \le \mathbb{P}[A_2 \mid
\mathcal{G}] \le \cdots \le \mathbb{P}[A \mid \mathcal{G}].
\]</span> Further, based on property 2, it holds with probability <span
class="math inline">\(1\)</span> that <span class="math display">\[
    \begin{aligned}
        \mathbb{P}\left[ A_n \middle| \mathcal{G} \right]
            = \mathbb{P}\left[ \bigcup_{i = 1}^n B_n \middle|
\mathcal{G} \right]
            = \sum_{i = 1}^n \mathbb{P}[B_n \mid \mathcal{G}], \\
        \mathbb{P}\left[ A \middle| \mathcal{G} \right]
            = \mathbb{P}\left[ \bigcup_{n \ge 1} B_n \mid \mathcal{G}
\right]
            = \sum_{n \ge 1} \mathbb{P}[B_n \mid \mathcal{G}].
    \end{aligned}
\]</span> Therefore, <span class="math display">\[
    \mathbb{P}\left[ A \middle| \mathcal{G} \right]
        = \lim_{n \rightarrow \infty} \sum_{i \in [n]} \mathbb{P}[B_n
\mid \mathcal{G}]
        = \lim_{n \rightarrow \infty} \mathbb{P}\left[ A_n \middle|
\mathcal{G} \right].
\]</span> <span class="math inline">\(\square\)</span></p></li>
<li><p>If <span class="math inline">\(A_n \downarrow A\)</span>, then
<span class="math display">\[
    \mathbb{P}[A_n \mid \mathcal{G}] \, \Big\downarrow \, \mathbb{P}[A
\mid \mathcal{G}].
\]</span></p>
<p><strong>Proof.</strong> Let <span class="math inline">\(B_n = A_1 -
A_n\)</span>, and <span class="math inline">\(B = A_1 - A\)</span>. Then
<span class="math inline">\(B_n \uparrow B\)</span>, based on property 4
we know that <span class="math display">\[
    \mathbb{P}[A_1 - A_n \mid \mathcal{G}] \, \Big\uparrow \,
\mathbb{P}[A_1 - A \mid \mathcal{G}].
\]</span> On the other hand, we know that <span class="math display">\[
    \begin{aligned}
        \mathbb{P}[A_1 - A_n \mid \mathcal{G}]
            &amp;= \mathbb{P}[A_1 \mid \mathcal{G}] - \mathbb{P}[A_n
\mid \mathcal{G}], \\
        \mathbb{P}[A_1 - A \mid \mathcal{G}]
            &amp;= \mathbb{P}[A_1 \mid \mathcal{G}] - \mathbb{P}[A \mid
\mathcal{G}].
    \end{aligned}
\]</span> Noting that <span class="math inline">\(\mathbb{P}[A_1 \mid
\mathcal{G}] \le 1\)</span> with probability <span
class="math inline">\(1\)</span> completes the proof.</p>
<p><span class="math inline">\(\square\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{P}(A) = 1 \implies
\mathbb{P}[A \mid \mathcal{G}] = 1\)</span>.</p>
<p><strong>Proof.</strong> The proof is similar to the proof in property
<span class="math inline">\(1\)</span>, in which the essential property
we are using are <span class="math inline">\(\mathbb{P}[\varnothing] =
0\)</span>, and <span class="math inline">\(\mathbb{P}[\Omega] =
1\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{P}(A) = 0 \implies
\mathbb{P}[A \mid \mathcal{G}] = 0\)</span>.</p>
<p><strong>Proof.</strong> Similar as property 6.</p>
<p><span class="math inline">\(\square\)</span></p></li>
</ol>
<h2 id="conditional-probability-distribution">Conditional Probability
distribution</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable on
<span class="math inline">\((\Omega, \mathcal{F}, \mathbb{P})\)</span>,
and let <span class="math inline">\(\mathcal{G}\)</span> be a <span
class="math inline">\(\sigma\)</span>-field in <span
class="math inline">\(\mathcal{F}\)</span>.</p>
<blockquote>
<p><strong>Theorem.</strong> There exists a function <span
class="math inline">\(\mu(H, \omega)\)</span>, defined on <span
class="math inline">\(\mathcal{R} \times \Omega\)</span>, with these two
properties:</p>
<ol type="1">
<li>For each <span class="math inline">\(\omega \in \Omega\)</span>,
<span class="math inline">\(\mu(\cdot, \mu)\)</span> is a probability
measurable on <span class="math inline">\(\mathcal{R}^1\)</span>.</li>
<li>For each <span class="math inline">\(H \in \mathcal{R}\)</span>,
<span class="math inline">\(\mu(H, \cdot)\)</span> is a version of <span
class="math inline">\(\mathbb{P}[X \in H \mid
\mathcal{G}]\)</span>.</li>
</ol>
</blockquote>
<p><strong>Proof.</strong> Define <span class="math inline">\(F(r,
\omega) = \mathbb{P}[X \le r \mid \mathcal{G}]_\omega\)</span>, for each
<span class="math inline">\(r \in \mathbb{Q}\)</span>. If <span
class="math inline">\(r \le s\)</span>, then based on the properties of
conditional probability, with probability <span
class="math inline">\(1\)</span>: <span class="math display">\[
\begin{aligned}
    F(r, \omega)
        &amp;\le F(s, \omega), \\
    F(r, \omega)
        &amp;= \lim_{n} F(r + n^{-1}, \omega), \\
    \lim_{r \rightarrow -\infty} F(r, \omega) &amp;= 0, \\
    \lim_{r \rightarrow \infty} F(r, \omega) &amp;= 1.
\end{aligned}
\]</span> As there are only countably many of these events, their
intersection, denoted by <span class="math inline">\(E\)</span>, has
probability <span class="math inline">\(1\)</span>.</p>
<ul>
<li><p>For each <span class="math inline">\(\omega \in E\)</span>,
extend <span class="math inline">\(F(\cdot , \omega)\)</span> to all of
<span class="math inline">\(\mathbb{R}\)</span> by <span
class="math inline">\(F(x, \omega) \doteq \inf[ F(r, \omega) : x &lt; r
]\)</span>. It is easy to see that <span class="math inline">\(F(\cdot,
\omega)\)</span> is non-decreasing, right-continuous and therefore a
probability distribution function. Therefore, there exists a probability
measure <span class="math inline">\(\mu(\cdot, \omega)\)</span> on <span
class="math inline">\((\mathbb{R}, \mathcal{R})\)</span> with
distribution function <span class="math inline">\(F(\cdot,
\omega)\)</span>.</p></li>
<li><p>For each <span class="math inline">\(\omega \notin E\)</span>,
take <span class="math inline">\(F(x, \omega) = F(x)\)</span> for some
arbitrary but fixed distribution function <span
class="math inline">\(F\)</span>, and let <span
class="math inline">\(\mu(\cdot, \omega)\)</span> be the probability
measure corresponding to <span
class="math inline">\(F(x)\)</span>.</p></li>
</ul>
<p>Thus, condition 1 is satisfied.</p>
<p>Next, consider the class of <span class="math inline">\(H\)</span>
for which <span class="math inline">\(\mu(H, \cdot)\)</span> is
measurable <span class="math inline">\(\mathcal{G}\)</span>: <span
class="math display">\[
    \mathcal{S} \doteq \{
        H \in \mathcal{R} : \mu(H, \cdot) \text{ is measurable }
\mathcal{G}
    \}.
\]</span> Then</p>
<ol type="1">
<li><p>By definition of <span class="math inline">\(F(r,
\omega)\)</span>, <span class="math inline">\(\mathcal{S}\)</span>
contains <span class="math inline">\((-\infty, r]\)</span> for rational
<span class="math inline">\(r\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal{S}\)</span> is a <span
class="math inline">\(\lambda\)</span>-system, since</p>
<ul>
<li><p><span class="math inline">\(\Omega \in
\mathcal{S}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(A, B \in cS\)</span>. Since <span
class="math inline">\(\mu\)</span> is a measure, <span
class="math inline">\(\mu(A - B, \cdot) = \mu(A, \cdot) - \mu(B,
\cdot)\)</span>. Since <span class="math inline">\(\mu(A,
\cdot)\)</span> and <span class="math inline">\(\mu(B, \cdot)\)</span>
are <span class="math inline">\(\mathcal{G}\)</span> measurable, so is
<span class="math inline">\(\mu(A - B, \cdot)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(A_n \in \mathcal{S}\)</span>,
<span class="math inline">\(n \ge 1\)</span> are mutually disjoint.
Since <span class="math inline">\(\mu\)</span> is a measure, <span
class="math inline">\(\mu(\bigcup_n A_n , \cdot) = \sum_{n \ge 1}
\mu(A_n, \cdot)\)</span>. Since <span class="math inline">\(\mu(A_n,
\cdot)\)</span> are <span class="math inline">\(\mathcal{G}\)</span>
measurable, so is <span class="math inline">\(\mu(\bigcup_{n \ge 1} A_n,
\cdot)\)</span>.</p></li>
</ul></li>
</ol>
<p>Therefore, <span class="math inline">\(\mathcal{S} =
\mathcal{G}\)</span>. Finally, for each <span class="math inline">\(r
\in \mathbb{Q}\)</span> and <span class="math inline">\(H = (-\infty,
r]\)</span>, with probability <span class="math inline">\(1\)</span>,
<span class="math inline">\(\mu(H, \omega) = \mathbb{P}[X \le r \mid
\mathcal{G}]_\omega\)</span>; Therefore, for each <span
class="math inline">\(G \in \mathcal{G}\)</span>, <span
class="math display">\[
    \int_G \mu(H, \omega) \, \mathbb{P} (d \omega)
        = \mathbb{P} ([X \in H] \cap G).
\]</span> Since the <span class="math inline">\(H = (-\infty, r], r \in
\mathbb{Q}\)</span> is a <span class="math inline">\(\pi\)</span> system
which generates <span class="math inline">\(\mathcal{R}\)</span>, this
holds for all <span class="math inline">\(H \in \mathcal{R}\)</span> as
well.</p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="reference">Reference</h1>
<ol type="1">
<li><em>Billingsley, Patrick. "Probability and Measure." 3rd ed. Wiley
Series in Probability and Mathematical Statistics. New York: Wiley,
1995.</em></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/23/Local-Sensitivity-Lower-Bound/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/23/Local-Sensitivity-Lower-Bound/" class="post-title-link" itemprop="url">Local Sensitivity Lower Bound</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-23 17:20:45" itemprop="dateCreated datePublished" datetime="2023-09-23T17:20:45-04:00">2023-09-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-11 16:40:43" itemprop="dateModified" datetime="2024-04-11T16:40:43-04:00">2024-04-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p><strong>Theorem.</strong> Let <span class="math inline">\(q :
\mathcal{X} \rightarrow \mathbb{R}\)</span> be a real-valued query and
<span class="math inline">\(M : \mathcal{X} \rightarrow Y\)</span> be an
<span class="math inline">\((\varepsilon,  \delta
)\)</span>-differentially private mechanism. Then 1. For every <span
class="math inline">\(x_0 ∼ x_1 \in \mathcal{X}\)</span>, at least one
of the following holds <span class="math display">\[
\Pr\left[ |M(x_0) − q(x_0)| &lt; \frac{|q(x_0) − q(x_1)|}{2} \right]
\le
\frac{1 +  \delta}
{1 + e^{−\varepsilon}}
= \frac{1}{2} + O( \delta  + \varepsilon).
\]</span> <span class="math display">\[
\Pr\left[ |M(x_1) − q(x_1)| &lt; \frac{|q(x_0) − q(x_1)|}{2} \right]
\le
\frac{1 +  \delta}
{1 + e^{−\varepsilon}}
= \frac{1}{2} + O( \delta  + \varepsilon). \\
\]</span></p>
</blockquote>
<blockquote>
<p><strong>Corollary.</strong> For every <span class="math inline">\(x
\in \mathcal{X}\)</span>, there is some <span
class="math inline">\(x&#39;\)</span> at Hamming distance at most <span
class="math inline">\(1\)</span> from <span
class="math inline">\(x\)</span> such that <span class="math display">\[
\Pr\left[ |M(x&#39;) − q(x&#39;)| &lt; \frac{LS_q(x)}{2} \right]
\le
\frac{1 +  \delta}
{1 + e^{−\varepsilon}}
= \frac{1}{2} + O( \delta  + \varepsilon). \\
\]</span></p>
</blockquote>
<p><strong>Proof.</strong> Let <span class="math display">\[
    \begin{align*}
        G_b
            &amp;= \left\{ y \in \mathbb{R} : |y − q(x_b)| &lt; |q(x_0)
− q(x_1)| \right\},
        \quad \forall b \in \{0, 1\}, \\
        p   &amp;= \min \{ \Pr[ M(x_0) \in G_0 ], \Pr[ M(x_1) \in G_1 ]
\}.
    \end{align*}
\]</span> To shed light on the final inequality, assume first that <span
class="math inline">\(M(x_0)\)</span> and <span
class="math inline">\(M(x_1)\)</span> have identical distributions. Then
<span class="math inline">\(M(x_0) \in G_0\)</span> and <span
class="math inline">\(M(x_1) \in G_1\)</span> are mutually disjoint
events, and therefore at least one of them has probability at most <span
class="math inline">\(1 / 2\)</span>.</p>
<p>In general, since <span class="math inline">\(M(x_0)\)</span> and
<span class="math inline">\(M(x_1)\)</span> have similar distributions,
we should expect similar results. We present two ways of deriving
it.</p>
<p>First, <span class="math display">\[
    \begin{align*}
        2 \cdot p
        &amp;\le \Pr[ M(x_0) \in G_0 ] + \Pr[ M(x_1) \in G_1 ] \\
        &amp;\le \Pr[ M(x_0) \in G_0 ] + e^\varepsilon \Pr[ M(x_0) \in
G_1 ] + \delta \\
        &amp;\le \Pr[ M(x_0) \in G_0 ] + e^\varepsilon \big(1 - \Pr[
M(x_0) \in G_0 ]\big) + \delta \\
        &amp;= e^\varepsilon + \delta + (1 - e^\varepsilon) \cdot \Pr[
M(x_0) \in G_0 ].
    \end{align*}
\]</span></p>
<p>Similarly, we have <span class="math inline">\(2 \cdot p \le
e^\varepsilon + \delta + (1 - e^\varepsilon) \cdot \Pr[ M(x_1) \in G_1
]\)</span>. It follows that <span class="math display">\[
    2 \cdot p \le e^\varepsilon + \delta + (1 - e^\varepsilon) \cdot p
    \Longleftrightarrow
    p \le \frac{e^\varepsilon + \delta}{e^\varepsilon + 1}.
\]</span></p>
<p>Second Proof: <span class="math display">\[
    \begin{align*}
        1 − p
            &amp;\ge \Pr[ M(x_0) \notin G_0 ] \\
            &amp;\ge \Pr[ M(x_0) \in G_1] \\
            &amp;\ge e^{− \varepsilon} \cdot \Pr[ M(x_1) \in G_1 ]
−  \delta \\
            &amp;\ge e^{− \varepsilon} \cdot p −  \delta.
    \end{align*}
\]</span> Solving, we deduce that <span class="math inline">\(p \le (1
+  \delta ) / (1 + e^{−\varepsilon})\)</span>.</p>
<p>Third Proof: there is an alternative proof, which is quite close the
proof of packing lower bound. <span class="math display">\[
    \begin{aligned}
        1   &amp;\ge \Pr[ M(x_0) \in G_0 ] + \Pr[ M(x_0) \in G_1]\\
            &amp;\ge \Pr[ M(x_0) \in G_0 ] + e^{− \varepsilon} \cdot
(\Pr[ M(x_1) \in G_1 ] −  \delta)\\
            &amp;\ge (1 + e^{− \varepsilon}) \cdot p −  e^{−
\varepsilon} \cdot \delta.
    \end{aligned}
\]</span> It concludes that <span class="math display">\[
    p \le \frac{e^\varepsilon + \delta}{e^\varepsilon + 1}.
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h1 id="reference">Reference</h1>
<ol type="1">
<li>Vadhan, Salil. “The Complexity of Differential Privacy.” 2017.
https://doi.org/10.1007/978-3-319-57048-8_7.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/16/Sub-Exponential-Random-Variable/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/Sub-Exponential-Random-Variable/" class="post-title-link" itemprop="url">Sub-Exponential Random Variable</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 13:35:04" itemprop="dateCreated datePublished" datetime="2023-06-16T13:35:04-04:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-11-22 14:59:27" itemprop="dateModified" datetime="2024-11-22T14:59:27-05:00">2024-11-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p><strong>Theorem (Sub-exponential properties).</strong> Let <span
class="math inline">\(X\)</span> be a random variable. Then the
following properties are equivalent; the parameters <span
class="math inline">\(K_i &gt; 0\)</span> appearing in these properties
differ from each other by at most an absolute constant factor. 1. The
tails of <span class="math inline">\(X\)</span> satisfy <span
class="math display">\[
\Pr \{ |X| \ge t \} \le 2 \exp( − t / K_1 ), \quad  \forall t \ge 0.
      \]</span> 2. The moments of <span class="math inline">\(X\)</span>
satisfy <span class="math display">\[
\Vert X \Vert_{L_p} = ( \mathbb{E} |X|^p )^{1/p}
\le
K_1 \cdot \sqrt[p]{2 p \cdot \Gamma (p)}
= K_2 p, \quad \forall p \ge 1.
      \]</span> 3. The MGF of <span class="math inline">\(|X|\)</span>
satisfies <span class="math display">\[
\mathbb{E} \exp( \lambda |X| ) \le \exp( K_3 \lambda ),
\quad
\forall \lambda, \, \text{ s.t., } \, 0 \le \lambda \le 1 / K_3.
      \]</span> 4. The MGF of <span class="math inline">\(|X|\)</span>
is bounded at some point, namely <span class="math display">\[
\mathbb{E} \exp( |X| / K_4 ) \le 2.
      \]</span> 5. Moreover, if <span class="math inline">\(\mathbb{E} X
= 0\)</span> then properties 1 - 4 are also equivalent to the following
property: The MGF of <span class="math inline">\(X\)</span> satisfies
<span class="math display">\[
\mathbb{E} \exp( \lambda X ) \le \exp( K_5^2 \lambda^2 ),
\quad
\forall \lambda, \, \text{ s.t., } \, | \lambda | \le 1/K_5.
      \]</span></p>
</blockquote>
<p><strong>Proof.</strong></p>
<p><span class="math inline">\(1 \rightarrow 2:\)</span> <span
class="math display">\[
    \begin{aligned}
        \Vert X \Vert_{L_p}^p
            &amp;= \mathbb{E}|X|^p \\
            &amp;= \int_{0}^\infty \Pr[ |X|^p \ge t ] \, dt \\
            &amp;= \int_{0}^\infty \Pr[ |X| \ge t^{1 / p} ] \, dt \\
            &amp;\le \int_{0}^\infty 2 \exp(− t^{1 / p} / K_1) \, dt
            &amp; u \doteq t^{1 / p} / K_1 \\
            &amp;\le \int_{0}^\infty 2 K_1^p \cdot p \cdot u^{p - 1}
\exp(− u ) \, du \\
            &amp;= 2 K_1^p \cdot p \cdot \Gamma (p).
    \end{aligned}
\]</span> Noting that <span class="math inline">\(\Gamma(p) \le
p^p\)</span> proves property 2.</p>
<p><span class="math inline">\(2 \rightarrow 3:\)</span> By monotone
convergence theorem, we have <span class="math display">\[
    \begin{aligned}
        \mathbb{E} \exp( \lambda |X|)
            &amp;= \mathbb{E} \left( \sum_{n \in \N} \frac{\lambda^n
|X|^n}{n!}\right) \\
            &amp;= 1 + \sum_{n \in \N^+} \frac{\lambda^n \mathbb{E}
|X|^n}{n!} \\
            &amp;\le 1 + \sum_{n \in \N^+} \frac{\lambda^n \cdot 2 K_1^n
\cdot n \cdot \Gamma(n)}{n!}
            &amp; \text{ since }\, \mathbb{E} |X|^n \le 2 K_1^n \cdot n
\cdot \Gamma(n) \\
            &amp;\le 1 + 2 \cdot \sum_{n \in \N^+} \lambda^n K_1^n \\
            &amp;= 1 + \frac{2 \lambda K_1}{1 - \lambda K_1}
            &amp; \text{ provided that }\, \lambda K_1 &lt; 1 \\
            &amp;= \frac{1 + \lambda K_1}{1 - \lambda K_1} \\
            &amp;\le e^{ 3 K_1 \lambda }
            &amp; \text{ provided that }\, \lambda K_1 \in [0, 4 / 5].
    \end{aligned}
\]</span></p>
<p><span class="math inline">\(3 \rightarrow 4:\)</span></p>
<p>It is trivial to prove property 4 by setting <span
class="math inline">\(\lambda = 1 / K_3\)</span> in property 3.</p>
<p><span class="math inline">\(4 \rightarrow 1:\)</span> <span
class="math display">\[
    \begin{aligned}
         \Pr \{ |X| \ge t \}
            &amp;=  \Pr \{ e^{|X| / K_4} \ge e^{t / K_4} \} \\
            &amp;\le \frac{\mathbb{E} e^{|X| / K_4}}{e^{t / K_4}} \\
            &amp;\le 2 \exp(−t / K_4),
            &amp;\forall t \ge 0.
    \end{aligned}
\]</span></p>
<p><span class="math inline">\(2 \rightarrow 5:\)</span> By monotone
convergence theorem, we have <span class="math display">\[
    \begin{aligned}
        \mathbb{E} \exp( \lambda X)
            &amp;= \mathbb{E} \left( \sum_{n \in \N} \frac{\lambda^n
X^n}{n!}\right) \\
            &amp;= 1 + \mathbb{E} (\lambda X) + \mathbb{E} \left(
\sum_{n \in \N} \frac{\lambda^n X^n}{n!}\right) \\
            &amp;= 1 + \mathbb{E} \left( \sum_{n \ge 2} \frac{\lambda^n
X^n}{n!}\right) \\
            &amp;\le 1 + \mathbb{E} \left( \sum_{n \ge 2}
\frac{\lambda^n |X|^n}{n!}\right) \\
            &amp;= 1 + \sum_{n \ge 2} \frac{\lambda^n \mathbb{E}
|X|^n}{n!} \\
            &amp;\le 1 + \sum_{n \ge 2} \frac{\lambda^n \cdot 2 K_1^n
\cdot n \cdot \Gamma(n)}{n!}
            &amp; \text{ since }\, \mathbb{E} |X|^n \le 2 K_1^n \cdot n
\cdot \Gamma(n) \\
            &amp;\le 1 + 2 \cdot \sum_{n \ge 2} \lambda^n K_1^n \\
            &amp;= 1 + \frac{2 \lambda^2 K_1^2}{1 - \lambda K_1}
            &amp; \text{ provided that }\, \lambda K_1 &lt; 1 \\
            &amp;= 1 + 4 \lambda^2 K_1^2
            &amp; \text{ provided that }\, \lambda K_1 \le 1 / 2 \\
            &amp;\le e^{ 4 K_1^2 \lambda^2 }.
    \end{aligned}
\]</span></p>
<p><span class="math inline">\(5 \rightarrow 2:\)</span> Taking <span
class="math inline">\(\lambda = 1 / K_5\)</span> and <span
class="math inline">\(\lambda = - 1 / K_5\)</span> gives <span
class="math display">\[
    \begin{aligned}
        \mathbb{E}\exp( X / K_5 ) \le \exp( 1 ),
        \qquad
        \mathbb{E}\exp( - X / K_5 ) \le \exp( 1 ),
    \end{aligned}
\]</span> Further, for each <span class="math inline">\(x \in
\R\)</span>, and <span class="math inline">\(p &gt; 0\)</span>, since
<span class="math inline">\(|x| / p \le 1 + |x / p| \le e^{|x| /
p}\)</span>, it holds that <span class="math display">\[
    |x|^p / p^p \le e^{|x|} \le e^x + e^{-x},
    \quad
    \Longleftrightarrow
    \quad
    |x / K_5|^p / p^p \le e^{|x / K_5|} \le e^{x / K_5} + e^{- x / K_5}.
\]</span> It follows that <span class="math display">\[
    \mathbb{E} \left[ |X / K_5|^p / p^p \right]
        \le \mathbb{E} \left[ e^{X / K_5} \right]
        + \mathbb{E} \left[ e^{- X / K_5} \right]
        \le 2,
\]</span> i.e., <span class="math display">\[
      \mathbb{E} \left[ |X|^p \right] \le 2 \cdot p^p \cdot K_5^p.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p>A random variable <span class="math inline">\(X\)</span> that
satisfies one of the equivalent properties 1 - 5 in previous theorem is
called a <strong><em>sub-exponential random variable</em></strong>.</p>
<blockquote>
<p><strong>Definition (Sub-Exponential Norm).</strong> The
sub-exponential norm of <span class="math inline">\(X\)</span>, denoted
<span class="math inline">\(\Vert X \Vert_{\psi_1}\)</span>, is defined
to be the smallest <span class="math inline">\(K_3\)</span> in property
3. In other words, <span class="math display">\[    
\Vert X \Vert_{\psi_1} = \inf \{t &gt; 0: \mathbb{E}\exp(|X|/t) \le 2
\}.
\]</span></p>
</blockquote>
<blockquote>
<p><strong>Lemma (Sub-gaussian squared is sub-exponential).</strong> A
random variable <span class="math inline">\(X\)</span> is sub-gaussian
if and only if <span class="math inline">\(X^2\)</span> is
sub-exponential. Moreover, <span class="math display">\[  
\Vert X^2 \Vert_{\psi_1} = \Vert X \Vert_{\psi_2}^2.
  \]</span></p>
</blockquote>
<p><strong>Proof.</strong> By definition we have <span
class="math display">\[
\begin{align*}
    \Vert X^2   \Vert_{\psi_1}
        &amp;\doteq \inf \{t &gt; 0: \mathbb{E}\exp(|X|^2/t) \le 2 \}.
\\
    \Vert X \Vert_{\psi_2}
        &amp;\doteq \inf \{t &gt; 0: \mathbb{E}\exp(|X|^2/t^2) \le 2 \}.
\end{align*}
\]</span> Hence the claim follows.</p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Lemma (The product of sub-gaussians is
sub-exponential).</strong> Let <span class="math inline">\(X\)</span>
and <span class="math inline">\(Y\)</span> be sub-gaussian random
variables. Then <span class="math inline">\(XY\)</span> is
sub-exponential. Moreover, <span class="math display">\[
\Vert XY \Vert_{\psi_1} \le \Vert X \Vert_{\psi_2} \Vert Y
\Vert_{\psi_2}.
  \]</span></p>
</blockquote>
<p><strong>Proof.</strong> Assume that <span class="math inline">\(\Vert
X \Vert_{\psi_2} = a\)</span>, and <span class="math inline">\(\Vert Y
\Vert_{\psi_2} = b.\)</span> Then by definition, we have <span
class="math display">\[
\begin{aligned}
    \mathbb{E}\exp( |X|^2 / a^2 ) \le 2, \\
    \mathbb{E}\exp( |Y|^2 / b^2 ) \le 2.
\end{aligned}
\]</span> Further <span class="math display">\[
\begin{aligned}
    \mathbb{E} \left[
        \exp ( | X Y | / (a \cdot b) )
    \right]
    &amp;\le \mathbb{E} \left[
        \exp \bigg( \frac{1}{2} \cdot (X^2 / a^2 + Y^2 / b^2) \bigg)
    \right] \\
    &amp;\le \mathbb{E} \left[
        \exp \bigg( \frac{1}{2} \cdot (X^2 / a^2 ) \bigg)
        \cdot
        \exp \bigg( \frac{1}{2} \cdot (Y^2 / b^2) \bigg)
    \right] \\
    &amp;\le \mathbb{E} \left[
        \frac{1}{2} \cdot \left( \exp \bigg( \frac{1}{2} \cdot (X^2 /
a^2 ) \bigg) \right)^2
        +
        \frac{1}{2} \cdot \left( \exp \bigg( \frac{1}{2} \cdot (Y^2 /
b^2) \bigg) \right)^2
    \right] \\
    &amp;\le \frac{1}{2} \cdot \left( 2 + 2 \right)
    = 2.
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Theorem (Centering).</strong> Given sub-exponential random
variables <span class="math inline">\(X\)</span>, it holds that <span
class="math display">\[
\Vert X − \mathbb{E} X \Vert_{\psi_1} \le C \Vert X \Vert_{\psi_1}.
\]</span></p>
</blockquote>
<p><strong>Proof.</strong> Since <span class="math inline">\(\Vert \cdot
\Vert_{\psi_1}\)</span> is a norm, it holds that <span
class="math display">\[
    \begin{aligned}
        \Vert X − \mathbb{E} X \Vert_{\psi_1}
            &amp;\le \Vert X \Vert_{\psi_1}
            + \Vert \mathbb{E} X \Vert_{\psi_1},
    \end{aligned}
\]</span> it suffices to bound <span class="math inline">\(\Vert
\mathbb{E} X \Vert_{\psi_1}\)</span>. Define <span
class="math inline">\(t = |\mathbb{E} X| / \ln 2\)</span>. Then <span
class="math display">\[
    \mathbb{E} \left[ \exp \left( |\mathbb{E} X| / t \right) \right]
        = \mathbb{E} \left[ \exp \left( \ln 2 \right) \right]
        = 2.
\]</span> Therefore, <span class="math display">\[
    \begin{aligned}
        \Vert \mathbb{E} X \Vert_{\psi_1}
            &amp;= \frac{1}{\ln 2} \cdot |\mathbb{E} X | \\
            &amp;\le \frac{1}{\ln 2} \cdot \mathbb{E} |X| \\
            &amp;= \frac{\Vert X \Vert_{\psi_1}}{\ln 2} \cdot \mathbb{E}
\left[ |X| / \Vert X \Vert_{\psi_1} \right] \\
            &amp;\le \frac{\Vert X \Vert_{\psi_1}}{\ln 2} \cdot \left(
                1 + \mathbb{E} \left[ \frac{|X|}{\Vert X \Vert_{\psi_1}}
\right] + \frac{1}{2!} \cdot \mathbb{E} \left[ \left( \frac{|X|}{\Vert X
\Vert_{\psi_1}} \right)^2 \right] + \frac{1}{3!} \cdot \mathbb{E} \left[
\left( \frac{|X|}{\Vert X \Vert_{\psi_1}} \right)^3 \right] + \cdots
            \right) \\
            &amp;\stackrel{(a)}{=} \frac{1}{\ln 2} \cdot \mathbb{E}
\left[ \exp \left( \frac{|X|}{\Vert X \Vert_{\psi_1}} \right) \right] \\
            &amp;= 2 \cdot \frac{ \Vert X \Vert_{\psi_1} }{\ln 2},
    \end{aligned}
\]</span> where <span class="math inline">\((a)\)</span> follows from
monotone convergence theorem.</p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Theorem (Bernstein’s Inequality).</strong> Let <span
class="math inline">\(X_1, \ldots, X_n\)</span> be independent mean-zero
sub-exponential random variables. Then, for every <span
class="math inline">\(t \ge 0\)</span>, we have <span
class="math display">\[
\Pr \left[ \sum_{i \in [n]} X_i \ge t \right]
\le 2 \exp \left( − c \min
\left(
\frac{t^2}{\sum_{i \in [n]} \Vert X_i \Vert_{\psi_1}^2},
\frac{t}{\max_{i \in [n]} \Vert X_i \Vert_{\psi_1}}
\right)
\right),
\]</span> where <span class="math inline">\(c &gt; 0\)</span> is an
absolute constant.</p>
</blockquote>
<p><strong>Proof.</strong> Let <span class="math inline">\(\lambda &gt;
0\)</span>. Since the <span class="math inline">\(K_i\)</span> in each
property in Theorem 1 differs by at most an absolute constant, and by
property (5) , when $ $ for some absolute constant <span
class="math inline">\(c\)</span>, it holds that <span
class="math display">\[
    \begin{aligned}
        \Pr \left[ \sum_{i \in [n]} X_i \ge t \right]
            \le \frac{
                \prod_{i \in [n]} \mathbb{E} \exp \left( \lambda \cdot
X_i \right)
            }{
                 \exp \left( \lambda \cdot t \right)
            }
            \le \exp \left( \sum_{i \in [n]} \lambda^2 \cdot c^* \cdot
\Vert X_i \Vert_{\psi_1}^2 - \lambda \cdot t \right),
    \end{aligned}
\]</span> where <span class="math inline">\(c^* = 1 / c&#39;\)</span>.
The RHS is minimized for <span class="math display">\[
    \lambda = \min \left\lbrace
        \frac{t}{
            2 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
        }
        ,\quad
        \frac{ c&#39; }{ \max_{i \in [n]}
            \Vert X_i \Vert_{\psi_1}
        }
    \right\rbrace,
\]</span> which gives <span class="math display">\[
    \begin{aligned}
        \Pr \left[ \sum_{i \in [n]} X_i \ge t \right]
            \le \exp \left(
                \sum_{i \in [n]} \lambda^2 \cdot c^* \cdot \Vert X_i
\Vert_{\psi_1}^2 - \lambda \cdot t
            \right).
    \end{aligned}
\]</span> When <span class="math inline">\(\lambda = \frac{t}{
            2 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
        } \le \frac{ c&#39; }{
           \max_{i \in [n]}  \Vert X_i \Vert_{\psi_1}
        },\)</span> we have <span class="math display">\[
    \sum_{i \in [n]} \lambda^2 \cdot c^* \cdot \Vert X_i
\Vert_{\psi_1}^2 - \lambda \cdot t
        = \frac{t^2}{
            4 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
        } - \frac{t^2}{
            2 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
        }
        = - \frac{t^2}{
            4 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
        }.
\]</span> When <span class="math inline">\(\lambda = \frac{ c&#39; }{
           \max_{i \in [n]}  \Vert X_i \Vert_{\psi_1}
        }
        \le \frac{t}{
            2 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
        },\)</span> we have <span class="math display">\[
\begin{aligned}
    \sum_{i \in [n]} \lambda^2 \cdot c^* \cdot \Vert X_i
\Vert_{\psi_1}^2 - \lambda \cdot t
        &amp;= \left(
            \frac{ c&#39; }{
                \max_{i \in [n]} \Vert X_i \Vert_{\psi_1}
            }
        \right)^2 \cdot \sum_{i \in [n]} c^* \cdot \Vert X_i
\Vert_{\psi_1}^2
        - \left(
            \frac{ c&#39; }{
                \max_{i \in [n]} \Vert X_i \Vert_{\psi_1}
            }
        \right) t \\
        &amp;\le \left(
            \frac{ c&#39; }{
                \max_{i \in [n]} \Vert X_i \Vert_{\psi_1}
            }
        \right)
        \cdot
        \frac{t}{
            2 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
        }
        \cdot \sum_{i \in [n]} c^* \cdot \Vert X_i \Vert_{\psi_1}^2
        - \left(
            \frac{ c&#39; }{
                \max_{i \in [n]} \Vert X_i \Vert_{\psi_1}
            }
        \right) t \\
        &amp;= - \frac{ c&#39; t }{
                2 \cdot \max_{i \in [n]} \Vert X_i \Vert_{\psi_1}
            }.
\end{aligned}
\]</span> It concludes that <span class="math display">\[
    \begin{aligned}
        \Pr \left[ \sum_{i \in [n]} X_i \ge t \right]
            &amp;\le \exp \left(
                \max \left\lbrace
                    - \frac{t^2}{
                        4 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
                    },
                    \,
                    - \frac{ c&#39; t }{
                        2 \cdot \max_{i \in [n]} \Vert X_i
\Vert_{\psi_1}
                    }
                \right\rbrace
            \right) \\
            &amp;= \exp \left(
                - \min \left\lbrace
                    \frac{t^2}{
                        4 \cdot c^* \cdot \sum_{i \in [n]} \Vert X_i
\Vert_{\psi_1}^2
                    },
                    \,
                    \frac{ c&#39; t }{
                        2 \cdot \max_{i \in [n]} \Vert X_i
\Vert_{\psi_1}
                    }
                \right\rbrace
            \right).
    \end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="reference">Reference</h1>
<p>[1] <em>R. Vershynin, "High-Dimensional Probability: An Introduction
with Applications in Data Science." in Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge: Cambridge University Press,
2018. doi: 10.1017/9781108231596.</em></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/67/">67</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">WOW</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
