<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/10/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/12/PAC-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/12/PAC-learning/" class="post-title-link" itemprop="url">PAC Learning Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-12 16:06:55" itemprop="dateCreated datePublished" datetime="2020-11-12T16:06:55+11:00">2020-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-19 15:16:12" itemprop="dateModified" datetime="2020-11-19T15:16:12+11:00">2020-11-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss a toy model of probably approximately correct (PAC) learning [1]. The setting focuses on a <span class="math inline">\(d\)</span>-dimension Boolean hypercube <span class="math inline">\(\{0, 1\}^d\)</span> and a set of functions <span class="math inline">\(\mathcal{H} = \{ h : \{0, 1\}^d \rightarrow \{0, 1 \} \}\)</span>. A function <span class="math inline">\(h \in \mathcal{H}\)</span> is called a hypothesis or a concept, which assigns each vertex in the hypercube a label 0 or 1.</p>
<p>The goal of PAC learning is to learn an unknown hypothesis, denoted as <span class="math inline">\(h^* \in \mathcal{H}\)</span>, from a dataset, which consists of <span class="math inline">\(n\)</span> points <span class="math inline">\(X_1, X_2, ..., X_n \in \{0, 1\}^n\)</span> sampled independently from an unknown distribution <span class="math inline">\(D\)</span> (over the hypercube), as well as their labels <span class="math inline">\(Y_1 = h^*(X_1), Y_2 = h^*(X_2), ..., Y_n = h^*(X_n)\)</span>.</p>
<p>We can state the algorithm for PAC learning in just one sentence.</p>
<blockquote>
<p>Output any hypothesis <span class="math inline">\(h&#39; \in \mathcal{H}\)</span> that is consistent with the sampled dataset, i.e., <span class="math display">\[
h&#39;(X_i) = Y_i, \qquad \forall i \in [n]
\]</span></p>
</blockquote>
<p>Such an hypothesis always exists, as <span class="math inline">\(h^*\)</span> satisfies this constraint. We have the classic PAC learning theorem.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The PAC learning theorem states that the probability that <span class="math inline">\(h&#39;\)</span> mislabels a point is bounded by <span class="math inline">\(\sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}\)</span>.</p>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\mathbb{E}[Z_i]\)</span> is an unbiased estimator of <span class="math inline">\(\mu_h\)</span>. Further, denote <span class="math inline">\(\hat \mu_h\)</span> the empirical mean of <span class="math inline">\(Z_i\)</span>'s <span class="math display">\[
\hat \mu_h \doteq \frac{1}{n} \sum_{i \in [n] } Z_i
\]</span></p>
<p>Note that for the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(\hat \mu_{h&#39;} = 0\)</span>.</p>
<p>By Hoeffding inequality, for any <span class="math inline">\(\delta&#39; &gt; 0\)</span>, <span class="math display">\[
\Pr[ \hat \mu_h \le \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} } ] \le \delta&#39;. 
\]</span></p>
<p>By union bound, the probability that <span class="math display">\[
\exists h \in \mathcal{H}, \hat \mu_h \ge \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} }
\]</span></p>
<p>is bounded by <span class="math inline">\(| \mathcal{H} | \delta&#39;\)</span>. By setting <span class="math inline">\(\delta&#39; = \frac{\delta}{ | \mathcal{H} | }\)</span>, it is guaranteed that <span class="math display">\[
0 = \hat \mu_{h&#39; } \le \mu_{h&#39;} -  \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span></p>
<p>with probability at most <span class="math inline">\(\delta\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>The previous theorem can be improved with the <span class="math inline">\(\sqrt{ \cdot }\)</span> removed. The key here is that the selected <span class="math inline">\(h&#39;\)</span> is error-free on the samples.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>We will prove that if <span class="math inline">\(\mu_h \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}\)</span>, then <span class="math inline">\(h\)</span>'s probability of being selected is very low.</p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\Pr[Z_i = 1] = \mu_h\)</span>. Therefore, <span class="math display">\[
    \begin{aligned}
        \Pr[ Z_1 = 0 \wedge Z_2 = 0 \wedge ... \wedge Z_n = 0] = (1 - \mu_h)^n \le \exp(- \mu_h n) = \frac{\delta}{ |\mathcal{H} | }
    \end{aligned}
\]</span></p>
<p>Taking union bound over all possible <span class="math inline">\(h\)</span> gives the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Caveat.</em></strong> <em>In both theorem, we see a <span class="math inline">\(\log \mathcal{H}\)</span> term in the failure probability, which is due to the use of union bound and can be roughly considered as the complexity of the hypothesis family. It is tempting to think that this could be avoid as follows.</em></p>
<blockquote>
<p>For the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(Z_1, ..., Z_n\)</span> are i.i.d samples. Therefore, the probability of all of them being <span class="math inline">\(0\)</span> is give by<br />
<span class="math display">\[
(1 - \mu_{h&#39;} )^n \le \exp( - \mu_{h&#39;} n). 
  \]</span><br />
Bounding this probability by <span class="math inline">\(\delta\)</span> gives <span class="math inline">\(\mu_{h&#39;} \le \frac{ \log \frac{1}{\delta} } {n}\)</span>.</p>
</blockquote>
<p><em>The argument is wrong. For the outputted <span class="math inline">\(h&#39;\)</span>, the <span class="math inline">\(Z_1, ..., Z_n\)</span> are not i.i.d samples, as <span class="math inline">\(h&#39;\)</span> is selected according to <span class="math inline">\(Z_1, ..., Z_n\)</span> and depends on them.</em></p>
<p><em>To make it more concrete, suppose that there is an <span class="math inline">\(h\)</span>, such that <span class="math inline">\(\mu_h = \frac{ \log \frac{1}{\delta} } {n}\)</span>. Consider the following experiment</em></p>
<blockquote>
<ol type="1">
<li>Sample <span class="math inline">\(n\)</span> points independently the distribution <span class="math inline">\(D\)</span>.<br />
</li>
<li>Pick an <span class="math inline">\(h&#39;\)</span> that makes no prediction error on the samples.</li>
</ol>
</blockquote>
<p><em>If we repeat the experiment <span class="math inline">\(N\)</span> times for some positive integer <span class="math inline">\(N\)</span>, then the fixed <span class="math inline">\(h\)</span> makes no mistake in roughly <span class="math inline">\((1 - \delta) N\)</span> of the experiments and constitutes an candidate of <span class="math inline">\(h&#39;\)</span>. However, on the other roughly <span class="math inline">\(\delta N\)</span> experiments, where <span class="math inline">\(h\)</span> makes prediction error, it is no longer considered as the candidate of <span class="math inline">\(h&#39;\)</span>. The selection procedure of <span class="math inline">\(h&#39;\)</span> ignores bad event of <span class="math inline">\(h\)</span> automatically.</em></p>
<p><em>We could also investigate the following example. Let <span class="math inline">\(h^* = h_0\)</span>. There are also other two hypothesis <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span>. Let <span class="math inline">\(S\)</span> be the sample space. It holds that</em> <span class="math display">\[
|h_1(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_1} \\
    0,\ \       \forall x \in S_{h_1}
\end{cases}
\]</span> <span class="math display">\[
|h_2(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_2} \\
    0,\ \       \forall x \in S_{h_2}
\end{cases}
\]</span> <em>Moreover, <span class="math inline">\(\Pr[S_{h_1}] = \Pr[S_{h_2} ] = \delta\)</span>. How, if we get a sample in <span class="math inline">\(S_{h_1}\)</span>, then we can output <span class="math inline">\(h&#39;\)</span> as <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_1\)</span>, since both of them make no error when <span class="math inline">\(x \in S_{h_1}\)</span>. Similarly, if <span class="math inline">\(x \in S_{h_2}\)</span>, we can output either <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_2\)</span>. We call <span class="math inline">\(S_{h_1}\)</span> and <span class="math inline">\(S_{h_2}\)</span> the bad areas. When <span class="math inline">\(x\)</span> is in the bad area of any hypothesis, we can't distinguish this hypothesis from the true hypothesis. That is why we need to use union bound to bound the total probabilities of these bad areas.</em></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/PAC-Learning.png?raw=true" width="400" height="340" /></p>
</div>
<h3 id="reference">Reference</h3>
<p>[1] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. 6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/Gaussian-Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/06/Gaussian-Mechanism/" class="post-title-link" itemprop="url">Gaussian Mechanism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-06 23:00:19" itemprop="dateCreated datePublished" datetime="2020-11-06T23:00:19+11:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-10 20:56:26" itemprop="dateModified" datetime="2020-11-10T20:56:26+11:00">2020-11-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="gaussian-distribution">Gaussian Distribution</h2>
<p>We illustrate how Gaussian distribution (a.k.a. normal distribution) is constructed and some of its basic properties.</p>
<p>Suppose we would like to have a distribution in <span class="math inline">\(\mathbb{R}\)</span>, such that</p>
<ol type="1">
<li>It is centered at the origin.<br />
</li>
<li>Its density function decreases exponentially with respect to the square distance to the origin.</li>
</ol>
<p><em>Remark: if we replace 2 with "a density function decreases exponentially with the distance to the origin", we obtain Laplace distribution.</em></p>
<p>By definition, the density function <span class="math inline">\(p(x)\)</span> should be inversely and exponentially proportional to <span class="math inline">\(x^2\)</span>:</p>
<p><span class="math display">\[
p(x) \propto \exp(-x^2 )
\]</span></p>
<p>If we define <span class="math inline">\(M\)</span> to be the value of<br />
<span class="math display">\[
M \doteq \int_{-\infty}^\infty \exp(-x^2) \ dx, 
\]</span></p>
<p>then we can give the precise formula of <span class="math inline">\(p(x)\)</span>: <span class="math display">\[
p(x)  = \frac{1}{M} \exp(- x^2)
\]</span></p>
<p>It is easy to see that <span class="math inline">\(p(x)\)</span> is a density function such that <span class="math inline">\(\int_{-\infty}^\infty p(x) \ dx = 1\)</span>.</p>
<h3 id="closed-form">Closed Form</h3>
<p>In general, it is not hard to construct a distribution. Let <span class="math inline">\(h(x)\)</span> be any integrable function on a domain <span class="math inline">\(\mathcal{D}\)</span> such that <span class="math display">\[
M = \int_\mathcal{D} h(x) \ dx &lt; \infty
\]</span></p>
<p>Then we can view <span class="math inline">\(h(x)\)</span> as almost an density function of some distribution. The only obstacle is that <span class="math inline">\(M\)</span> might not be <span class="math inline">\(1\)</span>. However this is easy to overcome. We just scale the value of <span class="math inline">\(h(x)\)</span> by a constant factor at each point on <span class="math inline">\(\mathbb{R}\)</span> and set <span class="math display">\[
p(x) = \frac{1}{M} h(x)
\]</span></p>
<p>then we have immediately a density function. We don't even need to know the exact value of <span class="math inline">\(M\)</span>. What we need to guarantee is just that</p>
<blockquote>
<p>The value of <span class="math inline">\(M\)</span> exists and is finite.</p>
</blockquote>
<p>And we can safely use <span class="math inline">\(M\)</span> to denote this value. For example, we know that <span class="math display">\[
\int_0^\infty x^{0.2} e^{-x} \ dx \le  \int_0^1 e^{-x} \ dx  + \int_1^\infty x e^{-x} \ dx &lt; \infty.
\]</span></p>
<p>We can construct a density function as above. It turns out that this is a special case of <em>Gamma distribution</em>.</p>
<p>For Gaussian distribution, we are lucky as we can compute a closed form of <span class="math inline">\(M\)</span>. This is done by a trick of computing its square: <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \big( \int_{-\infty}^\infty \exp( -x^2 ) \  dx \big)^2 \\
        &amp;= \int_{-\infty}^\infty \exp( -x^2 ) \  dx \ \cdot \int_{-\infty}^\infty \exp( -y^2 ) \  dy \\
        &amp;= \int_{-\infty}^\infty \int_{-\infty}^\infty \exp( -(x + y) ^2 ) \  dx  dy. 
\end{aligned}
\]</span></p>
<p>Denote <span class="math inline">\(r = \sqrt{ x^2 + y^2}\)</span> the distance of <span class="math inline">\((x, y)\)</span> to <span class="math inline">\((0, 0)\)</span>. We are integrating the function <span class="math inline">\(\exp( - r^2)\)</span> over the plane <span class="math inline">\(\mathbb{R}^2\)</span>. It is natural to switch to polar coordinate system <span class="math inline">\((r, \theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the angle of a point. Here is an informal but intuitive explanation. As the picture demonstrated below, if <span class="math inline">\(r\)</span> increases by <span class="math inline">\(dr\)</span> and <span class="math inline">\(\theta\)</span> increases by <span class="math inline">\(d\theta\)</span>, the new area spanned by <span class="math inline">\(dr\)</span> and <span class="math inline">\(d \theta\)</span> is roughly <span class="math inline">\(r dr d \theta\)</span>. Or we can calculate it algebraically <span class="math display">\[
\frac{1}{2} [(r + dr)^2 - r^2] d\theta = r dr d \theta + \frac{1}{2} (dr)^2 d \theta \approx r dr d \theta.
\]</span></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration1.png?raw=true" width="400" height="340" /></p>
</div>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \int_{0}^{ 2 \pi} \int_{0}^\infty \exp( -r^2 ) r d r d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2}  \int_{0}^\infty \exp( -r^2 ) d r^2 d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2} d \theta \\
        &amp;= \pi.
\end{aligned}
\]</span></p>
<p>Hence, <span class="math display">\[
p(x) = \frac{1}{ \sqrt \pi} \exp( - x^2 ).
\]</span></p>
<h3 id="basic-properties">Basic Properties</h3>
<ol type="1">
<li><blockquote>
<p><em>Expectation.</em> As the distribution is symmetric to <span class="math inline">\(0\)</span>, it has expectation 0.</p>
</blockquote></li>
<li><blockquote>
<p><em>Variance.</em> As its expectation is 0, the variance is given by</p>
</blockquote>
<span class="math display">\[
 \begin{aligned}
     \int_{-\infty}^\infty  \frac{ x^2 }{ \sqrt \pi } \exp( -x^2 ) \ dx 
         &amp;= -\frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty x \ d \exp( -x^2 ) \\
         &amp;= \frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty \exp( -x^2 ) d x\\
         &amp;= \frac{1}{2 \sqrt \pi } \sqrt \pi \\
         &amp;= \frac{1}{2}
 \end{aligned}
 \]</span></li>
</ol>
<h3 id="general-gaussian-distribution">General Gaussian Distribution</h3>
<p>We have proved that if <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(p(x) = \frac{1}{\sqrt \pi} \exp( -x^2)\)</span>, then <span class="math display">\[
\mathbb{Var}[X] = \frac{1}{2}
\]</span></p>
<p>Consider another random variable <span class="math inline">\(Y = \sqrt{2} X\)</span>. It has variance <span class="math display">\[
\mathbb{Var}[X] = 2 \mathbb{Var}[X] = 1
\]</span></p>
<p>To obtain its density function of <span class="math inline">\(Y\)</span>, we first scale the function <span class="math inline">\(p(x)\)</span> horizontally by a factor of <span class="math inline">\(\sqrt 2\)</span> to get <span class="math display">\[
\frac{1}{ \sqrt{ \pi } } \exp( -\frac{x^2}{2} ).
\]</span></p>
<p>Now the area under the curve is <span class="math inline">\(\sqrt 2\)</span>. We normalize this area to 1 and obtain <span class="math inline">\(Y\)</span>'s density function as <span class="math display">\[
p(y) = \frac{1}{ \sqrt{2 \pi } } \exp( -\frac{y^2}{2} )
\]</span></p>
<p>From now on, we use <span class="math inline">\(N(0, 1)\)</span> to denote a Gaussian distribution with mean 0 and variance <span class="math inline">\(1\)</span>. We can also scale <span class="math inline">\(X\)</span> by a factor of <span class="math inline">\(\sqrt 2 \sigma\)</span> for any <span class="math inline">\(\sigma &gt; 0\)</span>, to get <span class="math inline">\(Y = \sqrt 2 \sigma X\)</span>. It have variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math display">\[
p(y) = \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( - (\frac{y}{ \sqrt{2} \sigma })^2 ) = \frac{1}{ \sqrt{2 \pi \sigma^2 } } \exp( -\frac{y^2}{ 2 \sigma^2 } )
\]</span></p>
<p>Finally, we can shift the center of <span class="math inline">\(0\)</span> to any real number <span class="math inline">\(\mu \in \mathbb{R}\)</span>, and obtain a density function <span class="math display">\[
p(y) = \frac{1}{ \sqrt{ \pi\cdot 2  \sigma^2 } } \exp( -\frac{ (y - \mu)^2 }{ 2 \sigma^2 } )
\]</span></p>
<p>The corresponding distribution is denoted as <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<em>The picture below shows a few Gaussian distributions.</em>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution.png?raw=true" width="700" height="430" /></p>
</div>
<h3 id="further-properties">Further Properties</h3>
<ol type="1">
<li><blockquote>
<p>If <span class="math inline">\(X \sim N(0, a^2)\)</span> and <span class="math inline">\(Y \sim N(0, b^2)\)</span>, then <span class="math inline">\(X + Y \sim N(0, a^2 + b^2)\)</span>.</p>
</blockquote>
<p>The demonstration here follows from [1]. The key observation is that the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is rotation invariant with the origin.</p>
<p>As an example, we plot below the joint distribution of <span class="math inline">\(X, Y \sim N(0, 1)\)</span>.</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-0.png?raw=true" width="500" height="330" />
</div>
<p>The figure below shows the same distribution. Observe again that the probability mass concentrates at a small region centered at origin.</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-1.png?raw=true" width="500" height="330" />
</div>
<p>Therefore, for any <span class="math inline">\(t \in \mathbb{R}\)</span>, the set <span class="math display">\[
 \{ (x, y) \in \mathbb{R}^2 : ax + by \le t \}
 \]</span> has the same probability measure as the one <span class="math display">\[
 \{ (x, y) \in \mathbb{R}^2 : x \le \frac{ t}{ \sqrt{a^2 + b^2} } \}.
 \]</span></p>
<p>Observe that the latter can be obtained by rotating the former with respect to the origin (see the figure below).</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration2.png?raw=true" width="800" height="350" />
</div>
<p>Now, the probability of the latter set is given by <span class="math display">\[
 \Pr[ X \le \frac{ t}{ \sqrt{a^2 + b^2} } ] = \Pr[  \sqrt{a^2 + b^2} X \le t]
 \]</span></p>
<p>It concludes that <span class="math inline">\(X + Y\)</span> has the same distribution as <span class="math inline">\(\sqrt{a^2 + b^2} X \sim N(0, a^2 + b^2)\)</span>.</p>
<p><em>Remark:if we want, we can verify this algebraically:</em><br />
<span class="math display">\[
 \Pr[X + Y \le t] = \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} }\exp(-\frac{x^2}{2} ) \ dx 
 \]</span></p>
<p>hence <span class="math display">\[
 \begin{aligned}
     \frac{ \partial }{ \partial t} \Pr[X + Y \le t]
         &amp;= \frac{ \partial }{ \partial t} \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} }\exp(-\frac{x^2}{2} ) \ dx \\
         &amp;= \frac{1}{ \sqrt{2 \pi (a^2 + b^2) } }\exp(-\frac{x^2}{2 (a^2 + b^2) } )
 \end{aligned}
 \]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p></li>
<li><blockquote>
<p>If <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>, then <span class="math inline">\(\forall t &gt; 0\)</span>, <span class="math display">\[
\Pr[ |X| \ge t] \le \exp( -\frac{t^2}{ 2\sigma^2} )
\]</span></p>
</blockquote>
<p><em>Remark: it is possible to get a tighter bound by using more advanced techniques.</em></p>
<p>In previous figures, it seems Gaussian distributions have a shape bump around its mean. This is kind of mis-leading, because the x-axis and y-axis are scaled. In the figure below, we plot <span class="math inline">\(N(0, 1)\)</span>, with ratio between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes being approximate <span class="math inline">\(1:1\)</span>. We see only a small bump around its mean. Although the distribution span the entire range of <span class="math inline">\(\mathbb{R}\)</span>, the probability mass is centered tightly around the origin.</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Concentration.png?raw=true" width="800" height="550" />
</div>
<p>Let <span class="math inline">\(T = \Pr[ |X| \ge t]\)</span>. Then <span class="math display">\[
 T^2 = \int_{ |x| \ge t, |y| \ge t} \frac{1}{ 2 \pi \sigma^2 } \exp( - \frac{ x^2 + y^2}{2 \sigma^2} ) 
 \]</span></p>
<p>As <span class="math display">\[
 \{ (x, y) \in \mathbb{R}^2: |x| \ge t \wedge |y| \ge t \} \subset
 \{ (x, y) \in \mathbb{R}^2: x^2 + y^2 \ge 2 t^2 \} 
 \]</span></p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration3.png?raw=true" width="600" height="600" />
</div>
<p>It follows that <span class="math display">\[
 \begin{aligned}
     T^2 
         &amp;\le \int_{ \sqrt 2 t}^\infty \int_{0}^{2 \pi } \frac{1}{ 2 \pi \sigma^2 } \exp( - \frac{ r^2 }{2 \sigma^2} ) r \ d\theta dr\\
         &amp;\le \int_{ \sqrt 2 t }^\infty \exp( - \frac{ r^2 }{2 \sigma^2} ) \ d \frac{r^2}{ 2\sigma^2} \\
         &amp;\le \int_{ \frac{ t^2 }{  \sigma^2 } }^\infty \exp( - z ) \ d z \\
         &amp;= \exp( -\frac{t^2}{ \sigma^2} )
 \end{aligned}
 \]</span></p>
<p>Hence <span class="math inline">\(T \le \exp( -\frac{t^2}{ 2\sigma^2} )\)</span>.</p>
<p><em>Remark: we briefly discuss the algebraic approach here.</em> <span class="math display">\[
 \begin{aligned}
     \Pr[X &gt; t] = \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( - \frac{ x^2 }{ 2 \sigma^2 } ) \ dx
 \end{aligned}
 \]</span></p>
<p><em>When <span class="math inline">\(x \ge t\)</span>, <span class="math inline">\(\frac{x}{t} \ge 1\)</span>, therefore,</em> <span class="math display">\[
 \begin{aligned}
     \Pr[X &gt; t] 
         &amp;\le \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \frac{x}{t} \exp( - \frac{ x^2 }{ 2  \sigma^2 } ) \ dx \\
         &amp;\le  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \int_{t}^\infty  \exp( - \frac{ x^2 }{ 2  \sigma^2 } ) \ d \frac{ x^2 }{ 2 \sigma^2 } \\
         &amp;=  - \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp( - \frac{ x^2 }{ 2  \sigma^2 } ) \mid_{t}^\infty \\
         &amp;=  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp( - \frac{ t^2 }{ 2 \sigma^2 } )  \\
 \end{aligned}
 \]</span></p>
<p><em>This bound is more useful only when we know that <span class="math inline">\(t \ge \frac{ \sigma }{ \sqrt{2 \pi } }\)</span></em>. <!-- ![](https://www.mathworks.com/help/examples/stats/win64/ComputeTheMultivariateNormalPdfExample_01.png) --></p>
<p><span class="math inline">\(\blacksquare\)</span></p></li>
</ol>
<h2 id="gaussian-mechanism">Gaussian Mechanism</h2>
<p>Our discussion in this section focus on a metric space <span class="math inline">\((\mathcal{X}, d)\)</span> and a function <span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}^n\)</span>. The Gaussian mechanism adds Gaussian noise to the output of <span class="math inline">\(f\)</span>, such that for any neighboring pairs <span class="math inline">\(x, y \in \mathcal{X}\)</span> with <span class="math inline">\(d(x, y) = 1\)</span>, it is hard to distinguish the outputs, i.e., their outputs have similar distribution after adding noises.</p>
<blockquote>
<p><strong>Definition.</strong> The sensitivity <span class="math inline">\(\Delta\)</span> of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
\Delta = \max_{x, y \in \mathcal{X}, d(x, y) = 1 } ||f(x) - f(y) ||
\]</span> where <span class="math inline">\(|| \cdot ||\)</span> is the <span class="math inline">\(\ell_2\)</span>-norm of a vector in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Given <span class="math inline">\(f\)</span>, and a variance parameter <span class="math inline">\(\sigma^2\)</span>, the Gaussian mechanism is a random variable defines as <span class="math display">\[
g(x) = f(x) + X
  \]</span> where <span class="math inline">\(X = (X_1, X_2, ..., X_n) \sim N(0, \sigma^2 I)\)</span>.</p>
</blockquote>
<p>We are ready to state the main theorem of this section.</p>
<blockquote>
<p><strong>Theorem.</strong> For <span class="math inline">\(\delta, \epsilon \in (0, 1)\)</span>, if <span class="math inline">\(\sigma = \frac{ \Delta \log \frac{1}{\delta} }{ \epsilon }\)</span>, then <span class="math inline">\(\forall x, y \in \mathcal{X}\)</span>, s.t., <span class="math inline">\(d(x, y) = 1\)</span>, and for all measurable subset <span class="math inline">\(S \subset \mathbb{R}^n\)</span>, it holds</p>
<p><span class="math display">\[
\Pr[ g(x) \in S] \le e^\epsilon \cdot \Pr[ g(y) \in S] + \delta 
\]</span></p>
</blockquote>
<p>It is obvious that if we set <span class="math inline">\(\sigma \rightarrow \infty\)</span>, then the Gaussian distribution tends to be a uniform one over <span class="math inline">\(\mathbb{R}^n\)</span>. So we are interested in how small <span class="math inline">\(\sigma\)</span> can be.</p>
<p>We claim that <span class="math inline">\(\sigma = O(\frac{\Delta}{\epsilon } )\)</span> suffices. This is based on three observations: 1) Most of the probability mass of a Gaussian distribution concentrates on a ball centered at its mean with radius <span class="math inline">\(O(\sigma)\)</span>; 2) the density ratio of any two points within the ball is bounded by a constant; 3) the distance between the distribution center of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> is bounded by <span class="math inline">\(\Delta\)</span>.</p>
<p><strong><em>Proof.</em></strong> For convenience of discussion, we write <span class="math inline">\(g(y) = f(y) + Y\)</span> to distinguish it from <span class="math inline">\(g(x)\)</span>. By definition, <span class="math inline">\(g(x)\)</span> is a random variable that follows <span class="math inline">\(N(f(x), \sigma^2 I)\)</span>. Similarly, <span class="math inline">\(g(y) \sim N( f(y), \sigma^2 I)\)</span>.</p>
<p>Further, we define <span class="math inline">\(p_x\)</span> and <span class="math inline">\(p_y\)</span> the density function of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> respectively. The sketch of the proof is as follows: we partition <span class="math inline">\(\mathbb{R}^n\)</span> into two parts: <span class="math display">\[
\mathcal{Y}_1 = \{ t \in \mathcal{R}^n : p_x(t) \le e^\epsilon \cdot p_y(t) \} \\
\mathcal{Y}_2 = \{ t \in \mathcal{R}^n : p_x(t) &gt; e^\epsilon \cdot p_y(t) \}.
\]</span></p>
<p>If <span class="math inline">\(\Pr[ g(x) \in \mathcal{Y}_2 ] \le \delta\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[ g(x) \in S] 
        &amp;= \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[ g(x) \in S \cap \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[  g(x) \in \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \delta \\
        &amp;= \int_{S \cap \mathcal{Y}_1 } p_x(t) \ d t + \delta \\
        &amp;\le \int_{S \cap \mathcal{Y}_1 } e^\epsilon \cdot p_y(t) \ d t + \delta \\
        &amp;= e^\epsilon \cdot \Pr[ g(y) \in S \cap \mathcal{Y}_1] + \delta. \\
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(p_y(t) &gt; 0\)</span> for all <span class="math inline">\(t \in \mathbb{R}^n\)</span>, we can rewrite <span class="math display">\[
\mathcal{Y}_1 = \{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } \le \epsilon  \} \\
\mathcal{Y}_2 = \{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } &gt; \epsilon  \}.
\]</span></p>
<p>In literature, the ratio <span class="math inline">\(\ln \frac{p_x(t) }{ p_y(t) }\)</span> is know as <em>privacy loss</em>. Substituting <span class="math inline">\(p_x(t)\)</span> and <span class="math inline">\(p_y(t)\)</span> with their definitions, we get <span class="math display">\[
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &amp;= \ln \frac{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( -\frac{ ||t - f(x)||^2 }{ 2 \sigma^2 } ) }{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( -\frac{ ||t - f(y) ||^2 }{ 2 \sigma^2 } )  } \\
        &amp;=  -\frac{ || t - f(x) ||^2 - || t - f(x) + f(x) - f(y) ||^2 }{ 2 \sigma^2 } 
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(t \sim N( f(x), \sigma^2 I)\)</span>, it holds that <span class="math inline">\(t - f(x) \sim N( 0, \sigma^2 I)\)</span>. Let <span class="math inline">\(t&#39; = t - f(x)\)</span> and <span class="math inline">\(v = f(x) - f(y)\)</span>, then <span class="math display">\[
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &amp;=  -\frac{ || t&#39; ||^2 - || t&#39; + v ||^2 }{ 2 \sigma^2 }    \\
        &amp;= \frac{ 2 v^T t&#39; + || v ||^2 }{ 2 \sigma^2 }    \\
\end{aligned}
\]</span></p>
<p>But <span class="math inline">\(v^T t&#39; \sim N(0, || v ||^2 \sigma^2)\)</span>. Let <span class="math inline">\(Z \sim N(0, 1)\)</span>, then <span class="math inline">\(\ln \frac{ p_x( t) }{ p_y(t)}\)</span> has the same distribution as <span class="math display">\[
\frac{ 2 ||v|| \sigma Z + ||v||^2 }{ 2 \sigma^2  } = \frac{ ||v||^2 }{ 2 \sigma^2  } + \frac{ ||v|| }{  \sigma  } Z \sim N(\frac{ ||v||^2 }{ 2 \sigma^2  },  \frac{ ||v||^2 }{  \sigma^2  } ).   \\
\]</span></p>
<p>Now, <span class="math display">\[
\frac{ ||v||^2 }{ 2 \sigma^2  } + \frac{ ||v|| }{  \sigma  } Z  \ge \epsilon \\
\longleftrightarrow  Z  \ge \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  }
\]</span></p>
<p>As <span class="math inline">\(Z \sim N(0, 1)\)</span>, as shown in the previous section, <span class="math inline">\(\forall z \in \mathbb{R}\)</span>, <span class="math display">\[
\Pr[ Z \ge z] \le \exp( -\frac{ z^2 }{2} )
\]</span></p>
<p>Replacing <span class="math inline">\(z\)</span> with <span class="math inline">\(\frac{ \sigma }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma }\)</span>, <span class="math display">\[
\Pr[ z \ge t ] \le  \exp( -\frac{1}{2} (\frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } )^2 ).
\]</span></p>
<p>We would like to bound this probability by some <span class="math inline">\(\delta \in (0, 1)\)</span>, then <span class="math display">\[
\begin{aligned}
    &amp;\exp( -\frac{1}{2} ( \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } )^2 ) \le \delta \\
    &amp;\Longrightarrow \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } \ge \sqrt{2 \ln \frac{1}{\delta} } \\
    &amp;\Longrightarrow \frac{\epsilon }{||v|| } \sigma^2 - \sqrt{2 \ln \frac{1}{\delta} } \sigma - \frac{ ||v|| }{2} \ge 0
\end{aligned}
\]</span></p>
<p>Finally, we get <span class="math display">\[
\sigma \ge \frac{ \sqrt{ 2 \ln \frac{1}{\delta} } + \sqrt{ 2 \ln \frac{1}{\delta} + 2 \epsilon  }  }{ 2 \frac{\epsilon }{||v|| } }
\]</span></p>
<p>By concavity of the square root function, it suffices to take <span class="math display">\[
\sigma = \frac{ ||v|| }{ \epsilon } \sqrt{ 0.5 * 2 \ln \frac{1}{\delta} + 0.5 * (2 \ln \frac{1}{\delta} + 2\epsilon) } = \frac{ ||v|| }{ \epsilon } \sqrt{  2 \ln \frac{1}{\delta} + \epsilon },
\]</span></p>
<p>i.e., <span class="math display">\[
\sigma^2 = \frac{ ||v||^2 }{ \epsilon^2 } (2 \ln \frac{1}{\delta} + \epsilon) 
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] B. Eisenberg and R. Sullivan, “Why Is the Sum of Independent Normal Random Variables Normal?,” Mathematics Magazine, vol. 81, no. 5, pp. 362–366, Dec. 2008<br />
[2] G. Kamath, “Lecture 5 — Approximate Diﬀerential Privacy”, CS 860 - Algorithms for Private Data Analysis.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/03/Laplace-Distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/03/Laplace-Distribution/" class="post-title-link" itemprop="url">Laplace Distribution</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-03 10:22:41" itemprop="dateCreated datePublished" datetime="2020-11-03T10:22:41+11:00">2020-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-05 16:03:07" itemprop="dateModified" datetime="2020-11-05T16:03:07+11:00">2020-11-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The density function of a Laplace distribution (denoted as Laplace(<span class="math inline">\(\mu, b\)</span>) ) with location and scale parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(b\)</span> is given by <span class="math display">\[
p(x) = \frac{1}{2 b} \exp( -\frac{ | x - \mu |  }{ b } )
\]</span></p>
<p>The density function of the distribution is symmetric with respect to <span class="math inline">\(\mu\)</span>, where it achieves peak value <span class="math inline">\(\frac{1}{2b}\)</span>. We plot below a few Laplace distributions with different parameters of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(b\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Laplace-Distribution.png?raw=true" /></p>
<!-- <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Laplace-Distribution.png?raw=true" style="zoom: 67%;" /> -->
<h3 id="properties.">Properties.</h3>
<p>We list and prove a few properties of the distribution. Let <span class="math inline">\(X\)</span> be a random variable that follows Laplace(<span class="math inline">\(\mu, b\)</span>).</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbb{E}[X] = \mu\)</span>.</p>
<p><em>Proof:</em> <span class="math display">\[
 \begin{aligned}
     \frac{1}{2} \int_{\mu }^\infty \frac{1}{b} \exp( -\frac{x - \mu}{b} ) \ dx
     = \frac{1}{2} \int_{0 }^\infty \frac{1}{b} \exp( -\frac{x}{b} ) \ dx = \frac{1}{2}
 \end{aligned}
 \]</span> The proof follows from symmetry of Laplace(<span class="math inline">\(\mu, b\)</span>) at <span class="math inline">\(\mu\)</span>.</p></li>
<li><p><span class="math inline">\(\forall t \ge 0, \Pr[X - \mu \ge t] \le \frac{1}{2} \exp(-t)\)</span>.</p>
<p><em>Proof:</em> <span class="math display">\[
 \begin{aligned}
     \Pr[X - \mu \ge t] = \frac{1}{2} \int_{\mu + t }^\infty \frac{1}{b} \exp( -\frac{x - \mu}{b} ) \ dx
     = \frac{1}{2} \int_{t }^\infty \exp( -x ) \ dx = \frac{1}{2} \exp(-t)
 \end{aligned}
 \]</span> <span class="math inline">\(\square\)</span></p></li>
<li><p><span class="math inline">\(\forall t \ge 0, \Pr[|X - \mu| \ge t] \le \exp(-t)\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{Var}[X] = 2b^2\)</span>.</p>
<p><em>Proof:</em> First, for an integer <span class="math inline">\(n \ge 0\)</span>, we define <span class="math display">\[
 \Gamma(n) = \int_{0 }^\infty x^n \exp( -x ) \ dx
 \]</span></p>
<p>Therefore, <span class="math inline">\(\Gamma(0) = 1\)</span>. Suppose <span class="math inline">\(n \ge 1\)</span>, we have <span class="math display">\[
 \begin{aligned}
     \Gamma(n) 
         &amp;= -\int_{0 }^\infty x^n \ d \exp( -x ) \\
         &amp;= - x^n \exp(-x) \mid_0^\infty + \int_{0 }^\infty \exp( -x ) \ d x^n \\
         &amp;= n\int_{0 }^\infty x^{n - 1} \exp( -x ) \ dx \\
         &amp;= n \Gamma(n - 1)
 \end{aligned}
 \]</span></p>
<p>By induction, we conclude <span class="math inline">\(\Gamma(n) = n!\)</span>. Now, consider</p>
<p><span class="math display">\[
 \begin{aligned}
     \frac{1}{2} \int_{\mu }^\infty \frac{1}{b} (x - \mu)^2 \exp( -\frac{x - \mu}{b} ) \ dx
     &amp;= \frac{1}{2} \int_{0 }^\infty \frac{1}{b} x^2 \exp( -\frac{x}{b} ) \ dx \\
     &amp;= \frac{b^2}{2} \int_{0 }^\infty x^2 \exp( -x ) \ dx \\
     &amp;= \frac{b^2}{2} \Gamma(2) \\
     &amp;= b^2
 \end{aligned}
 \]</span> The proof follows from symmetry of Laplace(<span class="math inline">\(\mu, b\)</span>) at <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X&#39; \sim Laplace(\mu, b)\)</span> and define another random variable <span class="math inline">\(Y = X&#39; + b\)</span>, then the density function of <span class="math inline">\(Y\)</span> shares the same shape with that of <span class="math inline">\(X&#39;\)</span>, with its center shifted to the right by distance <span class="math inline">\(b\)</span>. To distinguish their density function, denote <span class="math inline">\(p_{X&#39;} (\cdot)\)</span> the one for random variable <span class="math inline">\(X&#39;\)</span> and <span class="math inline">\(p_Y(\cdot)\)</span> the one for <span class="math inline">\(Y\)</span>. For any <span class="math inline">\(t \in \mathbb{R}\)</span>,</p>
<p><span class="math display">\[
 \frac{ p_{X&#39;} (t) }{ p_Y(t) } = \exp( - \frac{ |t - \mu | - | t - ( \mu + b ) | }{ b  }) \in [ \exp(-1), \exp(1) \ ]
 \]</span></p></li>
<li><p>If <span class="math inline">\(X, X&#39; \sim Laplace(\mu, \frac{b}{\epsilon} )\)</span> be a pair of independent random variables, and <span class="math inline">\(Y = X&#39; + b\)</span>, then</p>
<p><span class="math display">\[
 \frac{ p_X(t) }{ p_Y(t) } = \exp( - \epsilon \frac{ |t - \mu | - | t - ( \mu + b ) | }{ b  }) \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
 \]</span></p></li>
<li><p>Continued. Let <span class="math inline">\(S\)</span> be any interval in <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p><span class="math display">\[
 \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } = \frac{ \int_{t \in S} p_X(t) \ dt }{ \int_{t \in S} p_Y(t) \ dt } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
 \]</span></p></li>
<li><p>Continued. Denote <span class="math inline">\(P_X\)</span> and <span class="math inline">\(P_Y\)</span> the probability functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> respectively. Let <span class="math inline">\(S\)</span> be any Borel set in <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p><span class="math display">\[
 \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } = \frac{ \int_S 1 \ d P_X }{ \int_S 1 \ d P_Y } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
 \]</span></p></li>
<li><p>Let <span class="math inline">\(X = (X_1, X_2, ..., X_n), X&#39; = (X_1&#39;, X_2&#39;, ..., X_n&#39;)\)</span> be independent random vectors with dimension <span class="math inline">\(n\)</span>, where <span class="math inline">\(X_i, X_i&#39; \sim Laplace(0, \frac{\Delta}{\epsilon} )\)</span> (for <span class="math inline">\(1 \le i \le n\)</span>) are independent random variables. Let <span class="math inline">\(Y \doteq X&#39; + \mu = (Y_1, Y_2, ..., Y_n)\)</span> be another random vector, such that <span class="math inline">\(Y_i = X_i&#39; + \mu_i\)</span> for <span class="math inline">\(1 \le i \le n\)</span> and <span class="math inline">\(|\mu| = \sum_{i \in [n] } \mu_i \le \Delta\)</span>. For any <span class="math inline">\(t \in \mathbb{R}^n\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
    \frac{ p_X(t) }{ p_Y(t) } &amp;= \frac{ \exp( - \frac{\epsilon}{\Delta} \sum_{i \in [n] } |t_i| ) }{ \exp( - \frac{\epsilon}{\Delta} \sum_{i \in [n] } |t_i - \mu_i| ) } \\
    &amp;= \exp( - \epsilon \frac{ |t | - | t - \mu| }{ \Delta  } ) \\
    &amp;\in [\exp( - \epsilon \frac{ |\mu| }{ \Delta  } ), \exp( \epsilon \frac{ |\mu| }{ \Delta  } )] \\
    &amp;\in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
\end{aligned}
\]</span></p></li>
<li><p>Continued. Let <span class="math inline">\(S \subset \mathbb{R}^n\)</span> be any Borel set. Then</p>
<p><span class="math display">\[
    \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
\]</span></p></li>
</ol>
<h3 id="laplacian-mechanism.">Laplacian Mechanism.</h3>
<p>Let <span class="math inline">\((\mathcal{X}, d)\)</span> be a metric space and <span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}^n\)</span> a function defined on <span class="math inline">\(\mathcal{X}\)</span>.</p>
<p><strong><em>Definition.</em></strong> <span class="math inline">\(x, x&#39; \in \mathcal{X}\)</span> are called neighboring points if <span class="math inline">\(d(x, x&#39;) = 1\)</span>.</p>
<p><strong><em>Definition.</em></strong> The sensitivity <span class="math inline">\(\Delta\)</span> of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
\Delta \doteq \max_{x, x&#39; \in \mathcal{X}, d(x, x&#39;) = 1} | f(x) - f(x&#39;) |
\]</span></p>
<p>where <span class="math inline">\(| \cdot |\)</span> is the <span class="math inline">\(\ell_1\)</span> distance. In other words, <span class="math inline">\(\Delta\)</span> is the maximum <span class="math inline">\(\ell_1\)</span> distance between the images of two neighboring points.</p>
<p>We now construct a new randomized function, called <em>Laplacian mechanism</em> from <span class="math inline">\(f\)</span>, as follows.</p>
<p><strong><em>Definition.</em></strong> The Laplacian mechanism of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
g(x) = f(x) + Y
\]</span></p>
<p>where <span class="math inline">\(Y = (Y_1, Y_2, ..., Y_n)\)</span> is a random vector with independent random variables <span class="math inline">\(Y_i \sim Laplace(\Delta / \epsilon)\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="theorem">Theorem</h4>
<p><em><span class="math inline">\(g\)</span> is <span class="math inline">\((\epsilon, 0)\)</span>-differentially private.</em></p>
<p><em>Proof.</em> Let <span class="math inline">\(S \subset \mathbb{R}^n\)</span> be an arbitrary Borel set. We need to prove for any pair of neighboring <span class="math inline">\(x, x&#39; \in \mathcal{X}\)</span>, <span class="math display">\[
    \frac{ \Pr[ g(x) \in S]}{ \Pr[ g(x&#39;) \in S] } \le \exp(\epsilon)
\]</span></p>
<p>Denote <span class="math inline">\(p_x\)</span> and <span class="math inline">\(p_{x&#39;}\)</span> the density functions for <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(x&#39;)\)</span> respectively. It reduces to prove the following property of the density function <span class="math display">\[
    \frac{ p_x(t) }{ p_{x&#39;} (t)  } \le \exp(\epsilon), \qquad \forall t \in \mathbb{R}
\]</span></p>
<p>But <span class="math display">\[
\begin{aligned}
    \frac{ p_x(t) }{ p_{x&#39;} (t)  } 
        &amp;= \frac{ \exp( - \frac{\epsilon |t - f(x)| }{\Delta} ) }{ \exp( - \frac{\epsilon |t - f(x&#39;)| }{\Delta} )  } \\
        &amp;= \exp\big( - \frac{\epsilon }{\Delta} (|t - f(x)| - |t - f(x&#39;) | )  \big) \\
        &amp;\le \exp \big( \frac{\epsilon }{\Delta}  |f(x) - f(x&#39;) |  \big) \\
        &amp;= \exp( \epsilon )
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/47/">47</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
