<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/06/Gradient-Descent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/06/Gradient-Descent/" class="post-title-link" itemprop="url">Gradient Descent</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-06 10:10:45" itemprop="dateCreated datePublished" datetime="2020-03-06T10:10:45+11:00">2020-03-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-09 00:46:03" itemprop="dateModified" datetime="2020-03-09T00:46:03+11:00">2020-03-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Gradient descent is a greedy search method for optimization. It approximates a function locally by a linear function. Its analysis varies depending on the what kinds of constraints you place on it. </p>
<p>We study a baby version of the method. The problem at hand is given by:<br>$$<br>\min_{x \in \mathbb{R }^n} f(x)<br>$$<br>where $f$ is twice continuous differentiable and convex. The convexity of $f$ implies that it has a unique minimum value $\min_{x \in \mathbb{R }^n} f(x)$. Assume that $f$ indeed achieves minimum at some point and denote the optimal point as $x^*$. Note that is some case, $f$ can not achieve minimum value at any point. Consider $f(x) = e^x$. Then $\min_{x \in \mathbb{R } } e^x = 0$. $f$ can approach arbitrary close to $0$ but never achieve that value. </p>
<p>Before we delve into the details, we need to define some frequently used notations. </p>
<ul>
<li><p>The gradient $\nabla f(x):\mathbb{R }^n \rightarrow \mathbb{R }^n$ is the vector of partial derivatives of $f$ at point $x$:<br>$$<br>\nabla f(x) = \left[ \frac{\partial f (x) }{\partial x_1}, \frac{\partial f (x)}{\partial x_2}, .., \frac{\partial f (x) }{\partial x_n} \right]<br>$$</p>
</li>
<li><p>The Hessian $\nabla^2 f(x) : \mathbb{R }^n \rightarrow \mathbb{R }^{n \times n}$ is the matrix of second order derivatives of $f$ at $x$:<br>  $$<br>  \nabla f(x) = </p>
<pre><code>  \left[
  \begin&#123;aligned&#125;
      \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_1 \partial x_1&#125;, \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_1 \partial x_2&#125;, &amp;..., \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_1 \partial x_n&#125; \\
      \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_2 \partial x_1&#125;, \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_2 \partial x_2&#125;, &amp;..., \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_2 \partial x_n&#125; \\
      &amp;...\\
      \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_n \partial x_1&#125;, \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_n \partial x_2&#125;, &amp;..., \frac&#123;\partial^2 f (x)&#125;&#123;\partial x_n \partial x_n&#125; \\
  \end&#123;aligned&#125;
  \right]
</code></pre>
<p>  $$<br>  Since $f$ is twice continuous differentiable, the Hessian is symmetric.</p>
</li>
</ul>
<p>The algorithm is itself very simple. </p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a><strong>Algorithm</strong></h3><p><strong><em>Gradient Descent</em></strong>  </p>
<ol>
<li>Begin from some $x_0 \in \mathbb{R }^n$.</li>
<li>$x^{t + 1} \leftarrow x^t - \eta \nabla f(x^t)$.</li>
</ol>
<p>where $\eta &gt; 0$ is called step size. Below is an example that show how $f$ decreases as the algorithm iterates. </p>
<p> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/GradientDescent.jpg"></p>
<h2 id="Convergence"><a href="#Convergence" class="headerlink" title="Convergence"></a><strong>Convergence</strong></h2><p>To analyze the convergence behavior, we need additional assumptions on $f$. We analyze a baby version of gradient descent with strong assumptions:</p>
<ol>
<li>$f$ is $\beta$ smooth, i.e., $v^T \nabla^2 f(z) v \le \beta | v |^2$ for $\forall z, v \in \mathbb{R }^n$. </li>
<li>$f$ is $\alpha$ strongly convex, i.e., $v^T \nabla^2 f(z) v \ge \alpha | v |^2$ for $\forall z, v \in \mathbb{R }^n$.</li>
</ol>
<p>The interpretation is that, $\beta$ is the upper bound of the largest possible eigenvalue of the Hessian at any point and $\alpha$ is the lower bound. By definition we have $\beta \ge \alpha$.</p>
<p>Now define<br>$$<br>\Delta_t = f(x^t) - f(x^*)<br>$$<br>and $\kappa = \frac{\beta}{\alpha}$. The following theorem characterize the convergence speed of the algorithm:</p>
<p><strong><em>Theorem.</em></strong> <em>If we set $\eta = \frac{1}{\beta}$, then it holds that</em><br>$$<br>\Delta_t \le \Delta_0 \left( 1 - \frac{1}{4 \kappa } \right)^t.<br>$$</p>
<p>By the theorem, given any $\epsilon &gt; 0$, after<br>$$<br>O \left( \kappa \log \frac{\Delta_0}{\epsilon} \right)<br>$$</p>
<p>iteration, the error $\Delta_t$ will be smaller than $\epsilon$. </p>
<p><em>Proof:</em></p>
<p>The proof consists of two parts. $\beta$-Smooth and $\alpha$-Convexity play a role in each part separately.</p>
<h4 id="beta-Smoothness"><a href="#beta-Smoothness" class="headerlink" title="$\beta$-Smoothness."></a>$\beta$-Smoothness.</h4><p>The property guarantees that $\epsilon_{t + 1} \le \epsilon_t - \frac{1}{2 \beta} | f(x_t) |^2$. </p>
<p>First, by Taylor series expansion, we have for $x, y \in \mathbb{R }^n$: </p>
<p>$$<br>\begin{aligned}<br>    f(y)    &amp;= f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f(z) (y - x) \<br>            &amp;\le f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} \beta |y - x |^2<br>\end{aligned}<br>$$<br>where $z = \lambda x + (1 - \lambda) y$ for $\lambda \in [0, 1]$. The second inequality comes from the $\beta$-smoothness.</p>
<p>Replacing $(y - x)$ as $-\eta \nabla f(x)$, we get<br>$$<br>\begin{aligned}<br>    f(y)<br>        &amp;\le f( x ) - \eta | \nabla f(x) |^2 + \frac{1}{2} \beta \eta^2 | \nabla f( x ) |^2 \<br>        &amp;\le f( x ) + (\frac{1}{2} \beta \eta^2 -\eta) | \nabla f(x ) |^2  \<br>\end{aligned}<br>$$</p>
<p>The second coefficient $(\frac{1}{2} \beta \eta^2 -\eta)$ is minimized when $\beta \eta - 1 = 0$, i.e., $\eta = \frac{1}{\beta}$. It follows<br>$$<br>\begin{aligned}<br>    f(y )<br>        &amp;\le f( x ) + (\frac{1}{2} \beta \eta^2 -\eta) | \nabla f(x ) |^2  \<br>        &amp;\le f( x  ) - \frac{1}{2 \beta}  | \nabla f(x ) |^2  \<br>\end{aligned}<br>$$</p>
<p>Finally, substituting $y$ with $x^{t + 1}$ and $x$ with $x^t$, it becomes<br>$$<br>\begin{aligned}<br>    f(x^{t + 1})<br>        &amp;\le f(x^t) - \frac{1}{2 \beta } | \nabla f(x^t) |^2 \<br>        &amp;\longleftrightarrow    \<br>    \Delta_{t + 1}<br>        &amp;\le \Delta_t - \frac{1}{2 \beta } | \nabla f(x^t) |^2<br>\end{aligned}<br>$$</p>
<h4 id="alpha-Convexity"><a href="#alpha-Convexity" class="headerlink" title="$\alpha$ Convexity."></a>$\alpha$ Convexity.</h4><p>We might still have a problem, if when $x^t$ approaches $x^*$, $| \nabla f(x^t) |$ decreases too fast. That where $\alpha$ convexity comes into play. We will show that<br>$$<br>| \nabla f(x^t) |^2 \ge \Delta_t \cdot\frac{1}{2} \alpha<br>$$</p>
<ol>
<li><p>Proof 1. By $\alpha$ convexity, we have for all $y \in \mathbb{R}^n$,<br>$$<br>f(y) \ge f(x) + \nabla f(x)^T (y - x) + \frac{\alpha}{2} | y - x |^2<br>$$<br>Taking minimum of both sides,<br>$$<br>\min_{y \in \R^n} f(y) \ge \min_{y \in \R^n} \left( f(x) + \nabla f(x)^T (y - x) + \frac{\alpha}{2} | y - x |^2\right)<br>$$<br>The left hand side equals to $f(x^*)$ while the right hand side minimized to $f(x) - \frac{1}{2 \alpha} | \nabla f(x) |^2$ when $y - x = -\frac{1}{\alpha} \nabla f(x)$.</p>
<p>Substituting $x$ with $x^t$, we get<br>$$<br>| \nabla f(x^t) |^2 \ge 2 \alpha \left( f(x^t) - f(x^*) \right) = 2\alpha \Delta_t<br>$$</p>
</li>
<li><p>Proof 2. By convexity, we have </p>
<p> $$<br> \begin{aligned}</p>
<pre><code> \nabla f(x)^T (x - x^*) 
     &amp;\ge f(x) - f(x^*) 
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> Then by Taylor expansion and $\alpha$ convexity,<br> $$<br> \begin{aligned}</p>
<pre><code> f(x) - f(x^*) 
     &amp;\ge \nabla f(x^*)^T (x - x^*) +  \frac&#123;1&#125;&#123;2&#125; \alpha \| x - x^* \|^2\\
     &amp;=\frac&#123;1&#125;&#123;2&#125; \alpha \| x - x^* \|^2
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> Concatenating the inequalities, we get<br> $$<br> | \nabla f(x) | | x - x^* | \ge \nabla f(x)^T (x - x^*) \ge f(x) - f(x^*) \ge \frac{1}{2} \alpha | x - x^* |^2<br> $$<br> where the first inequality is Cauchy-Schwartz inequality. Multiplying the inequalities we get</p>
</li>
</ol>
<pre><code>$$
\| \nabla f(x) \|^2 \| x - x^* \|^2 \ge (f(x) - f(x^*)) \cdot\frac&#123;1&#125;&#123;2&#125; \alpha \| x - x^* \|^2
$$

which implies that 
$$
\| \nabla f(x) \|^2 \ge (f(x) - f(x^*)) \cdot\frac&#123;1&#125;&#123;2&#125; \alpha 
$$

Replacing $x$ with $x^t$ and $f(x) - f(x^*)$ with $\Delta_t$, we have 
$$
\| \nabla f(x^t) \|^2 \ge \Delta_t \cdot\frac&#123;1&#125;&#123;2&#125; \alpha 
$$
</code></pre>
<p>In combination, we conclude that<br>$$<br>\begin{aligned}<br>    \Delta_{t + 1}<br>        &amp;\le \Delta_t \left( 1- \frac{\alpha }{4 \beta} \right)<br>\end{aligned}<br>$$</p>
<p>which finishes the proof. </p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/05/Beta-Smoothness/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/Beta-Smoothness/" class="post-title-link" itemprop="url">Characterizations of Beta-Smoothness</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-05 20:05:37" itemprop="dateCreated datePublished" datetime="2020-03-05T20:05:37+11:00">2020-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-06 17:01:27" itemprop="dateModified" datetime="2020-03-06T17:01:27+11:00">2020-03-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>A differential function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is $\beta$-smooth if<br>$$<br>|\nabla f(y) - \nabla f(x) | \le \beta |y - x|<br>$$<br>where $|\cdot |$ is $\ell_2$ norm. </p>
<h4 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem 1."></a>Theorem 1.</h4><p>A twice continuously differential function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is $\beta$-smooth if and only if<br>$$<br>v^T \nabla^2 f(z) v \le \beta |v|^2<br>$$<br>for any $z, v \in \mathbb{R}^n$. </p>
<p><strong><em>Proof.</em></strong> </p>
<ol>
<li><p>(<strong><em>ONLY IF</em></strong>). $v^T \nabla^2 f(z) v \le \beta |v|^2$ is equivalent to that the maximum eigenvalue of $\nabla^2 f(z)$ is at most $\beta$. Let $v$ be the eigenvector of $\nabla^2 f(z)$. By mean value theorem<br> $$</p>
<pre><code> \frac&#123;\partial f&#125;&#123;\partial x_i&#125; (z + v) - \frac&#123;\partial f&#125;&#123;\partial x_i&#125; (z) = \sum_&#123;j = 1&#125;^n \left( \frac&#123;\partial f&#125;&#123;\partial x_i \partial x_j&#125; (c) \right) v_j
</code></pre>
<p> $$</p>
<p> For some $c$ between $z$ and $z + v$. As $f$ is twice continuously differentiable, for any given $\epsilon &gt; 0$, $\exists v$ that is small enough, such that<br> $$<br> \left| \frac{\partial f}{\partial x_i} (z + v) - \frac{\partial f}{\partial x_i} (z) \right| \in \left| \sum_{j = 1}^n \left( \frac{\partial f}{\partial x_i \partial x_j} (z) \right) v_j \right| \pm \frac{\epsilon}{n}<br> $$</p>
<p> Hence,<br> $$<br> | \nabla f(z + v) - \nabla f(z) | \in |\nabla^2 f(z) v| + \epsilon<br> $$</p>
<p> By equivalence of $\ell_1$-norm and $\ell_2$ norm, we get for $v$ small enough,<br> $$<br> | \nabla f(z + v) - \nabla f(z) | \in | \nabla^2 f(z) v | + \epsilon<br> $$</p>
<p> As $\epsilon$ can be arbitrary small, it concludes that<br> $$<br> | \nabla^2 f(z) v | \le \beta |v |<br> $$</p>
<p> which is equivalent to that the eigenvalues of $\nabla^2 f(z)$ is at most $\beta$. </p>
</li>
<li><p>(<strong><em>IF</em></strong>). We prove a stronger result that<br> $$</p>
<pre><code> \| \nabla f(y) - \nabla f(x) \| \cdot \| \nabla f(y) - \nabla f(x) \| \le \beta \left( \nabla f(y) - \nabla f(x) \right)^T (y - x) 
</code></pre>
<p> $$<br> By Cauchy-Schwartz inequality, we have<br> $$<br> \left( \nabla f(y) - \nabla f(x) \right)^T (y - x) \le | \nabla f(y) - \nabla f(x) | \cdot | y - x  |<br> $$<br> which will finish our proof. </p>
<p> By Taylor series expansion, we have<br> $$<br> \begin{aligned}</p>
<pre><code> f(y) &amp;
     = f(x) + \nabla f(x)^T (y - x) + \frac&#123;1&#125;&#123;2&#125; (y - x)^T \nabla^2 f(z) (y - x)
 \\
     &amp;\le f(x) + \nabla f(x)^T (y - x) + \frac&#123;1&#125;&#123;2&#125; \beta \| y - x \|^2
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> for some $z$ between $x$ and $y$. The inequality holds since $v^T \nabla^2 f(z) v \le \beta |v|^2$ for any $v \in \mathbb{R}^n$. </p>
<p> By symmetry, we get<br> $$<br> \begin{aligned}</p>
<pre><code> f(x) &amp;\le f(y) + \nabla f(y)^T (x - y) + \frac&#123;1&#125;&#123;2&#125; \beta \| y - x \|^2
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> Summing up the two inequalities, we obtain<br> $$<br> \left( \nabla f(y) - \nabla f(x) \right)^T (y - x)^T  \le \beta | y - x |^2<br> $$</p>
</li>
</ol>
<p>$\square$</p>
<h4 id="Theorem-2"><a href="#Theorem-2" class="headerlink" title="Theorem 2."></a>Theorem 2.</h4><p>If $f$ is $\beta$-smooth, then for $x, y \in \mathbb{R}^n$, we have<br>$$<br>f(y) \le f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} \beta | y - x |^2<br>$$</p>
<p><em>Proof.</em><br>By fundamental theorem of calculus,<br>$$<br>\begin{aligned}<br>    f(y) - f(x) - \nabla f(x)^T (y - x)<br>        &amp;=<br>    \int_{0}^1 \nabla f(x + t(y - x))^T(y - x) \ dt - \nabla f(x)^T (y - x)<br>    \<br>        &amp;=<br>    \int_{0}^1 \left( \nabla f(x + t(y - x)) - \nabla f(x) \right)^T(y - x) \ dt<br>    \<br>        &amp;\le<br>    \int_{0}^1 | \nabla f(x + t(y - x)) - \nabla f(x) | \cdot | y - x | \ dt<br>    \<br>        &amp;\le<br>    \int_{0}^1 \beta t | y - x |  \cdot | y - x | \ dt<br>    \<br>        &amp;\le<br>    \frac{1}{2} \beta | y - x |^2<br>\end{aligned}<br>$$</p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/01/Counting-Triangle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/01/Counting-Triangle/" class="post-title-link" itemprop="url">Counting Triangle</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-01 22:20:20" itemprop="dateCreated datePublished" datetime="2020-03-01T22:20:20+11:00">2020-03-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-05 20:20:54" itemprop="dateModified" datetime="2020-07-05T20:20:54+10:00">2020-07-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given an undirected graph $G = \left&lt; V, E \right&gt;$ with $|V| = n$ vertices and $|E| = m$ edges, we can count the number of triangle in $O(m^{1.5})$ time. </p>
<ol>
<li><p>Label the vertices in ascending order according their degrees. Break tie arbitrarily. After ordering, we assume that $d_{v_1} \le d_{v_2} \le … \le d_{v_n}$. </p>
</li>
<li><p>Check all possible triangles $(u, v, w)$ such that $d_u \le d_v \le d_w$. This can be implemented as follows: </p>
<ul>
<li>Check all edges $(u, v)$ such that $d_u \le d_v$. </li>
<li>Check each neighbor $w$ of $u$ such that $d_w \ge d_v$. We can sort the neighbors of each vertex according to their degree. Then finding the first vertex with $d_w \ge d_v$ takes $O(\log d_u)$ time.</li>
</ul>
</li>
</ol>
<p><em>Claim: This algorithm has complexity $O(m^{1.5})$.</em></p>
<p><em>Proof.</em></p>
<ol>
<li>If $d_w \ge \sqrt m$, then the number of such possible $w$ is bounded by $O( m / \sqrt m) = O(\sqrt m)$. Therefore, the time spent on $(u,v )$ for ${w : d_w \ge \sqrt m}$ is $O(\sqrt m)$. The time spent over all edges is:<br> $$<br> \sum_{(u, v) \in E} \sqrt m = m\sqrt m = m^{1.5}<br> $$</li>
<li>If $d_w &lt; \sqrt m$, then $d_u \le d_v \le d_w &lt; \sqrt m$. The number of such triangles is bounded by:<br> $$<br> \sum_{u \in S} d_u^2 \le \sqrt m \left( \sum_{u \in S} d_u \right) \le \sqrt m \cdot 2m= 2m^{1.5}<br> $$</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/27/Primal-Dual-Algorithm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/27/Primal-Dual-Algorithm/" class="post-title-link" itemprop="url">Primal-Dual Algorithm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-27 21:57:12" itemprop="dateCreated datePublished" datetime="2020-02-27T21:57:12+11:00">2020-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-28 11:36:47" itemprop="dateModified" datetime="2020-02-28T11:36:47+11:00">2020-02-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Primal:<br>$$<br>\begin{aligned}<br>    &amp;\min       &amp;c^T x \<br>    &amp;s.t.       &amp;Ax = b\<br>    &amp;           &amp;x \ge 0<br>\end{aligned}<br>$$</p>
<p>We assume that $b \ge 0$. Otherwise we can flip the $i$ constraints $A^T_i x = b_i$ by $- A^T_i x = -b_i$, where $A^T_i$ is the $i$-th column of matrix $A^T$, i.e, the $i$-th row of matrix $A$.  </p>
<p>Dual:<br>$$<br>\begin{aligned}<br>    &amp;\max       &amp;y^T b \<br>    &amp;s.t.       &amp;y^TA \le c^T \<br>\end{aligned}<br>$$</p>
<p>Suppose that we have a feasible solution $y$ for the dual (if $c \ge 0$, then $y = 0$ is a feasible solution). Weak duality states that<br>$$<br>c^T x \ge y^T A x = y^T b<br>$$</p>
<p>Denote $S = { i : i \in [n], y^T A_i &lt; c_i }$ (the set of constraints that have slacks for increments) and $\bar S = [n] \setminus S$ (the set of indexes of tight dual constraints), where $A_i$ is the $i$-th columns of $A$.</p>
<p>By weak duality, if we can find an $x$ such that</p>
<ol>
<li>$x_{S } = 0$, where $x_{S} \doteq (x_i : i \in {S})$. </li>
<li>$x_{\bar S} \ge 0$ and $A_{\bar S} x_{\bar S} = b$, where $A_{\bar S}$ is the sub-matrix of $A$ containing only columns in ${\bar S}$.</li>
</ol>
<p>then both $y$ and $x$ are optimal. However, as $y$ is not necessarily optimal, we have the following relaxed optimization problem, which is called <em>restricted primal</em></p>
<p>Relaxation:<br>$$<br>\begin{aligned}<br>    &amp;\min       &amp;1^T \bar x \<br>    &amp;s.t.       &amp;A_{\bar S} x_{\bar S} + \bar x = b \<br>    &amp;           &amp;x_{\bar S} \ge 0 \<br>    &amp;           &amp;\bar x \ge 0<br>\end{aligned}<br>$$</p>
<p>Note that $x_{\bar S} = 0$ and $\bar x = b$ is a feasible solution as we assume that $b \ge 0$. Therefore, <em>restricted primal</em> is feasible. Further, it is bounded below by $1^{\bar S} 0 = 0$. Indeed, if it achieves optimal solution value $0$, it holds that $\bar x = 0$ so that $A_{\bar S} x_{\bar S} = b$. </p>
<p>Associated with the <em>restricted primal</em> is the <em>restricted dual</em><br>$$<br>\begin{aligned}<br>    &amp;\max       &amp; \bar y^T b \<br>    &amp;s.t.       &amp; \bar y^TA_{\bar S} \le 0 \<br>    &amp;           &amp; \bar y^T \le 1^T<br>\end{aligned}<br>$$</p>
<p>By strong duality, it optimal solution $\bar y^T b = 1^T \bar x$. If $1^T \bar x= \bar y^T b &gt; 0$, then we can improve the dual solution $y$ by<br>$$<br>y \leftarrow y + \epsilon \bar y<br>$$</p>
<p>for some $\epsilon$. To determine its value, first observe that<br>$$<br>(y + \epsilon \bar y)^{T} A_{\bar S} = y^T A_{\bar S} + \epsilon \bar y^T A_{\bar S} \le y^T A_{\bar S} = c_{\bar S}^T<br>$$</p>
<p>Hence it is left to guarantee that<br>$$<br>(y + \epsilon \bar y)^T A_S = y^T A_S + \epsilon \bar y^T A_S \le  c_S<br>$$</p>
<p>By definition of ${S}$, we have $y^T A_{S} &lt; c_{S}^T$. If that $\bar y^T A_i \le 0$ for $\forall i \in {S}$, then $\epsilon$ can be arbitrary large and the dual is unbounded. Otherwise, </p>
<p>$$<br>\epsilon = \min_{i \in S, \bar y^T A_i &gt; 0} \frac{c_i - y^T A_i } { \bar y^T A_i }<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/26/The-Remainder-Term-in-Taylor-Series/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/26/The-Remainder-Term-in-Taylor-Series/" class="post-title-link" itemprop="url">The Remainder Term in Taylor Series</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-26 13:57:00" itemprop="dateCreated datePublished" datetime="2020-02-26T13:57:00+11:00">2020-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-28 11:22:58" itemprop="dateModified" datetime="2020-02-28T11:22:58+11:00">2020-02-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Taylor-Series"><a href="#Taylor-Series" class="headerlink" title="Taylor Series"></a>Taylor Series</h2><p>The philosophy of Taylor series is to approximate a function with polynomials that use only the information of a single point in the domain of the function. Consider the function $f: \mathbb{R} \rightarrow \mathbb{R}$ and a point $a \in \mathbb{R}$. Further, we would like to use $(x-a)^0$, $(x- a)^1$, $(x- a)^2$, …, $(x- a)^n$ as the basic building blocks of our approximation. The polynomial constructed is of the form:<br>$$<br>p_n(x) = c_0 + c_1 (x - a) + c_2(x - a)^2 + … + c_n (x - a)^n<br>$$<br>Now, assume that the $n$-th derivative of $f$ exits. To approximate $f$ well, $p_n$ should satisfy<br>$$<br>p_n(a) = f(a)<br>$$<br>so that $c_0 = f(a)$. </p>
<p>Further, $f’(a)$ tell how the function $f(x)$ changes when $x$ moves from $a$ by a little bit. We require $p_n(x)$ to exhibit the same behavior:<br>$$<br>p’_n(a) = f’(a)<br>$$<br>so that $c_1 = f’(a)$. </p>
<p>To fit $f(x)$ even better, we force the second order derivative of $p_n(x)$ at $x$ comply with $f’’(a)$, hence<br>$$<br>p_n’’(a) = f’’(a)<br>$$<br>and $c_2 = \frac{1}{2} f’’(a)$. </p>
<p>Following the above procedure,we get<br>$$<br>c_i = \frac{1}{i!}f^{(i) } (a)<br>$$<br>and<br>$$<br>p_n(x) = \sum_{k = 0}^n \frac{ f^{( k ) }(a) }{ k! }  (x - a)^k<br>$$<br>where $f^{(0)} \doteq f$, $0! \doteq 1$.  We have just show that the construct polynomial satisfies </p>
<p><strong><em>Theorem. $p^{ (k) }_n (a) = f^{ (k) } (a)$, for $k \in [n]$.</em></strong></p>
<h2 id="Remainder"><a href="#Remainder" class="headerlink" title="Remainder"></a>Remainder</h2><p>One natural to ask is that, how accurate is the $p_n(x)$. In particular, given $x \neq a$, we want to measure the error<br>$$<br>r_n(x) = f(x) - p_n(x)<br>$$<br>quantitively. </p>
<p>Note that if the function $f$ is itself a polynomial up to order $n$, then the approximation is accurate that is $r_n(x) \equiv 0$ for $x \in \mathbb{R}$. In this case, we recover $f$ globally using only local information at $a$. </p>
<p>In general, if $f^{ (n + 1)} (x)$ exists and is continuous, then the error is characterized as </p>
<p><strong><em>Theorem</em></strong><br>$$<br>r_n(x) = \frac{1}{n!} \int_a^x f^{ (n + 1) } (t) (x - t)^{n } dt = -\int_a^x f^{ (n + 1) } (t)  d \left( \frac{(x - t)^{n + 1} }{(n + 1)!} \right)<br>$$<br><em>Proof.</em></p>
<p>By fundamental theorem of calculus, we have<br>$$<br>f(x) = f(a) + \int_{a}^x f’(t) dt<br>$$<br>On the other hand<br>$$<br>\frac{\partial [f’(t) (t - x)] }{\partial t} = f’’(t) (t - x) + f’(t)<br>$$<br>Therefore,<br>$$<br>f’(t) (t - x) \mid_{t = a}^x = f’(a) (x -a)  = \int_{a}^x f’’(t) (t - x) dt  + \int_{a}^x f’(t) dt<br>$$<br>and<br>$$<br>f(x) = f(a) + f’(a) (x - a) +  \int_{a}^x f’’(t) (x - t) dt<br>$$<br>Similarly,<br>$$<br>\begin{aligned}<br>\int_{a}^x f’’(t) (x - t) dt<br>    &amp;= - \int_{a}^x f’’(t) d \frac{ (x - t)^2}{2!} \<br>    &amp;= f’’(a) \frac{ (x - a)^2}{2!} + \int_{a}^x \frac{ (x - t)^2}{2!} f^{(3) } (t) dt<br>\end{aligned}<br>$$<br>As just shown, the proof can be finished by induction.<br>$\square$</p>
<p><strong><em>Weighted Mean Value Theorem.</em></strong><br>Given a continuous functions $f \ge 0$ and an Riemann integral function $g \ge 0$ defined on an closed interval $I$, then<br>$$<br>\int_I f(t) g(t) dt = f(c) \int_I g(t) dt<br>$$<br>for some $c \in I$. </p>
<p><em>Proof.</em> As $f$ is continuous and $I$ is closed, $\exists m, M \in \mathbb{R}$, s.t.,<br>$$<br>m \le f(t) \le M, \qquad \forall t \in I<br>$$</p>
<p>Therefore,<br>$$<br>mg(t) \le f(t) g(t) \le M g(t), \qquad \forall t \in I<br>$$</p>
<p>Define $S = \int_I g(t) dt$. By monotonicity of Riemann integration, we have<br>$$<br>m S \le \int_I f(t) g(t) dt \le M S<br>$$</p>
<p>Note that $S f(t)$ is a continuous function in $I$. Hence, by mean value theorem, $\exists c \in I$, such that<br>$$<br>S f(c) = \int_I f(t) g(t) dt<br>$$<br>$\square$</p>
<p><strong><em>Corollary 1.</em></strong> By weighted mean value theorem, we have<br>$$<br>\begin{aligned}<br>r_n(x)<br>    &amp;= \frac{1}{n!} \int_a^x f^{ (n + 1) } (t) (x - t)^{n } dt  \<br>    &amp;= f^{ (n + 1) } (c) \int_a^x  \frac{1}{n!} (x - t)^{n } dt  \<br>    &amp;= \frac{f^{ (n + 1) } (c ) }{(n + 1) ! } (x - a)^{n + 1 }<br>\end{aligned}<br>$$</p>
<p>for some $c \in [a, x]$. </p>
<p><strong><em>Corollary 2.</em></strong> Let $f:\rightarrow \mathbb{R}^n \rightarrow \mathbb{R}$ be a function with continuous second partial derivatives, then given $a \in \mathbb{R}^n$, it holds that<br>$$<br>f(x) = f(a) + [\nabla f(c)]^T (x - a)<br>$$<br>for some $c \in a + t(x - a)$, where $t \in [0, 1]$. </p>
<p><em>Proof:</em> Define $g(t) = f(a + t (x- a)) : [0,1] \rightarrow \mathbb{R}$. Then </p>
<ol>
<li>$g(0) = f(a)$.</li>
<li>$g(1) = f(x)$. </li>
<li>$g’(t) = [\nabla f(a + t (x- a) ) ]^T (x - a)$</li>
<li>By mean value theorem, $\exists t \in [0, 1]$, s.t.,<br>$$<br> \frac{g(1) - g(0)}{1} = f(x) - f(a) = g’(t) = [\nabla f(a + t (x- a) ) ]^T (x - a)<br>$$</li>
</ol>
<p>$\square$</p>
<p><strong><em>Corollary 3.</em></strong> Let $f$, $g$ and $c$ as defined before, then<br>$$<br>f(x) = f(a) + [\nabla f(a)]^T (x - a) + \frac{1}{2} (x - a)^T [\nabla^2 f(c)] (x - a)<br>$$</p>
<p><em>Proof:</em><br>$$<br>g’’(t) = (x - a)^T [\nabla^2 f(a + t (x- a) ) ] (x - a)<br>$$</p>
<p>Then applying Corollary 1 gives<br>$$<br>g(1) = g(0) + g’(0)(1 - 0) + \frac{1}{2} g’’(t) (1 - 0)^2<br>$$</p>
<p>for some $t \in [0, 1]$.</p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/24/Eigenvalues-and-Eigenvectors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/24/Eigenvalues-and-Eigenvectors/" class="post-title-link" itemprop="url">Eigenvalues, Eigenvectors and SVD</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-24 21:57:00 / Modified: 23:20:37" itemprop="dateCreated datePublished" datetime="2020-02-24T21:57:00+11:00">2020-02-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Eigenvector-and-Eigenvalues"><a href="#Eigenvector-and-Eigenvalues" class="headerlink" title="Eigenvector and Eigenvalues"></a>Eigenvector and Eigenvalues</h2><p><strong><em>Theorem. Given a symmetric matrix $A \in \mathbb{R}^{n \times n}$ that is positive semi-definitive, there exists an orthogonal matrix $V \in \mathbb{R}^{n \times n}$, such that</em></strong><br>$$<br>A = V \Sigma V^T<br>$$<br><strong><em>where $\Sigma \in \mathbb{R}^{n \times n}$ is a diagonal matrix with all diagonal elements non-negative.</em></strong></p>
<p>We need the following lemma for the proof. </p>
<p><strong>Lemma: If a function $f: S \rightarrow \mathbb{R}$ is continuous on a closed set $S$, then it takes maximum value at some point of $S$.</strong></p>
<p><em>Proof.</em><br>Consider the maximization problem:<br>$$<br>\begin{aligned}<br>&amp;\max &amp; \frac{1}{2} x^T A x \<br>&amp;s.t. &amp;  x^T x = 1<br>\end{aligned}<br>$$</p>
<p>*<strong>Question to ponder:</strong> Explain the geometric meaning of the term $x^T A x$.* </p>
<p>The corresponding Lagrange function is given by<br>$$<br>L(x, \lambda) = \frac{1}{2} x^T A x - \lambda (x^T x  - 1)<br>$$</p>
<p>Taking derivative with respect to $x$ gives<br>$$<br>\frac{\partial L}{\partial x} = Ax - \lambda x<br>$$</p>
<p><strong><em>Existence of maximum solution:</em></strong><br>Note that ${ x : x^T x = 1 }$ is defined the boundary of the unit cycle, which is a closed set. Combined with the fact that $f$ is a continuous function, it takes its maximum value at some point in ${ x : x^T x = 1 }$. Denote this point $v_1 \in { x : x^T x = 1 }$. </p>
<p><strong><em>Gradient at the maximum point:</em></strong> At this point, it holds that<br>$$<br>\frac{\partial L}{\partial x} \mid_{x = v_1} = Av_1 - \lambda v_1 = 0<br>$$</p>
<p>which implies that $v_1$ is an eigenvector of $A$. Denote its associated eigenvalue $\lambda_1$. </p>
<p>*<strong>Question to ponder:</strong> Prove that $\lambda_1$ is the largest eigenvalue.*</p>
<p>After finding $v_1$, consider a new optimization problem:<br>$$<br>\begin{aligned}<br>&amp;\max   &amp; \frac{1}{2} x^T A x \<br>&amp;s.t.   &amp;  x^T x = 1 \<br>&amp;       &amp;  v_1^T x = 0<br>\end{aligned}<br>$$</p>
<p>That is, we want to find a new unit vector that is orthogonal to $v_1$ while maximizing $\frac{1}{2} x^T A x$. </p>
<p><strong><em>Existence of maximum solution.</em></strong> By the lemma and the facts: </p>
<ol>
<li>${x : x^T x = 1} \cap { x : v_1^T x = 0 }$ is closed.   </li>
<li>$\frac{1}{2} x^T A x$ is continuous. </li>
</ol>
<p>Denote this maximum point $v_2$. Now consider the new Lagrange function for the optimization<br>$$<br>L(x, \lambda) = \frac{1}{2} x^T A x - \lambda (x^T x  - 1) - \beta(v_1^T x - 0)<br>$$</p>
<p><strong><em>Gradient at the maximum point:</em></strong> At this point, it holds that<br>$$<br>\frac{\partial L}{\partial x} \mid_{x = v_2} = Av_2 - \lambda v_2 - \beta v_1 = 0<br>$$</p>
<p>Left multiplying $v_1^T$ gives<br>$$<br>(v_1^T A)v_2 - \lambda v_1^T v_2 - \beta v_1^T v_1 = (A v_1)^T v_2  - \beta = \lambda_1 v_1^T v_2 - \beta = -\beta = 0<br>$$<br>where $v_1^T A = (A v_1)^T$ follows from the symmetry of $A$, i.e., $A = A^T$. </p>
<p>Therefore,<br>$$<br>Av_2 - \lambda v_2 = 0<br>$$</p>
<p>It concludes that $v_2$ is an eigenvector of $A$. Denote its associated eigenvalue $\lambda_2$. </p>
<p>In a similar manner, we can define<br>$$<br>\begin{aligned}<br>&amp;\max   &amp; \frac{1}{2} x^T A x \<br>&amp;s.t.   &amp;  x^T x = 1 \<br>&amp;       &amp;  v_1^T x = 0 \<br>&amp;       &amp;  v_2^T x = 0 \<br>\end{aligned}<br>$$</p>
<p>There exists an optimal point $v_3$. The derivative of Lagrange function at this point is<br>$$<br>\frac{\partial L}{\partial x} \mid_{x = v_3} = Av_3 - \lambda v_3 - \beta_1 v_1 - \beta_2 v_2 = 0<br>$$</p>
<p>for some parameters $\lambda, \beta_1, \beta_2$. Left multiplying $v_1^T$ or $v_2^T$ gives $\beta_1 = 0$ and $\beta_2 = 0$ so that $v_3$ is an eigenvector of $A$. </p>
<p>Following the procedure, we can find $n$ eigenvectors $v_1, v_2, …, v_n$ of $A$. Define<br>$$<br>\Sigma =<br>\begin{aligned}<br>    \left[<br>    \begin{matrix}<br>    &amp;\lambda_1 &amp;    &amp;   \<br>    &amp;          &amp;\lambda_2 &amp; \<br>    &amp;           &amp;   &amp;   … \<br>    &amp;           &amp;    &amp;      &amp; \lambda_n<br>    \end{matrix}<br>    \right]<br>\end{aligned}<br>\qquad<br>V =<br>\begin{aligned}<br>    \left[<br>    \begin{matrix}<br>    v_1, v_2, …, v_n<br>    \end{matrix}<br>    \right]<br>\end{aligned}<br>$$</p>
<p>Then<br>$$<br>A V = V \Sigma<br>$$</p>
<p>Hence,<br>$$<br>A = V \Sigma V^{-1} = V \Sigma V^T<br>$$</p>
<p>$\square$</p>
<h2 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h2><p>In general, a matrix $A \in \mathbb{R}^{m \times n}$ takes a vector $x \in \mathbb{R}^n$ to a vector $Ax \in \mathbb{R}^m$. It is natural to ask, is there a vector $x$, such that its length is enlarged by the largest factor after the transformation:<br>$$<br>\max_{x \in \mathbb{R}^n } \frac{ ||Ax || }{ ||x|| }<br>$$</p>
<p>As $||x||$ is a number, $\frac{ ||Ax || }{ ||x|| } = || A \frac{x}{||x||} ||$. This is equivalent to find a unit vector $x$ that maximize its length (or equivalently, the square of its length) after the transformation<br>$$<br>\begin{aligned}<br>&amp;\max &amp; \frac{1}{2} x^T A^T Ax \<br>&amp;s.t. &amp;  x^T x = 1<br>\end{aligned}<br>$$</p>
<p>In this case, the objective function is $||Ax||^2 = (Ax)^T (Ax) = x^T A^T A x$. </p>
<p>Following similar process of previous section, we can find a set of eigenvectors $v_1, v_2, …, v_n$ of $A^T A$. Define $U \in \mathbb{R}^{m \times n}$<br>$$<br>U = A V { \sqrt{\Sigma^{-1} } } = AV<br>\begin{aligned}<br>    \left[<br>    \begin{matrix}<br>    &amp;\sqrt { 1 / \lambda_1} &amp;    &amp;   \<br>    &amp;          &amp;\sqrt { 1 / \lambda_2} &amp; \<br>    &amp;           &amp;   &amp;   … \<br>    &amp;           &amp;    &amp;      &amp; \sqrt { 1 / \lambda_n}<br>    \end{matrix}<br>    \right]<br>\end{aligned}<br>$$</p>
<p>Then $U { \sqrt \Sigma } V^T = A$, which is exactly the SVD. </p>
<p>Finally, we can verify that the columns of $U$ are perpendicular, $\forall i \neq j, i, j \in [n]$,<br>$$<br>u_i^T u_j = (\sqrt { 1 / \lambda_i} A V_i)^T (\sqrt { 1 / \lambda_j} A V_j) = \sqrt { 1 / \lambda_i} \sqrt { 1 / \lambda_j} v_i^T A^T A v_j  = 0<br>$$</p>
<p>as $v_i$ and $v_j$ are orthogonal eigenvectors of $A^T A$. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/20/Ellipsoid/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/20/Ellipsoid/" class="post-title-link" itemprop="url">Ellipsoid method</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-20 21:19:42" itemprop="dateCreated datePublished" datetime="2020-02-20T21:19:42+11:00">2020-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-22 22:39:05" itemprop="dateModified" datetime="2020-02-22T22:39:05+11:00">2020-02-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Ellipsoid method is a weakly polynomial algorithm for solving linear programming. The preliminary version was introduced by the Russian mathematician Shor in 1977 for general convex optimization problems. Then it was applied to linear programming by Khachyan in 1979 [1]. Before interior point method, it was the first polynomial time algorithm for LP. </p>
<h2 id="Feasibility-implies-Optimality"><a href="#Feasibility-implies-Optimality" class="headerlink" title="Feasibility implies Optimality"></a>Feasibility implies Optimality</h2><p>Now, consider the primal LP:<br>$$<br>\mathcal{P}:  \qquad<br>\begin{aligned}<br>  &amp;\max   &amp;c^T x    &amp; \<br>  &amp;s.t.   &amp;Ax \le b &amp; \<br>  &amp;       &amp; x \ge 0 &amp;<br>\end{aligned}<br>$$</p>
<p>whose dual is given by<br>$$<br>\mathcal{D}:  \qquad<br>\begin{aligned}<br>  &amp;\min   &amp;y^Tb    \ \  &amp; \<br>  &amp;s.t.   &amp;y^TA \ge c^T &amp; \<br>  &amp;       &amp;y \ge 0 \ \  &amp;<br>\end{aligned}<br>$$</p>
<p>The ellipsoid does not optimize the primal directly. Instead, it try to return a feasible point $(x, y)$ that satisfies the following constraints:<br>$$<br>\begin{aligned}<br>         c^T x &amp;= y^Tb  &amp; \<br>         Ax    &amp;\le b   &amp; \<br>          x    &amp;\ge 0       &amp; \<br>         y^TA  &amp;\ge c^T &amp; \<br>         y     &amp;\ge 0 \ \  &amp;<br>\end{aligned}<br>$$</p>
<p>By weak duality, if such point exists, then $x$ is the optimal solution for the primal program. (Note that, if there is no feasible point for the third program, the primal could be either infeasible or unbounded.)</p>
<h2 id="Ellipsoid"><a href="#Ellipsoid" class="headerlink" title="Ellipsoid"></a>Ellipsoid</h2><p>Ellipsoid is a unit ball that has been squashed or stretched along orthogonal directions. We illustrate this by an example in two dimension. The following picture shows an ellipse obtained by enlarging the unit ball by a facto of $2$ along the $v_1$ direction and squashing it ball by half along the $v_2$ direction, where $v_1$ and $v_2$ constitute an orthogonal base. </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Ellipsoid/EllipsoidMethod1.jpg"></p>
<p>It is equivalent to first squash or stretch the ball along the directions of $e_1, e_2$, </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Ellipsoid/EllipsoidMethod2.jpg"></p>
<p>then rotate the resulting ellipsoid to corresponding orthogonal directions. </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Ellipsoid/EllipsoidMethod3.jpg"></p>
<p>The rotation can be done by left multiplying an orthogonal matrix<br>$$<br>V = [v_1, v_2]<br>$$</p>
<p>where $v_1, v_2$ are the orthogonal base for which the squash and stretch are performed. </p>
<p>To recover the unit ball, we can first perform the reverse rotation:<br>$$<br>x \leftarrow V^T x<br>$$</p>
<p>followed by a reverse scaling:<br>$$<br>x \leftarrow<br>  \begin{bmatrix}<br>    1/2  &amp; \<br>    &amp;  2<br>  \end{bmatrix}<br>  x<br>$$</p>
<p>Note that the reverse rotation is represented by matrix $V^T$ as $V V^T  = I$. Therefore, the ellipsoid is denoted as<br>$$<br>  \left(<br>  \begin{bmatrix}<br>    1/2  &amp; \<br>    &amp;  2<br>  \end{bmatrix}<br>  V^T x \right)^T<br>  \left(<br>  \begin{bmatrix}<br>    1/2  &amp; \<br>    &amp;  2<br>  \end{bmatrix}<br>  V^T x \right) \le 1<br>$$</p>
<p>That is<br>$$<br>  x^T \left( V<br>  \begin{bmatrix}<br>    1/2  &amp; \<br>    &amp;  2<br>  \end{bmatrix}^2<br>  V^T \right) x  \le 1<br>$$</p>
<p>In general, an ellipsoid is represented by<br>$$<br>E(s, P) = { x : (x - s)^T P^{-1} (x - s) \le 1}<br>$$</p>
<p>where $s \in \mathcal{R}^n$ is the center of the ellipsoid and $P$ is a positive semi-definite matrix. </p>
<p>Recall that a positive semi-definite matrix $P$ can be rewritten as<br>$$<br>P = V \Sigma V^T<br>$$<br>and<br>$$<br>P^{-1} = V \Sigma^{-1} V^T<br>$$</p>
<p>where $V$ is an orthogonal matrix and $\Sigma$ is a diagonal matrix. Geometrically, $V$ are the directions and $\sqrt \Sigma$ are the ratios of the squashes or stretches. If we define $B = V \sqrt \Sigma$,  then $B^T = \sqrt \Sigma V^T$, $B^{-1} = \sqrt \Sigma^{-1} V^T$ and $P = B B^T$, $P^{-1} = (B^{-1} )^T B^{-1}$.   </p>
<p>The representation can be viewed as merely a recipe for converting the ellipsoid back to the unit ball:</p>
<ol>
<li>First perform a translation $x \leftarrow (x - s)$ so that the center of the ellipsoid is now the center. </li>
<li>Rotate the ellipsoid, such that its axes are aligned with $e_1, e_2, …, e_n$. This is done by multiplying the matrix $V^T$: $x \leftarrow V^T x$. </li>
<li>Squash or stretch along the axes $e_1, e_2, …, e_n$: $x \leftarrow \sqrt \Sigma^{-1} x$. </li>
<li>Therefore, the linear transformation for the recovery is<br>$$<br>x \leftarrow B^{-1} (x - s)<br>$$</li>
</ol>
<p>Conversely, $E(s, P) = E(s, V \sqrt \Sigma \sqrt \Sigma V^T)$ tells us how to get the ellipsoid from the unit ball:</p>
<ol>
<li>Scale unit ball along the axes $e_1, e_2, .., e_n$ by factors that $\sqrt \Sigma_{1, 1}, \sqrt \Sigma_{2, 2}, …, \sqrt \Sigma_{n, n}$.</li>
<li>Rotate the ellipsoid by matrix $V$. </li>
<li>Move the ellipsoid by $s$. </li>
</ol>
<p>That is, $x \leftarrow V \sqrt \Sigma y + s$ takes the unit ball ${ y : y^T y \le 1 }$ to the corresponding ellipsoid ${ x : (x - s)^T V \Sigma^{-1} V^T (x - s) \le 1 }$.</p>
<h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>Now return to our discussion of ellipsoid method. Without lose of generality, we consider only the following  problem: </p>
<p><em>Given $F = {x : Ax \le b}$, output any $x \in F$ or report $F = \emptyset$, where $A \in \mathcal{R}^{m \times n}, x \in \mathcal{R}^n, b \in \mathcal{R}^m$</em>. </p>
<p>We assume further that</p>
<ol>
<li><p>$\exists R \in \mathcal{R}$, $F \subset B(0, R)$. Besides, if $F \neq \emptyset$, $\exists r \in \mathcal{R}$, and $t \in \mathcal{R}^n$, s.t., $B(t, r) \subset F$. Here $B(0, R) = { x : ||x||_2 \le R }$ and $B(t, r) = { x : ||x - t||_2 \le r }$. </p>
</li>
<li><p><em>Separation Oracle:</em> Given a vertex $s \in \mathcal{R}^n$, the separation oracle is a mechanism that asserts</p>
<ul>
<li>either $s \in F$</li>
<li>or $s \notin F$ and returns a direction (which is a unit vector) $d \in \mathcal{R}^n$, such that $d^T s \ge d^T x$ for $\forall x \in F$. </li>
</ul>
</li>
</ol>
<p>The first assumption says that $F$ is neither too “large” nor too “small”. Note that $B(t, r) \subset F$ also implies that $F$ has full dimension. </p>
<p>For LP, the separation is straightforward to implement: we just check whether $Ac \ge b$. If so, then $c \in F$. Otherwise, $\exists i \in [m]$, such that the $i$-th constraint is violated: $A_{i,\cdot} c &gt; b_i$. Just return $d^T = A_{i, \cdot}$. </p>
<p><strong><em>Question to ponder: the feasible region of an LP may not satisfy the first assumption? It is possible to reduce it to one satisfying the first assumption?</em></strong></p>
<h2 id="The-Algorithm"><a href="#The-Algorithm" class="headerlink" title="The Algorithm"></a>The Algorithm</h2><p>The algorithm begins with a special ellipsoid $E_0 = B(0, R)$. At each step, it queries the <em>separation oracle</em> and generates a smaller ellipsoid (with reduced volume). If $F$ is not empty, the ellipsoid at each step is guaranteed to contain it. The algorithm stops before the volume of the ellipsoid decreases below $vol(B(t, r))$. </p>
<p>The algorithmic flow is shown below:</p>
<hr>
<ol>
<li><p>$i = 0$.  </p>
</li>
<li><p>$E_0 = B(0, R)$.   </p>
</li>
<li><p><em>While</em> $vol(E_i(s_i, P_i)) \ge vol(B(t, r))$:  </p>
</li>
<li><p>$\qquad$ <em>if</em> $s_i \in F$, output $s_i$.  </p>
</li>
<li><p>$\qquad$ <em>else</em>,   </p>
</li>
<li><p>$\qquad$ $\qquad$ Obtain a direction $d_i$, s.t., $d_i^T s_i \ge d_i^T x$<br>for $x \in F$*.   </p>
</li>
<li><p>$\qquad$ $\qquad$ Calculate a ellipsoid $E_{i + 1} \supset E_{i} \cap {x : d_i^T s_i \ge d_i^T x }$.  </p>
</li>
<li><p>$\qquad$ $\qquad$ $i \leftarrow i + 1$.   </p>
</li>
<li><p> Output $F = \emptyset$.   </p>
</li>
</ol>
<hr>
<p>For the one dimension case, it is nothing more than a variant of binary search. </p>
<h2 id="Bounding-the-Number-of-Iterations"><a href="#Bounding-the-Number-of-Iterations" class="headerlink" title="Bounding the Number of Iterations"></a>Bounding the Number of Iterations</h2><p>The main result that characterizes the running time is </p>
<p><strong><em>Theorem: It is guaranteed that for $\forall i$</em></strong>,<br>$$<br>\frac{vol( E_{i + 1} ) } {vol(E_i) } \le \exp \left( -\frac{1}{2 (n + 1) } \right)<br>$$<br>$\square$.</p>
<p><em>Corollary: the number of iterations of the ellipsoid method is bounded by</em><br>$$<br>2n(n + 1) \ln \frac{R}{r}<br>$$<br>$\square$.</p>
<p>To simplify the notation further, we write $E_i = E(s, P)$, $d_i = d$ and $E_{i + 1} = E(s’, P’)$. The key lemma for the theorem is: </p>
<p><em>Lemma: Given an ellipsoid $E(s, P) = { x \in \mathcal{R}^n : (x - s)^T P^{-1} (x - s) \le 1}$ and the half space $H(s, d) = { x \in \mathcal{R}^n : d^T (x - s)\le 0 }$, we can find an ellipsoid $E(s’, P’)$, such that</em></p>
<ol>
<li><em>$E(s’, P’)$ contains the intersection of $E(s, P)$ and $H(s, d)$ : $E(c, P) \cap { x \in \mathcal{R}^n : d^T (x - s)\le 0 }$</em>.  </li>
<li><em>The volume of $E(s’, P’)$ is at most $\exp \left( -\frac{1}{2 (n + 1) } \right)$ times that of $E(s, P)$</em>. </li>
</ol>
<p>In particular, $s’$ and $P’$ is given by<br>$$<br>\begin{aligned}<br>s’ &amp;= s - \frac{1}{n + 1} \frac{P d}{\sqrt{d^T P d} } \<br>P’ &amp;= \frac{n^2}{n^2 - 1} \left( P - \frac{2}{n + 1} \frac{Pd}{\sqrt{d^T P d} } (\frac{Pd}{\sqrt{d^T P d} } )^T \right)<br>\end{aligned}<br>$$</p>
<p><em>Proof.</em> </p>
<p>The transformation $y = B^{-1} (x - s)$ maps $E(s, P)$ to $E(0， I)$, and ${x \in R^n: d^T B (B^{-1} (x - s)) \le 0}$ to $H(0, B^T d) = {y \in R^n: d^T B y \le 0}$. Let $e’ = \frac{B^T d}{\sqrt{d^T B B^T d} } = \frac{B^T d}{\sqrt{d^T P d} }$ so that $H(0, B^Td)$ is equivalent to $H(0, e’)$.</p>
<p>It suffices to find an ellipsoid that contains $E(0, I) \cap H(0, e’)$. Then applying the reverse transformation $x = By + s$ will give us an ellipsoid that contains $E(s, P) \cap H(s, d)$. </p>
<p>We can further reduce the problem to finding an ellipsoid that covers $E(0, I) \cap H(0, e_1)$ as we can recover the ellipsoid by rotating $e_1$ to $e’$. </p>
<p><strong><em>Claim.</em></strong><br>The ellipsoid that covers $E(0,I) \cap H(0, e_1)$ is given by<br>$$<br>E \left( -\frac{1}{n + 1}e_1, \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T) \right)<br>$$<br>and has minimum volume $\exp ( -\frac{1}{2(n + 1)} )$. </p>
<p>To under the ellipsoid, we check how it transforms the unit ball to an ellipsoid:</p>
<ol>
<li><p>It squashes the ball along the $e_1$ axis, by a factor of $\sqrt{ \frac{n^2} {n^2 - 1} \frac{n - 1} {n + 1} } = \frac{n}{ n + 1 }$.</p>
</li>
<li><p>Then it stretches all other directions $e_2, e_3, …, e_n$ by a factor of $\sqrt \frac{n^2}{n^2 - 1}$. </p>
</li>
<li><p>Finally, move the center of the ellipsoid to $-\frac{1}{n + 1}e_1$.</p>
<p>Note that the ellipsoid has volume $(1 - \frac{1}{n + 1}) (1 + \frac{1}{n^2 -1} )^{(n - 1) / 2} \le e^{- \frac{1}{n + 1} + \frac{1}{n^2 -1} (n - 1) / 2} = e^{- \frac{1}{2(n + 1)} }$. This is also the ratio of the ellipsoid over that of the unit ball, whose value is 1.</p>
</li>
</ol>
<p>Before we prove the claim, we show how to transform the ellipsoid back to $E(s’, P’)$ that covers $E(s, P) \cap H(s, d)$. First, by symmetry, the ellipsoid that covers $E(0, I) \cap H(0, e’)$ is given by<br>$$<br> E(-\frac{1}{n + 1}e’, \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e’ (e’)^T) )<br>$$</p>
<p>Applying the mapping $B y + s = x$:<br>$$<br>E(-\frac{1}{n + 1} e’, \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e’ (e’)^T) ) \rightarrow E(s - \frac{1}{n + 1} B^T e’, \frac{n^2}{n^2 - 1} B (I - \frac{2}{n + 1}e’ (e’)^T) B^T )<br>$$</p>
<p><strong>As linear transformation preserves the ratio of volumes, the ratio of the volume of the final ellipsoid over that of $E(s, P)$ is also $\exp(- \frac{1}{2 (n + 1)} )$.</strong></p>
<p><strong><em>Question to ponder:</em></strong> </p>
<ol>
<li>Show that $\frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T)$ is positive definite.</li>
<li>Show that $\left( \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T) \right)^{-1} =  \frac{n^2 - 1}{n^2} I + \frac{2n + 2}{n^2} e_1 e_1^T$.</li>
<li>Show that $\frac{n^2}{n^2 - 1} B (I - \frac{2}{n + 1}e’ (e’)^T) B^T$ is positive definite.</li>
</ol>
<p><em>Proof of the claim</em>. </p>
<p>As both $E(0,I)$ and $H(0, e_1)$ are symmetric to $e_1$, so should the ellipsoid covering the intersection of them. Therefore it should be of the form<br>$$<br>a_1 (x_1 - \lambda) + \sum_{i = 2}^n a_i x_i^2 \le 1<br>$$</p>
<p>where $a_i$’s and $\lambda$ ($0 &lt; \lambda &lt; 1$) are the parameters to be determined. The goal is to minimize its volume: </p>
<p>$$<br>\sqrt{\frac{1}{a_1} \prod_{i = 2}^n \frac{1}{a_i} } vol[ E(0, I) ]<br>$$</p>
<p>where $vol[ E(0, I) ] = 1$ is the volume of unit ball in $R^n$. It is equivalent to maximize<br>$$<br>a_1 \prod_{i = 2}^n a_i<br>$$</p>
<p>As the ellipsoid covers $E(0,I) \cap H(0, e_1)$ and should has as small volume as possible, the points $e_i$ ($i = 1, 2, …, n$) should be on its boundary. Therefore, </p>
<p>$$<br>\begin{aligned}<br>a_1 (1 - \lambda)^2 + \sum_{i = 2}^n a_i 0^2 &amp;= 1 &amp;\rightarrow a_1(1 - \lambda)^2 = 1 &amp;\<br>a_1 (0 - \lambda)^2 + a_i 1^2 &amp;= 1  &amp;\rightarrow a_i = 1 - a_1 \lambda^2  &amp;\quad  \forall i = 2, 3, …, n  \<br>\end{aligned}<br>$$</p>
<p>Replacing $a_i$ with $1 - a_1 \lambda^2$ and then $a_1$ with $\frac{1}{(1 - \lambda)^2}$, the goal becomes</p>
<p>$$<br>\max \quad \frac{1}{(1 - \lambda)^2} \left[ 1 - \frac{\lambda^2}{(1 - \lambda)^2} \right]^{n - 1}<br>$$</p>
<p>Substitute $\frac{1}{1 - \lambda}$ by a variable $x &gt; 1$, and let<br>$$<br>f \doteq x^2 [1 - (x - 1)^2]^{n - 1}<br>$$</p>
<p>Its derivative is<br>$$<br>f’ = 2x [1 - (x - 1)^2]^{n - 1} - 2 (n - 1)(x - 1) x^2 [1 - (x - 1)^2]^{n - 2}<br>$$</p>
<p>Setting the derivative to zero, we obtain </p>
<p>$$<br>\begin{aligned}<br>&amp;[1 - (x - 1)^2] - (n - 1)(x - 1)x = 0 \<br>&amp;\Rightarrow 1 - x^2 + 2x - 1- (n -1 )x^2 + (n -1 )x = 0 \<br>&amp;\Rightarrow - n x^2 + (n + 1) x = 0 \<br>&amp;\Rightarrow x = \frac{n + 1}{n}<br>\end{aligned}<br>$$</p>
<p>It follows that $a_1 = (\frac{n + 1}{n})^2$, $\lambda = \frac{1}{n + 1}$ and $a_i = \frac{n^2 - 1}{n^2}$ for $\forall i = 2, 3, …, n$. </p>
<!-- Therefore, 
$$
c_1 = \left[ 
    \begin{matrix}
\frac{1}{n + 1} \\
 0 \\
  ...\\
0
\end{matrix}
\right] 
= \frac{1}{n + 1} e_1
$$
and 
$$
P_1^{-1} = \left[ 
\begin{matrix}
(\frac{n + 1}{n} )^2 &                       &   ...     \\
                    &   \frac{n^2 - 1}{n^2}  &   ...     \\
                    &                        &   ...     \\
                    &                        &   &\frac{n^2 - 1}{n^2}
\end{matrix}
\right] = \frac{n^2 - 1}{n^2} I + 2\frac{n + 1}{n^2} e_1 e_1^T \\
P_1 = \left[ 
\begin{matrix}
(\frac{n}{n + 1} )^2 &                       &   ...     \\
                    &   \frac{n^2}{n^2 - 1}  &   ...     \\
                    &                        &   ...     \\
                    &                        &   &\frac{n^2}{n^2 - 1}
\end{matrix}
\right] = \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T)
$$

The volume ratio between $E(c_1, P_1)$ and $E(0, I)$ is 

$$
\begin{aligned}
\sqrt{\frac{1}{a_1} \prod_{i = 2}^n \frac{1}{a_i} } 
&= \frac{n}{n + 1} \left[ \frac{n^2 }{n^2 - 1} \right]^{(n - 1) / 2} \\
&\le \exp \left( -\frac{1}{n + 1} + \frac{1}{2}\frac{n - 1}{n^2 - 1} \right) \\
&= \exp \left( -\frac{1}{2} \frac{1}{n + 1} \right)
\end{aligned}
$$ -->

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Michel X. Goemans, 7. Lecture notes on the ellipsoid algorithm, 18.433: Combinatorial Optimization, MIT,</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/18/Determinant,%20Cross%20Product%20and%20Cramer's-Rule/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/18/Determinant,%20Cross%20Product%20and%20Cramer's-Rule/" class="post-title-link" itemprop="url">Determinant, Cross Product and Cramer's-Rule</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-18 23:36:52" itemprop="dateCreated datePublished" datetime="2020-02-18T23:36:52+11:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-21 19:01:14" itemprop="dateModified" datetime="2020-04-21T19:01:14+10:00">2020-04-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Determinant"><a href="#Determinant" class="headerlink" title="Determinant"></a><strong><em>Determinant</em></strong></h3><p>When studying linear algebra, the determinant function $\det(\cdot)$ is a horrifying subject. We try to give a friendly approach to it in this article. </p>
<p>I think it is the study of the volume of the parallelotope specified by $v_1, v_2, … ,v_n \in \mathbb{R}^n$ that gives it away. The parallelotope is defined as the set<br>$$<br>\left{ \sum_{i \in [n]} \lambda_i v_i :  \lambda_i \in [0, 1], \forall i \in [n] \right}<br>$$</p>
<p>Let’s denote $f(v_1, v_2, … ,v_n) : \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ the singed volume function, i.e., $|f(v_1, v_2, … ,v_n)|$ gives the volume of the parallelotope. We will show how to determine the sign of $f$ later. At this point, we do not know how $\det(\cdot)$ is related to the volume. Therefore, we use the notation $f$ instead.</p>
<p>We show how to derive the formula of $f$ by its properties. The two key properties associated with volume are </p>
<ol>
<li><p>$f$ is multi-linear. Specially, it is a linear function in any dimension when other dimensions are fixed: </p>
<ul>
<li><p>$f(v_1, v_2, …, cv_i, … ,v_n)=cf(v_1, v_2, … ,v_n)$ for $\forall i \in [n]$ and $c \in \mathbb{R}$. Geometrically, if we scale the $i$-th vector $v_i$ by a factor $c$, then the volume of the parallelotope should change by the same factor.<br><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity1.jpg"><br>In the above example, when $v_2$ is fixed, the area of the parallelogram is determined by its height, i.e., the projection of $v_1$ to the orthogonal direction of $v_2$. When $v_1$ doubles, the length of its projection doubles. </p>
</li>
<li><p>$f(v_1, v_2, …, v_i + u_i, … ,v_n)= f(v_1, v_2, …, v_i, … ,v_n) + f(v_1, v_2, …, u_i, … ,v_n)$.<br><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity2.jpg"><br>In the above example, when $v_2$ is fixed, the projection to the orthogonal direction of $v_2$ are additive. In particular, denote $\mathcal{P}(v_1)$ the length of the projection of $v_1$ to the orthogonal direction of $v_2$. Similarly we define $\mathcal{P}(v_2)$ and $\mathcal{P}(v_1 + v_2)$. Then<br>$$<br>\mathcal{P}(v_1 + v_2) = \mathcal{P}(v_1) + \mathcal{P}(v_2)<br>$$</p>
</li>
<li><p>Note that the projection of two vector can cancel each other in some cases, in the sense that they points to different direction.<br><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity4.jpg"><br>In the above example, if we write $v_1 = v_{1, 1} e_1 + v_{1, 2} e_2$, then $\mathcal{P} (v_{1, 1} e_1)$ and $\mathcal{P} (v_{1,2} e_2)$ points to opposite directions. This implies that we should give volume a sign. For example, we can define the direction pointed by purple vector points to be positive. The volume of the parallelogram increases if a vector’s projection moves along this direction and decrease otherwise.</p>
</li>
</ul>
</li>
<li><p>If $v_1, v_2, …, v_n$ are linearly independent, then the parallelotope is contained in a subspace with dimension smaller than $n$ and should have volume zero.<br><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/NonIndependent.jpg"><br>Intuitively, the parallelotope collapses to a lower dimension space. Examples include colinear vectors in two dimension case and the third vector lying in the hyperplane spanned by the other two in the three dimension case.<br>Finally, we emphases an important case here:<br>$$<br>f(…, v_i,…, v_j, … , ) = 0, \textbf{ if } v_i = v_j, \forall i, j \in [n], i \neq j<br>$$</p>
</li>
</ol>
<p>If we write<br>$$<br>v_i = v_{i, 1} e_1 + v_{i, 2} e_2 + … + v_{i, n} e_n<br>$$</p>
<p>for all $i \in [n]$, then by linearity<br>$$<br>\begin{aligned}<br>f(v_1, v_2, …, v_n)<br>    &amp;= f( v_{1, 1} e_1 + v_{1, 2} e_2 + … + v_{1, n} e_n, \<br>    &amp;\qquad v_{2, 1} e_1 + v_{2, 2} e_2 + … + v_{2, n} e_n, \<br>    &amp;\qquad …\<br>    &amp;\qquad v_{n, 1} e_1 + v_{n, 2} e_2 + … + v_{n, n} e_n) \<br>    &amp;= \sum_{k_1, k_2, …, k_n \in [n]} v_{1, k_1} v_{2, k_2} … v_{n, k_n} f(e_{k_1}, e_{k_2}, …, e_{k_n})<br>\end{aligned}<br>$$</p>
<p>Note that if $k_i = k_j$ for $i \neq j$, then $f(e_{k_1}, e_{k_2}, …, e_{k_n}) = 0$. Therefore,<br>$$<br>f(v_1, v_2, …, v_n)  = \sum_{ k  \text{ is a permutation} } v_{1, k_1} v_{2, k_2} … v_{n, k_n} f(e_{k_1}, e_{k_2}, …, e_{k_n})<br>$$</p>
<p>We are almost done. It is left to determined the value of $f(e_{k_1}, e_{k_2}, …, e_{k_n})$, where $k = (k_1, k_2, …, k_n)$ is a permutation of $(1, 2, …., n)$. Here we claim that is purely determined by the value of $f(e_1, e_2, …, e_n)$. By conversion, we define<br>$$<br>f(e_1, e_2, …, e_n) = 1<br>$$<br>Then we have the key lemma: </p>
<p><strong><em>Lemma</em></strong><br>$$<br>f(e_{k_1}, e_{k_2}, …, e_{k_n}) = -1<br>$$</p>
<p>if ${k_1}, {k_2}, …, {k_n}$ can be recovered to $1, 2, …, n$ by odd number of neighboring swaps and<br>$$<br>f(e_{k_1}, e_{k_2}, …, e_{k_n}) = 1<br>$$<br>if ${k_1}, {k_2}, …, {k_n}$ can be recovered to $1, 2, …, n$ by even number of neighboring swaps.</p>
<p><em>Proof.</em><br>The key is to prove that we swap two neighboring vectors, then the sign of the volume change:<br>$$<br>f(e_{k_1}, e_{k_2}, …, e_{k_{i } }, e_{k_{i + 1} }, …, e_{k_n}) = -f(e_{k_1}, e_{k_2}, …, e_{k_{i + 1 } }, e_{k_{i } }, …, e_{k_n})<br>$$</p>
<p>This is because<br>$$<br>\begin{aligned}<br>    f(e_{k_1}, e_{k_2}, …, e_{k_{i } } + e_{k_{i + 1} }, e_{k_{i } } + e_{k_{i + 1} }, …, e_{k_n})<br>        &amp;=<br>        f(e_{k_1}, e_{k_2}, …, e_{k_{i } }, e_{k_{i + 1} }, …, e_{k_n})\<br>        &amp;\quad +<br>        f(e_{k_1}, e_{k_2}, …, e_{k_{i + 1 } }, e_{k_{i } }, …, e_{k_n})<br>        \<br>        &amp;= 0<br>\end{aligned}<br>$$</p>
<p>$\square$</p>
<h3 id="Cross-Product"><a href="#Cross-Product" class="headerlink" title="Cross Product"></a><strong><em>Cross Product</em></strong></h3><p>If we fix $v_2, v_3, …, v_n$, then $\det(x, v_2, …, v_n): \R^n \rightarrow \R$ is a linear function. Then<br>$$<br>\begin{aligned}<br>    \det(x, v_2, …, v_n) &amp;= \sum_{i = 1}^n \det(x_i e_i, v_2, …, v_n) \<br>    &amp;= \sum_{i = 1}^n x_i \det(e_i, v_2, …, v_n)<br>\end{aligned}<br>$$</p>
<p>where $x_i$ is the i-th coordinator of $x$. If we write<br>$$<br>d = \left&lt; det(e_1, v_2, …, v_n), det(e_2, v_2, …, v_n), …, det(e_n, v_2, …, v_n)\right&gt;<br>$$</p>
<p>Then<br>$$<br>\det(x, v_2, …, v_n) = d \cdot x<br>$$</p>
<p>Indeed, in a similar manner, we can prove that for any $h: \R^{n \times n} \rightarrow \R^n$ linear function, there exists a vector $\vec h$, such that<br>$$<br>h(x) = \vec h \cdot x<br>$$ </p>
<p>Conventionally, $d$ is called the cross product of $v_2, …, v_n$ is written as<br>$$<br>d = v_2 \times v_3 \times … \times v_n<br>$$</p>
<p>It has some interesting properties: </p>
<ol>
<li><p>$d$ is perpendicular to the subspace spanned by $v_2, …, v_n$, as $d \cdot v_i = \det(v_i, v_2, …, v_n) = 0$ for $2 \le i \le n$. </p>
</li>
<li><p>For any $x \in \R^n$, $d \cdot x$ gives the signed volume of the parallelotope spanned by $x, v_2, …, v_n$.   </p>
<ul>
<li>When $n = 2$, the length of $d$ equals to that of $v_2$. </li>
<li>When $n = 3$, the length of $d$ equals to the area of the parallelogram spanned by $v_2, v_3$. Combined with the fact that $d$ is perpendicular to $v_2, v_3$, we see why $d \cdot x$ gives the volume of the parallelotope spanned by $x, v_2, v_3$: it takes the projection of $x$ to the direction that is perpendicular to $v_2$ and $v_3$, scaled by a factor of the area spanned by $v_2$ and $v_3$.  </li>
</ul>
</li>
<li><p>The direction given of $d$ is given by right hand rule, when $n = 2$ or $n = 3$. </p>
</li>
</ol>
<p>$\square$</p>
<h3 id="Cramer’s-Rule"><a href="#Cramer’s-Rule" class="headerlink" title="Cramer’s Rule"></a><strong><em>Cramer’s Rule</em></strong></h3><p>Cramer’s rule is a formula for solving linear equations. In particular, given an invertible matrix $A \in \R^{n \times n}$, and vector $b \in \R^n$, then the solution for the equation<br>$$<br>Ax = b<br>$$</p>
<p>is given by ($\forall i \in [n]$),<br>$$<br>x_i = \frac{\det(A_i) }{\det (A)}<br>$$</p>
<p>where $x_i$ is the i-th coordinate of $x$ and $A_i$ is the matrix formed by replacing $A$’s the $i$-th column by column vector $b$.</p>
<p>To understand the formula, we view $A$ as a mapping $T: \R^n \rightarrow \R^n$ that takes $e_i (i \in [n])$ to $A[:,i]$, i.e.,<br>$$<br>T(e_i) = A[:, i]<br>$$</p>
<p>where $A[:, i]$ is the i-th column of $A$. The image of the hypercube<br>$$<br>\left{ \sum_{i \in [n]} \lambda_i e_i :  \lambda_i \in [0, 1], \forall i \in [n] \right}<br>$$</p>
<p>is given as<br>$$<br>\left{ \sum_{i \in [n]} \lambda_i A[:, i] :  \lambda_i \in [0, 1], \forall i \in [n] \right}<br>$$</p>
<p>Original the hypercube has volume 1. After the transformation it has volume $\det(A)$. One interesting fact about linear transformation is that it preserves volumes ratio. Define $\mathbb{V}(\cdot)$ the volume of a parallelotope. </p>
<p><em>Fact.</em> For any parallelotope $P$, its volume under transformation $T$ is given by<br>$$<br>\det(A) \mathbb{V}(P) = \mathbb{V}(T(P))<br>$$</p>
<p>Now, we rewrite the Cramer’s rule as<br>$$<br>\det(A) x_i = \det(A_i)<br>$$</p>
<p>Its meaning is clear: there is a parallelotope $P$ whose volume is $x_i$ and after the transformation it has volume $\det(A_i)$. </p>
<p>It is left to verify that the parallelotope indeed exists.<br>We observe that $\det(A_i)$ can be viewed as the volume of the following parallelotope formed by<br>$$<br>\left{ \sum_{j \in [n], j \neq i} \lambda_j A[:, j] +\lambda_i b_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right}<br>$$</p>
<p>But this parallel0tope is the image of<br>$$<br>\left{ \sum_{j \in [n], j \neq i} \lambda_j e_j +\lambda_i x_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right}<br>$$</p>
<p>as<br>$$<br>\begin{aligned}<br>    &amp; T \left( \left{ \sum_{j \in [n], j \neq i} \lambda_j e_j +\lambda_i x_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right} \right) \<br>    &amp;= \left{ \sum_{j \in [n], j \neq i} \lambda_j T(e_j) +\lambda_i T(x_i):  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right} \<br>    &amp;= \left{ \sum_{j \in [n], j \neq i} \lambda_j A[:, j] +\lambda_i b_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right}<br>\end{aligned}<br>$$</p>
<p>$\square$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/07/Simplex/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/Simplex/" class="post-title-link" itemprop="url">Simplex</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-07 10:03:19" itemprop="dateCreated datePublished" datetime="2020-02-07T10:03:19+11:00">2020-02-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-18 12:10:34" itemprop="dateModified" datetime="2020-02-18T12:10:34+11:00">2020-02-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Simplex is the first systematically method for solving linear program via a sequence of Gaussian eliminations. It was invented by George Dantzig in 1947 [1]. Algebraically, it converts the linear program from one slack form into to an equivalent one whose objective value does not decrease and possibly increase. It keeps going until the optimal is reached. Geometrically, it is a greedy search strategy that moves from a vertex of the feasible region to another one, with the objective value at the new vertex as least as good as or possibly improved over the previous one. It is an exponential algorithm, although most of time it does a lot better. In this blog we walk through a simple example to illustrate this. </p>
<p>In this case we are operating a factory with two products. Both product have 1 unit of profit. Making the products needs to use two kind of machine. Product one uses 1 hour of machine one and product two uses 2 hours of machine one. Further, product one uses 2 hours of machine two and product two uses 1 hour of machine two. Both machines operate at most 6 hours each day. We want to maximize our profit:<br>$$<br>\begin{aligned}<br>&amp;\max     &amp; x_1 + x_2 \<br>&amp;s.t.     &amp; x_1 + 2x_2 \le 6 \<br>&amp;        &amp; 2x_1 + x_2 \le 6 \<br>&amp;        &amp;    x_1, x_2 \ge 0<br>\end{aligned}<br>$$</p>
<p>It can be represented concisely in matrix form:<br>$$<br>\begin{aligned}<br>&amp;\max     &amp; c^T x \<br>&amp;s.t.     &amp; A x \le b \<br>&amp;        &amp; x \ge 0<br>\end{aligned}<br>$$</p>
<p>where<br>$$<br>\begin{aligned}<br>A = \left[<br>    \begin{matrix}<br>        1 &amp; 2 \<br>        2 &amp;  1<br>    \end{matrix}<br>    \right],<br>\quad<br>b = \left[<br>    \begin{matrix}<br>        6 \<br>        6<br>    \end{matrix}<br>    \right],<br>\quad<br>c = \left[<br>    \begin{matrix}<br>        1 \<br>        1<br>    \end{matrix}<br>    \right],<br>\quad<br>x = \left[<br>    \begin{matrix}<br>        x_1 \<br>        x_2<br>    \end{matrix}<br>    \right],<br>\end{aligned}<br>$$</p>
<p><em>We call the set of points that satisfy the constraints of the optimization problem the feasible region</em><br>$$<br>P = {x : Ax \le b, x \ge 0}<br>$$</p>
<p>It is obvious that the optimal is obtained at $(2, 2)$, which gives the objective value $4$. It is a <em>vertex</em> of the feasible region. </p>
<h4 id="Slack-Form"><a href="#Slack-Form" class="headerlink" title="Slack Form"></a>Slack Form</h4><p>In general, it might not be obvious what the optimal point is. But if the elements of $b$ are non-negative, the origin is a feasible point, from which we can begin our search. To make the search easier, we are going to introduce an additional number of variables, the slack variables $s_1, s_2$ and $z$ that correspond to the constraints and objective value.<br>$$<br>\begin{aligned}<br>z =       &amp;    &amp; x_1          &amp;+x_2   \<br>s_1 =     &amp;6    &amp; -x_1      &amp;-2x_2  \<br>s_2 =     &amp;6    &amp; -2x_1     &amp;-x_2      \<br>\end{aligned}<br>$$</p>
<p>rewrite the optimization into <em>slack form</em>:<br>$$<br>\begin{aligned}<br>&amp;\max     &amp; x_1      &amp;+x_2       &amp;        &amp;            &amp;= z\<br>&amp;s.t.     &amp; x_1      &amp;+2x_2       &amp;+s_1      &amp;             &amp;= 6 \<br>&amp;        &amp; 2x_1  &amp;+x_2          &amp;        &amp;+s_2        &amp;= 6 \<br>&amp;        &amp; x_1,     &amp;x_2,         &amp;s_1     &amp;,s_2         &amp;\ge 0<br>\end{aligned}<br>$$</p>
<p>They are called slacks variables because they correspond how much slack you have in the inequalities in the original problem. For this slack form, we also call $s_1$ and $s_2$ basic variables, and the original variables non-basic variables. </p>
<p>Now the feasible region $F$ is considered as the intersection of the two subspaces<br>$$<br>F = {x : Ax = b } \cap { x \ge 0 }<br>$$</p>
<p>We can represent the LP in <em>slack form</em> even more concisely in a table:<br>$$<br>\begin{aligned}<br>    1    &amp;&amp;    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp; \mid 6 \<br>    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp;     1    &amp;&amp; \mid 6 \<br>    \hline<br>    1    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp;    0    &amp;&amp; \mid 0 \<br>\end{aligned}<br>$$</p>
<p>where the last column denotes the objective function. This example has a trivial starting point : $(0, 0, 6, 6)$. It is called a basic solution in which all non-basic variables are set to zero. It can be read from the tabular form: the identity matrix correspond to basic variables, and $(s_1, s_2) = b$. </p>
<h4 id="Pivoting"><a href="#Pivoting" class="headerlink" title="Pivoting"></a>Pivoting</h4><p>Look at the original LP:<br>$$<br>\begin{aligned}<br>&amp;\max     &amp; x_1      &amp;+x_2       &amp;        &amp;            &amp;= z\<br>&amp;s.t.     &amp; x_1      &amp;+2x_2       &amp;+s_1      &amp;             &amp;= 6 \<br>&amp;        &amp; 2x_1  &amp;+x_2          &amp;        &amp;+s_2        &amp;= 6 \<br>&amp;        &amp; x_1,     &amp;x_2,         &amp;s_1     &amp;,s_2         &amp;\ge 0<br>\end{aligned}<br>$$</p>
<p>As we would like to maximize the objective, we would like to increase the value of $x_1$ or $x_2$ as much as possible. Suppose we just increase one variable at a time. For example, we can increase variable $x_1$ and keep the value of $x_2$ to be $0$.</p>
<p>How much can we increase $x_1$? After increasing $x_1$, we need to maintain the inequality constraints:<br>$$<br>x_1 + s_1 = 6 \<br>2 x_1 + s_2 = 6<br>$$</p>
<p>The values of $s_1$ and $s_2$ decrease as $x_1$ increases. By they can be drop below $0$. The maximum possible increases of $x_1$ is bounded by $6 / 2 = 3$. If we set $x_1 = 3$, $s_1 = 6 - x_1 = 3$ and  $s_2 = 6 - 2x_1 = 0$. </p>
<p>Note that $x_1$ increases from $0$ to $3$ and $s_2$ decreases from $6$ to $0$. This process is called <em>pivoting</em>. </p>
<p>How do we perform pivoting in the tabular form? By the replacement we have just performed – normalizing the second row, and using it to eliminate all other elements in the first column. </p>
<p>Recall the original table:<br>$$<br>\begin{aligned}<br>    1    &amp;&amp;    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp; \mid 6 \<br>    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp;     1    &amp;&amp; \mid 6 \<br>    \hline<br>    1    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp;    0    &amp;&amp; \mid 0 \<br>\end{aligned}<br>$$</p>
<p>We divide the second row by a factor of 2 to normalize it, and subtract the first and third row by the second row<br>$$<br>\begin{aligned}<br>    0      &amp;&amp;    3/2    &amp;&amp;    1    &amp;&amp; -1/2    &amp;&amp; \mid 3 \<br>    1    &amp;&amp;    1/2    &amp;&amp;    0    &amp;&amp;     1/2    &amp;&amp; \mid 3 \<br>    \hline<br>    0    &amp;&amp;    1/2    &amp;&amp;    0    &amp;&amp; -1/2    &amp;&amp; \mid -3 \<br>\end{aligned}<br>$$</p>
<p>Observe the first two rows of the tabular form:<br>$$<br>\begin{aligned}<br>    0      &amp;&amp;    3/2    &amp;&amp;    1    &amp;&amp; -1/2    &amp;&amp; \mid 3 \<br>    1    &amp;&amp;    1/2    &amp;&amp;    0    &amp;&amp;     1/2    &amp;&amp; \mid 3 \<br>\end{aligned}<br>$$<br>If we set $x_2 = 0$ and $s_2 = 0$, the constraints they implies becomes<br>$$<br>s_1 = 3 \<br>x_1 = 3<br>$$<br>Again the unity columns correspond to non-zero elements and we can read the solution from the tabular form. </p>
<h4 id="Correctness-of-Pivoting"><a href="#Correctness-of-Pivoting" class="headerlink" title="Correctness of Pivoting"></a>Correctness of Pivoting</h4><p>Before we continue our search on the tabular form, we need to show that pivoting is correct in the sense that, after pivoting, the tabular form represents the same optimization. </p>
<p>In particular, we need to guarantee that the feasible regions are the same and the objective function are equivalent defined on the feasible regions. </p>
<p>We focus on the extended matrix that represents the constraints:<br>$$<br>\bar A = [A, b] =<br>\left[\begin{aligned}<br>    1    &amp;&amp;    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp; \mid 6 \<br>    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp;     1    &amp;&amp; \mid 6 \<br>\end{aligned}\right]<br>$$<br><em>FACT 1.</em> Doing a row replacement on $\bar A$ does not change $F$, where row replacement is defined as<br>$$<br>\bar A[i: ] \leftarrow \bar A[i: ] + k \cdot \bar A[j: ]<br>$$<br>i.e., adding $k$ times the $j$-th row to the $i$-th row, for $i \neq j, k \in R$. </p>
<p>To see this, denote $F$ and $F’$ the sets before and after the replacement respectively. $F = F’$ implies that each point $x$ belonging to $F$ belongs to $F’$, and vice-versa. Specifically, the row replacement corresponds to left multiplying $\bar A$ by a matrix</p>
<p>$$<br>\begin{aligned}<br>E_{(i,j), k} \doteq<br>    \left[ \begin{matrix}<br>        1 &amp; \<br>          &amp; 1 \<br>          &amp;        &amp; \quad…         \<br>          &amp;     &amp; k      &amp; 1     \<br>          &amp;        &amp;         &amp;         &amp;     …  \<br>          &amp;        &amp;         &amp;        &amp;            &amp;     1<br>    \end{matrix} \right]<br>\end{aligned}<br>$$</p>
<p>which is a diagonal matrix with an additional $k$ in the $(i, j)$ position. It is invertible with inverse matrix<br>$$<br>\begin{aligned}<br>E_{(i,j), -k} \doteq<br>    \left[ \begin{matrix}<br>        1 &amp; \<br>          &amp; 1 \<br>          &amp;        &amp; \quad…         \<br>          &amp;     &amp; -k      &amp; 1     \<br>          &amp;        &amp;         &amp;         &amp;     …  \<br>          &amp;        &amp;         &amp;        &amp;            &amp;     1<br>    \end{matrix} \right]<br>\end{aligned}<br>$$</p>
<p>The previous claim becomes ${ x : E_{(i, j), k} A x = E_{(i, j), k} b } = { x : A x =  b }$. </p>
<p>But wait. Why do we subtract the last row from the second one? <em>Fact 1</em> has justified the replacement operation on $\bar A$, but not on the objective coefficients $c$ (the last row in the tabular form). To illustrate the meaning of the operation, recall that<br>$$<br>x_1 + x_2 + 0 \cdot s_1 + 0 \cdot s_2 = z<br>$$<br>Hence $z$ is the value of the objective function. After normalizing the second row, it becomes<br>$$<br>x_1 + 1 / 2 x_2 +  0 s_1 + 1/2 s_2 = 3 \<br>$$</p>
<p>The constraint holds for any feasible point of the linear programming. Therefore,<br>$$<br>\begin{aligned}<br>z - 3<br>    &amp;= (x_1 + x_2 + 0 s_1 + 0 s_2)  - (x_1 + 1 / 2 x_2 +  0 s_1 + 1/2 s_2) \<br>    &amp;= 0x_1 + 1/2x_2 + 0s_1 - 1/2 s_2 \<br>    &amp;\Leftrightarrow \<br>    z<br>    &amp;= 0x_1 + 1/2x_2 + 0s_1 - 1/2 s_2 + 3<br>\end{aligned}<br>$$</p>
<p>The last $-3$ in the table is the inverse constant in the objective functions after the replacement of variable. </p>
<h4 id="Back-to-our-search"><a href="#Back-to-our-search" class="headerlink" title="Back to our search"></a>Back to our search</h4><p>To improve the current solution further, we note that the coefficient of $x_2$ is positive. We can increase the value of $x_2$. The maximum increase is given by $\min { 3 / (1 / 2), 3 / (3/ 2) } = 2$, determined by the first row:<br>$$<br>\begin{aligned}<br>    0      &amp;&amp;    3/2    &amp;&amp;    1    &amp;&amp; -1/2    &amp;&amp; \mid 3 \<br>    1    &amp;&amp;    1/2    &amp;&amp;    0    &amp;&amp;     1/2    &amp;&amp; \mid 3 \<br>    \hline<br>    0    &amp;&amp;    1/2    &amp;&amp;    0    &amp;&amp; -1/2    &amp;&amp; \mid -3 \<br>\end{aligned}<br>$$<br>To make the change explicitly, we perform elimination by the first row<br>$$<br>\begin{aligned}<br>    0      &amp;&amp;    1    &amp;&amp;    2/3    &amp;&amp; -1/3    &amp;&amp; \mid 2 \<br>    1    &amp;&amp;    0    &amp;&amp; -1/3 &amp;&amp;     2/3    &amp;&amp; \mid 2 \<br>    \hline<br>    0    &amp;&amp;    0    &amp;&amp; -1/3    &amp;&amp; -1/3    &amp;&amp; \mid -4 \<br>\end{aligned}<br>$$<br>which give solution $x = (2, 2, 0, 0)$ and objective value $4$. It is indeed optimal solution for the problem, as all coefficients of the optimization are non-positive. </p>
<h4 id="Optimality-Condition-and-Strong-Duality"><a href="#Optimality-Condition-and-Strong-Duality" class="headerlink" title="Optimality Condition and Strong Duality"></a>Optimality Condition and Strong Duality</h4><p>We investigate the result deeper by viewing the initial tabular form in abbreviation as<br>$$<br>\left[ \begin{matrix}<br>A \quad I \mid b \ c^T  \quad 0 \mid 0<br>\end{matrix} \right]<br>$$<br>The series of $m$ Gaussian elimination can be interpreted as left multiplying a matrix:<br>$$<br>\begin{aligned}<br>\left[ \begin{matrix}<br>    &amp; R     &amp; 0 \<br>    &amp; -y^T     &amp; 1 \<br>\end{matrix} \right] \doteq E_m…E_3 E_2 E_1<br>\end{aligned}<br>$$<br>where each $E_j$ ( $1 \le j \le m$) matrix is invertible and represents a Gaussian elimination, i.e., if we left multiple a matrix by $E_j$, it is equivalent to perform Gaussian elimination on the matrix. The matrix product $E_m…E_3 E_2 E_1$ has the form<br>$$<br>\begin{aligned}<br>\left[ \begin{matrix}<br>    &amp; R     &amp; 0 \<br>    &amp; -y^T     &amp; 1 \<br>\end{matrix} \right]<br>\end{aligned}<br>$$<br>because we never use the last row to eliminate the other rows.    </p>
<p>Now:<br>$$<br>\begin{aligned}<br>\left[ \begin{matrix}<br>    &amp; R     &amp; 0 \<br>    &amp; -y^T     &amp; 1 \<br>\end{matrix} \right]<br>\left[ \begin{matrix}<br>    &amp; A &amp; I \mid     b \<br>    &amp; c^T  &amp; 0 \mid 0<br>\end{matrix} \right]<br>=<br>\left[ \begin{matrix}<br>    &amp; RA                &amp; R     &amp; \mid &amp; Rb \<br>    &amp; -y^TA+c^T       &amp; -y^T     &amp;\mid &amp; -y^T b<br>\end{matrix} \right]<br>\end{aligned}<br>$$<br>In our example,<br>$$<br>\begin{aligned}<br>    1    &amp;&amp;    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp; \mid 6 \<br>    2    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp;     1    &amp;&amp; \mid 6 \<br>    \hline<br>    1    &amp;&amp;    1    &amp;&amp;    0    &amp;&amp;    0    &amp;&amp; \mid 0 \<br>\end{aligned}<br>\Rightarrow … \Rightarrow<br>\begin{aligned}<br>    0      &amp;&amp;    1    &amp;&amp;    2/3    &amp;&amp; -1/3    &amp;&amp; \mid 2 \<br>    1    &amp;&amp;    0    &amp;&amp; -1/3 &amp;&amp;     2/3    &amp;&amp; \mid 2 \<br>    \hline<br>    0    &amp;&amp;    0    &amp;&amp; -1/3    &amp;&amp; -1/3    &amp;&amp; \mid -4 \<br>\end{aligned}<br>$$<br>we have<br>$$<br>R = \begin{aligned}<br>\left[ \begin{matrix}<br>    2 / 3 &amp; -1 / 3 \<br>    -1 / 3 &amp; 2 /3<br>\end{matrix} \right]<br>\end{aligned}<br>\qquad<br>y^T = [ -1 / 3, -1  / 3]<br>$$</p>
<p>The optimality condition of simplex algorithm implies that<br>$$<br>\begin{aligned}<br>-y^T A + c^T &amp;\le 0 \<br>y^T &amp;\le 0 \<br>y^T b &amp;= c^T x<br>\end{aligned}<br>$$</p>
<p>This is the optimal solution for the dual program of the primal:<br>$$<br>\min \ b^T y, \qquad s.t.,  A^T y \ge c, \quad y^T \ge 0<br>$$</p>
<p>If simplex algorithm stops, it implies strong duality holds: </p>
<ul>
<li>The optimal value of the primal program equals to the optimal one of the dual program, assuming that both values exist and are bounded. </li>
</ul>
<h4 id="Geometric-Interpretation"><a href="#Geometric-Interpretation" class="headerlink" title="Geometric Interpretation"></a>Geometric Interpretation</h4><p>In the beginning we claim that the simplex algorithm move from one vertex to another. We prove it rigorously in this section. First we need a few definitions:</p>
<h5 id="Vertex"><a href="#Vertex" class="headerlink" title="Vertex"></a>Vertex</h5><p><em>A vertex is a point $v \in F$, such that $\nexists v_1, v_2 \in F, v_1 \neq v_2$ and $\lambda \in (0, 1)$, s.t., $v = \lambda v_1 + (1 - \lambda) v_2$.</em></p>
<p>In other words, $v$ is a vertex of $F$ if it is not contained in a line segment of $P$. </p>
<p><strong>Theorem.</strong> If LP has an optimal solution, then it has an optimal solution that is a vertex of its feasible set. </p>
<p><em>Proof.</em> Denote $v$ the optimal solution of the LP with maximum number of zero components. If $v$ is not a vertex, then $\exists v_1, v_2 \neq v, v_1, v_2 \in F$ and $\lambda &gt; 0$, such that $\lambda v_1 + (1  - \lambda) v_2 = v$. </p>
<p>As $v$ is optimal, we have $c^T v \ge c^T v_1$ and $c^T v \ge c^T v_2$. By $\lambda v_1 + (1  - \lambda) v_2 = v$, it concludes that $c_T v = c^T v_1 = c^T v_2$. </p>
<p>Define $I = { i : v[i] &gt; 0 }$. As $v_1 \ge 0, v_2 \ge 0$ and $\lambda v_1 + (1  - \lambda) v_2 = v$ so that for $i \neq I$, $v_1[i] = v_2[i] = v[i] = 0$. </p>
<p>But $A_I v_1[I] = A_I v_2[I] = b$. Let $x = \epsilon (v_1 - v_2) + v$. </p>
<ol>
<li>$x$ is feasible, for small enough $\epsilon$. </li>
<li>$x$ is optimal, since $c^T x= \epsilon c^T (v_1 - v_2) + c^T v = c^T v$. </li>
<li>$x_i = 0$ for $i \notin I$. </li>
</ol>
<p>We can find some $\epsilon$ (either positive or false) to make $x_i = 0$ for some $i \in I$. Now $x$ contains more zeros than $v$, a contradiction. $\square$</p>
<h5 id="Basic-Solution"><a href="#Basic-Solution" class="headerlink" title="Basic Solution"></a>Basic Solution</h5><p><em>A solution $x$ of $Ax = b$ is called a basic solution if ${A_i : x_i \neq 0 }$ (columns in $A$ that correspond to non-zero components in $x$ ) are linearly independent.</em></p>
<h5 id="Basic-Feasible-Solution"><a href="#Basic-Feasible-Solution" class="headerlink" title="Basic Feasible Solution"></a>Basic Feasible Solution</h5><p><em>A basic solution $x$ is called basic feasible solution if $x \ge 0$, i.e., $x \in { x : Ax = b} \cap { x \ge 0 }$.</em> </p>
<p>With respect to the feasible region of LP in slack form, the <em>vertices</em> have have following property:</p>
<p><em>Lemma:</em>. A point $v \in F = {x : Ax = b} \cap { x: x \ge 0}$ is a vertex of $F$ if and only if it is a basic feasible solution. Here $A \in R^{m \times n}, x \in R^n, b \in R^{m}$ and $rank(A) = m$ (otherwise some row constraints of the $Ax = b$ are redundant). </p>
<p><em>Proof</em>: Denote $S$ the set of indices of positive components in $v$ and $A_S$ the set of </p>
<p><em>Only if:</em> If $A_S$ does not have full column rank, then $\exists u \in R^{|S|}$, such that $A_S u = 0$. Then for small enough $\epsilon \in R, \epsilon &gt; 0$,<br>both $v + \epsilon u$ and $v - \epsilon u$ are feasible point in $F$. Now $v = 0.5(v + \epsilon u) + 0.5(v - \epsilon u)$, a contradiction. </p>
<p><em>If:</em> As columns of $A_S$ are linearly independent, the only solution of to $A_S x_S = 0$ is given by $x_S = 0$. Hence, $v_S$ is now the unique solution to $A_S v_S = b$. If $v$ is not a vertex, $\exists v_1, v_2, \lambda$, such that $v = \lambda v_1 + (1 - \lambda) v_2$. But in this case we must have $v_1 = v_2 = v$, as $(v_1)<em>S = (v_2)_S = v_S$ and $(v_1)</em>{\bar S} = (v_2)<em>{\bar S} = v</em>{\bar S} = 0$. A contradiction. </p>
<p>$\blacksquare$. </p>
<p><em>Corollary:</em> The simplex algorithm moves from vertex to vertex.</p>
<p><em>Proof:</em> The positive components in the solution correspond to the columns in the identity matrix in the tabular form of simplex, which are linearly independent. </p>
<h4 id="Correctness-of-Simplex-TO-FINISH"><a href="#Correctness-of-Simplex-TO-FINISH" class="headerlink" title="Correctness of Simplex (TO FINISH)"></a>Correctness of Simplex (TO FINISH)</h4><p>Though not shown in the example, there are two issue associated with simplex algorithm. </p>
<ol>
<li>How do we find an initial solution? If there are negative component in $b$, then $x =  0 \wedge  s = b$ is not a feasible solution. </li>
<li>How do we guarantee that simplex terminates?</li>
</ol>
<p>In the previous example, at each step we choose to increase the variable with largest coefficient. In some rare cases, this results in a loop of the algorithm. </p>
<p>One method for avoiding looping is called Bland’s rule: </p>
<ol>
<li>If there is a positive coefficient in the objective function, choose the one with largest coefficient. </li>
<li>If there are multiple rows with tight constraints, choose the one with largest coefficient. </li>
</ol>
<p><em>Theorem.</em> Blands’ rule guarantees that the algorithm stops.</p>
<p><em>Proof.</em> Suppose that on the contrary, the algorithm loops. Denote $B_1, B_2, …, B_k$ the set of basis in the loop. In each base, there is entering variable and one leaving variable. Every variable that leaves the base must enter another base later. We called these variable <em>fickle variables</em>. Denote $x_t$ the fickle variable with the largest index. </p>
<ol>
<li><p>$D$ the dictionary that $x_t$ leaves the base, and $x_e$ the variable that enters the base. </p>
</li>
<li><p>$D’$ the dictionary that $x_t$ enters the base again.<br>$$<br>D =<br>\begin{aligned}<br> I    &amp;&amp;    A_N    &amp;&amp; \mid b \<br> \hline<br> 0    &amp;&amp;    c_N    &amp;&amp; \mid \alpha \<br>\end{aligned}<br>\Rightarrow … \Rightarrow<br>D’ =<br>\begin{aligned}<br> R    &amp;&amp;    R A_N    &amp;&amp; \mid R b \<br> \hline<br> y^T    &amp;&amp;    y^T A_N + c_N    &amp;&amp; \mid y^T b + \alpha \<br>\end{aligned}<br>$$<br>We have<br>$$<br>c_e’ = c_e + y^T A_N[:, e]<br>$$</p>
</li>
<li><p>$x_e$ is the entering variable in $D$, therefore $c_e &gt; 0$. </p>
</li>
<li><p>$x_t$ is the entering variable in $D’$, and $e &lt; t$, by the definition of $x_t$, we have $x_e’ \le 0$. </p>
</li>
<li><p>Hence, $y^T A_N[:, e] = c_e’ - c_e &lt; 0$. </p>
</li>
<li><p>$\exists i, s.t., y_i A_{i, e} &lt; 0$. Then $y_i \neq 0$, and $x_i$ is fickle. By definition, $i &lt; t$. But $x_i$ is not entering $D’$, we know that $y_i &lt; 0$. </p>
</li>
<li><p>$A_{i, e} &gt; 0$. But since $x_i$ is fickle, we have $b_i = 0$. In this case, $x_i$ is chosen in preference to $x_t$. A contradiction.</p>
</li>
</ol>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1]. Dantzig, George B. “Origins of the simplex method.” A history of scientific computing. 1990. 141-151.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/30/LP%20-%20Weak%20duality,%20Complementary%20Slackness,%20Strong%20Duality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/30/LP%20-%20Weak%20duality,%20Complementary%20Slackness,%20Strong%20Duality/" class="post-title-link" itemprop="url">LP - Dual Program, Weak Duality, Complementary Slackness, Strong Duality</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-30 11:47:54" itemprop="dateCreated datePublished" datetime="2020-01-30T11:47:54+11:00">2020-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-01 11:26:59" itemprop="dateModified" datetime="2020-02-01T11:26:59+11:00">2020-02-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Dual-Program-and-Weak-Duality"><a href="#Dual-Program-and-Weak-Duality" class="headerlink" title="Dual Program and Weak Duality"></a>Dual Program and Weak Duality</h1><p>Linear programming studies the optimization problems in which both its objective function and feasible region are represented by linear relationship. </p>
<p>Every linear program can be converted into Canonical form</p>
<p>$$<br>\begin{aligned}<br>&amp; \max &amp; c^T x \<br>&amp; s.t., &amp; Ax \le b \<br>&amp;&amp; x \ge 0<br>\end{aligned}<br>$$</p>
<p>where $c, x \in \mathcal{R}^n$, $A \in \mathcal{R}^{m \times n}$ and $b \in \mathcal{R}^m$. </p>
<p>Suppose we have a conic combination ($y \in \mathcal{R}^m$ and $y \ge 0$) of the linear constraints:<br>$$<br>y^T (Ax) \le y^T b<br>$$</p>
<p>Such that $y^T A \ge c^T$, then $y^T b$ gives an upper bound of the original program. We want to find a $y$, such that the bound is as tight as possible. This gives rise to another linear program:<br>$$<br>\begin{aligned}<br>&amp; \min &amp; y^T b \<br>&amp; s.t., &amp; y^T A \ge c^T \<br>&amp;&amp; y \ge 0<br>\end{aligned}<br>$$</p>
<p>We call the original program the primal and the derived program its dual. </p>
<p>The philosophy of dual program is to use the combination of primal constraints to construct a bound of its objective function. More formally, the goal is that the <em>weak duality</em> holds:</p>
<p>$$<br>c^T x \le (y^T A) x = y^T A x = y^T (Ax) \le y^T b<br>$$</p>
<p>Note that $c^T x \le (A^T y)^T x$ is true because $y^T A \ge c$ and $x \ge 0$. And $y^T (Ax) \le y^T b$ follows from $Ax \le b$ and $y^T \ge 0$. </p>
<p><em>Corollary 1:</em> If the primal is unbounded, then the dual is infeasible. If the dual is unbounded, then the primal is infeasible.</p>
<p>In general, the primal may not be given in Canonical form and can be formulated as</p>
<p>$$<br>\begin{aligned}<br>&amp; \max &amp; c^T x + \bar c^T \bar x + \hat c^T \hat x \<br>&amp; s.t., &amp; Ax + \bar A \bar x + \hat A \hat x \le b \<br>&amp;&amp; Bx + \bar B \bar x + \hat B \hat x =  \bar b \<br>&amp;&amp; Cx + \bar C \bar x + \hat C \hat x \ge \hat b \<br>&amp;&amp; x \ge 0 \<br>&amp;&amp; \hat x \le 0<br>\end{aligned}<br>$$</p>
<p>Note that the $\bar x$’s are unconstrained variables. Guided by similar philosophy, we hope to derive an upper bound by using the constraints:<br>$$<br>\begin{aligned}<br>    c^T x + \bar c^T \bar x + \hat c^T \hat x<br>        &amp; \le y^T (Ax + \bar A \bar x + \hat A \hat x) \<br>        &amp; + \bar y^T (Bx + \bar B \bar x + \hat B \hat x) \<br>        &amp; + \hat y^T (Cx + \bar C \bar x + \hat C \hat x) \<br>        &amp;\le y^T b + \bar y^T \bar b + \hat y^T \hat b<br>\end{aligned}<br>$$</p>
<ol>
<li><p>The first inequality implies that (by proper rearranging):<br> $$<br> \begin{aligned}</p>
<pre><code> c^T x + \bar c^T \bar x + \hat c^T \hat x 
     &amp; \le (y^T A + \bar y^T B + \hat y^T C)x \\
     &amp; + (y^T \bar A + \bar y^T \bar B + \hat y^T \bar C) \bar x \\
     &amp; + (y^T \hat A + \bar y^T \hat B + \hat y^T \hat C) \hat x
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> As $x \ge 0$, $\bar x$ unconstrained and $\hat x \le 0$, a sufficient condition for the inequality to become true is<br> $$<br> \begin{aligned}</p>
<pre><code> c^T      &amp; \le (y^T A + \bar y^T B + \hat y^T C) \\
 \bar c^T &amp; =   (y^T \bar A + \bar y^T \bar B + \hat y^T \bar C) \\
 \hat c^T &amp; \ge (y^T \hat A + \bar y^T \hat B + \hat y^T \hat C) \\
</code></pre>
<p> \end{aligned}<br> $$</p>
<p> That gives the first set of constraints the dual variables $y$, $\bar y$ and $\hat y$ need to satisfies. We have just seen that the sign of the primal variable determines the type of constraint in the dual. </p>
<p> Of particular interest is the inequality constraint, which we will dive into details in the next section. </p>
</li>
<li><p>By similar argument, the second inequality is true if $y^T \ge 0$, $\bar y^T$ unconstrained, $\hat y^T \le 0$. That is, the type of constraint in the primal determines the sign of variable in the dual. </p>
</li>
</ol>
<p>Finally, the rules of obtaining dual are summarized as follows: </p>
<p>$$<br>\begin{aligned}<br>\hline<br>&amp;&amp;&amp; \text{Primal}       &amp;&amp;&amp;                    \max   \quad                      &amp;&amp;&amp; \qquad \qquad                      \min                    &amp;&amp;&amp;  \text{Dual}        &amp;&amp;&amp; \<br>\hline<br>&amp;&amp;&amp; Constraints     &amp;&amp;&amp;      \begin{matrix} \le b_i \ \ge b_i \ = b_i  \end{matrix} \quad        &amp;&amp;&amp; \qquad \quad \begin{matrix} \ge 0 \ \le 0 \ \text{Both sides} \end{matrix}     &amp;&amp;&amp;  Variables      &amp;&amp;&amp; \<br>\hline<br>&amp;&amp;&amp;  Variables      &amp;&amp;&amp;\qquad \begin{matrix} \ge 0 \ \le 0 \ \text{Both sides} \end{matrix}     &amp;&amp;&amp; \qquad \qquad  \begin{matrix}   \ge c_j \ \le c_i \ = c_j   \end{matrix}    &amp;&amp;&amp; Constraints     &amp;&amp;&amp; \<br>\hline<br>\end{aligned}<br>$$</p>
<h1 id="Complementary-Slackness"><a href="#Complementary-Slackness" class="headerlink" title="Complementary Slackness"></a>Complementary Slackness</h1><p>The strong duality states that, if both primal and dual are feasible, then the objective function values of their optimal solutions equal. </p>
<p>If we expand the weak duality expression for the standard form, we get<br>$$<br>\sum_{i = 1}^n c_i x_i \le \sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j} \right) x_i =  \sum_{j = 1}^m  y_j \left( \sum_{i = 1}^n A_{i, j} x_i \right) \le \sum_{j = 1}^m y_j b_j<br>$$</p>
<p>Then $c^T x= y^T b$ implies<br>$$<br>\sum_{i = 1}^n c_i x_i = \sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j}  \right) x_i \<br>\sum_{j = 1}^m  y_j \left( \sum_{i = 1}^n A_{i, j} x_i \right)  = \sum_{j = 1}^m b_j y_j<br>$$</p>
<p>i.e.,<br>$$<br>\sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j} - c_i \right) x_i  = 0\<br>\sum_{j = 1}^m  y_j \left( b_j - \sum_{i = 1}^n A_{i, j} x_i \right) = 0<br>$$</p>
<p>As $\sum_{j = 1}^m A_{i, j} y_j - c_i \ge 0$, $x_i \ge 0$ for all $i$ and $b_j - \sum_{i = 1}^n A_{i, j} x_i  \ge 0$ and $y_j \ge 0$ for all $j$, we conclude that </p>
<p>$$<br>\begin{aligned}<br>&amp;\sum_{j = 1}^m y_j A_{i, j}  - c_i = 0 &amp;\vee &amp;&amp;    x_i = 0 &amp; \forall i \<br>&amp;b_j - \sum_{i = 1}^n A_{i, j} x_i = 0 &amp;\vee &amp;&amp;     y_j  = 0, &amp; \forall j<br>\end{aligned}<br>$$</p>
<p>Or more compactly<br>$$<br>(y^T A - c^T) x = 0 \y^T (b - Ax) = 0<br>$$</p>
<p>If the primal is in canonical form, then the fact that $A^T \ge c, x \ge 0, y \ge 0, b - Ax \ge 0$ implies that<br>$$<br>\begin{aligned}<br>&amp;(y^T A - c^T)_i = 0  &amp;&amp;\vee &amp;&amp; x_i = 0         &amp;&amp;&amp;\forall \ i \<br>&amp;y_j = 0             &amp;&amp;\vee   &amp;&amp; (b - Ax)_j = 0     &amp;&amp;&amp;\forall \ j<br>\end{aligned}<br>$$<br>where $(y^T A - c^T)$ is a vector and $(y^T A - c^T)_i$ is its $i$-th coordinate. </p>
<p>This is known as complementary slackness. </p>
<h1 id="Strong-Duality"><a href="#Strong-Duality" class="headerlink" title="Strong Duality"></a>Strong Duality</h1><p>Question to ponder: what is the geometric interpretation of complementary slackness? </p>
<h2 id="Special-Case"><a href="#Special-Case" class="headerlink" title="Special Case"></a>Special Case</h2><p>To illustrate its underlying meaning, first consider the special case<br>$$<br>\begin{aligned}<br>&amp; \min &amp; y^T b \<br>&amp; s.t., &amp; y^T A = c^T<br>\end{aligned}<br>$$</p>
<p>whose dual form is given by<br>$$<br>\begin{aligned}<br>&amp; \max &amp; c^T x \<br>&amp; s.t., &amp; Ax = b<br>\end{aligned}<br>$$</p>
<p>First note that $y^T A = c^T$ means that $c^T$ is a linear combination of rows of $A$. The primal is feasible if $c$ is in the row space of $A$. </p>
<p>In this special case, if both primal and dual are feasible, then for any feasible solutions $x$ and $y$, strong duality holds, since $c^T x = (y^T A) x = y^T (Ax) = y^T b$. </p>
<p>The dual search for some $x$ such that<br>$$<br>Ax = b<br>$$</p>
<p>If we can find such $x$, it implies that $b$ lies in the column space of $A$, denoted as $C(A)$ (the subspace in $\mathcal{R}^n$ spanned by the row vectors of $A$). What if not? We can decompose $b$ into two part: $b = b_1 + b_2$, such that $b_1 \in C(A)$ and $0 \neq b_2 \in N(A^T)$ (the subspace orthogonal to $C(A)$). Here is the problem: if $y$ is a feasible solution for the primal, then we can decrease $y$ by arbitrary amount along the direction orthogonal to $C(A)$, while maintaining its feasibility. In particular, if we add $k \cdot b_2$ to $y$ ($k &gt; 0$), then $(y - k b_2)$ is still feasible as $(y - k b_2)^T A = y^T A - k b_2^T A = c^T$. But now $(y - k \cdot b_2)^T b = y^T b - k b_2^T b_2 &lt; y^T b$,  which implies that the primal is unbounded. </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/StrongDualitySpecialCase.jpg"></p>
<h2 id="General-Case"><a href="#General-Case" class="headerlink" title="General Case"></a>General Case</h2><p>Now we return our discussion to the more general form:<br>$$<br>\begin{aligned}<br>&amp; \min &amp; y^T b \<br>&amp; s.t., &amp; y^T A \ge c^T<br>\end{aligned}<br>$$</p>
<p>Its dual is given by<br>$$<br>\begin{aligned}<br>&amp; \max &amp; c^T x \<br>&amp; s.t., &amp; Ax = b \<br>&amp;&amp; x \ge 0<br>\end{aligned}<br>$$</p>
<p>Every LP can be converted into standard form.</p>
<p>The weak duality states that<br>$$<br>c^T x \le y^T A x = y^T b<br>$$</p>
<p>If both primal and dual are feasible, then strong duality reduce to show that the first inequality holds with equality, that is, $\exists$ feasible $x, y$, s.t., $c^T x = y^T A x$. In particular,  complementary slackness, this is equivalent to proving that<br>$$<br>\begin{aligned}<br>&amp;(y^T A - c^T)_i = 0  &amp;&amp;\vee &amp;&amp;x_i = 0         &amp;&amp;\forall \ i \<br>\end{aligned}<br>$$</p>
<p>We begin by assuming that $y$ is the optimal solution for the dual. We claim that $y$ can not be an interior point of the feasible region. Otherwise, $y$ can move along arbitrary direction by a little while maintaining feasibility. If it moves along the direction pointed by $-b$, then the objective function value decreases, contradicting $y$ being optimal. </p>
<p>Therefore, $y$ must be on the boundary of the feasible region.</p>
<h4 id="Question-to-ponder-prove-the-existence-of-optimal-solution-of-the-primal-assuming-feasibility-of-the-dual"><a href="#Question-to-ponder-prove-the-existence-of-optimal-solution-of-the-primal-assuming-feasibility-of-the-dual" class="headerlink" title="Question to ponder: prove the existence of optimal solution of the primal, assuming feasibility of the dual."></a><em>Question to ponder: prove the existence of optimal solution of the primal, assuming feasibility of the dual.</em></h4><p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/MinimizeYB.jpg"></p>
<p>The boundary $y$ touches are specified by tight constraints. The intuition behind is that, $b$ must be the positive combination of the normal vector of the tight constraints. Otherwise, we can find some direction that decreases objective function value while maintaining feasibility. </p>
<p>Denote $S \subset [m]$ the set of indexes, such that $\forall i \in S$, $y^T A_i = c_i$, where $A_i$ is the $i$-th column of matrix $A$ and $c_i$ is the $i$-th coordinator of vector $c$. In other words, $S$ is the set of indexes for which the inequality is tight.</p>
<p>We claim that $b$ is in the conic hull of columns of $A_s$. That is, $\exists x_S \in R^{|S|}$, such that </p>
<ol>
<li>$A_S x_S = b$. </li>
<li>$x_S \ge 0$.</li>
</ol>
<p>Such $x_S$ gives a feasible solution to the primal, by setting $x_{\bar S} = 0$ and $x = x_S \cup x_{\bar S}$. Further, strong duality holds as $c^T x = y^T A x = y^T b$.</p>
<p><em>Proof:</em> </p>
<p>It suffices to show that $b$ is a conic combination of columns of $A_S$. Suppose not. Then by <a target="_blank" rel="noopener" href="https://wuhao-wu-jiang.github.io/2019/01/17/Farkas-Lemma/">Farkas’ Lemma</a>, $\exists v \in \R^m$, such that </p>
<ol>
<li>$v^T A_S \ge 0$. </li>
<li>$v^T b &lt; 0$. </li>
</ol>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/FarkasLemma3.jpg"></p>
<p>Then we can move the vector $y$ along the direction of $v$. But this time we need to be careful, in order not to violate the constraints. </p>
<p>For each inequality constraint $y^T A_i &gt; c_i$ ($i \in \bar S$), $\exists t_i &gt; 0$, such that $(y + t_i v)^T A_i &gt; c_i$. Denote $t = \min_{i \in \bar S} t_i$. </p>
<p>As $(y + t v)^T A_S = c_S + t v^T A_S \ge c_S$, $(y + t v)$ is still feasible. But $(y + t v)^T b &lt; y^T b$. A contradiction.</p>
<h4 id="Question-to-ponder-relate-the-above-discussion-to-Lagrange-multiplier-and-K-K-T-condition"><a href="#Question-to-ponder-relate-the-above-discussion-to-Lagrange-multiplier-and-K-K-T-condition" class="headerlink" title="Question to ponder: relate the above discussion to Lagrange multiplier and K.K.T condition."></a>Question to ponder: relate the above discussion to Lagrange multiplier and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">K.K.T condition</a>.</h4><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference."></a>Reference.</h1><ol>
<li>David P. Williamson, ORIE 6300 Mathematical Programming I, <a target="_blank" rel="noopener" href="https://people.orie.cornell.edu/dpw/orie6300/Lectures/lec08.pdf">Lecture 8</a>  </li>
<li>Boyd, S. and Vandenberghe, L., 2004. Convex optimization. Cambridge university press.</li>
</ol>
<!-- 1. $A_S x_S = b$:   
   We first show $b$ in the linear hull of $A_S$, i.e., $\exists x_S$, s.t., $A_S x_S = b$. Otherwise, the case is similar to the first example we have just discussed. We decompose $b = b_1 + b_2$, where $b_2$ is orthogonal to $C(A_S)$. Then we can move the vector $y$ along the reverse direction of $b_2$. But this time we need to be careful, in order not to violate the inequality constraints. 

    For each inequality constraint $y^T A_i > c_i$ ($i \in \bar S$), $\exists t_i > 0$, such that $(y - t_i b_2)^T A_i > c_i$. Denote $t = \min_{i \in \bar S} t_i$. 
    
    As $(y - tb_2)^T A_S = c_S$, $(y - tb_2)$ is still feasible. But $(y - tb_2)^T b < y^T b$. A contradiction.

1. $x_S \ge 0$. 
   
    For simplicity, consider just the case where the columns of $A_S$ are linearly independent. If $\exists i \in S$, such that $x_i < 0$, then we can find a better solution as follows. Let $u$ be the projection of $A_i$ to $C(A_{S \setminus [i]})$, the subspace spanned by columns of $A_{S \setminus [i]}$. Define $v = A_i - u$. We have $v \neq 0$ since $A_i$ is not inside $C(A_{S \setminus [i]})$. Further $v$ is orthogonal to columns of $A_{S \setminus [i]}$ and $v^T A_i > 0$. 

    Moreover, $v^T b = v^T A_S x_S = (v^T A_i) x_i < 0$. 

    We can move $y$ along the direction of $v$, before some constraint in $\bar S$ becomes tight. But this decreases the value of the objective function, contradicting $y$ being optimal. 

    Remark: when the columns of $A_S$ are linearly dependent, there are few cases.
    1. If $A_i$ is outside $C(A_{S \setminus [i]})$, then the proof still holds. 
    2. Otherwise, $A_i$ can be expressed by a linear combination by columns in $A_{S \setminus [i]}$.  -->
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/14/Sparse-Recovery-with-Count-Sketch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/14/Sparse-Recovery-with-Count-Sketch/" class="post-title-link" itemprop="url">Sparse Recovery with Count-Sketch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-14 22:51:28" itemprop="dateCreated datePublished" datetime="2020-01-14T22:51:28+11:00">2020-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-15 23:01:24" itemprop="dateModified" datetime="2020-01-15T23:01:24+11:00">2020-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose that a stream of items comes online $S = { a_1, a_2, …, a_m }$ where $a_i \in [n]$. Denote the frequency of $S$ as $f = &lt;f_1, f_2, …, f_n&gt;$, where $f_j = |{ a_i \in S : a_i = j }|$. By running the count-sketch algorithm, we can obtain an estimation $\hat f$ of $f$ with space $O(\frac{1}{\epsilon^2} \log n)$, such that with high probability, for $i \in [n]$,<br>$$<br>|\hat f_i - f_i| \le \epsilon |f|_2<br>$$<br>where $|f|<em>2 = \sqrt {\sum</em>{i = 1}^n f_i^2 }$. </p>
<p>In general, a sketch is a concise representation of $f$. Suppose that we limit our space usage to $k$ words (i.e., $|\hat f|_0 = k$), and assume that $H$ is the set of items of $k$ most frequent items, then the sketch $\hat f$ that minimize the $l_2$ error keeps all items in $H$ and the error between $f$ and $\hat f$ is<br>$$<br>|f - \hat f|_2 \doteq err_2^k (f) = \sqrt {\sum_{i \in H} f_i^2}<br>$$</p>
<p>In practice, however, $S$ is not given offline. As we don’t know $H$, we can only compute an $\hat f$ with $k$ words that is as good as possible. The question is, how far $|f - \hat f|_2$ can be from the minimum possible value, namely $err_2^k(f)$? </p>
<p>One possible solution, proposed by Cormode and Muthukrishnan [1], is to construct $\hat f$ vis Count-Sketch with $d = O(\log n)$ hash tables each of which has size $w = \frac{3k}{\epsilon^2}$.</p>
<p><em>Lemma 1.</em>  For any $i \in [n]$, we have $|f_i - \hat f_i| \le \frac{\epsilon}{\sqrt k } err_2^k(f)$ with high probability. </p>
<p><em>Proof</em><br>Denote $h_j(i)$ the value of item $i$ in the $j$-th hash table. Let $A_i$ be the event such that any of the $k$ items with largest frequency is hashed to position $h_j(i)$ in the $j$-th table. Then by union bound,<br>$$<br>\Pr[A_i] \le \frac{k}{w} = \frac{\epsilon^2}{3}<br>$$</p>
<p>Now, </p>
<p>$$<br>\begin{aligned}<br>    \Pr[|f_i - h_i(j)| \ge \frac{\epsilon}{\sqrt k } err_2^k(f)    ]<br>        &amp;= \Pr[|f_i - h_i(j) | \ge  \frac{\epsilon}{\sqrt k } err_2^k(f) \mid A_i ] + \Pr[|f_i - h_i(j) | \ge  \frac{\epsilon}{\sqrt k } err_2^k(f) \mid \bar A_i ] \<br>        &amp;\le \Pr[A_i] + \frac{ Var[ f_i - h_i(j)  \mid \bar A_i ] }{ (\frac{\epsilon}{\sqrt k } err_2^k(f) )^2 } \<br>        &amp;\le \frac{\epsilon^2}{3} + \frac{(err_2^k(f))^2}{ w (\frac{\epsilon}{\sqrt k } err_2^k(f) )^2 } \<br>        &amp;\le \frac{\epsilon^2}{3} + \frac{1}{3}<br>\end{aligned}<br>$$</p>
<p>As long as $\epsilon^2 / 3 &lt; 1 / 6$, this probability is smaller than $1 / 2$. We can boost the failure probability to $1/ n^2$ by repeating the hash tables $d = O(\log n)$ times and taking the median as estimation.</p>
<p>$\blacksquare$</p>
<p>The next step is keep only set $\hat H$ of the $k$ largest items in $\hat f$. Note that $\hat H$ might not equals to $H$. To bound the value $|f - \hat f|_2$, we divide $[n]$ into three subsets: $\hat H$, $H \setminus \hat H$, and $[n]$\setminus (H \cup \hat H)$</p>
<p><em>Lemma 2</em>. $\sum_{i \in \hat H} (f_i - \hat f_i)^2 \le \epsilon^2 (err_2^k(f))^2$.   </p>
<p><em>Proof.</em> As $|\hat H| = k$, this follow directly from lemma 1. </p>
<p>$\blacksquare$</p>
<p><em>Lemma 3</em>. $\sum_{i \in H \setminus \hat H} f_i ^2 \le  \sum_{j \in \hat H \setminus H} \hat f_j^2  + (2 \epsilon +  \epsilon^2) ( err_2^k(f))^2$</p>
<p><em>Proof.</em> $|H| = |\hat H|$, we have $|H \setminus \hat H| = |\hat H \setminus H|$. </p>
<p>If $H \setminus \hat H = \emptyset$, the claim is trivial. Otherwise, we can construct a bijection between $H \setminus \hat H$ and $\hat H \setminus H$. Then for $i \in H \setminus \hat H$, </p>
<p>$$<br>f_i = f_i - \hat f_i + \hat f_i - \hat f_j + \hat f_j<br>$$</p>
<p>As $\hat H$ contains the largest $k$ items in $\hat f$, it must be that $\hat f_i - \hat f_j \le 0$. Therefore, </p>
<p>$$<br>f_i \le f_i - \hat f_i + \hat f_j<br>$$</p>
<p>By lemma 1, we have $f_i - \hat f_i \le \frac{\epsilon}{\sqrt k } err_2^k(f)$, we have </p>
<p>$$<br>f_i \le \frac{\epsilon}{\sqrt k } err_2^k(f) + \hat f_j<br>$$</p>
<p>Summing over all possible $i$, </p>
<p>$$<br>\begin{aligned}<br>    \sum_{i \in H \setminus \hat H} f_i ^2<br>    &amp;\le \sum_{j \in \hat H \setminus H} (\frac{\epsilon}{\sqrt k } err_2^k(f) + \hat f_j)^2 \<br>    &amp;= \sum_{j \in \hat H \setminus H} \hat f_j^2  + 2 \left( \sum_{j \in \hat H \setminus H} \hat f_j \right) \frac{\epsilon}{\sqrt k } err_2^k(f) +  \epsilon^2 ( err_2^k(f))^2 \<br>    &amp;\le \sum_{j \in \hat H \setminus H} \hat f_j^2  + 2 k \sqrt{ \sum_{j \in \hat H \setminus H} \frac{1}{k} \hat f_j^2} \frac{\epsilon}{\sqrt k } err_2^k(f) +  \epsilon^2 ( err_2^k(f))^2 \<br>    &amp;= \sum_{j \in \hat H \setminus H} \hat f_j^2  + 2 \epsilon (err_2^k(f)) \sqrt{ \sum_{j \in \hat H \setminus H} \hat f_j^2} +  \epsilon^2 ( err_2^k(f))^2 \<br>    &amp;\le \sum_{j \in \hat H \setminus H} \hat f_j^2  + (2 \epsilon +  \epsilon^2) ( err_2^k(f))^2<br>\end{aligned}<br>$$</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] G. Cormode and S. Muthukrishnan. Combinatorial algorithms for Compressed Sensing.<br>[2] Moses Charikar, Lecture 9, CS 369G: Algorithmic Techniques for Big Data</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/12/Priority-Sampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/12/Priority-Sampling/" class="post-title-link" itemprop="url">Priority Sampling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-12 14:32:57" itemprop="dateCreated datePublished" datetime="2020-01-12T14:32:57+11:00">2020-01-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-23 15:27:41" itemprop="dateModified" datetime="2020-01-23T15:27:41+11:00">2020-01-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose there are a set of $n$ items with nonnegative weights $w_1, w_2, …, w_n$. We want to maintain a subset of samples $S \subset [n]$ in order to answer queries like given arbitrary $I \subset [n]$ , what is $\sum_{i \in I} w_i$?</p>
<p>A trivial solution is to set $S = [n]$ and we can answer the query exactly. This is an overkill if our goal is to only answer the query approximately. </p>
<p>Alternatively, we can sample each item $i \in [n]$ independently with probability $p_i$ (to be determined), and maintain a random variable<br>$$<br>\begin{aligned}<br>X_i =<br>    \begin{cases}<br>        \frac{w_i}{p_i} &amp; i \text{ is chosen} \<br>        0               &amp;   \text{ else }<br>    \end{cases}<br>\end{aligned}<br>$$</p>
<p>Note that<br>$$<br>E \left[ X_i \right] = w_i<br>$$</p>
<p>And therefore,<br>$$<br>E \left[\sum_{i \in I} X_i \right] = \sum_{i \in I} E[X_i] = \sum_{i \in I} w_i<br>$$</p>
<p>One possible materialization of this sampling scheme is called <em>threshold sampling</em>. First a threshold $T$ is selected. Then each $i \in [n]$ is kept with probability with $p_i = \min { 1 , \frac{w_i}{T} }$. In particular, for each $i \in [n]$, we sample a uniform random variable $u_i \in [0, 1]$, and set<br>$$<br>\begin{aligned}<br>X_i =<br>    \begin{cases}<br>        \max { w_i, T } &amp; u_i &lt; \frac{w_i}{T} \<br>        0               &amp;   \text{ else }<br>    \end{cases}<br>\end{aligned}<br>$$</p>
<p>Note that for $i \in [n]$ such that $w_i &gt; T$, it always holds that $u_i &lt; \frac{w_i}{T}$ and the definition reduce to $X_i = w_i$. For $i \in [n]$ such that $w_i &lt; T$,<br>$$<br>\begin{aligned}<br>X_i =<br>    \begin{cases}<br>        T = \frac{w_i}{p_i} &amp; u_i &lt; \frac{w_i}{T} = p_i \<br>        0               &amp;   \text{ else }<br>    \end{cases}<br>\end{aligned}<br>$$</p>
<p>(Note that we take $u_i &lt; \frac{w_i}{T}$ instead of $u_i \le \frac{w_i}{T}$ here). Clearly, in this case we have $E[X_i] = \frac{w_i}{T} T = w_i$. </p>
<p>We consider the space usage of <em>threshold sampling</em>. Denote $S$ the set of items with $X_i \neq 0$. Then the expected size of $S$ is given by<br>$$<br>\sum_{i = 1}^n p_i = \sum_{i = 1}^n \min \left{ 1 , \frac{w_i}{T} \right}<br>$$</p>
<p>The problem with threshold sampling is that it is hard to control the memory usage of the sample set $S$. If the $n$ items are known offline, and we would like $|S| \approx k$ for some integer $k \le n$, then we can search for some $T$ such that<br>$$<br>\sum_{i = 1}^n p_i = k<br>$$</p>
<p>Then $|S| = k$ in expectation. </p>
<p>The question is, is it possible to force $|S|= k$ exactly? Further, what if the items come online (one by one)? </p>
<h3 id="Priority-Sampling"><a href="#Priority-Sampling" class="headerlink" title="Priority Sampling"></a>Priority Sampling</h3><p><em>Priority sampling</em> achieves this. First recall that in <em>threshold sampling</em> we decide whether to keep $i$ by checking whether a uniform sampled real number $u_i$ is less than $\frac{w_i}{T}$:<br>$$<br>u_i &lt; \frac{w_i}{T}<br>$$</p>
<p>This is equivalent to<br>$$<br>T &lt; \frac{w_i}{u_i}<br>$$</p>
<p>Define the <em>priority</em> of an item $i \in [n]$ as $q_i = \frac{w_i}{u_i}$. Note that $q_i$ is a random variable. In <em>threshold sampling</em>, item $i$ is kept with weight $\max { w_i, T }$ if<br>$$<br>T &lt; q_i<br>$$</p>
<p>Now, instead of using a fixed value of $T$, the <em>Priority sampling</em> sets<br>$$<br>T = (k + 1) \text{-th largest } { q_i }<br>$$</p>
<p>By the definition of $T$, only the $k$ items with largest priorities $q_i$’s are kept in the sample set $S$:<br>$$<br>S = { k \text{ items with largest } q_i \text{ ‘s } }<br>$$</p>
<p>It can be seen that $S$ and $T$ can be easily maintained by a binary heap when the items come in stream fashion. </p>
<p>The challenge is not to analyze the behaviors of the r.v.’s:<br>$$<br>\begin{aligned}<br>X_i =<br>    \begin{cases}<br>        \max { w_i, T } &amp; T &lt; \frac{w_i}{u_i} \equiv u_i &lt; \frac{w_i}{T} \<br>        0               &amp;   \text{ else }<br>    \end{cases}<br>\end{aligned}<br>$$</p>
<p>since now $T$ is its self a random variable. </p>
<p>We claim that it still holds<br><em>Theorem 1</em><br>$$<br>E[X_i] = w_i<br>$$</p>
<p>The trick is to conditioned $X_i$ on the $k$-th and $k+1$-th largest priority of the rest items. Define the random variables<br>$$<br>L = (k + 1) \text{-th largest } { q_j }_{j \neq i} \<br>U = (k) \text{-th largest } { q_j }_{j \neq i}<br>$$</p>
<p>Consider the case $L = a$ and $U = b$, where $a \le b$. There two possible cases</p>
<ol>
<li><p>If $w_i &gt; b$, then $T = b$ and surely $\frac{w_i}{u_i} &gt; T$ holds. In this case  $X_i = w_i$ and<br> $$</p>
<pre><code> E[X_i | U = b] = w_i
</code></pre>
<p> $$</p>
</li>
<li><p>If $w_i &lt; b$, then there are four sub-cases:</p>
<ul>
<li><p>$\frac{w_i}{u_i} \le a$, then $T = b$ and $i$ is not selected into $S$. Therefore $X_i = 0$.   </p>
</li>
<li><p>$a &lt; \frac{w_i}{u_i} &lt; b$, now $\frac{w_i}{u_i}$ becomes the $k+1$-th largest priority of all items and $T = \frac{w_i}{u_i}$. In this case $i$ is not selected into $S$, and $X_i = 0$ (since $\frac{w_i}{u_i} &lt; T$ does not hold). </p>
</li>
<li><p>$\frac{w_i}{u_i} = b$, then there is a tie and $T = b$ in this case. Depending on how the tie is broken, either $i$ is put into $S$ or not. But in both cases, $X_i = 0$ since $\frac{w_i}{u_i} &lt; T$ does not hold. </p>
</li>
<li><p>$\frac{w_i}{u_i} &gt; b$, then $T = b$ and $i$ is put into $S$. As $w_i &lt; T$, we have $X_i = T = b$. </p>
<p>Therefore,<br>$$<br>   E[X_i | U = b] = b \cdot \Pr\left[ \frac{w_i}{u_i} &gt; b \mid w_i &lt; b \wedge U = b \right] = b \cdot \frac{w_i}{b} = w_i<br>$$</p>
</li>
</ul>
</li>
</ol>
<p>In conclusion,<br>$$<br>E[X_i] = E[E[X_i | U]]  = w_i<br>$$</p>
<p><em>Theorem 2</em>.<br>For $I \subset [n]$,<br>$$<br>\begin{aligned}<br>    E\left[ \prod_{i \in I} X_i \right] =<br>    \begin{cases}<br>         \prod_{i \in I} w_i     &amp; \text{ if } |I| \le k \<br>         0                      &amp; else<br>    \end{cases}<br>\end{aligned}<br>$$ </p>
<p><em>Proof:</em> Clearly, when $|I| &gt; k$, then $\exists i \in I$, such that $q_i$ is not one of the $k$ largest priorities. Hence $X_i = 0$ for sure. </p>
<p>If $|I| \le k$, we show the claims holds by induction. The base case of $|I| = 1$ reduces to <em>Theorem 1</em>. Suppose that the claim hold for $|I| \le t &lt; k$, we show it holds for $|I| = t + 1$. </p>
<p>Similarly to <em>Theorem 1</em>, define $U$ to be the $k + 1 - |I|$-th largest priority in $[n] \setminus I$. Conditioning on $U = b$, then we can divide $I$ into two sets: $I_1 = { i \in I :   w_i &gt; b}$ and $I_2 = I \setminus I_1$.  For $i \in I_1$, we have $X_i = w_i$ by definition. Now consider $I_2$. Let $j = \arg \min_{i \in I_2} q_i$.<br>If $q_j \le b$, then it holds that $b \ge T \ge q_j$ and $X_j = 0$. In this case it does not contribute to $\prod_{i \in I} X_i$. Hence<br>$$<br>\begin{aligned}<br>E \left[ \prod_{i \in I} X_i \mid U = b \right]<br>    &amp;= \prod_{i \in I_1} w_i ( b^{|I_2| } \cdot \prod_{i \in I_2} \Pr \left[ \frac{w_i}{u_i} &gt; b \right] ) \<br>    &amp;= \prod_{i \in I_1} w_i ( b^{|I_2| } \cdot \prod_{i \in I_2} \frac{w_i}{b} ) \<br>    &amp;= \prod_{i \in I} w_i<br>\end{aligned}<br>$$</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Nick Duffield, Carsten Lund, and Mikkel Thorup. Priority sampling for estimation fo arbitrary subset sums. Journal of ACM, 2007</li>
<li>Chandra Chekuri, Lecture note, CS 598: Algorithms for Big Data</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/02/Newton-s-Method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/02/Newton-s-Method/" class="post-title-link" itemprop="url">Newton's Method</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-01-02 16:25:30 / Modified: 16:54:02" itemprop="dateCreated datePublished" datetime="2020-01-02T16:25:30+11:00">2020-01-02</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Newton’s method is an algorithm for finding root of a function $f(x)$ numerically. If $f$ is differentially at a point $x$, then we can use this local information at $x$ to approximate $f$:<br>$$<br>\bar f(y) = f(x) + f’(x) (y - x)<br>$$</p>
<p>This is the tangent line at $x$. We use its intersection with the $x$-axis as the guess of root:<br>$$<br>\bar f(y) = 0 = f(x) + f’(x) (y - x)<br>$$</p>
<p>If $f’(x) \neq 0$, then $y = x - \frac{f(x)}{f’(x)}$. Intuitively, $f(x)$ is the height and $f’(x)$ is the slope, hence $\frac{f(x)}{f’(x)} = x - y$ gives the distance between $x$ and $y$. Note that if $f’(x) = 0$, the tangent line does not intersect with $x$-axis. </p>
<p>With a starting point $x_1$, the Newton’s Method updates its value iteratively:<br>$$<br>x_{n + 1} = x_n - \frac{f(x_n)  }{ f’(x_n) }<br>$$</p>
<p>Let $z$ be the root of $f$ ($f(z) = 0$). Then under proper condition, $x_n$ converges to $z$ quadratically, that is, $|x_n - z| \le O(2^{-n})$. </p>
<h3 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem."></a>Theorem.</h3><p>Let $f \in C^2$ in some interval $[z \pm c]$ and $f’(x) \neq 0$, $f’’(x)$ is bounded for $x \in [z \pm c]$. If $x_0 \in [z \pm c]$, then Newton’s Method achieves quadratic convergence. </p>
<p><em>Proof:</em> By Taylor expansion, $\forall x \in [z \pm c]$, $\exists y$ that lies between $x$ and $z$, such that<br>$$<br>0 = f(z) = f(x) + f’(x) (x - z) + \frac{f’’(y)}{2} (z - x)^2<br>$$</p>
<p>Hence<br>$$<br>x - \frac{f(x)}{f’(x)} - z = \frac{f’’(y)}{2 f’(x)} (z - x)^2<br>$$</p>
<p>Define $\epsilon_n = (z - x_n)$, and replacing $x$ with $x_n$, we have<br>$$<br>|\epsilon_{n + 1}| = |x_{n + 1} - z| = |x_n - \frac{f(x_n)}{f’(x_n)} - z| = |\frac{f’’(y)}{2 f’(x_n)}| (z - x_n)^2 = |\frac{f’’(y)}{2 f’(x_n)}| \epsilon_n^2<br>$$</p>
<p>Since $f$’s second derivative is continuous, $f’’(y)$ is bounded on the closed interval $[z \pm c]$. Further, as $f’(x) \neq 0$, then $\min_{x [z \pm c]} f’(x) &gt; 0$ as $f’(x)$ is continuous on the closed interval $[z \pm c]$. Therefore,<br>$$<br>C \doteq \max |\frac{f’’(y)}{2 f’(x_n)}|<br>$$<br>is bounded. Now<br>$$<br>|\epsilon_{n + 1}| \le |C| \epsilon_n^2<br>$$</p>
<p>If for some $n$ such that $\epsilon_n &lt; 1$, then the number of precise digits doubles for $\epsilon_{n + 1}$.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/01/Online-Matrix-Sketch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/01/Online-Matrix-Sketch/" class="post-title-link" itemprop="url">Online Matrix Sketch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-01 21:04:12" itemprop="dateCreated datePublished" datetime="2020-01-01T21:04:12+11:00">2020-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-25 17:36:33" itemprop="dateModified" datetime="2020-12-25T17:36:33+11:00">2020-12-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given a matrix $A \in \R^{m \times n}$, the goal of low approximation is to find a matrix $B \in R^{k \times n}$ with rank at most $k$ that approximates $A$ as well as possible. This can be measured by $2$-norm, i.e.,<br>$$<br>||A - B|| = \max_{x \in R^n, ||x|| = 1} ||(A - B)x||<br>$$</p>
<p>If we know the entire matrix $A$, its best approximation is given by the singular vector decomposition,<br>$$<br> B \doteq \sum_{i = 1}^k \sigma_i u_i v_i^T<br>$$<br>where $\sigma_i$, $u_i$ and $v_i$ are the $i$-th largest singular value, left singular vector and right singular vector of $A$, respectively. </p>
<p>When some elements of $A$ are missing, finding such a matrix $B$ to approximate $A$ as well as to predict the missing entries of $A$ is known as collaborative filtering, which is used in recommendation system.</p>
<p>Another complicated scenario is to approximate $A$ in an online fashion, i.e.,  the rows of $A$ comes on by one (no missing values). Denote the $i$-th row of $A$ as $A_i$. We need to maintain a smaller matrix $B \in \R^{k \times n}$, termed the matrix sketch of $A$, that approximate $A$ well, in the sense that<br>$$<br>|| A - B || \le \frac{2 ||A||_F^2}{ k },<br>$$</p>
<p>where $|| A ||<em>F \doteq \sqrt{ \sum</em>{i \in [n], j \in [n] } A_{i,j}^2 }$ is the Frobenius norm of $A$. </p>
<blockquote>
<p> Algorithm [1]  </p>
<ol>
<li>$B \leftarrow$ ${k \times n}$ all zero matrix.</li>
<li>For $i \in [1:m]$:</li>
<li>$\qquad$ Insert $A_i$ into a zero row of $B$.</li>
<li>$\qquad$ If $B$ has no zero row after insertion:</li>
<li>$\qquad$ $\qquad$ $U_i, \Sigma_{i-}, V_i \leftarrow SVD(B)$, such that $B = U_i \Sigma_{i-} V_i^T$. </li>
<li>$\qquad$$\qquad$ $\Sigma_{i} \leftarrow \sqrt{ \max { \Sigma_{i-}^2 - \sigma_{i, k / 2}^2 I, 0 } }$</li>
<li>$\qquad$$\qquad$ $B \leftarrow \Sigma_{i} V_i^T$. </li>
</ol>
</blockquote>
<p>Here $\sigma_{i, k / 2}$ is the $k / 2$-th largest singular value of $\Sigma_{i -}$. The line 6 is throw away all singular values that are less than $\sigma_{i, k / 2}$, and then subtract the squares of all singular values that are greater by $\sigma_{i, k / 2}^2$. </p>
<h4 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h4><h5 id="Theorem-1-1-B-T-B-preccurlyeq-A-T-A-at-any-time"><a href="#Theorem-1-1-B-T-B-preccurlyeq-A-T-A-at-any-time" class="headerlink" title="Theorem 1 [1]. $B^T B \preccurlyeq A^T A$ at any time."></a>Theorem 1 [1]. $B^T B \preccurlyeq A^T A$ at any time.</h5><p><em>Proof:</em> To show $A^T A - B^T B$ is semi-positive definitive, we need to show that $\forall x \in R^n$, we have<br>$$<br>x^T A^T A x - x^T B^T B x \ge 0<br>\equiv ||Ax||^2 \ge ||Bx||^2<br>$$</p>
<p>Denote $B_i$ the matrix of $B$ after the $i$-th iteration of the algorithm. Note that $B_0 = 0^{k \times n}$. Then it holds that<br>$$<br>||B_i x||^2 + (A_i \ x)^2 \ge ||B_{i + 1} x||^2<br>$$</p>
<p>There are two possible cases. In the first case, after inserting $A_i$ into $B_i$, $B_i$ still contains empty row, then the if-clause does not execute and the quality holds. Otherwise, if the if-clause is executed, then<br>$$<br>||B_{i + 1} x||^2  = ||\Sigma_i V_i^T x||^2<br>$$</p>
<p>Moreover<br>$$<br>||B_i x||^2 + (A_i \ x)^2 = ||U_i \Sigma_{i-} V_i^T x||^2 = ||\Sigma_{i-} V_i^T x||^2<br>$$<br>Since $U_i$ is an orthogonal matrix and corresponds to a left rotation</p>
<p>By the transformation of the algorithm, we have $\Sigma_{i-}^T \Sigma_{i-} \preccurlyeq \Sigma_{i}^T  \Sigma_i$, therefore,<br>$$<br> x V_i \Sigma_{i}^T \Sigma_{i} V_i^T x - x V_i \Sigma_{i-}^T \Sigma_{i-} V_i^T x \ge 0<br>$$</p>
<h5 id="Theorem-2-1-A-T-A-B-T-B-le-2-A-F-2-k"><a href="#Theorem-2-1-A-T-A-B-T-B-le-2-A-F-2-k" class="headerlink" title="Theorem 2 [1]. $||A^T A - B^T B|| \le 2 ||A||_F^2 / {k}$"></a>Theorem 2 [1]. $||A^T A - B^T B|| \le 2 ||A||_F^2 / {k}$</h5><p><em>Proof:</em> We first show that for a semi-positive definite matrix $S$, we have<br>$$<br>||S|| = \max_{x \in R^n, ||x|| = 1} ||S x|| = \max_{x \in R^n, ||x|| = 1} \sqrt {x^T S^T S x} = \max_{x \in R^n, ||x|| = 1} \sqrt {x^T S^2 x}<br>$$</p>
<p>But $S^2$ has the same eigenvectors as $S$. Denote $x$ be $S$’s largest unit eigenvector and $x^T S x = x^T \lambda_1 x_1 = \lambda_1$ its corresponding eigenvalues. Then<br>$$<br>||S|| = \sqrt {x^T S^2 x} = \lambda_1 = x^T Sx<br>$$<br>and $||S|| = \lambda_1$. Geometrically, as $x$ is eigenvector, it is colinear with $Sx$. Further, since $x$ is a unit vector, the length of $Sx$ is given by $x^T Sx$. </p>
<p>Now, let $x$ the largest unit eigenvector of $A^T A - B^T B$. As $B^T B \ll A^T A$,<br>$$<br>\begin{aligned}<br>||A^T A - B^T B||<br>    &amp;= x^TA^T A x- x^T B^T Bx \<br>    &amp;= ||Ax||^2 - ||Bx||^2 \<br>    &amp;= \sum_{i = 1}^n (A_i^T x)^2  + ||B_{i - 1} x||^2 - ||B_{i} x||^2 \<br>    &amp;= \sum_{i = 1}^n ||\Sigma_{i-} V_i^T x||^2 - ||\Sigma_i V_i^T x||^2 \<br>    &amp;\le \sum_{i = 1}^n ||V_i \Sigma_{i-}^T \Sigma_{i-} V_i^T - V_i \Sigma_i^T \Sigma_i V_i^T || \<br>    &amp;= \sum_{i = 1}^n ||\Sigma_{i-}^T \Sigma_{i-}  - \Sigma_i^T \Sigma_i || \<br>    &amp;= \sum_{i = 1}^n \sigma_{i, k /2 }^2<br>\end{aligned}<br>$$</p>
<p>It is left to bound the size of $\sum_{i = 1}^n \sigma_{i, k /2 }^2$:<br>$$<br>\begin{aligned}<br>||A||<em>F - ||B||<em>F<br>    &amp;= \sum</em>{i = 1}^n ||A_i||^2  + ||B_{i - 1}||<em>F - ||B</em>{i}||<em>F \<br>    &amp;= \sum</em>{i = 1}^n ||U_i \Sigma_{i-} V_i^T||_F - ||\Sigma_i V_i^T||<em>F \<br>    &amp;= \sum</em>{i = 1}^n ||\Sigma</em>{i-} V_i^T||<em>F - ||\Sigma_i V_i^T||<em>F \<br>    &amp;= \sum</em>{i = 1}^n ||\Sigma</em>{i-}||<em>F - ||\Sigma_i||_F \<br>    &amp;\ge (k / 2) \sum</em>{i = 1}^n \sigma_{i, k /2 }^2<br>\end{aligned}<br>$$</p>
<p>The equality holds since left or right multiplying a orthogonal matrix does not change Frobenius norm (the sum of square distance of the row vectors to the origin). The last inequality holds by the definition of $\Sigma_{i - }$ and $\Sigma_i$. </p>
<p>In conclusion,<br>$$<br>\begin{aligned}<br>    \frac{||A^T A - B^T B||}{||A||_F} \le \frac{||A^T A - B^T B||}{||A||_F - ||B||_F} \le \frac{2}{k}<br>\end{aligned}<br>$$</p>
<p>$\square$.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference."></a>Reference.</h2><p>[1]. Liberty, Edo. “Simple and deterministic matrix sketching.” In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 581-588. ACM, 2013.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/12/13/Polyhydra-and-Polytope/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/13/Polyhydra-and-Polytope/" class="post-title-link" itemprop="url">Polyhydra and Polytopes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-13 23:54:11" itemprop="dateCreated datePublished" datetime="2019-12-13T23:54:11+11:00">2019-12-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-15 22:37:11" itemprop="dateModified" datetime="2019-12-15T22:37:11+11:00">2019-12-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>When studying linear program, it necessary that we understand the structure of the feasible region. We investigate here some critical ideas of characterizing the feasible region. </p>
<ol>
<li><em>Polyhedron</em>: $P$ is a polyhedron in $R^n$ if it is the intersection of a set of linear constraints: $P = { x: Ax \le b }$ for some $A \in R^{m \times n}, b \in R^n$. A polyhedron is a convex set. </li>
<li><em>Polytope</em>: $P$ is a polytope if it consists of points that are convex combination of finite number of points $v_1, v_2, …, v_k$, i.e., $P = { x: x = \sum_{i = 1}^k \mu_i v_i , \ s.t., \ \sum_{i  =1}^k \mu_i = 1 \wedge \mu_i \in [0, 1] \quad \forall i \in [k] }$. </li>
</ol>
<p>Of interest are the vertices, or extreme points or basic feasible solution of a polyhedron $P$:</p>
<ol>
<li><em>Vertex</em>: $v \in P$ is called a vertex if $\exists c \in R^n$, s.t., $c^T v &gt; c^T x$ $\forall x \neq v \wedge x\in R$. </li>
<li><em>Extreme Point</em>: $v \in R$ is an extreme point of $P$ if $\nexists x, y \neq v, x, y \in P$, s.t., $v = \mu_1 x + \mu_2 y$, $\mu_1, \mu_2 \in [0, 1]$ and $\mu_1 + \mu_2 = 1$. </li>
<li><em>Basic Feasible Solution:</em> $v \in P$ is a basic feasible solution of $P = { x: Ax \le b }$, if the number of tight constraints $A v \le b$ equals to $n$. In particular, denote $A_{i, :}$ the $i$-th row of matrix $A$ and $b_i$ the $i$-th element of $b$. The constraint $A_{i, :} v \le b_i$ is called tight if equality holds. For convenience of discussion, denote $A_{=} x = b_{=}$ the set of tight constraints and $A_{&lt;} x &lt; b_{&lt;}$ the set of non-tight constraints. </li>
</ol>
<p>It turns out that the three definitions are equivalent. Here is the proof. </p>
<p><em>Proof:</em>  </p>
<p>$1 \rightarrow 2:$ $c^T(\mu_1 x + \mu_2 y) = \mu_1 c^T x + \mu_2 c^T y &lt; \mu_1 c^T v + \mu_2 c^T v = c^T v$, it concludes that $\mu_1 x + \mu_2 y \neq v$.   </p>
<p>$2 \rightarrow 3:$ suppose that $rank(A_{=}) \neq n$. There $\exists y \in R^n, y \neq 0$, and $A_{=} y = 0$. For inequality constraints, we can find small enough $\epsilon$, such that $A_{&lt; } (x \pm \epsilon y) &lt; b$. Hence $x \pm \epsilon y \in P$. But $\frac{1}{2} (x + \epsilon y) + \frac{1}{2} (x - \epsilon y) = x$, a contradiction.   </p>
<p>$3 \rightarrow 1:$ just define $c^T = \sum_{i = 1}^m A_{i, :}$. If $x \in P$, then $\sum_{i = 1}^m A_{i, :} x \le \sum_{i = 1}^m b_i = \sum_{i = 1}^m A_{i, :} v$. As $A_{=}$ has rank $n$, when equality holds, it implies $x = y$.  </p>
<p>The relationship between polyhedron and polytope is stated as follows: </p>
<p><em>Theorem:</em> if a polyhedron $P$ is bounded, that is, $\exists M &gt; 0$, $\forall x \in P$, we have $x^T x &lt; M$, then $P$ is a polytope. </p>
<p><em>Proof:</em> It suffice to show that every $x \in P$ is a convex combination of vertices of $P$. The proof is by induction on the rank of $A_{=}$, given $x \in A$. </p>
<ol>
<li>If $rank(A_{=}) = n$, then by definition $x$ is a vertex. </li>
<li>Suppose the claim holds for all ranks $\ge k + 1$ for some $k &lt; n$. We show it carries to $x \in P$ with $rank(A_{=}) = k$.  <ul>
<li>As $k &lt; n$, $\exists y \in R^n, y \neq 0$, such that $A_{=} y  = 0$.</li>
<li>We extend $x$ from the directions of $y$ and $-y$ as much as possible. We find the largest $\lambda_+ \in R, \lambda_+ &gt; 0$, such that $x + \lambda_+ y \in P$. Since $A_{=} (x + \lambda_+ y) = b_{=}$, these constraints will never be violated. Hence some constraints $A_{&lt; } x &lt; b_{&lt;}$ becomes tight for $A_{&lt;} (x + \lambda_+ y)$. Which implies that the rank of $x + \lambda_+ y$ is at least $k + 1$ and thus can be expressed a convex combination of vertices. Similarly, we can find some $x - \lambda_- y$ that is a convex combination of vertices. </li>
<li>$x$ can now be express a convex combination of $x + \lambda_+ y$ and $x - \lambda_- y$, therefore a convex combination of vertices. </li>
</ul>
</li>
</ol>
<p>The reverse is also true:</p>
<p><em>Theorem:</em> Let $P$ be a polytope, then $P$ is a bounded polyhedron. </p>
<p>To prove this, we need an intermediate definition–<em>polar set</em>. It means something on the opposite direction. </p>
<p><em>Definition:</em> Given a set $P$, its polar set is defined as $\bar P = { y \in R^n, y^T x \le 1, \forall x \in P}$. </p>
<p><em>Lemma</em>: If $P$ is closed, convex and $0$ is the interior point of $P$, then $\overline{\bar P} = P$, that is the polar set of the polar set of $P$ is $P$ itself. </p>
<p><em>Proof of the Lemma</em>:   </p>
<ol>
<li>$P \subset \overline{\bar P}$. If $x \in P$, then by definition of $\bar P$, $\forall y \in \overline{\bar P}$, we have $y^T x \le 1$, then $x \in P^{oo}$. </li>
<li>$(R^n - P) \cap \overline{\bar P} = \emptyset$, i.e., $\forall x \notin P$, we have $v \notin \overline{\bar P}$. Since $P$ is convex and closed, $\exists y \in R^n$, such that $y^T v &gt; k$ and $y^T x &lt; k$ for all $x \in P$. The fact that $0 \in P$ implies $k &gt; 0$. Replace $y$ with $\bar y = y / k$, we have $\bar y^T x &lt; 1$ for all $x \in P$. Hence $\bar y \in \bar P$. But now $\bar y^T v &gt; 1$. By definition, $v$ does not belong to the polar set of $\bar P$. </li>
</ol>
<p><em>Proof of the Theorem:</em> We assume that $P$ contains the point $0$. Otherwise, we perform a translation $R^n \rightarrow R^n: f(x) = x + t, \forall x \in R^n$, such that $0 \in f(P)$. If we can show $f(P)$ is a polyhedron, then $f(P) = {f(x) :  A f(x) \le b }$, then we have $P = { x : Ax \le b - At }$. </p>
<p>Back to our proof. As $P$ is a polytope, then it can be written as $P = { x: x = \sum_{i = 1}^k \mu_i v_i , \ s.t., \ \sum_{i  =1}^k \mu_i = 1 \wedge \mu_i \in [0, 1] \quad \forall i \in [k] }$. A point $y$ belongs to the polar set of $P$ if and only if $y^T v_i \le 1, \forall i \in [k]$. Therefore, $\bar P = {y : y^T v_i \le 1, \forall i \in [k] }$, which is a polyhedron. </p>
<p>Next, we show that $\bar P$ is bounded. This results from the assumption that $0$ is an interior point of $P$. Therefore, $\exists \epsilon &gt; 0$, such that $x \in P, \forall x \in R^n, |x|_2 &lt; \epsilon$. For any $y \in \bar P$, with proper scaling, we have $\frac{\epsilon}{|y|_2} y \in P$. Which further implies that $y^T \frac{\epsilon}{|y|} y = \epsilon |y|_2 \le 1$, and $|y|_2 \le \frac{1}{\epsilon}$. </p>
<p>Now, $\bar P$ is a bounded polyhedron, hence a polytope. By previous lemma, $P = \overline{\bar P}$, hence a polyhedron. </p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a target="_blank" rel="noopener" href="https://people.orie.cornell.edu/dpw/orie6300/Lectures/lec03.pdf">David P. Williamson, ORIE 6300, Lecture 03, Polyhedra and polytopes.</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/12/02/Unique-Factorization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/02/Unique-Factorization/" class="post-title-link" itemprop="url">Unique Factorization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-02 21:23:35" itemprop="dateCreated datePublished" datetime="2019-12-02T21:23:35+11:00">2019-12-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-03 16:33:24" itemprop="dateModified" datetime="2019-12-03T16:33:24+11:00">2019-12-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>A prime number is an positive integer greater than 1 and whose divisor is 1 and itself. We claim that<br>$$<br>\forall n \in N^+, n &gt; 1,<br>$$<br>$n$ has a unique prime factorization, i.e., we can write $n$ as<br>$$<br>n = p_1 p_2 p_3 … p_k<br>$$<br>where $k \ge 1$ and the $p_i$’s are primes and<br>$$<br>p_1 \le p_2 \le p_3 \le … \le p_k<br>$$<br>The claim uses Euclid’s Lemma, which states: </p>
<ul>
<li>Lemma: If $p$ is a prime and $p | ab$, then $p | a$ or $p | b$. </li>
<li>Proof: if $p$ is prime, and $p \nmid a$, then exists $a^{-1} \in F_p$, s.t, $a^{-1} a \equiv 1 \mod p$. It concludes that $b \equiv a^{-1}a b \equiv 0 \mod p$. Similar results holds when $p \nmid b$. </li>
<li>Corollary: if $p | q_1 q_2 q_3 … q_l$, then $p | q_i$ for some $1 \le i \le l$. </li>
</ul>
<p>Now, suppose $n$ has another prime decomposition<br>$$<br>n = q_1 q_2 …    q_l<br>$$<br>Then by assumption, $q_1 | q_i$ for some $1 \le i \le l$. But both $q_1$ and $q_i$ are prime. Hence $q_1 = q_i$. By cancellation $q_1$ and $q_i$ and induct on the rest products, we get the desired result. </p>
<p><em>Corollary:</em> For integers $n$ and $m$, if we write them as the prime decompositions:<br>$$<br>n = p_1^{d_1} p_2^{d_2} … p_k^{d_k} \<br>m = q_1^{f_1} q_2^{f_2} … q_l^{f_l}<br>$$<br>such that $p_i \neq p_j$ for $1 \le i &lt; j \le k$ and $q_i \neq q_j$ for $1 \le i &lt; j \le l$, and<br>$$<br>n \mid m<br>$$<br>Then for any $1 \le i \le k$, $\exists 1 \le j \le l$, such that<br>$$<br>p_i = q_j \<br>d_i \le f_j<br>$$<br><em>Proof:</em> By Euclid’s Lemma, and proper cancelation, we have the desired result. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/07/Approximate-Membership-Problem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/07/Approximate-Membership-Problem/" class="post-title-link" itemprop="url">Approximate Membership Problem</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-07 16:14:36" itemprop="dateCreated datePublished" datetime="2019-11-07T16:14:36+11:00">2019-11-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 21:33:25" itemprop="dateModified" datetime="2020-04-06T21:33:25+10:00">2020-04-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the approximate membership problem, we have subset $S \subset U$, that supports two operations:  </p>
<ol>
<li><em>update($x$)</em>: $S \leftarrow S \cup {x}$</li>
<li><em>query($x$)</em>: return True if $x\in S$ else False.</li>
</ol>
<p>where $U$ is the domain that elements $x$ belongs to. If $U = 2^{32}$, then each representation of $x$ takes $32$ bits. If $U = 2^{64}$, then each $x$ needs $64$ bits. If $U$ is the website address, the length of its elements might is unlimited. Suppose that the average length is about $15$ chars, and each char takes $8$ bits, then the representation could occupy most $120$ bits. </p>
<p>We can solve the membership problem exactly by dictionaries, such as balanced-binary search tree and hash set. This requires space $O(n \log U)$ bits. If we allows false positive, we can break this bound. </p>
<p>The idea is to have a bit array $A$ with length $O(kn)$ bits and a pairwise hash family $H$ that maps the elements in $U$ uniformly at random to each bit in the array. After picking a hash function uniformly at random from $H$, the operation on $S$ is performed as follows: </p>
<ol>
<li><em>update($x$)</em>: $A[h(x)] \leftarrow 1$</li>
<li><em>query($x$)</em>: return True if $A[h(x)] = 1$ else False.</li>
</ol>
<p>We can only make a mistake if an element $y \notin S$ and $A[h(y)] = 1$. However, the probability of such event is bounded by<br>$$<br>\Pr[\exists x \in S, s.t., h(x) = h(y)] \le \sum_{x \in S} \Pr[h(x) = h(y)] = \frac{n}{m} = \frac{1}{k}<br>$$</p>
<p>Remark: the accurate probability is given by<br>$$<br>1 - \Pr[\forall x \in S, s.t., h(x) \neq h(y)] = 1 - (1 - \frac{1}{m})^n \le \frac{n}{m}<br>$$ </p>
<p>To amplify the failure probability to a specified threshold $\delta$, we can set $k = \frac{1}{\delta}$. A more efficient way is by repetition the array $\ln \frac{1}{\delta} /\ln k$ times. Then the overall space usage is<br>$$<br>\frac{ n k \ln \frac{1}{\delta} }{ \ln k }<br>$$</p>
<p>Note that $f(k) = \frac{k}{\ln k} \ge e$,  since it holds that <a target="_blank" rel="noopener" href="https://www.google.com/search?q=y=x-e*ln(x)">$y = x - e\ln x$</a> (click to see the plot) for $x &gt; 0$:<br>$$<br>y’ = 1 - e / x = 0 \longrightarrow x = e.<br>$$</p>
<p>When $k = e$ the space usage is<br>$$<br>ne \ln \frac{1}{\delta}<br>$$</p>
<p>The compress ratio is given by<br>$$<br>\frac{\log U}{e \ln \frac{1}{\delta} }<br>$$</p>
<p>If we require the failure probability $\delta = 1 / 100$, then $\ln 1 / \delta \approx 4.6$, and $e \ln 1/\delta \approx 12.5 \le 13$. That is, we need on $13$ bits for each element in $S$, and the average number of operations for updating $S$ is $5$, which is about $1/2$ space when $U = 2^{32}$, $1/5$ space when $U = 2^{64}$, and much less when $U$ is the set of possible web addresses.</p>
<h4 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h4><p>There is another implementation that use a single bit array $A$ of size $m$ but $k$ hash functions $h_1, h_2, …, h_k$. We initialization by setting </p>
<blockquote>
<p>For $x \in S$:<br>$\qquad$ set $A[h_i(x)] = 1$ for all $i \in [k]$.</p>
</blockquote>
<p>As a result, at most $kn$ bits of $A$ are initialized to be 1. </p>
<p>Now, for an element $x’ \notin S$, we accidentally report it as a member $\in S$ if $h_i(x’) = 1$ for all $i \in [k]$. But the probability that<br>$$<br>\Pr[h_i(x’) = 1] \le \frac{kn}{m}<br>$$</p>
<p>and<br>$$<br>\Pr[h_i(x’) = 1, \forall i \in [k]] \le (\frac{kn}{m} )^k<br>$$</p>
<p>If we use $k = \ln \frac{1}{\delta}$ hash functions and set the size $m = ne \ln \frac{1}{\delta}$, we can also achieve failure probability $\delta$. </p>
<p>Question to ponder: what is $S$ is dynamic? </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/02/Frequency-Estimators/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/Frequency-Estimators/" class="post-title-link" itemprop="url">Frequency Estimators</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-02 20:47:52" itemprop="dateCreated datePublished" datetime="2019-11-02T20:47:52+11:00">2019-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-21 23:06:51" itemprop="dateModified" datetime="2021-01-21T23:06:51+11:00">2021-01-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The task of frequency estimation is to count the number of times each element appears in the multi-set $S$. There are $n$ elements in $S$ and each element belongs to a finite domain $U = [1, , 2, …, u]$. If we use a balanced binary search, this can be done in $O(n \log \min{n, u})$ time and $\Theta(\min{n, u})$ space. If we use a hash table, this can be done in $\Theta(\min{n, u})$ time and space.</p>
<p>Here is the question: can we do it in sub-linear space? </p>
<p>If we are allowed to make some errors, this is possible. Let $a = \left&lt; a_1, a_2, …, a_{u} \right&gt;$ be the frequency of elements in $U$ and $b = \left&lt; b_1, …, b_{u} \right&gt;$ be our estimation. Our goal is to minimize the maximum possible error $\max_i |b_i - a_i|$. </p>
<h1 id="Count-Min-Sketch"><a href="#Count-Min-Sketch" class="headerlink" title="Count-Min Sketch"></a>Count-Min Sketch</h1><p>If we use array $A$ with length $m &lt; \min{n, u}$ to count the frequency of the elements, then some elements must share the same slot in the array. Now we apply the following strategy: </p>
<blockquote>
<ol>
<li>Pick a perfect hash function $h : U \rightarrow [m]$ (this condition can be weaken, but for convenience of discussion, we assume perfectness here), that maps the elements uniformly to every bucket in the array.   </li>
<li>For $\forall s \in S$,<br>   $\qquad A[h(s)] \leftarrow A[h(s)] + 1$    .</li>
<li>Return $b_i = A[h(i)]$ as the estimation of $a_i$.   </li>
</ol>
</blockquote>
<p>Intuitively, each slot obtains $\frac{1}{m}$ fraction of the total mass $\sum_i a_i = |a|$ (where $|\cdot |$ denotes the $l_1$ norm). Conditioned on $i$-th element is hashed to the $h(i)$ slots, $\frac{1}{m}$ fraction of the rest mass $\sum_{j \neq i} a_j = n - a_i$ is hashed to the $h(i)$-th slot. Therefore, in expectation,<br>$$<br>0 \le \mathbb{E}[b_i - a_i] \le \frac{|a|}{m}<br>$$</p>
<p>An interesting property is that, if some $a_j$ ($j \neq i$) (or a small subset of $a_j$’s) constitutes, say $0.99$ percent of the mass $|a|$, as long as it is not hashed to $h(i)$, then the expected error is less than $0.01 \frac{|a|}{m}$. </p>
<p>Formally, define the binary random variables $X_j$ for $j \neq i$ such that<br>$$<br>X_j = \begin{cases}<br>    a_j     &amp;\quad \text{if} \quad h(i) = h(j)\<br>    0       &amp;\quad \text{otherwise}<br>\end{cases}<br>$$<br>Then by perfectness of $h$, $\Pr[X_j = a_j] = \frac{1}{m}$ and $\mathbb{E}[X_j] = \frac{a_j}{m}$. (Indeed, pairwise independence of $h$ suffices to guarantee this condition). </p>
<p>Then<br>$$<br>    \mathbb{E} [b_i - a_i] = \mathbb{E} \left[ \sum_{j \neq i} X_j \right] = \sum_{j \neq i} \mathbb{E}[X_j] \le \frac{|a|}{m}<br>$$</p>
<p>As $b_i - a_i$ is non-negative, by Markov inequality,<br>$$<br>\Pr \left[ b_i - a_i \ge \epsilon |a| \right] \le \frac{\mathbb{E}[b_i - a_i]}{\epsilon |a|}  \le \frac{1}{\epsilon m}<br>$$<br>If we would like to achieve failure probability $\delta$, we can set $m = \epsilon^{-1} \delta^{-1}$. </p>
<p>Question to ponder: can we improve this? </p>
<p>Yes, by repetition. Instead of using just one array, we set $d$ arrays (to be set latter) and choose $d$ hash functions independently. Then the probability that all estimations deviate more than $\epsilon |a|$ is at most<br>$$<br>\left( \frac{1}{\epsilon m} \right)^d<br>$$</p>
<p>When $d = \ln{\delta} / \ln \frac{1}{\epsilon m} = \ln{ \frac{1}{\delta} } / \ln \epsilon m$, the failure probability decreases to $\delta$. To minimize the space usage, we solve the following optimization problem:<br>$$<br>\begin{aligned}<br>    &amp;\min           &amp;\ln \frac{1} {\delta} \frac{m}{\ln m + \ln \epsilon} \<br>    &amp;\text{s.t. }   &amp;m \ge \frac{1}{\epsilon}<br>\end{aligned}<br>$$</p>
<p>Solving the equation gives $m = e \epsilon^{-1}$. The space consumption is $\frac{e}{\epsilon} \ln \frac{1}{\delta}$.</p>
<p>Note: suppose that $|a| = 10000$ and $\epsilon = 0.1$, $\delta = 0.01$, then with $10 \cdot e \ln 100 \approx 125$ words, we can estimate a particular $a_i$ with the absolute error less than $1000$. If $a_i = 9000$, the relative error is small. However, if $a_i = 1$, basically our estimation if rather inaccurate.</p>
<p>Question to ponder: what if we analyze this by Chernoff style inequality?</p>
<p>If we make $d$ repetition, then the confidence interval of the mean estimation is given by (Hoeffding’s inequality)<br>$$<br>|a|\sqrt{\frac{\log \frac{2}{\delta} }{2 d} }<br>$$<br>If we want this interval to be smaller than $\epsilon |a|$, then we need $d = \frac{ \log \frac{2}{\delta} }{2 \epsilon^2}$, which is a $\frac{1}{\epsilon }$ worse than the above analysis. Intuitively, we require stronger condition for concentration of mean value than that for existence of near-min value. </p>
<h1 id="Count-Sketch"><a href="#Count-Sketch" class="headerlink" title="Count-Sketch"></a>Count-Sketch</h1><p>One drawback of Count-Min Sketch is that the estimator constructed is one-sided and not unbiased. This can be fixed by introducing an additional function $g : U \rightarrow { - 1, 1 }$ that gives each elements in $U$ a sign (“+” or “-“) independently and uniformly at random.  Now, </p>
<ol start="2">
<li>For each element $s \in S$, we increase the counter $h(s)$ in the array by 1 if $g(s) &gt; 0$ or $-1$ if $g(s) &lt; 0$. </li>
<li>Finally, we set $b_i = A[h(i)] \cdot g(i)$ as the estimation of $a_i$. </li>
</ol>
<p>In expectation, each slot obtains $\frac{1}{m}$ fraction of the total mass $\sum_i a_i = |a|$. But as each element “flips” it sign randomly, the expected value obtained is 0. Now, conditioned on $i$-th element is hashed to the $h(i)$ slots, $\frac{1}{m}$ fraction of the rest mass $\sum_{j \neq i} a_j = n - a_i$ is hashed to the $h(i)$-th slot. With their signs flipped randomly, they contribute 0 to the $h(i)$-th slot. Hence,<br>$$<br>\mathbb{E}[b_i - a_i] = 0<br>$$<br>Formally, define the binary random variables $X_j$ for $j \neq i$ such that<br>$$<br>X_j = \begin{cases}<br>    a_j     &amp;\text{if} \quad h(i) = h(j) \wedge g(j) = g(i) \<br>    - a_j   &amp;\text{if} \quad h(i) = h(j) \wedge g(j) \neq g(i) \<br>    0       &amp;\text{otherwise}<br>\end{cases}<br>$$<br>Then by perfectness and independence of $h$ and $g$, </p>
<p>$$<br>    \Pr[X_j = a_j] = \frac{1}{2m} \<br>    \Pr[X_j = -a_j] = \frac{1}{2m} \<br>    \Pr[X_j = 0] = 1 - \frac{1}{m}<br>$$</p>
<p>and<br>$$<br>\begin{aligned}<br>    \mathbb{E}[X_j]<br>    &amp;= 0 \<br>    \mathbb{E}[b_i - a_i]<br>    &amp;= \mathbb{E}[\sum_{j \neq i} X_j]<br>    = \sum_{j \neq i} \mathbb{E}[X_j]<br>    = 0 \<br>    \mathbb{Var} \left[ X_j \right]<br>    &amp;= \mathbb{E} [X_j^2] - (\mathbb{E}[X_j])^2<br>    = \frac{a_j^2}{m} \<br>    \mathbb{Var} \left[ \sum_{j \neq i} X_j \right]<br>    &amp;= \sum_{j \neq i} \mathbb{Var} [X_j]<br>    = \sum_{j \neq i} \frac{a_j^2}{m}<br>    \le \frac{|a|_2^2}{m}<br>    \end{aligned}<br>$$</p>
<p>where $|a|_2^2$ is the $l_2$ norm of $a$. Note that the above analysis requires only pair independence. The random variable $X_j$ is two sided, therefore we can not use Markov Inequality for analysis. </p>
<p>Question to ponder: is it possible to analyze this directly by Chernoff style inequality? </p>
<p>The tool we resort to is Chebyshev’s inequality:<br>$$<br>\Pr \left[ |\sum_{j \neq i} X_j - 0| \ge \epsilon |a|<em>2 \right] \le \frac{\mathbb{Var} [\sum</em>{j \neq i} X_j]}{\epsilon^2 |a|_2^2 } \le \frac{1}{m\epsilon^2 }<br>$$<br>we take $m = k\epsilon^{-2}$ for some integer $k&gt; 2$. </p>
<p>How do we amplify the successful probability? We repeat the experiment $d$ times, and take the median. The only way that the median makes a mistake is when more than half of the estimations deviate more  $\epsilon |a|_2$.  Denote $Y$ the number of estimation such that the error is more than $\epsilon |a|_2$. By Chernoff,<br>$$<br>\begin{aligned}<br>    \Pr \left[ Y - \frac{d}{k}\ge \frac{d}{2} - \frac{d}{k} \right]<br>    &amp;= \Pr \left[ Y - \frac{d}{k}\ge (\frac{1}{2} - \frac{1}{k})\cdot k \cdot \frac{d}{k} \right] \<br>    &amp;\le \exp \big( -\frac{d}{3 k} (\frac{k}{2} - 1)^2  \big) \<br>    &amp;= \delta<br>\end{aligned}<br>$$</p>
<p>which solves to<br>$$<br>d= \frac{3k}{(k/2  -1)^2} \log \frac{1}{\delta}<br>$$<br>The overall space usage is therefore<br>$$<br>md = \frac{3k^2}{\epsilon^2 (k/2  -1)^2} \log \frac{1}{\delta}<br>= \frac{3}{\epsilon^2 (1/2  - \frac{1}{k} )^2} \log \frac{1}{\delta}<br>$$<br>Question to ponder: the expression is wired, which implies that $k \rightarrow \infty$, the space usage converges to $\frac{12}{\epsilon^2} \log \frac{1}{\delta}$, which is worse than count-min hash than a $\frac{1}{\epsilon}$ factor. </p>
<h4 id="Relation-between-a-1-and-a-2"><a href="#Relation-between-a-1-and-a-2" class="headerlink" title="Relation between $|a|_1$ and $|a|_2$."></a>Relation between $|a|_1$ and $|a|_2$.</h4><p>Their relation is given as follows (by concavity of the $\sqrt \cdot$ function)<br>$$<br>|a|_2 \le |a|_1 \le \sqrt{ |u| } |a|_2<br>$$<br>In particular, when $|u| = 2$, the inequality becomes<br>$$<br>|a_1 + a_2| \le \sqrt 2 \sqrt{a_1^2 + a_2^2 }<br>$$<br>When the dimension is higher than $2$, the factor in the RHS can not bounded to $\sqrt 2$. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/02/AM-GM-Inequality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/AM-GM-Inequality/" class="post-title-link" itemprop="url">AM-GM Inequality</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-02 10:48:17" itemprop="dateCreated datePublished" datetime="2019-11-02T10:48:17+11:00">2019-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-03 10:18:10" itemprop="dateModified" datetime="2019-12-03T10:18:10+11:00">2019-12-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The AM-GM inequality states that for $x_1, x_2, …, x_n &gt;0$ and $\lambda_1, \lambda_2, …, \lambda_n \ge 0$, such that $\sum_{i = 1}^n \lambda_i = 1$, we have<br>$$<br>\prod_{i} x_i^{\lambda_i} \le \sum_i \lambda_i x_i<br>$$<br>If particular, when $\lambda_i = \frac{1}{n}$, the inequality becomes<br>$$<br> \sqrt[^n]{\prod_i x_i} \le \frac{\sum_i x_i}{n}<br>$$<br>Replaying $x_i = \frac{1}{y_i}$, we obtain<br>$$<br> \frac{n}{\sum_i \frac{1}{y_i}} \le \sqrt[^n]{\prod_i y_i}<br>$$<br>Another famous case is $n = 2$ and $\lambda_1 = \frac{1}{p}$, $\lambda_2 = \frac{1}{q}$ and $x_1 = x^p$, $x_2 = y^q$, we get the Young’s inequality<br>$$<br>xy = (x^p)^\frac{1}{p} (y^q)^\frac{1}{q} \le \frac{x^p}{p} + \frac{y^q}{q}<br>$$<br>The proof of AM-GM inequality is rather simple and follows immediately from the concavity of $\log$ function:<br>$$<br>\sum_i \lambda_i \log x_i \le \log \sum_i \lambda_i x_i<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/31/Polynomials/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/31/Polynomials/" class="post-title-link" itemprop="url">Polynomials</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-31 21:38:36" itemprop="dateCreated datePublished" datetime="2019-10-31T21:38:36+11:00">2019-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-28 10:47:03" itemprop="dateModified" datetime="2020-11-28T10:47:03+11:00">2020-11-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Polynomials constitute of a rich class of functions. A polynomial in a single variable defined on a field $F$ is a function of the form:<br>$$<br>p(x) = a_d x^d + a_{d - 1} x^{d - 1} + … + a_1 x + a_0<br>$$<br>where the variable $x$ and coefficients $a_i$ belongs to $F$. The integer $d$ is called the degree of $p(x)$. </p>
<p>Polynomials have two fundamental properties:  </p>
<ol>
<li>A non-zero polynomial of degree $d$ has at most $d$ distinct roots. </li>
<li>Given $d + 1$ pairs $(x_i, y_i)$ ($1 \le i \le d + 1$), such that $x_i \neq x_j$ for $i \neq j$, there is a unique polynomial $p(x)$ with degree at most $d$ that goes through all these points. </li>
</ol>
<p>Let’s prove claim 2 first. If claim 1 holds, <em>existence</em> implies <em>uniqueness</em> for claim 2. Suppose for contradiction that there is another polynomial $q(x)$ such that $p(x) = q(x)$ for all $(x_i, y_i)$ pairs. Now consider the polynomial $r(x) = p(x) - q(x)$. If $r(x) \equiv 0$, then the claim holds trivially. Otherwise, $r(x)$ is a non-zero polynomial with degree at most $d$. Claim 1 asserts that $r(x)$ has at most $d$ distinct roots, contradicting that $r(x_i) = 0$ for $1 \le i \le d + 1$. </p>
<p>Question to ponder: does claim 2 implies claim 1? </p>
<h3 id="Property-2"><a href="#Property-2" class="headerlink" title="Property 2"></a>Property 2</h3><p>Now we return to the construction of a feasible for claim 2. The method is called Lagrange Interpolation. </p>
<p>First, we would like to construct polynomial $\delta_i(x)$, such that </p>
<ol>
<li>It has degree $d$. </li>
<li>$\delta_i(x_j) = 0$ for $j \neq i$. </li>
<li>$\delta_i(x_i) = 1$</li>
</ol>
<p>As it takes zero on $x_j, \forall j \neq i$, we can simply try a prototype $\delta_i’$ as:<br>$$<br>\delta_i’(x) = \prod_{j \neq i} (x - x_j).<br>$$</p>
<p>Observe that it has degree $d$. However, at point $x_i$, $\delta_i’(x_i)$ may not equal to 1. This issue is not hard to fix, we can just normalize it by $\delta_i’(x_i)$, and define </p>
<p>$$<br>\delta_i(x) = \frac{\delta_i’(x)}{\delta_i’(x_i)} = \frac{\prod_{j \neq i} (x - x_j) }{ \prod_{j \neq i} (x_i - x_j) } = \prod_{1 \le j \le d + 1, j \neq i} \frac{x - x_j}{x_i - x_j}.<br>$$</p>
<p>With proper scaling and summation, $p(x)$ is given by<br>$$<br>p(x) = \sum_{i = 1}^{d + 1} y_i \delta_i(x)<br>$$</p>
<p><em>Remark</em>: uniqueness can also be proved by showing the following equation has unique solution:<br>$$<br>\left[\begin{matrix}<br>x_1^d &amp; x_1^{d - 1} &amp; … &amp;  x_1 &amp; 1\<br>x_2^d &amp; x_2^{d - 1} &amp; … &amp;  x_2 &amp; 1\<br>…\<br>x_{d + 1}^d &amp; x_{d + 1}^{d - 1} &amp; … &amp;  x_{d + 1} &amp; 1<br>\end{matrix}\right]<br>\left[\begin{matrix}<br>a_d\<br>a_{d - 1} \<br>…\<br>a_0<br>\end{matrix}\right] =<br>\left[\begin{matrix}<br>y_1 \<br>y_{1} \<br>…\<br>y_{d + 1}<br>\end{matrix}\right] \<br>$$<br>The left matrix is called Vandermonde Matrix, whose determinant is<br>$$<br>\begin{aligned}<br>&amp;\det<br>\left[\begin{matrix}<br>x_1^d &amp; x_1^{d - 1} &amp; … &amp;  x_1 &amp; 1\<br>x_2^d &amp; x_2^{d - 1} &amp; … &amp;  x_2 &amp; 1\<br>…\<br>x_{d + 1}^d &amp; x_{d + 1}^{d - 1} &amp; … &amp;  x_{d + 1} &amp; 1<br>\end{matrix}\right]  \<br>=&amp;<br>\det<br>\left[\begin{matrix}<br>0&amp; 0 &amp; … &amp;  0 &amp; 1\<br>x_2^d - x_1 x_2^{d - 1} &amp; x_2^{d - 1} - x_1 x_2^{d - 2}&amp; … &amp;  x_2 - x_1 &amp; 1\<br>…\<br>x_{d + 1}^d - x_1 x_{d + 1}^{d - 1} &amp;  x_{d + 1}^{d - 1} - x_1 x_{d + 1}^{d - 2}&amp; … &amp;  x_{d + 1} - x_ 1&amp; 1<br>\end{matrix}\right]  \<br>=&amp;\prod_{i = 1}^{d + 1} (x_i - x_1) \det<br>\left[\begin{matrix}<br>x_2^d &amp; x_2^{d - 1} &amp; … &amp;  x_2 &amp; 1\<br>…\<br>x_{d + 1}^d &amp; x_{d + 1}^{d - 1} &amp; … &amp;  x_{d + 1} &amp; 1<br>\end{matrix}\right]  \<br> …\<br>=&amp; \prod_{i &lt; j} (x_j - x_i)<br>\end{aligned}<br>$$<br>which is non-zero when $x_i \neq x_j$. </p>
<h3 id="Property-1"><a href="#Property-1" class="headerlink" title="Property 1"></a>Property 1</h3><p>Finally, we prove claim 1: </p>
<ol>
<li>A non-zero polynomial of degree $d$ has at most $d$ distinct roots. </li>
</ol>
<p>The prove is by induction. </p>
<ol>
<li><p>The base case is $d = 0$. As the polynomial is non-zero, it does not have any root. </p>
</li>
<li><p>Suppose the claim holds for $d$. Then the claim holds for $d + 1$. If $p(x)$ has a root $a_{d + 1}$, by polynomial division, we can rewrite $p(x)$ as<br> $$<br> p(x) = (x - a_{d + 1}) q(x)<br> $$<br> and $q(x)$ has degree $d$. If for any $a \neq a_{d + 1}$, $p(a) = (a - a_{d +1}) q(a) = 0$, then $q(a) = 0$, which implies that $a$ is a root of $q(x)$. By applying the inductive hypothesis to $q(x)$, we see that it has at most $d$ distinct roots. </p>
</li>
</ol>
<h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><h4 id="Pairwise-Independent-Hash-Function"><a href="#Pairwise-Independent-Hash-Function" class="headerlink" title="Pairwise Independent Hash Function."></a>Pairwise Independent Hash Function.</h4><p>Consider a set of elements ${0, 1, …, u - 1}$ and the family of functions $\mathcal{H}$ defined on $[u - 1]$. Given $i \in [u - 1]$, if we take a function $h$ uniformly at random from $\mathcal{H}$, the value $Y_i = h(i)$ is a random variable. The $Y_i$’s are called pairwise independent, if for $i, j \in [u - 1], i \neq j$ and $y_i, y_j \in \cup_{h \in \mathcal{H}} h([u - 1])$,<br>$$<br>\Pr_{h \in \mathcal{H} } [Y_i = y_i, Y_j = y_j] = \Pr_{h \in \mathcal{H} } [Y_i = y_i] \Pr_{h \in \mathcal{H} } [Y_j = y_j]<br>$$</p>
<p>Similarly we can define $k$-wise independence. For any $0 \le i_1 &lt; i_2 &lt;… &lt; i_k &lt; u$, and $y_{i_1} , y_{i_2}, …, y_{i_k} \in \cup_{h \in \mathcal{H}} h([u - 1])$, we have<br>$$<br>\Pr_{h \in \mathcal{H} } [ Y_{i_1} = y_{i_1}, … Y_{i_k} = y_{i_k} ] = \prod_{j} \Pr_{h \in \mathcal{H} } [ Y_{i_j} = y_{i_j}]<br>$$</p>
<p>Now we restrict our discussion to the finite field $F_p = [p - 1] = {0, 1, …, p - 1}$, where $p$ is a prime larger than $u$. </p>
<p>Claim: The set of all possible functions from $[u - 1]$ to $[p - 1]$ is pairwise independent. </p>
<p>Pf:<br>$$<br>\begin{aligned}<br>&amp;\Pr[Y_i = a, Y_j = b] \<br>= &amp;\frac{p^{u - 2}}{p^u} \<br>= &amp;\frac{1}{p^2} \<br>= &amp;\Pr[Y_i = a] \cdot \Pr[Y_j = b]<br>\end{aligned}<br>$$</p>
<p>However, each of the functions takes $u \log p$ bits to represent. To save to space of representing the functions, we restrict to a smaller subset all possible functions:<br>$$<br>\mathcal{H_1} = { p(x) = a_1 x + a_0 \mid a_1, a_0 \in [p - 1] }<br>$$</p>
<p>That is, all polynomials with degree at most 1 in the field $[p - 1]$. By claim 2, we conclude that for any pair $(x_1, y_1), (x_2, y_2)$, where $x_1, x_2 \in [u - 1], x_1 \neq x_2$ and $y_1, y_2 \in [p - 1]$, there is a unique polynomial with degree at most 1 that goes through these two points. If we pick a polynomial uniformly at random from $\mathcal{H}$, then the probability is<br>$$<br>\frac{1}{p^2}<br>$$</p>
<p>This already implies pairwise independence, as<br>$$<br>\Pr[h(x_1) = y_1 ] = \sum_{y_2 = 0}^{p - 1} \Pr[h(x_1) = y_1, h(x_2) = y_2] = \frac{1}{p}<br>$$</p>
<p>In a similar manner, we can extend the idea to $k$-wise independent functions:<br>$$<br>\mathcal{H_{k - 1}} = { p(x) = a_{k - 1} x^{k - 1} + a_{k - 2} x^{k - 2} + … + a_1 x_1 + a_0 \mid a_i \in [p - 1] }<br>$$</p>
<p>$|\mathcal{H}| = p^{k}$ and there is a unique polynomial that goes through any pairs $(x_1, y_1), (x_2, y_2), …, (x_k, y_k)$ for $x_1 \neq x_2 \neq …\neq x_k$. </p>
<p>Another application is key-sharing. Suppose that we have $n$-candidates who knows some information about the key and we require that only when no less than $k$ of them aggragate their information can they figure the actual key. No group of less than $k$ candidates can achieve this. The way we do it is as follows:  </p>
<ol>
<li>Select a large prime number $p$  </li>
<li>Select $k+1$ random numbers $y_0, y_1, …, y_k$ from $[p-1]$.  </li>
<li>Generate the unique polynomial $p(x)$ with degree $k$ that goes through $(0, y_0), (1, y_1), …, (k, y_k)$.   </li>
<li>Inform the value of $p(1)$ to the first person, $p(2)$ to the second one, … and $p(n)$ to the $n$-th person. </li>
</ol>
<p>If $k - 1$ people pool their keys together, then there are $p$ possible polynomials that goes through all their keys, each has different value at point 0. The probability of guessing the correct key is given by $\frac{1}{p}$. </p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><p>CS 70. Discrete Mathematics and Probability Theory. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description">你生之前悠悠千載已逝，未來還會有千年沉寂的期待</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
