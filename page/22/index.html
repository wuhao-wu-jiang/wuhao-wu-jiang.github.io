<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/22/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/22/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/21/Median-of-Mean/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/21/Median-of-Mean/" class="post-title-link" itemprop="url">Median-of-Mean</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-06-21 21:23:19 / Modified: 21:54:42" itemprop="dateCreated datePublished" datetime="2020-06-21T21:23:19+10:00">2020-06-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose that we have an algorithm <span class="math inline">\(A\)</span> that estimates the mean <span class="math inline">\(E[Y]\)</span> of a random variable <span class="math inline">\(Y\)</span> within some specified range <span class="math inline">\(r\)</span> with probability <span class="math inline">\(\rho &gt; \frac{1}{2}\)</span>. We can derive a new algorithm <span class="math inline">\(A^*\)</span> from <span class="math inline">\(A\)</span> that boosts the successful probability to <span class="math inline">\(1 - \delta\)</span>, as follows:</p>
<blockquote>
<ol type="1">
<li>Repeat <span class="math inline">\(A\)</span> for <span class="math inline">\(m = \frac{1}{2 (\rho - 0.5)^2 } \ln \frac{1}{\delta}\)</span> times.</li>
<li>Take the median of the <span class="math inline">\(m\)</span> outputs.</li>
</ol>
</blockquote>
<p>To prove it, define the indicator random variable <span class="math display">\[
X_i = \begin{cases}
    1, \text{ if the } i \text{-th output of } A \text{ is within the range } r \\
    0, \text{otherwise}
\end{cases}
\]</span></p>
<p>for <span class="math inline">\(1 \le i \le m\)</span>. Then <span class="math inline">\(E[X_i] = \rho &gt; 0.5\)</span>. Let <span class="math inline">\(\mu = E \left[\sum_{i = 1}^m X_i \right] = m\rho\)</span>.</p>
<p>By Hoeffding's inequality, for $&gt; 0 $, <span class="math display">\[
\Pr \left[ \left| \sum_{i = 1}^m X_i - \mu \right| \ge \lambda \right] \le \exp \left(- \frac{2\lambda^2}{m} \right)
\]</span></p>
<p>Replacing <span class="math inline">\(\lambda\)</span> with <span class="math inline">\(\mu - 0.5m\)</span>, and <span class="math inline">\(m\)</span> with <span class="math inline">\(m = \frac{1}{2 (\rho - 0.5)^2 }\ln \frac{1}{\delta}\)</span>, we get <span class="math display">\[
\begin{aligned}
    \Pr \left[ \sum_{i = 1}^m X_i \le 0.5m \right] &amp;\le \Pr \left[ \left| \sum_{i = 1}^m X_i - \mu \right| \ge m(\rho - 0.5) \right] \\
    &amp;\le \exp \left(- 2m(\rho - 0.5)^2 \right) \\
    &amp;= \delta   
\end{aligned}
\]</span></p>
<p>Note that if more than half of <span class="math inline">\(A\)</span> outputs are correct, then the output of <span class="math inline">\(A^*\)</span> is correct. This happens with probability at least <span class="math inline">\(1 - \delta\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/18/Entropy-and-Message-Transmission/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/Entropy-and-Message-Transmission/" class="post-title-link" itemprop="url">Entropy and Message Transmission</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-18 20:26:03" itemprop="dateCreated datePublished" datetime="2020-06-18T20:26:03+10:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-01 11:06:27" itemprop="dateModified" datetime="2020-07-01T11:06:27+10:00">2020-07-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss another application that sheds light on meaning of entropy, the message transmission problem. In this model, a sender transmits a message to the receiver through a channel. In real world, the channel could be an optical fiber, a wireless channel, a hard disk etc. In the final application, the computer that writes and reads information from the hard disk is both the sender and the receiver.</p>
<p>In real world the channel is not perfect and the message transmitted is distorted by noise. It is natural to ask whether the message can be transmitted accurately under the noise, i.e., whether the receiver can recover the noisy message.</p>
<p>To study the problem, we need a mathematical model for both the channel and the noise. We focus on the simplest binary channel with bits of 0 and 1. The noise causes the bits to flip and is modelled by the distribution on the bit-flips. We study the simplest one that flips each bit independently with some identical probability <span class="math inline">\(p &lt; 0.5\)</span>. For a message with length <span class="math inline">\(n\)</span>, the number of bit flips follows a Binomial distribution <span class="math inline">\(B(n, p)\)</span>. We call such channel a <em>binary symmetric channel</em>.</p>
<p>To protect the message against noise, a naïve way is to send the each bit multiple times. For example, if the sender wants to send a bit 1, it sends <span class="math inline">\(10\)</span> copies of <span class="math inline">\(1\)</span> as <span class="math inline">\(1111111111\)</span>. The receiver decides that the bit sent is 1, if the majority of the <span class="math inline">\(10\)</span> bits it receives is 1.</p>
<p>Could it be improved? Repeating each bit may not be the mot efficient way against the noise. In general, if the sender wants to send a message of <span class="math inline">\(k\)</span> bits, it can convert it into a new message of <span class="math inline">\(n\)</span> bits and sends the new one. The receiver considers the <span class="math inline">\(n\)</span> bits received as a whole, and try to recover the <span class="math inline">\(k\)</span>-bit message the sender wants to send.</p>
<p>We call the method used by the sender to convert the original message the <em>encoding function</em>, and the one used by the receiver to recover the message the <em>decoding function.</em></p>
<p><strong><em>Definition.</em></strong> A <span class="math inline">\((k, n)\)</span> encoding function <span class="math inline">\(\text{Enc}: \{0, 1\}^k \rightarrow \{0, 1\}^n\)</span> and a <span class="math inline">\((k, n)\)</span> decoding function <span class="math inline">\(\text{Dec}: \{0, 1\}^n \rightarrow \{0, 1\}^k\)</span>, where <span class="math inline">\(n \ge k\)</span>.</p>
<p>Since the noise flips the bits randomly, it is impossible to have an encoding function and a decoding function without error (unless <span class="math inline">\(p = 0\)</span>). Instead, we aim to control the probability of error within some specified threshold <span class="math inline">\(\delta\)</span>. To achieve this, we add redundancy to the message and encode a <span class="math inline">\(k\)</span>-bit one into an <span class="math inline">\(n\)</span>-bit one. Now, <span class="math inline">\(n - k\)</span> is the amount of redundancy introduced. We would like to make <span class="math inline">\(n - k\)</span> as small as possible. On the other hand, the value of <span class="math inline">\(n-k\)</span> should positively related to <span class="math inline">\(p\)</span>. The larger <span class="math inline">\(p\)</span> is, the noisier the channel is and the larger <span class="math inline">\(n - k\)</span> should be.</p>
<p>For fixed <span class="math inline">\(\delta\)</span> and <span class="math inline">\(p\)</span>, the <em>Shannon's Theorem</em> says that the smallest possible value of <span class="math inline">\(n - k\)</span> we can achieve is roughly <span class="math inline">\(n\mathbf{H}(p)\)</span>. Intuitively, <span class="math inline">\(\mathbf{H}(p)\)</span> is the amount of randomness the noise exerts on each bit. Therefore <span class="math inline">\(1 - \mathbf{H}(p)\)</span> is the maximum amount of information we can transmit by one bit.</p>
<p>Before we proceed, we prove the following lemma.</p>
<p><strong><em>Lemma 1</em></strong><br />
For <span class="math inline">\(q \in \mathbb{R}, q \le 1 / 2\)</span> and <span class="math inline">\(n \in \mathbb{N}\)</span>, it hold that <span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor q n \rfloor} \binom{n}{i} 
    &amp;\le 2^{ n \mathbf{H} ( q)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p><strong><em>Proof.</em></strong><br />
<span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor q n \rfloor} \binom{n}{i} 
    &amp;\le \frac{n + 1}{2} \binom{n }{\lfloor q n \rfloor }  \\
    &amp;\le \frac{n + 1}{2} 2^{ n \mathbf{H} \left( { \lfloor q n \rfloor } / { n } \right) } \\
    &amp;\le 2^{ n \mathbf{H} ( q)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p>The first inequality follows from that <span class="math inline">\(\binom{n}{i}\)</span> is increasing for <span class="math inline">\(i \le n / 2\)</span> and that the summation is over at most <span class="math inline">\((n + 1) /2\)</span> terms. The last one follows from that <span class="math inline">\(\mathbf{H}(x)\)</span> is increasing for <span class="math inline">\(x \le 1 / 2\)</span> and that <span class="math inline">\({ \lfloor q n \rfloor } / { n } \le { q n } / { n } = q &lt; 1/2\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Shannon's Theorem</em></strong><br />
Given a binary symmetric channel with flip probability <span class="math inline">\(p &lt; 1 / 2\)</span> and for any any <span class="math inline">\(\epsilon, \delta &gt; 0\)</span>, when <span class="math inline">\(n\)</span> is large enough,</p>
<ol type="1">
<li>if <span class="math inline">\(k \le n (1 - \mathbf{H}(p) - \epsilon)\)</span>, there exist <span class="math inline">\((k, n)\)</span> encoding and decoding functions such that the receiver fails to obtain the correct message with probability at most <span class="math inline">\(2\delta\)</span>.<br />
</li>
<li><span class="math inline">\(\nexists (k, n)\)</span> encoding and decoding functions with <span class="math inline">\(k \ge n (1 - \mathbf{H}(p) + \epsilon)\)</span> such that the decoding is correctly is at most <span class="math inline">\(\delta\)</span> for a <span class="math inline">\(k\)</span>-bit input message chosen uniformly at random.</li>
</ol>
<p><strong><em>Proof of 1.</em></strong></p>
<p><strong>Intuition.</strong> When an <span class="math inline">\(n\)</span>-bit message <span class="math inline">\(s\)</span> is transmitted through the channel, the number of flipped bits is roughly <span class="math inline">\(np\)</span>. As <span class="math inline">\(p &lt; {1} / {2}\)</span>, we can find a <span class="math inline">\(\lambda &gt; 0\)</span> such that <span class="math inline">\(p + \lambda &lt; {1} / {2}\)</span>. Let <span class="math inline">\(R\)</span> be the received <span class="math inline">\(n\)</span>-bit message and define <span class="math inline">\(d(s, R)\)</span> the number of different bits between <span class="math inline">\(s\)</span> and <span class="math inline">\(R\)</span>, i.e., the Hamming distance between <span class="math inline">\(s\)</span> and <span class="math inline">\(R\)</span>. Given <span class="math inline">\(s\)</span> and <span class="math inline">\(p\)</span>, let <span class="math inline">\(\mathfrak{D}(s, p)\)</span> be the distribution of <span class="math inline">\(R\)</span> on <span class="math inline">\(\{0, 1\}^n\)</span>.</p>
<p>By Hoeffding's inequality, it holds that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s, p) } [ |d(s, R) - pn | \ge \lambda n] \le \exp(- 2 \lambda^2 n).
\]</span></p>
<p>Define the <span class="math inline">\((p + \lambda)n\)</span> ball centered <span class="math inline">\(s\)</span> as <span class="math display">\[
B_{s, (p + \lambda)n} \doteq \{ r \in \{0, 1\}^n : d(s, r) &lt; (p + \lambda)n \}.
\]</span></p>
<p>The the Hoeffding's inequality implies that for large enough <span class="math inline">\(n\)</span>, we have <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s, p) } [ R \notin B_{s, (p + \lambda)n}] \le \delta.
\]</span></p>
<p>That is, with probability at most <span class="math inline">\(\delta\)</span>, the received message fall outside the ball <span class="math inline">\(B_{s, (p + \lambda)n}\)</span>. This motivates to decode each message in <span class="math inline">\(B_{s, (p + \lambda)n}\)</span> as the original message the sender wants to send.</p>
<p>Further, by Lemma 1, the size of <span class="math inline">\(B_{s, (p + \lambda) n}\)</span> is bounded by <span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor (p + \lambda) n \rfloor} \binom{n}{i} 
    &amp;\le 2^{ n \mathbf{H} ( p + \lambda)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p>Therefore, the maximum number of non-overlapped balls we can pack into the message space <span class="math inline">\(\{0, 1\}^n\)</span> is<br />
<span class="math display">\[
\frac{2^n}{ 2^{ n \mathbf{H} ( p + \lambda)  + \log \frac{n + 1}{2} }  } = 2^{ n (1 - \mathbf{H} ( p + \lambda) - \frac{1}{n} \log \frac{n + 1}{2})   }
\]</span></p>
<p>When <span class="math inline">\(n\)</span> is large enough, this is at least <span class="math inline">\(2^{ n (1 - \mathbf{H} ( p ) - \epsilon) }\)</span>.</p>
<p><em>Remark: Hoeffding's inequality is an overkill. Chebyshev's inequality suffices for this proof.</em></p>
<p><strong>Designing Encoding and Decoding Functions.</strong></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Balls.png" /></p>
<p>This motivates one to design of encoding and decoding function as follows: find a set of messages <span class="math inline">\(s_1, s_2, ..., s_{2^k} \in \{0, 1\}^n\)</span>, such that <span class="math display">\[
B_{s_1, (p + \lambda)n}, B_{s_2, (p + \lambda)n}, ..., B_{s_{2^k} , (p + \lambda)n}
\]</span></p>
<p>are disjoint balls, and assign the messages in <span class="math inline">\(\{0, 1\}^k\)</span> to the ones in <span class="math inline">\(\{ s_i \}&#39;s\)</span> one by one. If the sending want to send <span class="math inline">\(i\)</span>, it sends <span class="math inline">\(s_i\)</span>. On receiving a message <span class="math inline">\(r\)</span>, the receiver determines which ball <span class="math inline">\(r\)</span> belongs to. The probability of decoding error is at most <span class="math inline">\(\delta\)</span>.</p>
<p><em>Question to ponder: can we find such a set efficiently, such that</em><br />
<span class="math display">\[
B_{s_1, (p + \lambda)n}, B_{s_2, (p + \lambda)n}, ..., B_{s_{2^k} , (p + \lambda)n}
\]</span> <em>are disjoint?</em></p>
<p><strong>Finding the Codewords.</strong></p>
<p>It is left to find a set of satisfactory <span class="math inline">\(s_1, s_2, ..., s_{2^k} \in \{0, 1\}^n\)</span>. The method we show here does not find a set of non-overlapped balls. Instead it aims to find a set such that</p>
<blockquote>
<p>if <span class="math inline">\(s_i\)</span> is transmitted, then the probability received message <span class="math inline">\(r\)</span> falls into the another ball, i.e., <span class="math inline">\(r \in B_{s_{j} , (p + \lambda)n}\)</span> (<span class="math inline">\(j \neq i\)</span>), is less than <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The property implies that the overlap between <span class="math inline">\(B_{s_{i} , (p + \lambda)n}\)</span> and <span class="math inline">\(\cup_{j \neq i} B_{s_{j} , (p + \lambda)n}\)</span> is less than <span class="math inline">\(\delta\)</span> (measured by probability). To formalize the statement, define the random variable <span class="math inline">\(S\)</span> to be the message sent and <span class="math inline">\(R\)</span> to be the one received. Given that <span class="math inline">\(S = s_i\)</span> is sent, the conditional probability of receiving <span class="math inline">\(R = r\)</span> is <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] = p^{d(s_i,r)} (1 - p)^{1 - d(s_i, r)}
\]</span></p>
<p>To goal is to prove that it holds for all <span class="math inline">\(i\)</span>, <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] = \sum_{r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n} } p_r \le \delta
\]</span></p>
<p>Then , by union bound, it holds that <span class="math display">\[
\begin{aligned} 
\Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \text{ is not recovered correctly} \mid S = s_i] 
    &amp;= \Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \notin B_{s_i, (p + \lambda)n}  \vee  R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  \mid S = s_i] \\
    &amp;\le \Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \notin B_{s_i, (p + \lambda)n}   \mid S = s_i]  + \Pr[ R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  \mid S = s_i] \\
    &amp;\le 2 \delta
\end{aligned}
\]</span> That is, <span class="math inline">\(s_i\)</span> is transmitted and decoded correctly with probability at least <span class="math inline">\(1 - 2 \delta\)</span>.</p>
<p>In what follows, we will prove the following claim:</p>
<p><strong>Claim 1</strong>. <em>If we generate <span class="math inline">\(2^{k + 1}\)</span> codewords uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>, then in expectation</em> <span class="math display">\[
\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} }  \left[ \sum_{i = 1}^{2^{k + 1} }  \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \le  2^k \delta
\]</span></p>
<p>Claim 1 implies that there exits a set of codewords <span class="math inline">\(s_1, s_2, ..., s_{2^{k + 1} }\)</span> with <span class="math display">\[
\sum_{i = 1}^{2^{k + 1} } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]    \le 2^k \delta
\]</span></p>
<p>If we send each <span class="math inline">\(s_i\)</span> with probability <span class="math inline">\(1 / 2^k\)</span>, then the expected error probability is already <span class="math inline">\(\delta\)</span>. But we can have a stronger result: we can find a set of <span class="math inline">\(2^k\)</span> codewords, such that for each <span class="math inline">\(i\)</span>, <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \le 2 \delta
\]</span></p>
<p>W.L.O.G., suppose that <span class="math inline">\(s_1, s_2, ..., s_{2^k}\)</span> are the ones with the smallest <span class="math inline">\(\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]\)</span> 's, it must be that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]  \le \frac{2^k \delta }{2^k } = \delta
\]</span></p>
<p><strong><em>Proof of Claim 1</em></strong>. By Lemma 1, for a fixed <span class="math inline">\(r\)</span>, the number of strings <span class="math inline">\(s \in \{0, 1\}^n\)</span> such that <span class="math inline">\(d(s, r) &lt; (p + \lambda) n\)</span> is bounded by <span class="math display">\[
\begin{aligned}
    \sum_{k = 0}^{\lfloor (p + \lambda) n \rfloor} \binom{n}{k} 
        &amp;\le \frac{n + 1}{2} 2^{ n \mathbf{H} ( p + \lambda) }
\end{aligned}
\]</span></p>
<p>If a message <span class="math inline">\(s\)</span> is picked uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>, the probability such that <span class="math display">\[
\Pr_{s \sim \mathbf{U}(\{0, 1\}^n) } [d(s, r) &lt; (p + \lambda) n ] \le \frac{n + 1}{2^{n + 1} } 2^{ n \mathbf{H} \left(  p + \lambda \right) }
\]</span></p>
<p>By union bound,<br />
<span class="math display">\[
\begin{aligned}
    \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] 
    &amp;&lt; 2^k \frac{n + 1}{2^{n + 1} } 2^{ n \mathbf{H}( p + \lambda) }  \\
    &amp;= 2^{ k + \log (n + 1) +  n \mathbf{H} ( p + \lambda) - n - 1 } 
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(k \le n(1 - \mathbf{H}(p) - \epsilon)\)</span>, when <span class="math inline">\(n\)</span> is sufficient large, it follows<br />
<span class="math display">\[
\begin{aligned}
     \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] 
        &lt; 2^{ \log \frac{n + 1}{2} +  n ( \mathbf{H} ( p + \lambda) - \mathbf{H} ( p) - \delta) } 
        \le \frac{\delta}{2}
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    &amp;\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} } \left[ \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \\
    &amp;= \sum_{r \in \{0, 1\}^n }     \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] \cdot  \Pr_{R \sim \mathfrak{D}(s_i, p) } [ R = r \mid S = s_i] \\
    &amp;\le \frac{\delta}{2}
\end{aligned}
\]</span></p>
<p>and <span class="math display">\[
\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} }  \left[ \sum_{i = 1}^{2^{k + 1} }  \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \le 2^{k + 1} \frac{\delta}{2} = 2^k \delta
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Proof of 2.</em></strong></p>
<p>The difficulty here is to prove that the claim holds for arbitrary <span class="math inline">\((k, n)\)</span> encoding and decoding functions. The proof relies on an important property:</p>
<blockquote>
<p>the decoding function <span class="math inline">\(\text{Dec}\)</span> is a function on <span class="math inline">\(\{0, 1\}^n\)</span></p>
</blockquote>
<p>For any <span class="math inline">\(r \in \{0, 1\}^n\)</span>, there is a unique output <span class="math inline">\(\text{Dec}(r) \in \{0, 1\}^k\)</span>. The decoding function needs to decide the unique message <span class="math inline">\(\{0, 1\}^n\)</span> the sender want to send.</p>
<p>Now, define <span class="math display">\[
S_i \doteq \{ r \in \{0, 1\}^n :  r \text{ is decoded correctly if } s_i \text{ is sent}   \}
\]</span></p>
<p>The <span class="math inline">\(S_i\)</span>'s are non-overlapped and we have, <span class="math display">\[
\cup_{i = 1}^{2^k} S_i \subset \{0, 1 \}^n
\]</span></p>
<p>If <span class="math inline">\(s_i\)</span> is sent, then by Hoeffding inequality, with probability at least <span class="math inline">\(1 - \exp(- 2\lambda^2 n)\)</span>, the received message <span class="math inline">\(R\)</span> is likely to fall into a ring centered at <span class="math inline">\(s_i\)</span>: <span class="math display">\[
\text{ring} (s_i) \doteq \{ r \in \{0, 1\}^n : |d(r, s_i) - pn |&lt; \lambda n \}
\]</span></p>
<p>Finally, if the messages <span class="math inline">\(s_1, s_2, ..., s_{2^k}\)</span> are sent uniformly at random, i.e., <span class="math inline">\(i \sim \mathbf{U}[1, 2^k]\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[ R \text{ is decoded correctly}]   &amp;=  \sum_{i = 1}^{2^k} \sum_{r \in S_i } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \Pr_{i \sim \mathbf{U}[1, 2^k]} [ S = s_i] \\ 
        &amp;=  \frac{1}{2^k} \sum_{i = 1}^{2^k} \sum_{r \in S_i } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] 
\end{aligned}
\]</span></p>
<p>Observed that <span class="math inline">\(S_i \subset \text{ring} (s_i) \cup (S_i \cap \text{ring} (s_i) )\)</span>, we get</p>
<p><span class="math display">\[
\begin{aligned}
 &amp;\Pr[ R \text{ is decoded correctly}] \\
        &amp;\le \frac{1}{2^k}  \sum_{i = 1}^{2^k} \left( \sum_{r \notin \text{ring} (s_i)  } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i]  +\sum_{r \in S_i \cap \text{ring} (s_i) } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \right) \\ 
        &amp;\le \frac{1}{2^k}  \sum_{i = 1}^{2^k} \left( \exp( -2\lambda^2n) +\sum_{r \in S_i \cap \text{ring} (s_i) } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \right) 
\end{aligned}
\]</span></p>
<p>Note that for <span class="math inline">\(r \in S_i \cap \text{ring} (s_i)\)</span>, as <span class="math inline">\(p &lt; 1 / 2\)</span>, it holds that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \le p^{ \lceil(p - \lambda) n \rceil} (1 - p)^{ n - \lceil(p - \lambda) n \rceil }
\]</span></p>
<p>We obtain</p>
<p><span class="math display">\[
\begin{aligned}
        &amp;\Pr[ R \text{ is decoded correctly}] \\
        &amp;\le \exp( -2\lambda^2n) +  \frac{1}{2^k}  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } p^{ \lceil(p - \lambda) n \rceil} (1 - p)^{ n - \lceil(p - \lambda) n \rceil } \\
        &amp;\le \exp( -2\lambda^2n) +  \frac{1}{2^k}  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } p^{ (p - \lambda) n } (1 - p)^{ n - (p - \lambda) n  } \\
        &amp;= \exp( -2\lambda^2n) +  p^{ (p - \lambda) n } (1 - p)^{ n - (p - \lambda) n  }  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } \frac{1}{2^k}  \\
        &amp;= \exp( -2\lambda^2n) +   p^{ pn } (1 - p)^{ n - pn } \left(\frac{ 1 - p}{ p} \right)^{\lambda n} 2^{n - k}\\
        &amp;= \exp( -2\lambda^2n) +  2^{n - k - n \mathbf{H}(p)}  \left(\frac{ 1 - p}{ p} \right)^{\lambda n}
\end{aligned}
\]</span></p>
<p>Conditioning on that <span class="math inline">\(k \ge n (1 - \mathbf{H}(p) + \delta)\)</span>, <span class="math display">\[
\Pr[ R \text{ is decoded correctly}] \le \exp( -2\lambda^2n) +  2^{- n (\delta + \lambda \log \frac{ 1 - p}{ p} ) }  
\]</span></p>
<p>By choosing sufficient small <span class="math inline">\(\lambda\)</span> and large enough <span class="math inline">\(n\)</span>, <span class="math inline">\(\Pr[ R \text{ is decoded correctly}]\)</span> can be arbitrary small.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h2 id="reference">Reference</h2>
<p>[1] M. Mitzenmacher and E. Upfal, Probability and computing: an introduction to randomized algorithms and probabilistic analysis. New York: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/07/Entropy-and-Compression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/07/Entropy-and-Compression/" class="post-title-link" itemprop="url">Entropy and Compression</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-07 12:24:22" itemprop="dateCreated datePublished" datetime="2020-06-07T12:24:22+10:00">2020-06-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-11 23:49:47" itemprop="dateModified" datetime="2021-07-11T23:49:47+10:00">2021-07-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>One interpretation of entropy of a random variable is as a measure of how many unbiased, independent random bits in expectation we can extract from the random variable.<br>Another one comes from compression of a sequence.</p>
<p>Assume that the sequence studied (denoted as $s$) is a binary one.<br>It consists of a concatenation of the outcomes of $n$ independent Bernoulli random variables.<br>Without lose of generality, we assume that each bit comes up $1$ with probability $p \le  1 / 2$.<br>This is one of the simplest models of a sequence. </p>
<p>The fact that sequence could be a biased one makes it possible to represent it by a new sequence with shorter length in expectation.<br>For example, suppose that $p = \frac{1}{4}$, then for a pair of consecutive bits, it have   </p>
<ol>
<li>probability $\frac{1}{16}$ of being $11$,   </li>
<li>probability $\frac{3}{16}$ of being $01$, </li>
<li>probability $\frac{3}{16}$ of being $10$,</li>
<li>probability $\frac{9}{16}$ of being $00$.  </li>
</ol>
<p>If we use $111$ to represent $11$, $110$ to represent $01$, $10$ to represent $10$, and $0$ to represent $00$,  then the expected number of representation bits per pair is </p>
<script type="math/tex; mode=display">
\begin{array}{r}
    3 \cdot \frac{1}{16} + 3 \cdot \frac{3}{16} + 2 \cdot  \frac{3}{16} + 1 \cdot  \frac{9}{16} 
    = \frac{27}{16} 
    < 2
\end{array}</script><p>In general, compression is not limited to the way described above.<br>We call the rule with which we compress a string <em>a compression function</em>. </p>
<h1 id="Compression-Function"><a href="#Compression-Function" class="headerlink" title="Compression Function"></a>Compression Function</h1><blockquote>
<p><strong>Definition.</strong> Let $S = \{0, 1\}^n$ be the set of binary sequence of length $n$, and $T = \{ 0, 1\}^+$ the set of binary sequence of any positive length. A compression function $h: S \rightarrow T$ is an injective (one-to-one) function from $S$ to $T$.    </p>
</blockquote>
<p>In other words, each $s \in S$ is assigned a unique non-empty sequence (of arbitrary length) by $h$. </p>
<p>Observe that the size of $S$ is $2^n$. On the other hand, the number of sequences with length less than $n$ is $\sum_{i = 1}^{n - 1} 2^{i} = 2^n - 1 &lt; 2^n$.<br>For any $h$, it is impossible for $h$ to map every $s \in S$ to a sequence with length less than $n$.<br>Under adversarial input, $h$ can not compress at all.</p>
<p>The hope is that when there is a distribution $\mathfrak{D}$ on $S$, the expected length of the compressed sequences would be small: </p>
<script type="math/tex; mode=display">
    \mathbf{E} [ |h(s)| ] = \sum_{s \in S} p_s \cdot |h(s)|</script><p>To illustrate the meaning of entropy, we study a simple case where $\mathfrak{D}$ is the joint distribution of $n$ <em>i.i.d.</em> Bernoulli random variables that come up $1$ with probability $p \le { 1 / 2 }$ (by symmetry, the claim holds for the case of $p &gt; { 1 / 2 }$).</p>
<p>The following theorem formalizes how good a compression function we can find. </p>
<h2 id="Entropy-as-Lower-and-Upper-Bounds"><a href="#Entropy-as-Lower-and-Upper-Bounds" class="headerlink" title="Entropy as Lower and Upper Bounds"></a><strong>Entropy as Lower and Upper Bounds</strong></h2><blockquote>
<p><strong>Theorem.</strong></p>
<ol>
<li>$\forall \delta &gt; 0$, $\exists$ a compression $h$ and integer $N &gt; 0$, s.t., $\forall n \ge N$, it holds that <script type="math/tex; mode=display">
 \mathbf{E}[ |h(s)| ] \le (1 + \delta) n \mathbf{H}(p)</script></li>
<li>$\forall \delta &gt; 0$, $\exists$ integer $N &gt; 0$, s.t., $\forall n \ge N$, it holds that for any compression $h$, <script type="math/tex; mode=display">
 \mathbf{E}[ |h(s)| ] \ge (1 - \delta) n \mathbf{H}(p)</script></li>
</ol>
</blockquote>
<p><em>Intuitively, the entropy $\mathbf{H}(p)$ is the measure of the average number of bits in expectation we need after compression to represent a Bernoulli random variable that comes up $1$ with probability $p$.</em> </p>
<p><em>The high level idea of the proof is that, with high probability, the number of ones in a $s \in S$ is roughly $np$. There are roughly $2^{n\mathbf{H}(p)}$ such sequences. Therefore, we can use $n \mathbf{H}(p)$ bits to represent each sequence.</em></p>
<p><strong>Proof.</strong> The claim is trivially true if $p = { 1 / 2 }$. Otherwise, $p &lt; { 1 / 2 }$. We can pick some $\epsilon &gt; 0$, such that $p + \epsilon &lt; { 1 / 2 }$. Further, if $n$ is large enough,  $np + n\epsilon \le n / 2 - 1$. Hence, we may assume that $\lceil np + n\epsilon \rceil \le n / 2$ . </p>
<p>Denote $Z$ be the number of ones in the string $s$. By Hoeffding Inequality, it holds that </p>
<script type="math/tex; mode=display">
    \Pr[ |Z - np| \ge n\epsilon ] \le \exp(-2n \epsilon^2)</script><p>By the fact that $Z$ takes only integer values, we have </p>
<script type="math/tex; mode=display">
    \Pr[Z \ge \lceil np + n\epsilon \rceil] \le \exp(-2 n \epsilon^2 )</script><p>We use the first bit output of $h$ as a flag. For a string $s$ with $Z \ge \lceil np + n\epsilon \rceil$, we set the first bit to $0$ and then output the same sequence, i.e., $h(s) = 0 \circ s$. In such case we do not compress the string at all and use $n + 1$ bits to represent it. </p>
<p>We set the first bit output to $1$ if $Z &lt; \lceil np + n \epsilon \rceil \le n / 2$.  The number of such sequences is bounded by</p>
<script type="math/tex; mode=display">
    \sum_{k = 0}^{\lceil np + n\epsilon \rceil - 1} \binom{n}{k} \le  \frac{n}{2} \binom{n}{\lceil np + n\epsilon \rceil - 1 } \le \frac{n}{2} 2^{ n \cdot \mathbf{H} \left( \frac{ \lceil np + n\epsilon \rceil - 1 }{n} \right) } \le \frac{n}{2} 2^{ n \mathbf{H} \left( p + \epsilon  \right) }</script><p>The first inequality holds because $\binom{n}{k}$ increases for $k &lt; n / 2$ and that the summation is over $\lceil np + n \epsilon \rceil \le n / 2$ terms. The last inequality holds since $(\lceil np + n\epsilon \rceil - 1) / n \le ( np + n\epsilon ) / n = p + \epsilon &lt; 1 / 2$ and $\mathbf{H}( \cdot )$ is a increasing function in the range $[0, 1/2]$. </p>
<p>We compress these sequences by representing each of them with a unique sequence of at most  </p>
<script type="math/tex; mode=display">
\begin{array}{rl}
    \log \left[ (n / 2) \cdot  2^{ n \cdot\mathbf{H} \left( p + \epsilon  \right) } \right]
    = n \cdot\mathbf{H} \left( p + \epsilon  \right)  + \log n -1
\end{array}</script><p>bits. Considering the leading flag bit 1 (to indicate the sequence is a compressed one), we output at most $n \mathbf{H} \left( p + \epsilon  \right)  + \log n$ bits for sequences with $Z &lt; \lceil np + n \epsilon \rceil$. This can be written as $\left[ \mathbf{H} \left( p + \epsilon  \right)  + (1 / n) \cdot (\log n - 1) \right] n$ bits and is smaller than </p>
<script type="math/tex; mode=display">
(1 + \delta /{2} ) \cdot \mathbf{H} (p) \cdot n</script><p>is $\epsilon$ is smaller enough and $n$ is large enough.</p>
<p>We are almost done with the proof. It is left to consider sequences with $Z \ge \lceil np + n \epsilon \rceil$. For those sequence, we don’t compress them and and set the leading flag bit to 0. The outputs for those sequences consist of at most $1 + n$ bits. However, by Hoeffding inequality, such sequences do not appear frequently and the expected output length can be made arbitrary small.</p>
<p>Specifically, $\forall \delta &gt; 0$, we can find large enough $N$, such that $\forall n &gt; N$, it holds  </p>
<script type="math/tex; mode=display">
    (n + 1 ) \cdot \exp(-2 n \epsilon^2 ) \le n \cdot (\delta / 2)  \cdot  \mathbf{H}(p)</script><p>Then, in expectation, the bits outputted by $h$ is at most</p>
<script type="math/tex; mode=display">
\begin{aligned}
    &(n + 1) \cdot \exp(-2 n \epsilon^2 ) + [n \cdot \mathbf{H} \left( p + \epsilon  \right)   + \log n ] \cdot [ 1 - \exp(-2 n \epsilon^2 ) ] \\
    \le& n \cdot (\delta / 2)  \cdot  \mathbf{H}(p) + [ n \cdot \mathbf{H} \left( p + \epsilon  \right)   + \log n ]\\
    \le& n \cdot (\delta / 2)  \cdot  \mathbf{H}(p)  +  (1 + \delta /{2} ) \cdot \mathbf{H} (p) \cdot n\\
    \le& \left(  1 + \delta \right) \cdot \mathbf{H}(p) \cdot n
\end{aligned}</script><p>To prove the lower bound, first we need the following lemma.</p>
<blockquote>
<p><strong>Lemma.</strong> <em>for $s_1, s_2 \in S$, if $s_1$ has more ones than $s_2$, i.e., $\Pr(s_1) \ge \Pr(s_2)$, then the $h$ that minimizes the expected output length $\mathbf{E}[ |h(s)| ]$ should assign $s_1$ a sequence at most as long as $s_2$, i.e., $|h(s_1)| \le |h(s_2)|$.</em> </p>
</blockquote>
<p><strong>Proof</strong>. Otherwise, if we swap the output sequences of $h(s_1)$ and $h(s_2)$, we lower value of  $\mathbf{E}[ |h(s)| ]$.  </p>
<p>$\square$</p>
<p>Further, consider the number of $s \in S$ with $\lfloor np - n \epsilon \rfloor$ ones, </p>
<script type="math/tex; mode=display">
\begin{aligned}
\binom{n}{\lfloor np - n\epsilon \rfloor } 
    &\ge \frac{1}{n + 1} 2^{ n \cdot \mathbf{H} \left( { \lfloor np - n\epsilon \rfloor } / { n} \right) } \\
    &\ge \frac{1}{n + 1} 2^{ n \cdot \mathbf{H}( { (np - n\epsilon - 1) } / {n} )} \\
    &\ge 2^{ \lfloor n \cdot \mathbf{H}( p - \epsilon -1 / n) -  \log (n + 1)  \rfloor }
\end{aligned}</script><p>Let $k = \lfloor n \cdot \mathbf{H}( p - \epsilon -1 / n) - \log (n + 1)  \rfloor$. For large enough $n$, it holds that </p>
<script type="math/tex; mode=display">
k \ge (1 - \delta / 2) \cdot n \cdot \mathbf{H}(p)</script><p>Further, since</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{  k - 1  } 2^i \le \sum_{i = 0}^{  k - 1  } 2^i = 2^{  k } - 1 < 2^k,</script><p>there are strictly less than $2^k$ distinct binary sequences with length at most $k - 1$. </p>
<p>Therefore, for the set of $s \in S$ with $\lfloor np - n\epsilon \rfloor$ ones, at least one of them has output length at least $k$. </p>
<p>By the previous lemma, all sequences with more than $\lfloor np - n\epsilon \rfloor$ ones has length at least $k$. </p>
<p>Denote $Z$ be the number of ones in the string $s$. By Hoeffding Inequality, it holds that </p>
<script type="math/tex; mode=display">
\Pr[ |Z - np| \ge n\epsilon ] \le \exp(-2n \epsilon^2)</script><p>By the fact that $Z$ takes only integer values, we have </p>
<script type="math/tex; mode=display">
\Pr[Z \le \lfloor np - n\epsilon \rfloor] \le \exp(-2 n \epsilon^2 )</script><p>The expected output length of the sequences with more than $\lfloor np - n\epsilon \rfloor$ ones is at least </p>
<script type="math/tex; mode=display">
\begin{aligned}
    &[ 1 - \exp(-2n\epsilon^2) ] \cdot k \\
    =&[ 1 - \exp(-2n\epsilon^2) ] \cdot  (1 - \delta / 2) \cdot n \cdot \mathbf{H}(p)
\end{aligned}</script><p>This is at least $(1 - \delta) \cdot n \cdot \mathbf{H}(p)$ when $n$ is large enough. </p>
<p>$\blacksquare$</p>
<h2 id="Huffman-Code"><a href="#Huffman-Code" class="headerlink" title="Huffman Code"></a><strong>Huffman Code</strong></h2><p>In this section, we show that the upper bound can be achieved by Huffman code. </p>
<blockquote>
<p><em>$\forall \delta &gt; 0$, $\exists$ a compression $h$ and integer $N &gt; 0$, s.t., $\forall n \ge N$, it holds that</em> </p>
<script type="math/tex; mode=display">
   \mathbf{E}[ |h(s)| ] \le (1 + \delta) n \mathbf{H}(p)</script></blockquote>
<p>We begin with an important property of Huffman Code. Suppose that we have an alphabet $\mathbb{U}$ such that probability associated with each element in $\mathbb{U}$ is $2^{-l}$ for some integer $l$. Then $\exists h$, such that </p>
<script type="math/tex; mode=display">
   \mathbf{E}_{X \in \mathbb{U} } [ |h(X)| ] =  n \mathbf{H}(X)</script><p>For example, if $\mathbb{U} = \{0, 1, 2, 3 \}$ and the distribution $p = \{ 0.5, 0.25, 0.125, 0.125 \}$, then we can have the following encoding </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Encoding.png" alt=""></p>
<p>The expected coding length is </p>
<script type="math/tex; mode=display">
0.5 \cdot 1 + 0.25 \cdot 2 + 2 \cdot 0.125 \cdot 3 = 1.75</script><p>which is exactly the entropy of the random variable. </p>
<p>In general, if we have a random variable $X$, we can round down its probability to the nearest integer negative power of $2$. The expected code length is given by </p>
<script type="math/tex; mode=display">
\sum_{i } p_i \left\lceil \log \frac{1}{p_i} \right\rceil \le \mathbf{H}(p) + 1</script><p>Intuitively, the $\left\lceil \log \frac{1}{p_i} \right\rceil$ has enough slot to accommodate all elements with probabilities $p_i$’s. </p>
<p>In particular, we view $\{0, 1\}^n$ as a large alphabet $\mathbf{\Sigma }$. The alphabet has entropy $n \cdot \mathbf{H}(p)$. We can have a encoding such that the expected output length is at most $n \cdot \mathbf{H}(p) + 1$. For large enough $n$, this is at most $(1 + \delta) n \cdot \mathbf{H}(p)$. </p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <em>M. Mitzenmacher and E. Upfal, Probability and computing: an introduction to randomized algorithms and probabilistic analysis. New York: Cambridge University Press, 2005.</em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/21/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/21/">21</a><span class="page-number current">22</span><a class="page-number" href="/page/23/">23</a><span class="space">&hellip;</span><a class="page-number" href="/page/56/">56</a><a class="extend next" rel="next" href="/page/23/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">166</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
