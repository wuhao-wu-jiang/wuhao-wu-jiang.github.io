<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/06/Gradient-Descent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/06/Gradient-Descent/" class="post-title-link" itemprop="url">Gradient Descent</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-06 10:10:45" itemprop="dateCreated datePublished" datetime="2020-03-06T10:10:45+11:00">2020-03-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-09 00:46:03" itemprop="dateModified" datetime="2020-03-09T00:46:03+11:00">2020-03-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Gradient descent is a greedy search method for optimization. It approximates a function locally by a linear function. Its analysis varies depending on the what kinds of constraints you place on it.</p>
<p>We study a baby version of the method. The problem at hand is given by: <span class="math display">\[
\min_{x \in \mathbb{R }^n} f(x)
\]</span> where <span class="math inline">\(f\)</span> is twice continuous differentiable and convex. The convexity of <span class="math inline">\(f\)</span> implies that it has a unique minimum value <span class="math inline">\(\min_{x \in \mathbb{R }^n} f(x)\)</span>. Assume that <span class="math inline">\(f\)</span> indeed achieves minimum at some point and denote the optimal point as <span class="math inline">\(x^*\)</span>. Note that is some case, <span class="math inline">\(f\)</span> can not achieve minimum value at any point. Consider <span class="math inline">\(f(x) = e^x\)</span>. Then <span class="math inline">\(\min_{x \in \mathbb{R } } e^x = 0\)</span>. <span class="math inline">\(f\)</span> can approach arbitrary close to <span class="math inline">\(0\)</span> but never achieve that value.</p>
<p>Before we delve into the details, we need to define some frequently used notations.</p>
<ul>
<li><p>The gradient <span class="math inline">\(\nabla f(x):\mathbb{R }^n \rightarrow \mathbb{R }^n\)</span> is the vector of partial derivatives of <span class="math inline">\(f\)</span> at point <span class="math inline">\(x\)</span>: <span class="math display">\[
\nabla f(x) = \left[ \frac{\partial f (x) }{\partial x_1}, \frac{\partial f (x)}{\partial x_2}, .., \frac{\partial f (x) }{\partial x_n} \right]
\]</span></p></li>
<li><p>The Hessian <span class="math inline">\(\nabla^2 f(x) : \mathbb{R }^n \rightarrow \mathbb{R }^{n \times n}\)</span> is the matrix of second order derivatives of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>: <span class="math display">\[
  \nabla f(x) = 
      \left[
      \begin{aligned}
          \frac{\partial^2 f (x)}{\partial x_1 \partial x_1}, \frac{\partial^2 f (x)}{\partial x_1 \partial x_2}, &amp;..., \frac{\partial^2 f (x)}{\partial x_1 \partial x_n} \\
          \frac{\partial^2 f (x)}{\partial x_2 \partial x_1}, \frac{\partial^2 f (x)}{\partial x_2 \partial x_2}, &amp;..., \frac{\partial^2 f (x)}{\partial x_2 \partial x_n} \\
          &amp;...\\
          \frac{\partial^2 f (x)}{\partial x_n \partial x_1}, \frac{\partial^2 f (x)}{\partial x_n \partial x_2}, &amp;..., \frac{\partial^2 f (x)}{\partial x_n \partial x_n} \\
      \end{aligned}
      \right]
  \]</span> Since <span class="math inline">\(f\)</span> is twice continuous differentiable, the Hessian is symmetric.</p></li>
</ul>
<p>The algorithm is itself very simple.</p>
<h3 id="algorithm"><strong>Algorithm</strong></h3>
<p><strong><em>Gradient Descent</em></strong></p>
<ol type="1">
<li>Begin from some <span class="math inline">\(x_0 \in \mathbb{R }^n\)</span>.</li>
<li><span class="math inline">\(x^{t + 1} \leftarrow x^t - \eta \nabla f(x^t)\)</span>.</li>
</ol>
<p>where <span class="math inline">\(\eta &gt; 0\)</span> is called step size. Below is an example that show how <span class="math inline">\(f\)</span> decreases as the algorithm iterates.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/GradientDescent.jpg" /></p>
<h2 id="convergence"><strong>Convergence</strong></h2>
<p>To analyze the convergence behavior, we need additional assumptions on <span class="math inline">\(f\)</span>. We analyze a baby version of gradient descent with strong assumptions:</p>
<ol type="1">
<li><span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span> smooth, i.e., <span class="math inline">\(v^T \nabla^2 f(z) v \le \beta \| v \|^2\)</span> for <span class="math inline">\(\forall z, v \in \mathbb{R }^n\)</span>.</li>
<li><span class="math inline">\(f\)</span> is <span class="math inline">\(\alpha\)</span> strongly convex, i.e., <span class="math inline">\(v^T \nabla^2 f(z) v \ge \alpha \| v \|^2\)</span> for <span class="math inline">\(\forall z, v \in \mathbb{R }^n\)</span>.</li>
</ol>
<p>The interpretation is that, <span class="math inline">\(\beta\)</span> is the upper bound of the largest possible eigenvalue of the Hessian at any point and <span class="math inline">\(\alpha\)</span> is the lower bound. By definition we have <span class="math inline">\(\beta \ge \alpha\)</span>.</p>
<p>Now define <span class="math display">\[
\Delta_t = f(x^t) - f(x^*)
\]</span> and <span class="math inline">\(\kappa = \frac{\beta}{\alpha}\)</span>. The following theorem characterize the convergence speed of the algorithm:</p>
<p><strong><em>Theorem.</em></strong> <em>If we set <span class="math inline">\(\eta = \frac{1}{\beta}\)</span>, then it holds that</em> <span class="math display">\[
\Delta_t \le \Delta_0 \left( 1 - \frac{1}{4 \kappa } \right)^t.
\]</span></p>
<p>By the theorem, given any <span class="math inline">\(\epsilon &gt; 0\)</span>, after <span class="math display">\[
O \left( \kappa \log \frac{\Delta_0}{\epsilon} \right) 
\]</span></p>
<p>iteration, the error <span class="math inline">\(\Delta_t\)</span> will be smaller than <span class="math inline">\(\epsilon\)</span>.</p>
<p><em>Proof:</em></p>
<p>The proof consists of two parts. <span class="math inline">\(\beta\)</span>-Smooth and <span class="math inline">\(\alpha\)</span>-Convexity play a role in each part separately.</p>
<h4 id="beta-smoothness."><span class="math inline">\(\beta\)</span>-Smoothness.</h4>
<p>The property guarantees that <span class="math inline">\(\epsilon_{t + 1} \le \epsilon_t - \frac{1}{2 \beta} \| f(x_t) \|^2\)</span>.</p>
<p>First, by Taylor series expansion, we have for <span class="math inline">\(x, y \in \mathbb{R }^n\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    f(y)    &amp;= f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f(z) (y - x) \\
            &amp;\le f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} \beta \|y - x \|^2
\end{aligned}
\]</span> where <span class="math inline">\(z = \lambda x + (1 - \lambda) y\)</span> for <span class="math inline">\(\lambda \in [0, 1]\)</span>. The second inequality comes from the <span class="math inline">\(\beta\)</span>-smoothness.</p>
<p>Replacing <span class="math inline">\((y - x)\)</span> as <span class="math inline">\(-\eta \nabla f(x)\)</span>, we get <span class="math display">\[
\begin{aligned}
    f(y)  
        &amp;\le f( x ) - \eta \| \nabla f(x) \|^2 + \frac{1}{2} \beta \eta^2 \| \nabla f( x ) \|^2 \\
        &amp;\le f( x ) + (\frac{1}{2} \beta \eta^2 -\eta) \| \nabla f(x ) \|^2  \\ 
\end{aligned}
\]</span></p>
<p>The second coefficient <span class="math inline">\((\frac{1}{2} \beta \eta^2 -\eta)\)</span> is minimized when <span class="math inline">\(\beta \eta - 1 = 0\)</span>, i.e., <span class="math inline">\(\eta = \frac{1}{\beta}\)</span>. It follows <span class="math display">\[
\begin{aligned}
    f(y ) 
        &amp;\le f( x ) + (\frac{1}{2} \beta \eta^2 -\eta) \| \nabla f(x ) \|^2  \\ 
        &amp;\le f( x  ) - \frac{1}{2 \beta}  \| \nabla f(x ) \|^2  \\ 
\end{aligned}
\]</span></p>
<p>Finally, substituting <span class="math inline">\(y\)</span> with <span class="math inline">\(x^{t + 1}\)</span> and <span class="math inline">\(x\)</span> with <span class="math inline">\(x^t\)</span>, it becomes <span class="math display">\[
\begin{aligned}
    f(x^{t + 1}) 
        &amp;\le f(x^t) - \frac{1}{2 \beta } \| \nabla f(x^t) \|^2 \\
        &amp;\longleftrightarrow    \\
    \Delta_{t + 1} 
        &amp;\le \Delta_t - \frac{1}{2 \beta } \| \nabla f(x^t) \|^2 
\end{aligned}
\]</span></p>
<h4 id="alpha-convexity."><span class="math inline">\(\alpha\)</span> Convexity.</h4>
<p>We might still have a problem, if when <span class="math inline">\(x^t\)</span> approaches <span class="math inline">\(x^*\)</span>, <span class="math inline">\(\| \nabla f(x^t) \|\)</span> decreases too fast. That where <span class="math inline">\(\alpha\)</span> convexity comes into play. We will show that <span class="math display">\[
\| \nabla f(x^t) \|^2 \ge \Delta_t \cdot\frac{1}{2} \alpha
\]</span></p>
<ol type="1">
<li><p>Proof 1. By <span class="math inline">\(\alpha\)</span> convexity, we have for all <span class="math inline">\(y \in \mathbb{R}^n\)</span>,<br />
<span class="math display">\[
f(y) \ge f(x) + \nabla f(x)^T (y - x) + \frac{\alpha}{2} \| y - x \|^2
\]</span> Taking minimum of both sides, <span class="math display">\[
\min_{y \in \R^n} f(y) \ge \min_{y \in \R^n} \left( f(x) + \nabla f(x)^T (y - x) + \frac{\alpha}{2} \| y - x \|^2\right)
\]</span> The left hand side equals to <span class="math inline">\(f(x^*)\)</span> while the right hand side minimized to <span class="math inline">\(f(x) - \frac{1}{2 \alpha} \| \nabla f(x) \|^2\)</span> when <span class="math inline">\(y - x = -\frac{1}{\alpha} \nabla f(x)\)</span>.</p>
<p>Substituting <span class="math inline">\(x\)</span> with <span class="math inline">\(x^t\)</span>, we get <span class="math display">\[
\| \nabla f(x^t) \|^2 \ge 2 \alpha \left( f(x^t) - f(x^*) \right) = 2\alpha \Delta_t
\]</span></p></li>
<li><p>Proof 2. By convexity, we have</p>
<p><span class="math display">\[
 \begin{aligned}
     \nabla f(x)^T (x - x^*) 
         &amp;\ge f(x) - f(x^*) 
 \end{aligned}
 \]</span></p>
<p>Then by Taylor expansion and <span class="math inline">\(\alpha\)</span> convexity, <span class="math display">\[
 \begin{aligned}
     f(x) - f(x^*) 
         &amp;\ge \nabla f(x^*)^T (x - x^*) +  \frac{1}{2} \alpha \| x - x^* \|^2\\
         &amp;=\frac{1}{2} \alpha \| x - x^* \|^2
 \end{aligned}
 \]</span></p>
<p>Concatenating the inequalities, we get <span class="math display">\[
 \| \nabla f(x) \| \| x - x^* \| \ge \nabla f(x)^T (x - x^*) \ge f(x) - f(x^*) \ge \frac{1}{2} \alpha \| x - x^* \|^2
 \]</span> where the first inequality is Cauchy-Schwartz inequality. Multiplying the inequalities we get</p>
<p><span class="math display">\[
 \| \nabla f(x) \|^2 \| x - x^* \|^2 \ge (f(x) - f(x^*)) \cdot\frac{1}{2} \alpha \| x - x^* \|^2
 \]</span></p>
<p>which implies that <span class="math display">\[
 \| \nabla f(x) \|^2 \ge (f(x) - f(x^*)) \cdot\frac{1}{2} \alpha 
 \]</span></p>
<p>Replacing <span class="math inline">\(x\)</span> with <span class="math inline">\(x^t\)</span> and <span class="math inline">\(f(x) - f(x^*)\)</span> with <span class="math inline">\(\Delta_t\)</span>, we have <span class="math display">\[
 \| \nabla f(x^t) \|^2 \ge \Delta_t \cdot\frac{1}{2} \alpha 
 \]</span></p></li>
</ol>
<p>In combination, we conclude that <span class="math display">\[
\begin{aligned}
    \Delta_{t + 1}
        &amp;\le \Delta_t \left( 1- \frac{\alpha }{4 \beta} \right)
\end{aligned}
\]</span></p>
<p>which finishes the proof.</p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/05/Beta-Smoothness/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/Beta-Smoothness/" class="post-title-link" itemprop="url">Characterizations of Beta-Smoothness</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-05 20:05:37" itemprop="dateCreated datePublished" datetime="2020-03-05T20:05:37+11:00">2020-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-06 17:01:27" itemprop="dateModified" datetime="2020-03-06T17:01:27+11:00">2020-03-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="definition">Definition</h4>
<p>A differential function <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is <span class="math inline">\(\beta\)</span>-smooth if <span class="math display">\[
\|\nabla f(y) - \nabla f(x) \| \le \beta \|y - x\|
\]</span> where <span class="math inline">\(\|\cdot \|\)</span> is <span class="math inline">\(\ell_2\)</span> norm.</p>
<h4 id="theorem-1.">Theorem 1.</h4>
<p>A twice continuously differential function <span class="math inline">\(f : \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is <span class="math inline">\(\beta\)</span>-smooth if and only if <span class="math display">\[
v^T \nabla^2 f(z) v \le \beta \|v\|^2
\]</span> for any <span class="math inline">\(z, v \in \mathbb{R}^n\)</span>.</p>
<p><strong><em>Proof.</em></strong></p>
<ol type="1">
<li><p>(<strong><em>ONLY IF</em></strong>). <span class="math inline">\(v^T \nabla^2 f(z) v \le \beta \|v\|^2\)</span> is equivalent to that the maximum eigenvalue of <span class="math inline">\(\nabla^2 f(z)\)</span> is at most <span class="math inline">\(\beta\)</span>. Let <span class="math inline">\(v\)</span> be the eigenvector of <span class="math inline">\(\nabla^2 f(z)\)</span>. By mean value theorem <span class="math display">\[
     \frac{\partial f}{\partial x_i} (z + v) - \frac{\partial f}{\partial x_i} (z) = \sum_{j = 1}^n \left( \frac{\partial f}{\partial x_i \partial x_j} (c) \right) v_j
 \]</span></p>
<p>For some <span class="math inline">\(c\)</span> between <span class="math inline">\(z\)</span> and <span class="math inline">\(z + v\)</span>. As <span class="math inline">\(f\)</span> is twice continuously differentiable, for any given <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(\exists v\)</span> that is small enough, such that <span class="math display">\[
 \left| \frac{\partial f}{\partial x_i} (z + v) - \frac{\partial f}{\partial x_i} (z) \right| \in \left| \sum_{j = 1}^n \left( \frac{\partial f}{\partial x_i \partial x_j} (z) \right) v_j \right| \pm \frac{\epsilon}{n}
 \]</span></p>
<p>Hence, <span class="math display">\[
 | \nabla f(z + v) - \nabla f(z) | \in |\nabla^2 f(z) v| + \epsilon 
 \]</span></p>
<p>By equivalence of <span class="math inline">\(\ell_1\)</span>-norm and <span class="math inline">\(\ell_2\)</span> norm, we get for <span class="math inline">\(v\)</span> small enough, <span class="math display">\[
 \| \nabla f(z + v) - \nabla f(z) \| \in \| \nabla^2 f(z) v \| + \epsilon 
 \]</span></p>
<p>As <span class="math inline">\(\epsilon\)</span> can be arbitrary small, it concludes that <span class="math display">\[
 \| \nabla^2 f(z) v \| \le \beta \|v \|
 \]</span></p>
<p>which is equivalent to that the eigenvalues of <span class="math inline">\(\nabla^2 f(z)\)</span> is at most <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>(<strong><em>IF</em></strong>). We prove a stronger result that <span class="math display">\[
     \| \nabla f(y) - \nabla f(x) \| \cdot \| \nabla f(y) - \nabla f(x) \| \le \beta \left( \nabla f(y) - \nabla f(x) \right)^T (y - x) 
 \]</span> By Cauchy-Schwartz inequality, we have <span class="math display">\[
 \left( \nabla f(y) - \nabla f(x) \right)^T (y - x) \le \| \nabla f(y) - \nabla f(x) \| \cdot \| y - x  \|
 \]</span> which will finish our proof.</p>
<p>By Taylor series expansion, we have <span class="math display">\[
 \begin{aligned}
     f(y) &amp;
         = f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f(z) (y - x)
     \\
         &amp;\le f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} \beta \| y - x \|^2
 \end{aligned}
 \]</span></p>
<p>for some <span class="math inline">\(z\)</span> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. The inequality holds since <span class="math inline">\(v^T \nabla^2 f(z) v \le \beta \|v\|^2\)</span> for any <span class="math inline">\(v \in \mathbb{R}^n\)</span>.</p>
<p>By symmetry, we get <span class="math display">\[
 \begin{aligned}
     f(x) &amp;\le f(y) + \nabla f(y)^T (x - y) + \frac{1}{2} \beta \| y - x \|^2
 \end{aligned}
 \]</span></p>
<p>Summing up the two inequalities, we obtain <span class="math display">\[
 \left( \nabla f(y) - \nabla f(x) \right)^T (y - x)^T  \le \beta \| y - x \|^2
 \]</span></p></li>
</ol>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="theorem-2.">Theorem 2.</h4>
<p>If <span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span>-smooth, then for <span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, we have <span class="math display">\[
f(y) \le f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} \beta \| y - x \|^2
\]</span></p>
<p><em>Proof.</em> By fundamental theorem of calculus, <span class="math display">\[
\begin{aligned}
    f(y) - f(x) - \nabla f(x)^T (y - x)
        &amp;=
    \int_{0}^1 \nabla f(x + t(y - x))^T(y - x) \ dt - \nabla f(x)^T (y - x)
    \\
        &amp;= 
    \int_{0}^1 \left( \nabla f(x + t(y - x)) - \nabla f(x) \right)^T(y - x) \ dt 
    \\
        &amp;\le
    \int_{0}^1 \| \nabla f(x + t(y - x)) - \nabla f(x) \| \cdot \| y - x \| \ dt  
    \\
        &amp;\le
    \int_{0}^1 \beta t \| y - x \|  \cdot \| y - x \| \ dt  
    \\
        &amp;\le 
    \frac{1}{2} \beta \| y - x \|^2
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/01/Counting-Triangle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/01/Counting-Triangle/" class="post-title-link" itemprop="url">Counting Triangle</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-01 22:20:20" itemprop="dateCreated datePublished" datetime="2020-03-01T22:20:20+11:00">2020-03-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-05 20:20:54" itemprop="dateModified" datetime="2020-07-05T20:20:54+10:00">2020-07-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given an undirected graph <span class="math inline">\(G = \left&lt; V, E \right&gt;\)</span> with <span class="math inline">\(|V| = n\)</span> vertices and <span class="math inline">\(|E| = m\)</span> edges, we can count the number of triangle in <span class="math inline">\(O(m^{1.5})\)</span> time.</p>
<ol type="1">
<li><p>Label the vertices in ascending order according their degrees. Break tie arbitrarily. After ordering, we assume that <span class="math inline">\(d_{v_1} \le d_{v_2} \le ... \le d_{v_n}\)</span>.</p></li>
<li><p>Check all possible triangles <span class="math inline">\((u, v, w)\)</span> such that <span class="math inline">\(d_u \le d_v \le d_w\)</span>. This can be implemented as follows:</p>
<ul>
<li>Check all edges <span class="math inline">\((u, v)\)</span> such that <span class="math inline">\(d_u \le d_v\)</span>.</li>
<li>Check each neighbor <span class="math inline">\(w\)</span> of <span class="math inline">\(u\)</span> such that <span class="math inline">\(d_w \ge d_v\)</span>. We can sort the neighbors of each vertex according to their degree. Then finding the first vertex with <span class="math inline">\(d_w \ge d_v\)</span> takes <span class="math inline">\(O(\log d_u)\)</span> time.</li>
</ul></li>
</ol>
<p><em>Claim: This algorithm has complexity <span class="math inline">\(O(m^{1.5})\)</span>.</em></p>
<p><em>Proof.</em></p>
<ol type="1">
<li>If <span class="math inline">\(d_w \ge \sqrt m\)</span>, then the number of such possible <span class="math inline">\(w\)</span> is bounded by <span class="math inline">\(O( m / \sqrt m) = O(\sqrt m)\)</span>. Therefore, the time spent on <span class="math inline">\((u,v )\)</span> for <span class="math inline">\(\{w : d_w \ge \sqrt m\}\)</span> is <span class="math inline">\(O(\sqrt m)\)</span>. The time spent over all edges is: <span class="math display">\[
 \sum_{(u, v) \in E} \sqrt m = m\sqrt m = m^{1.5}
 \]</span></li>
<li>If <span class="math inline">\(d_w &lt; \sqrt m\)</span>, then <span class="math inline">\(d_u \le d_v \le d_w &lt; \sqrt m\)</span>. The number of such triangles is bounded by: <span class="math display">\[
 \sum_{u \in S} d_u^2 \le \sqrt m \left( \sum_{u \in S} d_u \right) \le \sqrt m \cdot 2m= 2m^{1.5}
 \]</span></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/27/Primal-Dual-Algorithm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/27/Primal-Dual-Algorithm/" class="post-title-link" itemprop="url">Primal-Dual Algorithm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-27 21:57:12" itemprop="dateCreated datePublished" datetime="2020-02-27T21:57:12+11:00">2020-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-28 11:36:47" itemprop="dateModified" datetime="2020-02-28T11:36:47+11:00">2020-02-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Primal: <span class="math display">\[
\begin{aligned}
    &amp;\min       &amp;c^T x \\
    &amp;s.t.       &amp;Ax = b\\
    &amp;           &amp;x \ge 0
\end{aligned}
\]</span></p>
<p>We assume that <span class="math inline">\(b \ge 0\)</span>. Otherwise we can flip the <span class="math inline">\(i\)</span> constraints <span class="math inline">\(A^T_i x = b_i\)</span> by <span class="math inline">\(- A^T_i x = -b_i\)</span>, where <span class="math inline">\(A^T_i\)</span> is the <span class="math inline">\(i\)</span>-th column of matrix <span class="math inline">\(A^T\)</span>, i.e, the <span class="math inline">\(i\)</span>-th row of matrix <span class="math inline">\(A\)</span>.</p>
<p>Dual: <span class="math display">\[
\begin{aligned}
    &amp;\max       &amp;y^T b \\
    &amp;s.t.       &amp;y^TA \le c^T \\
\end{aligned}
\]</span></p>
<p>Suppose that we have a feasible solution <span class="math inline">\(y\)</span> for the dual (if <span class="math inline">\(c \ge 0\)</span>, then <span class="math inline">\(y = 0\)</span> is a feasible solution). Weak duality states that <span class="math display">\[
c^T x \ge y^T A x = y^T b
\]</span></p>
<p>Denote <span class="math inline">\(S = \{ i : i \in [n], y^T A_i &lt; c_i \}\)</span> (the set of constraints that have slacks for increments) and <span class="math inline">\(\bar S = [n] \setminus S\)</span> (the set of indexes of tight dual constraints), where <span class="math inline">\(A_i\)</span> is the <span class="math inline">\(i\)</span>-th columns of <span class="math inline">\(A\)</span>.</p>
<p>By weak duality, if we can find an <span class="math inline">\(x\)</span> such that</p>
<ol type="1">
<li><span class="math inline">\(x_{S } = 0\)</span>, where <span class="math inline">\(x_{S} \doteq (x_i : i \in {S})\)</span>.</li>
<li><span class="math inline">\(x_{\bar S} \ge 0\)</span> and <span class="math inline">\(A_{\bar S} x_{\bar S} = b\)</span>, where <span class="math inline">\(A_{\bar S}\)</span> is the sub-matrix of <span class="math inline">\(A\)</span> containing only columns in <span class="math inline">\({\bar S}\)</span>.</li>
</ol>
<p>then both <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are optimal. However, as <span class="math inline">\(y\)</span> is not necessarily optimal, we have the following relaxed optimization problem, which is called <em>restricted primal</em></p>
<p>Relaxation: <span class="math display">\[
\begin{aligned}
    &amp;\min       &amp;1^T \bar x \\
    &amp;s.t.       &amp;A_{\bar S} x_{\bar S} + \bar x = b \\
    &amp;           &amp;x_{\bar S} \ge 0 \\
    &amp;           &amp;\bar x \ge 0
\end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(x_{\bar S} = 0\)</span> and <span class="math inline">\(\bar x = b\)</span> is a feasible solution as we assume that <span class="math inline">\(b \ge 0\)</span>. Therefore, <em>restricted primal</em> is feasible. Further, it is bounded below by <span class="math inline">\(1^{\bar S} 0 = 0\)</span>. Indeed, if it achieves optimal solution value <span class="math inline">\(0\)</span>, it holds that <span class="math inline">\(\bar x = 0\)</span> so that <span class="math inline">\(A_{\bar S} x_{\bar S} = b\)</span>.</p>
<p>Associated with the <em>restricted primal</em> is the <em>restricted dual</em> <span class="math display">\[
\begin{aligned}
    &amp;\max       &amp; \bar y^T b \\
    &amp;s.t.       &amp; \bar y^TA_{\bar S} \le 0 \\
    &amp;           &amp; \bar y^T \le 1^T
\end{aligned}
\]</span></p>
<p>By strong duality, it optimal solution <span class="math inline">\(\bar y^T b = 1^T \bar x\)</span>. If <span class="math inline">\(1^T \bar x= \bar y^T b &gt; 0\)</span>, then we can improve the dual solution <span class="math inline">\(y\)</span> by <span class="math display">\[
y \leftarrow y + \epsilon \bar y
\]</span></p>
<p>for some <span class="math inline">\(\epsilon\)</span>. To determine its value, first observe that <span class="math display">\[
(y + \epsilon \bar y)^{T} A_{\bar S} = y^T A_{\bar S} + \epsilon \bar y^T A_{\bar S} \le y^T A_{\bar S} = c_{\bar S}^T
\]</span></p>
<p>Hence it is left to guarantee that <span class="math display">\[
(y + \epsilon \bar y)^T A_S = y^T A_S + \epsilon \bar y^T A_S \le  c_S
\]</span></p>
<p>By definition of <span class="math inline">\({S}\)</span>, we have <span class="math inline">\(y^T A_{S} &lt; c_{S}^T\)</span>. If that <span class="math inline">\(\bar y^T A_i \le 0\)</span> for <span class="math inline">\(\forall i \in {S}\)</span>, then <span class="math inline">\(\epsilon\)</span> can be arbitrary large and the dual is unbounded. Otherwise,</p>
<p><span class="math display">\[
\epsilon = \min_{i \in S, \bar y^T A_i &gt; 0} \frac{c_i - y^T A_i } { \bar y^T A_i }
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/26/The-Remainder-Term-in-Taylor-Series/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/26/The-Remainder-Term-in-Taylor-Series/" class="post-title-link" itemprop="url">The Remainder Term in Taylor Series</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-26 13:57:00" itemprop="dateCreated datePublished" datetime="2020-02-26T13:57:00+11:00">2020-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-28 11:22:58" itemprop="dateModified" datetime="2020-02-28T11:22:58+11:00">2020-02-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="taylor-series">Taylor Series</h2>
<p>The philosophy of Taylor series is to approximate a function with polynomials that use only the information of a single point in the domain of the function. Consider the function <span class="math inline">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span> and a point <span class="math inline">\(a \in \mathbb{R}\)</span>. Further, we would like to use <span class="math inline">\((x-a)^0\)</span>, <span class="math inline">\((x- a)^1\)</span>, <span class="math inline">\((x- a)^2\)</span>, ..., <span class="math inline">\((x- a)^n\)</span> as the basic building blocks of our approximation. The polynomial constructed is of the form: <span class="math display">\[
p_n(x) = c_0 + c_1 (x - a) + c_2(x - a)^2 + ... + c_n (x - a)^n
\]</span> Now, assume that the <span class="math inline">\(n\)</span>-th derivative of <span class="math inline">\(f\)</span> exits. To approximate <span class="math inline">\(f\)</span> well, <span class="math inline">\(p_n\)</span> should satisfy <span class="math display">\[
p_n(a) = f(a)
\]</span> so that <span class="math inline">\(c_0 = f(a)\)</span>.</p>
<p>Further, <span class="math inline">\(f&#39;(a)\)</span> tell how the function <span class="math inline">\(f(x)\)</span> changes when <span class="math inline">\(x\)</span> moves from <span class="math inline">\(a\)</span> by a little bit. We require <span class="math inline">\(p_n(x)\)</span> to exhibit the same behavior: <span class="math display">\[
p&#39;_n(a) = f&#39;(a)
\]</span> so that <span class="math inline">\(c_1 = f&#39;(a)\)</span>.</p>
<p>To fit <span class="math inline">\(f(x)\)</span> even better, we force the second order derivative of <span class="math inline">\(p_n(x)\)</span> at <span class="math inline">\(x\)</span> comply with <span class="math inline">\(f&#39;&#39;(a)\)</span>, hence <span class="math display">\[
p_n&#39;&#39;(a) = f&#39;&#39;(a)
\]</span> and <span class="math inline">\(c_2 = \frac{1}{2} f&#39;&#39;(a)\)</span>.</p>
<p>Following the above procedure,we get <span class="math display">\[
c_i = \frac{1}{i!}f^{(i) } (a)
\]</span> and <span class="math display">\[
p_n(x) = \sum_{k = 0}^n \frac{ f^{( k ) }(a) }{ k! }  (x - a)^k
\]</span> where <span class="math inline">\(f^{(0)} \doteq f\)</span>, <span class="math inline">\(0! \doteq 1\)</span>. We have just show that the construct polynomial satisfies</p>
<p><strong><em>Theorem. <span class="math inline">\(p^{ (k) }_n (a) = f^{ (k) } (a)\)</span>, for <span class="math inline">\(k \in [n]\)</span>.</em></strong></p>
<h2 id="remainder">Remainder</h2>
<p>One natural to ask is that, how accurate is the <span class="math inline">\(p_n(x)\)</span>. In particular, given <span class="math inline">\(x \neq a\)</span>, we want to measure the error <span class="math display">\[
r_n(x) = f(x) - p_n(x)
\]</span> quantitively.</p>
<p>Note that if the function <span class="math inline">\(f\)</span> is itself a polynomial up to order <span class="math inline">\(n\)</span>, then the approximation is accurate that is <span class="math inline">\(r_n(x) \equiv 0\)</span> for <span class="math inline">\(x \in \mathbb{R}\)</span>. In this case, we recover <span class="math inline">\(f\)</span> globally using only local information at <span class="math inline">\(a\)</span>.</p>
<p>In general, if <span class="math inline">\(f^{ (n + 1)} (x)\)</span> exists and is continuous, then the error is characterized as</p>
<p><strong><em>Theorem</em></strong> <span class="math display">\[
r_n(x) = \frac{1}{n!} \int_a^x f^{ (n + 1) } (t) (x - t)^{n } dt = -\int_a^x f^{ (n + 1) } (t)  d \left( \frac{(x - t)^{n + 1} }{(n + 1)!} \right)
\]</span> <em>Proof.</em></p>
<p>By fundamental theorem of calculus, we have <span class="math display">\[
f(x) = f(a) + \int_{a}^x f&#39;(t) dt
\]</span> On the other hand <span class="math display">\[
\frac{\partial [f&#39;(t) (t - x)] }{\partial t} = f&#39;&#39;(t) (t - x) + f&#39;(t)
\]</span> Therefore, <span class="math display">\[
f&#39;(t) (t - x) \mid_{t = a}^x = f&#39;(a) (x -a)  = \int_{a}^x f&#39;&#39;(t) (t - x) dt  + \int_{a}^x f&#39;(t) dt
\]</span> and <span class="math display">\[
f(x) = f(a) + f&#39;(a) (x - a) +  \int_{a}^x f&#39;&#39;(t) (x - t) dt
\]</span> Similarly, <span class="math display">\[
\begin{aligned}
\int_{a}^x f&#39;&#39;(t) (x - t) dt 
    &amp;= - \int_{a}^x f&#39;&#39;(t) d \frac{ (x - t)^2}{2!} \\
    &amp;= f&#39;&#39;(a) \frac{ (x - a)^2}{2!} + \int_{a}^x \frac{ (x - t)^2}{2!} f^{(3) } (t) dt
\end{aligned}
\]</span> As just shown, the proof can be finished by induction. <span class="math inline">\(\square\)</span></p>
<p><strong><em>Weighted Mean Value Theorem.</em></strong> Given a continuous functions <span class="math inline">\(f \ge 0\)</span> and an Riemann integral function <span class="math inline">\(g \ge 0\)</span> defined on an closed interval <span class="math inline">\(I\)</span>, then <span class="math display">\[
\int_I f(t) g(t) dt = f(c) \int_I g(t) dt
\]</span> for some <span class="math inline">\(c \in I\)</span>.</p>
<p><em>Proof.</em> As <span class="math inline">\(f\)</span> is continuous and <span class="math inline">\(I\)</span> is closed, <span class="math inline">\(\exists m, M \in \mathbb{R}\)</span>, s.t., <span class="math display">\[
m \le f(t) \le M, \qquad \forall t \in I
\]</span></p>
<p>Therefore, <span class="math display">\[
mg(t) \le f(t) g(t) \le M g(t), \qquad \forall t \in I
\]</span></p>
<p>Define <span class="math inline">\(S = \int_I g(t) dt\)</span>. By monotonicity of Riemann integration, we have <span class="math display">\[
m S \le \int_I f(t) g(t) dt \le M S
\]</span></p>
<p>Note that <span class="math inline">\(S f(t)\)</span> is a continuous function in <span class="math inline">\(I\)</span>. Hence, by mean value theorem, <span class="math inline">\(\exists c \in I\)</span>, such that <span class="math display">\[
S f(c) = \int_I f(t) g(t) dt
\]</span> <span class="math inline">\(\square\)</span></p>
<p><strong><em>Corollary 1.</em></strong> By weighted mean value theorem, we have <span class="math display">\[
\begin{aligned}
r_n(x) 
    &amp;= \frac{1}{n!} \int_a^x f^{ (n + 1) } (t) (x - t)^{n } dt  \\
    &amp;= f^{ (n + 1) } (c) \int_a^x  \frac{1}{n!} (x - t)^{n } dt  \\
    &amp;= \frac{f^{ (n + 1) } (c ) }{(n + 1) ! } (x - a)^{n + 1 }  
\end{aligned}
\]</span></p>
<p>for some <span class="math inline">\(c \in [a, x]\)</span>.</p>
<p><strong><em>Corollary 2.</em></strong> Let <span class="math inline">\(f:\rightarrow \mathbb{R}^n \rightarrow \mathbb{R}\)</span> be a function with continuous second partial derivatives, then given <span class="math inline">\(a \in \mathbb{R}^n\)</span>, it holds that <span class="math display">\[
f(x) = f(a) + [\nabla f(c)]^T (x - a)
\]</span> for some <span class="math inline">\(c \in a + t(x - a)\)</span>, where <span class="math inline">\(t \in [0, 1]\)</span>.</p>
<p><em>Proof:</em> Define <span class="math inline">\(g(t) = f(a + t (x- a)) : [0,1] \rightarrow \mathbb{R}\)</span>. Then</p>
<ol type="1">
<li><span class="math inline">\(g(0) = f(a)\)</span>.</li>
<li><span class="math inline">\(g(1) = f(x)\)</span>.</li>
<li><span class="math inline">\(g&#39;(t) = [\nabla f(a + t (x- a) ) ]^T (x - a)\)</span></li>
<li>By mean value theorem, <span class="math inline">\(\exists t \in [0, 1]\)</span>, s.t., <span class="math display">\[
 \frac{g(1) - g(0)}{1} = f(x) - f(a) = g&#39;(t) = [\nabla f(a + t (x- a) ) ]^T (x - a)
\]</span></li>
</ol>
<p><span class="math inline">\(\square\)</span></p>
<p><strong><em>Corollary 3.</em></strong> Let <span class="math inline">\(f\)</span>, <span class="math inline">\(g\)</span> and <span class="math inline">\(c\)</span> as defined before, then <span class="math display">\[
f(x) = f(a) + [\nabla f(a)]^T (x - a) + \frac{1}{2} (x - a)^T [\nabla^2 f(c)] (x - a)
\]</span></p>
<p><em>Proof:</em> <span class="math display">\[
g&#39;&#39;(t) = (x - a)^T [\nabla^2 f(a + t (x- a) ) ] (x - a)
\]</span></p>
<p>Then applying Corollary 1 gives <span class="math display">\[
g(1) = g(0) + g&#39;(0)(1 - 0) + \frac{1}{2} g&#39;&#39;(t) (1 - 0)^2
\]</span></p>
<p>for some <span class="math inline">\(t \in [0, 1]\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/24/Eigenvalues-and-Eigenvectors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/24/Eigenvalues-and-Eigenvectors/" class="post-title-link" itemprop="url">Eigenvalues, Eigenvectors and SVD</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-24 21:57:00 / Modified: 23:20:37" itemprop="dateCreated datePublished" datetime="2020-02-24T21:57:00+11:00">2020-02-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="eigenvector-and-eigenvalues">Eigenvector and Eigenvalues</h2>
<p><strong><em>Theorem. Given a symmetric matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> that is positive semi-definitive, there exists an orthogonal matrix <span class="math inline">\(V \in \mathbb{R}^{n \times n}\)</span>, such that</em></strong> <span class="math display">\[
A = V \Sigma V^T 
\]</span> <strong><em>where <span class="math inline">\(\Sigma \in \mathbb{R}^{n \times n}\)</span> is a diagonal matrix with all diagonal elements non-negative.</em></strong></p>
<p>We need the following lemma for the proof.</p>
<p><strong>Lemma: If a function <span class="math inline">\(f: S \rightarrow \mathbb{R}\)</span> is continuous on a closed set <span class="math inline">\(S\)</span>, then it takes maximum value at some point of <span class="math inline">\(S\)</span>.</strong></p>
<p><em>Proof.</em> Consider the maximization problem: <span class="math display">\[
\begin{aligned}
&amp;\max &amp; \frac{1}{2} x^T A x \\
&amp;s.t. &amp;  x^T x = 1
\end{aligned}
\]</span></p>
<p><em><strong>Question to ponder:</strong> Explain the geometric meaning of the term <span class="math inline">\(x^T A x\)</span>.</em></p>
<p>The corresponding Lagrange function is given by <span class="math display">\[
L(x, \lambda) = \frac{1}{2} x^T A x - \lambda (x^T x  - 1)
\]</span></p>
<p>Taking derivative with respect to <span class="math inline">\(x\)</span> gives <span class="math display">\[
\frac{\partial L}{\partial x} = Ax - \lambda x
\]</span></p>
<p><strong><em>Existence of maximum solution:</em></strong> Note that <span class="math inline">\(\{ x : x^T x = 1 \}\)</span> is defined the boundary of the unit cycle, which is a closed set. Combined with the fact that <span class="math inline">\(f\)</span> is a continuous function, it takes its maximum value at some point in <span class="math inline">\(\{ x : x^T x = 1 \}\)</span>. Denote this point <span class="math inline">\(v_1 \in \{ x : x^T x = 1 \}\)</span>.</p>
<p><strong><em>Gradient at the maximum point:</em></strong> At this point, it holds that <span class="math display">\[
\frac{\partial L}{\partial x} \mid_{x = v_1} = Av_1 - \lambda v_1 = 0
\]</span></p>
<p>which implies that <span class="math inline">\(v_1\)</span> is an eigenvector of <span class="math inline">\(A\)</span>. Denote its associated eigenvalue <span class="math inline">\(\lambda_1\)</span>.</p>
<p><em><strong>Question to ponder:</strong> Prove that <span class="math inline">\(\lambda_1\)</span> is the largest eigenvalue.</em></p>
<p>After finding <span class="math inline">\(v_1\)</span>, consider a new optimization problem: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; \frac{1}{2} x^T A x \\
&amp;s.t.   &amp;  x^T x = 1 \\
&amp;       &amp;  v_1^T x = 0
\end{aligned}
\]</span></p>
<p>That is, we want to find a new unit vector that is orthogonal to <span class="math inline">\(v_1\)</span> while maximizing <span class="math inline">\(\frac{1}{2} x^T A x\)</span>.</p>
<p><strong><em>Existence of maximum solution.</em></strong> By the lemma and the facts: 1. <span class="math inline">\(\{x : x^T x = 1\} \cap \{ x : v_1^T x = 0 \}\)</span> is closed.<br />
2. <span class="math inline">\(\frac{1}{2} x^T A x\)</span> is continuous.</p>
<p>Denote this maximum point <span class="math inline">\(v_2\)</span>. Now consider the new Lagrange function for the optimization <span class="math display">\[
L(x, \lambda) = \frac{1}{2} x^T A x - \lambda (x^T x  - 1) - \beta(v_1^T x - 0)
\]</span></p>
<p><strong><em>Gradient at the maximum point:</em></strong> At this point, it holds that <span class="math display">\[
\frac{\partial L}{\partial x} \mid_{x = v_2} = Av_2 - \lambda v_2 - \beta v_1 = 0
\]</span></p>
<p>Left multiplying <span class="math inline">\(v_1^T\)</span> gives <span class="math display">\[
(v_1^T A)v_2 - \lambda v_1^T v_2 - \beta v_1^T v_1 = (A v_1)^T v_2  - \beta = \lambda_1 v_1^T v_2 - \beta = -\beta = 0
\]</span> where <span class="math inline">\(v_1^T A = (A v_1)^T\)</span> follows from the symmetry of <span class="math inline">\(A\)</span>, i.e., <span class="math inline">\(A = A^T\)</span>.</p>
<p>Therefore, <span class="math display">\[
Av_2 - \lambda v_2 = 0
\]</span></p>
<p>It concludes that <span class="math inline">\(v_2\)</span> is an eigenvector of <span class="math inline">\(A\)</span>. Denote its associated eigenvalue <span class="math inline">\(\lambda_2\)</span>.</p>
<p>In a similar manner, we can define <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; \frac{1}{2} x^T A x \\
&amp;s.t.   &amp;  x^T x = 1 \\
&amp;       &amp;  v_1^T x = 0 \\
&amp;       &amp;  v_2^T x = 0 \\
\end{aligned}
\]</span></p>
<p>There exists an optimal point <span class="math inline">\(v_3\)</span>. The derivative of Lagrange function at this point is <span class="math display">\[
\frac{\partial L}{\partial x} \mid_{x = v_3} = Av_3 - \lambda v_3 - \beta_1 v_1 - \beta_2 v_2 = 0
\]</span></p>
<p>for some parameters <span class="math inline">\(\lambda, \beta_1, \beta_2\)</span>. Left multiplying <span class="math inline">\(v_1^T\)</span> or <span class="math inline">\(v_2^T\)</span> gives <span class="math inline">\(\beta_1 = 0\)</span> and <span class="math inline">\(\beta_2 = 0\)</span> so that <span class="math inline">\(v_3\)</span> is an eigenvector of <span class="math inline">\(A\)</span>.</p>
<p>Following the procedure, we can find <span class="math inline">\(n\)</span> eigenvectors <span class="math inline">\(v_1, v_2, ..., v_n\)</span> of <span class="math inline">\(A\)</span>. Define <span class="math display">\[
\Sigma =
\begin{aligned}
    \left[
    \begin{matrix}
    &amp;\lambda_1 &amp;    &amp;   \\
    &amp;          &amp;\lambda_2 &amp; \\
    &amp;           &amp;   &amp;   ... \\
    &amp;           &amp;    &amp;      &amp; \lambda_n 
    \end{matrix} 
    \right]
\end{aligned}
\qquad
V =
\begin{aligned}
    \left[
    \begin{matrix}
    v_1, v_2, ..., v_n
    \end{matrix} 
    \right]
\end{aligned}
\]</span></p>
<p>Then <span class="math display">\[
A V = V \Sigma
\]</span></p>
<p>Hence, <span class="math display">\[
A = V \Sigma V^{-1} = V \Sigma V^T
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="svd">SVD</h2>
<p>In general, a matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> takes a vector <span class="math inline">\(x \in \mathbb{R}^n\)</span> to a vector <span class="math inline">\(Ax \in \mathbb{R}^m\)</span>. It is natural to ask, is there a vector <span class="math inline">\(x\)</span>, such that its length is enlarged by the largest factor after the transformation: <span class="math display">\[
\max_{x \in \mathbb{R}^n } \frac{ ||Ax || }{ ||x|| }
\]</span></p>
<p>As <span class="math inline">\(||x||\)</span> is a number, <span class="math inline">\(\frac{ ||Ax || }{ ||x|| } = || A \frac{x}{||x||} ||\)</span>. This is equivalent to find a unit vector <span class="math inline">\(x\)</span> that maximize its length (or equivalently, the square of its length) after the transformation <span class="math display">\[
\begin{aligned}
&amp;\max &amp; \frac{1}{2} x^T A^T Ax \\
&amp;s.t. &amp;  x^T x = 1
\end{aligned}
\]</span></p>
<p>In this case, the objective function is <span class="math inline">\(||Ax||^2 = (Ax)^T (Ax) = x^T A^T A x\)</span>.</p>
<p>Following similar process of previous section, we can find a set of eigenvectors <span class="math inline">\(v_1, v_2, ..., v_n\)</span> of <span class="math inline">\(A^T A\)</span>. Define <span class="math inline">\(U \in \mathbb{R}^{m \times n}\)</span> <span class="math display">\[
U = A V { \sqrt{\Sigma^{-1} } } = AV 
\begin{aligned}
    \left[
    \begin{matrix}
    &amp;\sqrt { 1 / \lambda_1} &amp;    &amp;   \\
    &amp;          &amp;\sqrt { 1 / \lambda_2} &amp; \\
    &amp;           &amp;   &amp;   ... \\
    &amp;           &amp;    &amp;      &amp; \sqrt { 1 / \lambda_n} 
    \end{matrix} 
    \right]
\end{aligned}
\]</span></p>
<p>Then <span class="math inline">\(U { \sqrt \Sigma } V^T = A\)</span>, which is exactly the SVD.</p>
<p>Finally, we can verify that the columns of <span class="math inline">\(U\)</span> are perpendicular, <span class="math inline">\(\forall i \neq j, i, j \in [n]\)</span>, <span class="math display">\[
u_i^T u_j = (\sqrt { 1 / \lambda_i} A V_i)^T (\sqrt { 1 / \lambda_j} A V_j) = \sqrt { 1 / \lambda_i} \sqrt { 1 / \lambda_j} v_i^T A^T A v_j  = 0
\]</span></p>
<p>as <span class="math inline">\(v_i\)</span> and <span class="math inline">\(v_j\)</span> are orthogonal eigenvectors of <span class="math inline">\(A^T A\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/20/Ellipsoid/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/20/Ellipsoid/" class="post-title-link" itemprop="url">Ellipsoid method</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-20 21:19:42" itemprop="dateCreated datePublished" datetime="2020-02-20T21:19:42+11:00">2020-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-22 22:39:05" itemprop="dateModified" datetime="2020-02-22T22:39:05+11:00">2020-02-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Ellipsoid method is a weakly polynomial algorithm for solving linear programming. The preliminary version was introduced by the Russian mathematician Shor in 1977 for general convex optimization problems. Then it was applied to linear programming by Khachyan in 1979 [1]. Before interior point method, it was the first polynomial time algorithm for LP.</p>
<h2 id="feasibility-implies-optimality">Feasibility implies Optimality</h2>
<p>Now, consider the primal LP: <span class="math display">\[
\mathcal{P}:  \qquad
\begin{aligned}
  &amp;\max   &amp;c^T x    &amp; \\
  &amp;s.t.   &amp;Ax \le b &amp; \\
  &amp;       &amp; x \ge 0 &amp;
\end{aligned}
\]</span></p>
<p>whose dual is given by <span class="math display">\[
\mathcal{D}:  \qquad
\begin{aligned}
  &amp;\min   &amp;y^Tb    \ \  &amp; \\
  &amp;s.t.   &amp;y^TA \ge c^T &amp; \\
  &amp;       &amp;y \ge 0 \ \  &amp;
\end{aligned}
\]</span></p>
<p>The ellipsoid does not optimize the primal directly. Instead, it try to return a feasible point <span class="math inline">\((x, y)\)</span> that satisfies the following constraints: <span class="math display">\[
\begin{aligned}
         c^T x &amp;= y^Tb  &amp; \\
         Ax    &amp;\le b   &amp; \\
          x    &amp;\ge 0       &amp; \\
         y^TA  &amp;\ge c^T &amp; \\
         y     &amp;\ge 0 \ \  &amp;
\end{aligned}
\]</span></p>
<p>By weak duality, if such point exists, then <span class="math inline">\(x\)</span> is the optimal solution for the primal program. (Note that, if there is no feasible point for the third program, the primal could be either infeasible or unbounded.)</p>
<h2 id="ellipsoid">Ellipsoid</h2>
<p>Ellipsoid is a unit ball that has been squashed or stretched along orthogonal directions. We illustrate this by an example in two dimension. The following picture shows an ellipse obtained by enlarging the unit ball by a facto of <span class="math inline">\(2\)</span> along the <span class="math inline">\(v_1\)</span> direction and squashing it ball by half along the <span class="math inline">\(v_2\)</span> direction, where <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> constitute an orthogonal base.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Ellipsoid/EllipsoidMethod1.jpg" /></p>
<p>It is equivalent to first squash or stretch the ball along the directions of <span class="math inline">\(e_1, e_2\)</span>,</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Ellipsoid/EllipsoidMethod2.jpg" /></p>
<p>then rotate the resulting ellipsoid to corresponding orthogonal directions.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Ellipsoid/EllipsoidMethod3.jpg" /></p>
<p>The rotation can be done by left multiplying an orthogonal matrix <span class="math display">\[
V = [v_1, v_2]
\]</span></p>
<p>where <span class="math inline">\(v_1, v_2\)</span> are the orthogonal base for which the squash and stretch are performed.</p>
<p>To recover the unit ball, we can first perform the reverse rotation: <span class="math display">\[
x \leftarrow V^T x
\]</span></p>
<p>followed by a reverse scaling: <span class="math display">\[
x \leftarrow  
  \begin{bmatrix}
    1/2  &amp; \\
    &amp;  2
  \end{bmatrix} 
  x
\]</span></p>
<p>Note that the reverse rotation is represented by matrix <span class="math inline">\(V^T\)</span> as <span class="math inline">\(V V^T = I\)</span>. Therefore, the ellipsoid is denoted as <span class="math display">\[
  \left( 
  \begin{bmatrix}
    1/2  &amp; \\
    &amp;  2
  \end{bmatrix} 
  V^T x \right)^T
  \left( 
  \begin{bmatrix}
    1/2  &amp; \\
    &amp;  2
  \end{bmatrix} 
  V^T x \right) \le 1
\]</span></p>
<p>That is <span class="math display">\[
  x^T \left( V
  \begin{bmatrix}
    1/2  &amp; \\
    &amp;  2
  \end{bmatrix}^2 
  V^T \right) x  \le 1
\]</span></p>
<p>In general, an ellipsoid is represented by <span class="math display">\[
E(s, P) = \{ x : (x - s)^T P^{-1} (x - s) \le 1\}
\]</span></p>
<p>where <span class="math inline">\(s \in \mathcal{R}^n\)</span> is the center of the ellipsoid and <span class="math inline">\(P\)</span> is a positive semi-definite matrix.</p>
<p>Recall that a positive semi-definite matrix <span class="math inline">\(P\)</span> can be rewritten as <span class="math display">\[
P = V \Sigma V^T
\]</span> and <span class="math display">\[
P^{-1} = V \Sigma^{-1} V^T
\]</span></p>
<p>where <span class="math inline">\(V\)</span> is an orthogonal matrix and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix. Geometrically, <span class="math inline">\(V\)</span> are the directions and <span class="math inline">\(\sqrt \Sigma\)</span> are the ratios of the squashes or stretches. If we define <span class="math inline">\(B = V \sqrt \Sigma\)</span>, then <span class="math inline">\(B^T = \sqrt \Sigma V^T\)</span>, <span class="math inline">\(B^{-1} = \sqrt \Sigma^{-1} V^T\)</span> and <span class="math inline">\(P = B B^T\)</span>, <span class="math inline">\(P^{-1} = (B^{-1} )^T B^{-1}\)</span>.</p>
<p>The representation can be viewed as merely a recipe for converting the ellipsoid back to the unit ball:</p>
<ol type="1">
<li>First perform a translation <span class="math inline">\(x \leftarrow (x - s)\)</span> so that the center of the ellipsoid is now the center.</li>
<li>Rotate the ellipsoid, such that its axes are aligned with <span class="math inline">\(e_1, e_2, ..., e_n\)</span>. This is done by multiplying the matrix <span class="math inline">\(V^T\)</span>: <span class="math inline">\(x \leftarrow V^T x\)</span>.</li>
<li>Squash or stretch along the axes <span class="math inline">\(e_1, e_2, ..., e_n\)</span>: <span class="math inline">\(x \leftarrow \sqrt \Sigma^{-1} x\)</span>.</li>
<li>Therefore, the linear transformation for the recovery is <span class="math display">\[
x \leftarrow B^{-1} (x - s)
\]</span></li>
</ol>
<p>Conversely, <span class="math inline">\(E(s, P) = E(s, V \sqrt \Sigma \sqrt \Sigma V^T)\)</span> tells us how to get the ellipsoid from the unit ball:</p>
<ol type="1">
<li>Scale unit ball along the axes <span class="math inline">\(e_1, e_2, .., e_n\)</span> by factors that <span class="math inline">\(\sqrt \Sigma_{1, 1}, \sqrt \Sigma_{2, 2}, ..., \sqrt \Sigma_{n, n}\)</span>.</li>
<li>Rotate the ellipsoid by matrix <span class="math inline">\(V\)</span>.</li>
<li>Move the ellipsoid by <span class="math inline">\(s\)</span>.</li>
</ol>
<p>That is, <span class="math inline">\(x \leftarrow V \sqrt \Sigma y + s\)</span> takes the unit ball <span class="math inline">\(\{ y : y^T y \le 1 \}\)</span> to the corresponding ellipsoid <span class="math inline">\(\{ x : (x - s)^T V \Sigma^{-1} V^T (x - s) \le 1 \}\)</span>.</p>
<h2 id="goal">Goal</h2>
<p>Now return to our discussion of ellipsoid method. Without lose of generality, we consider only the following problem:</p>
<p><em>Given <span class="math inline">\(F = \{x : Ax \le b\}\)</span>, output any <span class="math inline">\(x \in F\)</span> or report <span class="math inline">\(F = \emptyset\)</span>, where <span class="math inline">\(A \in \mathcal{R}^{m \times n}, x \in \mathcal{R}^n, b \in \mathcal{R}^m\)</span></em>.</p>
<p>We assume further that</p>
<ol type="1">
<li><p><span class="math inline">\(\exists R \in \mathcal{R}\)</span>, <span class="math inline">\(F \subset B(0, R)\)</span>. Besides, if <span class="math inline">\(F \neq \emptyset\)</span>, <span class="math inline">\(\exists r \in \mathcal{R}\)</span>, and <span class="math inline">\(t \in \mathcal{R}^n\)</span>, s.t., <span class="math inline">\(B(t, r) \subset F\)</span>. Here <span class="math inline">\(B(0, R) = \{ x : ||x||_2 \le R \}\)</span> and <span class="math inline">\(B(t, r) = \{ x : ||x - t||_2 \le r \}\)</span>.</p></li>
<li><p><em>Separation Oracle:</em> Given a vertex <span class="math inline">\(s \in \mathcal{R}^n\)</span>, the separation oracle is a mechanism that asserts</p>
<ul>
<li>either <span class="math inline">\(s \in F\)</span></li>
<li>or <span class="math inline">\(s \notin F\)</span> and returns a direction (which is a unit vector) <span class="math inline">\(d \in \mathcal{R}^n\)</span>, such that <span class="math inline">\(d^T s \ge d^T x\)</span> for <span class="math inline">\(\forall x \in F\)</span>.</li>
</ul></li>
</ol>
<p>The first assumption says that <span class="math inline">\(F\)</span> is neither too "large" nor too "small". Note that <span class="math inline">\(B(t, r) \subset F\)</span> also implies that <span class="math inline">\(F\)</span> has full dimension.</p>
<p>For LP, the separation is straightforward to implement: we just check whether <span class="math inline">\(Ac \ge b\)</span>. If so, then <span class="math inline">\(c \in F\)</span>. Otherwise, <span class="math inline">\(\exists i \in [m]\)</span>, such that the <span class="math inline">\(i\)</span>-th constraint is violated: <span class="math inline">\(A_{i,\cdot} c &gt; b_i\)</span>. Just return <span class="math inline">\(d^T = A_{i, \cdot}\)</span>.</p>
<p><strong><em>Question to ponder: the feasible region of an LP may not satisfy the first assumption? It is possible to reduce it to one satisfying the first assumption?</em></strong></p>
<h2 id="the-algorithm">The Algorithm</h2>
<p>The algorithm begins with a special ellipsoid <span class="math inline">\(E_0 = B(0, R)\)</span>. At each step, it queries the <em>separation oracle</em> and generates a smaller ellipsoid (with reduced volume). If <span class="math inline">\(F\)</span> is not empty, the ellipsoid at each step is guaranteed to contain it. The algorithm stops before the volume of the ellipsoid decreases below <span class="math inline">\(vol(B(t, r))\)</span>.</p>
<p>The algorithmic flow is shown below:</p>
<table style="width:53%;">
<colgroup>
<col style="width: 52%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">1. <span class="math inline">\(i = 0\)</span>.</td>
</tr>
<tr class="even">
<td style="text-align: left;">2. <span class="math inline">\(E_0 = B(0, R)\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3. <em>While</em> <span class="math inline">\(vol(E_i(s_i, P_i)) \ge vol(B(t, r))\)</span>:</td>
</tr>
<tr class="even">
<td style="text-align: left;">4. <span class="math inline">\(\qquad\)</span> <em>if</em> <span class="math inline">\(s_i \in F\)</span>, output <span class="math inline">\(s_i\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5. <span class="math inline">\(\qquad\)</span> <em>else</em>,</td>
</tr>
<tr class="even">
<td style="text-align: left;">6. <span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> Obtain a direction <span class="math inline">\(d_i\)</span>, s.t., <span class="math inline">\(d_i^T s_i \ge d_i^T x\)</span> for <span class="math inline">\(x \in F\)</span>*.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">7. <span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> Calculate a ellipsoid <span class="math inline">\(E_{i + 1} \supset E_{i} \cap \{x : d_i^T s_i \ge d_i^T x \}\)</span>.</td>
</tr>
<tr class="even">
<td style="text-align: left;">8. <span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> <span class="math inline">\(i \leftarrow i + 1\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">9. Output <span class="math inline">\(F = \emptyset\)</span>.</td>
</tr>
</tbody>
</table>
<p>For the one dimension case, it is nothing more than a variant of binary search.</p>
<h2 id="bounding-the-number-of-iterations">Bounding the Number of Iterations</h2>
<p>The main result that characterizes the running time is</p>
<p><strong><em>Theorem: It is guaranteed that for <span class="math inline">\(\forall i\)</span></em></strong>, <span class="math display">\[
\frac{vol( E_{i + 1} ) } {vol(E_i) } \le \exp \left( -\frac{1}{2 (n + 1) } \right) 
\]</span> <span class="math inline">\(\square\)</span>.</p>
<p><em>Corollary: the number of iterations of the ellipsoid method is bounded by</em> <span class="math display">\[
2n(n + 1) \ln \frac{R}{r}
\]</span> <span class="math inline">\(\square\)</span>.</p>
<p>To simplify the notation further, we write <span class="math inline">\(E_i = E(s, P)\)</span>, <span class="math inline">\(d_i = d\)</span> and <span class="math inline">\(E_{i + 1} = E(s&#39;, P&#39;)\)</span>. The key lemma for the theorem is:</p>
<p><em>Lemma: Given an ellipsoid <span class="math inline">\(E(s, P) = \{ x \in \mathcal{R}^n : (x - s)^T P^{-1} (x - s) \le 1\}\)</span> and the half space <span class="math inline">\(H(s, d) = \{ x \in \mathcal{R}^n : d^T (x - s)\le 0 \}\)</span>, we can find an ellipsoid <span class="math inline">\(E(s&#39;, P&#39;)\)</span>, such that</em></p>
<ol type="1">
<li><em><span class="math inline">\(E(s&#39;, P&#39;)\)</span> contains the intersection of <span class="math inline">\(E(s, P)\)</span> and <span class="math inline">\(H(s, d)\)</span> : <span class="math inline">\(E(c, P) \cap \{ x \in \mathcal{R}^n : d^T (x - s)\le 0 \}\)</span></em>.<br />
</li>
<li><em>The volume of <span class="math inline">\(E(s&#39;, P&#39;)\)</span> is at most <span class="math inline">\(\exp \left( -\frac{1}{2 (n + 1) } \right)\)</span> times that of <span class="math inline">\(E(s, P)\)</span></em>.</li>
</ol>
<p>In particular, <span class="math inline">\(s&#39;\)</span> and <span class="math inline">\(P&#39;\)</span> is given by <span class="math display">\[
\begin{aligned}
s&#39; &amp;= s - \frac{1}{n + 1} \frac{P d}{\sqrt{d^T P d} } \\
P&#39; &amp;= \frac{n^2}{n^2 - 1} \left( P - \frac{2}{n + 1} \frac{Pd}{\sqrt{d^T P d} } (\frac{Pd}{\sqrt{d^T P d} } )^T \right)
\end{aligned}
\]</span></p>
<p><em>Proof.</em></p>
<p>The transformation <span class="math inline">\(y = B^{-1} (x - s)\)</span> maps <span class="math inline">\(E(s, P)\)</span> to <span class="math inline">\(E(0， I)\)</span>, and <span class="math inline">\(\{x \in R^n: d^T B (B^{-1} (x - s)) \le 0\}\)</span> to <span class="math inline">\(H(0, B^T d) = \{y \in R^n: d^T B y \le 0\}\)</span>. Let <span class="math inline">\(e&#39; = \frac{B^T d}{\sqrt{d^T B B^T d} } = \frac{B^T d}{\sqrt{d^T P d} }\)</span> so that <span class="math inline">\(H(0, B^Td)\)</span> is equivalent to <span class="math inline">\(H(0, e&#39;)\)</span>.</p>
<p>It suffices to find an ellipsoid that contains <span class="math inline">\(E(0, I) \cap H(0, e&#39;)\)</span>. Then applying the reverse transformation <span class="math inline">\(x = By + s\)</span> will give us an ellipsoid that contains <span class="math inline">\(E(s, P) \cap H(s, d)\)</span>.</p>
<p>We can further reduce the problem to finding an ellipsoid that covers <span class="math inline">\(E(0, I) \cap H(0, e_1)\)</span> as we can recover the ellipsoid by rotating <span class="math inline">\(e_1\)</span> to <span class="math inline">\(e&#39;\)</span>.</p>
<p><strong><em>Claim.</em></strong> The ellipsoid that covers <span class="math inline">\(E(0,I) \cap H(0, e_1)\)</span> is given by <span class="math display">\[
E \left( -\frac{1}{n + 1}e_1, \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T) \right)
\]</span> and has minimum volume <span class="math inline">\(\exp ( -\frac{1}{2(n + 1)} )\)</span>.</p>
<p>To under the ellipsoid, we check how it transforms the unit ball to an ellipsoid:</p>
<ol type="1">
<li>It squashes the ball along the <span class="math inline">\(e_1\)</span> axis, by a factor of <span class="math inline">\(\sqrt{ \frac{n^2} {n^2 - 1} \frac{n - 1} {n + 1} } = \frac{n}{ n + 1 }\)</span>.</li>
<li>Then it stretches all other directions <span class="math inline">\(e_2, e_3, ..., e_n\)</span> by a factor of <span class="math inline">\(\sqrt \frac{n^2}{n^2 - 1}\)</span>.</li>
<li>Finally, move the center of the ellipsoid to <span class="math inline">\(-\frac{1}{n + 1}e_1\)</span>.</li>
</ol>
<p>Note that the ellipsoid has volume <span class="math inline">\((1 - \frac{1}{n + 1}) (1 + \frac{1}{n^2 -1} )^{(n - 1) / 2} \le e^{- \frac{1}{n + 1} + \frac{1}{n^2 -1} (n - 1) / 2} = e^{- \frac{1}{2(n + 1)} }\)</span>. This is also the ratio of the ellipsoid over that of the unit ball, whose value is 1.</p>
<p>Before we prove the claim, we show how to transform the ellipsoid back to <span class="math inline">\(E(s&#39;, P&#39;)\)</span> that covers <span class="math inline">\(E(s, P) \cap H(s, d)\)</span>. First, by symmetry, the ellipsoid that covers <span class="math inline">\(E(0, I) \cap H(0, e&#39;)\)</span> is given by <span class="math display">\[
 E(-\frac{1}{n + 1}e&#39;, \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e&#39; (e&#39;)^T) )
\]</span></p>
<p>Applying the mapping <span class="math inline">\(B y + s = x\)</span>: <span class="math display">\[
E(-\frac{1}{n + 1} e&#39;, \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e&#39; (e&#39;)^T) ) \rightarrow E(s - \frac{1}{n + 1} B^T e&#39;, \frac{n^2}{n^2 - 1} B (I - \frac{2}{n + 1}e&#39; (e&#39;)^T) B^T )
\]</span></p>
<p><strong>As linear transformation preserves the ratio of volumes, the ratio of the volume of the final ellipsoid over that of <span class="math inline">\(E(s, P)\)</span> is also <span class="math inline">\(\exp(- \frac{1}{2 (n + 1)} )\)</span>.</strong></p>
<p><strong><em>Question to ponder:</em></strong> 1. Show that <span class="math inline">\(\frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T)\)</span> is positive definite. 2. Show that <span class="math inline">\(\left( \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T) \right)^{-1} = \frac{n^2 - 1}{n^2} I + \frac{2n + 2}{n^2} e_1 e_1^T\)</span>. 3. Show that <span class="math inline">\(\frac{n^2}{n^2 - 1} B (I - \frac{2}{n + 1}e&#39; (e&#39;)^T) B^T\)</span> is positive definite.</p>
<p><em>Proof of the claim</em>.</p>
<p>As both <span class="math inline">\(E(0,I)\)</span> and <span class="math inline">\(H(0, e_1)\)</span> are symmetric to <span class="math inline">\(e_1\)</span>, so should the ellipsoid covering the intersection of them. Therefore it should be of the form <span class="math display">\[
a_1 (x_1 - \lambda) + \sum_{i = 2}^n a_i x_i^2 \le 1
\]</span></p>
<p>where <span class="math inline">\(a_i\)</span>'s and <span class="math inline">\(\lambda\)</span> (<span class="math inline">\(0 &lt; \lambda &lt; 1\)</span>) are the parameters to be determined. The goal is to minimize its volume:</p>
<p><span class="math display">\[
\sqrt{\frac{1}{a_1} \prod_{i = 2}^n \frac{1}{a_i} } vol[ E(0, I) ]
\]</span></p>
<p>where <span class="math inline">\(vol[ E(0, I) ] = 1\)</span> is the volume of unit ball in <span class="math inline">\(R^n\)</span>. It is equivalent to maximize <span class="math display">\[
a_1 \prod_{i = 2}^n a_i
\]</span></p>
<p>As the ellipsoid covers <span class="math inline">\(E(0,I) \cap H(0, e_1)\)</span> and should has as small volume as possible, the points <span class="math inline">\(e_i\)</span> (<span class="math inline">\(i = 1, 2, ..., n\)</span>) should be on its boundary. Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
a_1 (1 - \lambda)^2 + \sum_{i = 2}^n a_i 0^2 &amp;= 1 &amp;\rightarrow a_1(1 - \lambda)^2 = 1 &amp;\\
a_1 (0 - \lambda)^2 + a_i 1^2 &amp;= 1  &amp;\rightarrow a_i = 1 - a_1 \lambda^2  &amp;\quad  \forall i = 2, 3, ..., n  \\
\end{aligned}
\]</span></p>
<p>Replacing <span class="math inline">\(a_i\)</span> with <span class="math inline">\(1 - a_1 \lambda^2\)</span> and then <span class="math inline">\(a_1\)</span> with <span class="math inline">\(\frac{1}{(1 - \lambda)^2}\)</span>, the goal becomes</p>
<p><span class="math display">\[
\max \quad \frac{1}{(1 - \lambda)^2} \left[ 1 - \frac{\lambda^2}{(1 - \lambda)^2} \right]^{n - 1} 
\]</span></p>
<p>Substitute <span class="math inline">\(\frac{1}{1 - \lambda}\)</span> by a variable <span class="math inline">\(x &gt; 1\)</span>, and let <span class="math display">\[
f \doteq x^2 [1 - (x - 1)^2]^{n - 1}
\]</span></p>
<p>Its derivative is <span class="math display">\[
f&#39; = 2x [1 - (x - 1)^2]^{n - 1} - 2 (n - 1)(x - 1) x^2 [1 - (x - 1)^2]^{n - 2}
\]</span></p>
<p>Setting the derivative to zero, we obtain</p>
<p><span class="math display">\[
\begin{aligned}
&amp;[1 - (x - 1)^2] - (n - 1)(x - 1)x = 0 \\
&amp;\Rightarrow 1 - x^2 + 2x - 1- (n -1 )x^2 + (n -1 )x = 0 \\
&amp;\Rightarrow - n x^2 + (n + 1) x = 0 \\
&amp;\Rightarrow x = \frac{n + 1}{n}
\end{aligned}
\]</span></p>
<p>It follows that <span class="math inline">\(a_1 = (\frac{n + 1}{n})^2\)</span>, <span class="math inline">\(\lambda = \frac{1}{n + 1}\)</span> and <span class="math inline">\(a_i = \frac{n^2 - 1}{n^2}\)</span> for <span class="math inline">\(\forall i = 2, 3, ..., n\)</span>.</p>
<!-- Therefore, 
$$
c_1 = \left[ 
    \begin{matrix}
\frac{1}{n + 1} \\
 0 \\
  ...\\
0
\end{matrix}
\right] 
= \frac{1}{n + 1} e_1
$$
and 
$$
P_1^{-1} = \left[ 
\begin{matrix}
(\frac{n + 1}{n} )^2 &                       &   ...     \\
                    &   \frac{n^2 - 1}{n^2}  &   ...     \\
                    &                        &   ...     \\
                    &                        &   &\frac{n^2 - 1}{n^2}
\end{matrix}
\right] = \frac{n^2 - 1}{n^2} I + 2\frac{n + 1}{n^2} e_1 e_1^T \\
P_1 = \left[ 
\begin{matrix}
(\frac{n}{n + 1} )^2 &                       &   ...     \\
                    &   \frac{n^2}{n^2 - 1}  &   ...     \\
                    &                        &   ...     \\
                    &                        &   &\frac{n^2}{n^2 - 1}
\end{matrix}
\right] = \frac{n^2}{n^2 - 1} (I - \frac{2}{n + 1}e_1 e_1^T)
$$

The volume ratio between $E(c_1, P_1)$ and $E(0, I)$ is 

$$
\begin{aligned}
\sqrt{\frac{1}{a_1} \prod_{i = 2}^n \frac{1}{a_i} } 
&= \frac{n}{n + 1} \left[ \frac{n^2 }{n^2 - 1} \right]^{(n - 1) / 2} \\
&\le \exp \left( -\frac{1}{n + 1} + \frac{1}{2}\frac{n - 1}{n^2 - 1} \right) \\
&= \exp \left( -\frac{1}{2} \frac{1}{n + 1} \right)
\end{aligned}
$$ -->
<h2 id="reference">Reference</h2>
<ol type="1">
<li>Michel X. Goemans, 7. Lecture notes on the ellipsoid algorithm, 18.433: Combinatorial Optimization, MIT,</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/18/Determinant,%20Cross%20Product%20and%20Cramer's-Rule/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/18/Determinant,%20Cross%20Product%20and%20Cramer's-Rule/" class="post-title-link" itemprop="url">Determinant, Cross Product and Cramer's-Rule</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-18 23:36:52" itemprop="dateCreated datePublished" datetime="2020-02-18T23:36:52+11:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-21 19:01:14" itemprop="dateModified" datetime="2020-04-21T19:01:14+10:00">2020-04-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="determinant"><strong><em>Determinant</em></strong></h3>
<p>When studying linear algebra, the determinant function <span class="math inline">\(\det(\cdot)\)</span> is a horrifying subject. We try to give a friendly approach to it in this article.</p>
<p>I think it is the study of the volume of the parallelotope specified by <span class="math inline">\(v_1, v_2, ... ,v_n \in \mathbb{R}^n\)</span> that gives it away. The parallelotope is defined as the set <span class="math display">\[
\left\{ \sum_{i \in [n]} \lambda_i v_i :  \lambda_i \in [0, 1], \forall i \in [n] \right\}
\]</span></p>
<p>Let's denote <span class="math inline">\(f(v_1, v_2, ... ,v_n) : \mathbb{R}^{n \times n} \rightarrow \mathbb{R}\)</span> the singed volume function, i.e., <span class="math inline">\(|f(v_1, v_2, ... ,v_n)|\)</span> gives the volume of the parallelotope. We will show how to determine the sign of <span class="math inline">\(f\)</span> later. At this point, we do not know how <span class="math inline">\(\det(\cdot)\)</span> is related to the volume. Therefore, we use the notation <span class="math inline">\(f\)</span> instead.</p>
<p>We show how to derive the formula of <span class="math inline">\(f\)</span> by its properties. The two key properties associated with volume are</p>
<ol type="1">
<li><p><span class="math inline">\(f\)</span> is multi-linear. Specially, it is a linear function in any dimension when other dimensions are fixed:</p>
<ul>
<li><p><span class="math inline">\(f(v_1, v_2, ..., cv_i, ... ,v_n)=cf(v_1, v_2, ... ,v_n)\)</span> for <span class="math inline">\(\forall i \in [n]\)</span> and <span class="math inline">\(c \in \mathbb{R}\)</span>. Geometrically, if we scale the <span class="math inline">\(i\)</span>-th vector <span class="math inline">\(v_i\)</span> by a factor <span class="math inline">\(c\)</span>, then the volume of the parallelotope should change by the same factor. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity1.jpg" /> In the above example, when <span class="math inline">\(v_2\)</span> is fixed, the area of the parallelogram is determined by its height, i.e., the projection of <span class="math inline">\(v_1\)</span> to the orthogonal direction of <span class="math inline">\(v_2\)</span>. When <span class="math inline">\(v_1\)</span> doubles, the length of its projection doubles.</p></li>
<li><p><span class="math inline">\(f(v_1, v_2, ..., v_i + u_i, ... ,v_n)= f(v_1, v_2, ..., v_i, ... ,v_n) + f(v_1, v_2, ..., u_i, ... ,v_n)\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity2.jpg" /> In the above example, when <span class="math inline">\(v_2\)</span> is fixed, the projection to the orthogonal direction of <span class="math inline">\(v_2\)</span> are additive. In particular, denote <span class="math inline">\(\mathcal{P}(v_1)\)</span> the length of the projection of <span class="math inline">\(v_1\)</span> to the orthogonal direction of <span class="math inline">\(v_2\)</span>. Similarly we define <span class="math inline">\(\mathcal{P}(v_2)\)</span> and <span class="math inline">\(\mathcal{P}(v_1 + v_2)\)</span>. Then<br />
<span class="math display">\[
\mathcal{P}(v_1 + v_2) = \mathcal{P}(v_1) + \mathcal{P}(v_2)
\]</span></p></li>
<li><p>Note that the projection of two vector can cancel each other in some cases, in the sense that they points to different direction. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity4.jpg" /> In the above example, if we write <span class="math inline">\(v_1 = v_{1, 1} e_1 + v_{1, 2} e_2\)</span>, then <span class="math inline">\(\mathcal{P} (v_{1, 1} e_1)\)</span> and <span class="math inline">\(\mathcal{P} (v_{1,2} e_2)\)</span> points to opposite directions. This implies that we should give volume a sign. For example, we can define the direction pointed by purple vector points to be positive. The volume of the parallelogram increases if a vector's projection moves along this direction and decrease otherwise.</p></li>
</ul></li>
<li><p>If <span class="math inline">\(v_1, v_2, ..., v_n\)</span> are linearly independent, then the parallelotope is contained in a subspace with dimension smaller than <span class="math inline">\(n\)</span> and should have volume zero. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/NonIndependent.jpg" /> Intuitively, the parallelotope collapses to a lower dimension space. Examples include colinear vectors in two dimension case and the third vector lying in the hyperplane spanned by the other two in the three dimension case.<br />
Finally, we emphases an important case here: <span class="math display">\[
f(..., v_i,..., v_j, ... , ) = 0, \textbf{ if } v_i = v_j, \forall i, j \in [n], i \neq j
\]</span></p></li>
</ol>
<p>If we write <span class="math display">\[
v_i = v_{i, 1} e_1 + v_{i, 2} e_2 + ... + v_{i, n} e_n
\]</span></p>
<p>for all <span class="math inline">\(i \in [n]\)</span>, then by linearity <span class="math display">\[
\begin{aligned}
f(v_1, v_2, ..., v_n) 
    &amp;= f( v_{1, 1} e_1 + v_{1, 2} e_2 + ... + v_{1, n} e_n, \\
    &amp;\qquad v_{2, 1} e_1 + v_{2, 2} e_2 + ... + v_{2, n} e_n, \\
    &amp;\qquad ...\\
    &amp;\qquad v_{n, 1} e_1 + v_{n, 2} e_2 + ... + v_{n, n} e_n) \\
    &amp;= \sum_{k_1, k_2, ..., k_n \in [n]} v_{1, k_1} v_{2, k_2} ... v_{n, k_n} f(e_{k_1}, e_{k_2}, ..., e_{k_n}) 
\end{aligned}
\]</span></p>
<p>Note that if <span class="math inline">\(k_i = k_j\)</span> for <span class="math inline">\(i \neq j\)</span>, then <span class="math inline">\(f(e_{k_1}, e_{k_2}, ..., e_{k_n}) = 0\)</span>. Therefore, <span class="math display">\[
f(v_1, v_2, ..., v_n)  = \sum_{ k  \text{ is a permutation} } v_{1, k_1} v_{2, k_2} ... v_{n, k_n} f(e_{k_1}, e_{k_2}, ..., e_{k_n})
\]</span></p>
<p>We are almost done. It is left to determined the value of <span class="math inline">\(f(e_{k_1}, e_{k_2}, ..., e_{k_n})\)</span>, where <span class="math inline">\(k = (k_1, k_2, ..., k_n)\)</span> is a permutation of <span class="math inline">\((1, 2, ...., n)\)</span>. Here we claim that is purely determined by the value of <span class="math inline">\(f(e_1, e_2, ..., e_n)\)</span>. By conversion, we define <span class="math display">\[
f(e_1, e_2, ..., e_n) = 1
\]</span> Then we have the key lemma:</p>
<p><strong><em>Lemma</em></strong> <span class="math display">\[
f(e_{k_1}, e_{k_2}, ..., e_{k_n}) = -1
\]</span></p>
<p>if <span class="math inline">\({k_1}, {k_2}, ..., {k_n}\)</span> can be recovered to <span class="math inline">\(1, 2, ..., n\)</span> by odd number of neighboring swaps and <span class="math display">\[
f(e_{k_1}, e_{k_2}, ..., e_{k_n}) = 1
\]</span> if <span class="math inline">\({k_1}, {k_2}, ..., {k_n}\)</span> can be recovered to <span class="math inline">\(1, 2, ..., n\)</span> by even number of neighboring swaps.</p>
<p><em>Proof.</em> The key is to prove that we swap two neighboring vectors, then the sign of the volume change: <span class="math display">\[
f(e_{k_1}, e_{k_2}, ..., e_{k_{i } }, e_{k_{i + 1} }, ..., e_{k_n}) = -f(e_{k_1}, e_{k_2}, ..., e_{k_{i + 1 } }, e_{k_{i } }, ..., e_{k_n})
\]</span></p>
<p>This is because <span class="math display">\[
\begin{aligned}
    f(e_{k_1}, e_{k_2}, ..., e_{k_{i } } + e_{k_{i + 1} }, e_{k_{i } } + e_{k_{i + 1} }, ..., e_{k_n})  
        &amp;=
        f(e_{k_1}, e_{k_2}, ..., e_{k_{i } }, e_{k_{i + 1} }, ..., e_{k_n})\\
        &amp;\quad + 
        f(e_{k_1}, e_{k_2}, ..., e_{k_{i + 1 } }, e_{k_{i } }, ..., e_{k_n})
        \\
        &amp;= 0
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="cross-product"><strong><em>Cross Product</em></strong></h3>
<p>If we fix <span class="math inline">\(v_2, v_3, ..., v_n\)</span>, then <span class="math inline">\(\det(x, v_2, ..., v_n): \R^n \rightarrow \R\)</span> is a linear function. Then <span class="math display">\[
\begin{aligned}
    \det(x, v_2, ..., v_n) &amp;= \sum_{i = 1}^n \det(x_i e_i, v_2, ..., v_n) \\
    &amp;= \sum_{i = 1}^n x_i \det(e_i, v_2, ..., v_n)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is the i-th coordinator of <span class="math inline">\(x\)</span>. If we write <span class="math display">\[
d = \left&lt; det(e_1, v_2, ..., v_n), det(e_2, v_2, ..., v_n), ..., det(e_n, v_2, ..., v_n)\right&gt;
\]</span></p>
<p>Then <span class="math display">\[
\det(x, v_2, ..., v_n) = d \cdot x
\]</span></p>
<p>Indeed, in a similar manner, we can prove that for any <span class="math inline">\(h: \R^{n \times n} \rightarrow \R^n\)</span> linear function, there exists a vector <span class="math inline">\(\vec h\)</span>, such that <span class="math display">\[
h(x) = \vec h \cdot x
\]</span></p>
<p>Conventionally, <span class="math inline">\(d\)</span> is called the cross product of <span class="math inline">\(v_2, ..., v_n\)</span> is written as <span class="math display">\[
d = v_2 \times v_3 \times ... \times v_n
\]</span></p>
<p>It has some interesting properties:</p>
<ol type="1">
<li><p><span class="math inline">\(d\)</span> is perpendicular to the subspace spanned by <span class="math inline">\(v_2, ..., v_n\)</span>, as <span class="math inline">\(d \cdot v_i = \det(v_i, v_2, ..., v_n) = 0\)</span> for <span class="math inline">\(2 \le i \le n\)</span>.</p></li>
<li><p>For any <span class="math inline">\(x \in \R^n\)</span>, <span class="math inline">\(d \cdot x\)</span> gives the signed volume of the parallelotope spanned by <span class="math inline">\(x, v_2, ..., v_n\)</span>.</p>
<ul>
<li>When <span class="math inline">\(n = 2\)</span>, the length of <span class="math inline">\(d\)</span> equals to that of <span class="math inline">\(v_2\)</span>.</li>
<li>When <span class="math inline">\(n = 3\)</span>, the length of <span class="math inline">\(d\)</span> equals to the area of the parallelogram spanned by <span class="math inline">\(v_2, v_3\)</span>. Combined with the fact that <span class="math inline">\(d\)</span> is perpendicular to <span class="math inline">\(v_2, v_3\)</span>, we see why <span class="math inline">\(d \cdot x\)</span> gives the volume of the parallelotope spanned by <span class="math inline">\(x, v_2, v_3\)</span>: it takes the projection of <span class="math inline">\(x\)</span> to the direction that is perpendicular to <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span>, scaled by a factor of the area spanned by <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span>.<br />
</li>
</ul></li>
<li><p>The direction given of <span class="math inline">\(d\)</span> is given by right hand rule, when <span class="math inline">\(n = 2\)</span> or <span class="math inline">\(n = 3\)</span>.</p></li>
</ol>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="cramers-rule"><strong><em>Cramer's Rule</em></strong></h3>
<p>Cramer's rule is a formula for solving linear equations. In particular, given an invertible matrix <span class="math inline">\(A \in \R^{n \times n}\)</span>, and vector <span class="math inline">\(b \in \R^n\)</span>, then the solution for the equation <span class="math display">\[
Ax = b
\]</span></p>
<p>is given by (<span class="math inline">\(\forall i \in [n]\)</span>), <span class="math display">\[
x_i = \frac{\det(A_i) }{\det (A)}
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is the i-th coordinate of <span class="math inline">\(x\)</span> and <span class="math inline">\(A_i\)</span> is the matrix formed by replacing <span class="math inline">\(A\)</span>'s the <span class="math inline">\(i\)</span>-th column by column vector <span class="math inline">\(b\)</span>.</p>
<p>To understand the formula, we view <span class="math inline">\(A\)</span> as a mapping <span class="math inline">\(T: \R^n \rightarrow \R^n\)</span> that takes <span class="math inline">\(e_i (i \in [n])\)</span> to <span class="math inline">\(A[:,i]\)</span>, i.e., <span class="math display">\[
T(e_i) = A[:, i]
\]</span></p>
<p>where <span class="math inline">\(A[:, i]\)</span> is the i-th column of <span class="math inline">\(A\)</span>. The image of the hypercube <span class="math display">\[
\left\{ \sum_{i \in [n]} \lambda_i e_i :  \lambda_i \in [0, 1], \forall i \in [n] \right\}
\]</span></p>
<p>is given as <span class="math display">\[
\left\{ \sum_{i \in [n]} \lambda_i A[:, i] :  \lambda_i \in [0, 1], \forall i \in [n] \right\}
\]</span></p>
<p>Original the hypercube has volume 1. After the transformation it has volume <span class="math inline">\(\det(A)\)</span>. One interesting fact about linear transformation is that it preserves volumes ratio. Define <span class="math inline">\(\mathbb{V}(\cdot)\)</span> the volume of a parallelotope.</p>
<p><em>Fact.</em> For any parallelotope <span class="math inline">\(P\)</span>, its volume under transformation <span class="math inline">\(T\)</span> is given by <span class="math display">\[
\det(A) \mathbb{V}(P) = \mathbb{V}(T(P))
\]</span></p>
<p>Now, we rewrite the Cramer's rule as <span class="math display">\[
\det(A) x_i = \det(A_i)
\]</span></p>
<p>Its meaning is clear: there is a parallelotope <span class="math inline">\(P\)</span> whose volume is <span class="math inline">\(x_i\)</span> and after the transformation it has volume <span class="math inline">\(\det(A_i)\)</span>.</p>
<p>It is left to verify that the parallelotope indeed exists. We observe that <span class="math inline">\(\det(A_i)\)</span> can be viewed as the volume of the following parallelotope formed by <span class="math display">\[
\left\{ \sum_{j \in [n], j \neq i} \lambda_j A[:, j] +\lambda_i b_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\}
\]</span></p>
<p>But this parallel0tope is the image of <span class="math display">\[
\left\{ \sum_{j \in [n], j \neq i} \lambda_j e_j +\lambda_i x_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\}
\]</span></p>
<p>as <span class="math display">\[
\begin{aligned}
    &amp; T \left( \left\{ \sum_{j \in [n], j \neq i} \lambda_j e_j +\lambda_i x_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\} \right) \\
    &amp;= \left\{ \sum_{j \in [n], j \neq i} \lambda_j T(e_j) +\lambda_i T(x_i):  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\} \\
    &amp;= \left\{ \sum_{j \in [n], j \neq i} \lambda_j A[:, j] +\lambda_i b_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/07/Simplex/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/Simplex/" class="post-title-link" itemprop="url">Simplex</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-07 10:03:19" itemprop="dateCreated datePublished" datetime="2020-02-07T10:03:19+11:00">2020-02-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-18 12:10:34" itemprop="dateModified" datetime="2020-02-18T12:10:34+11:00">2020-02-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Simplex is the first systematically method for solving linear program via a sequence of Gaussian eliminations. It was invented by George Dantzig in 1947 [1]. Algebraically, it converts the linear program from one slack form into to an equivalent one whose objective value does not decrease and possibly increase. It keeps going until the optimal is reached. Geometrically, it is a greedy search strategy that moves from a vertex of the feasible region to another one, with the objective value at the new vertex as least as good as or possibly improved over the previous one. It is an exponential algorithm, although most of time it does a lot better. In this blog we walk through a simple example to illustrate this.</p>
<p>In this case we are operating a factory with two products. Both product have 1 unit of profit. Making the products needs to use two kind of machine. Product one uses 1 hour of machine one and product two uses 2 hours of machine one. Further, product one uses 2 hours of machine two and product two uses 1 hour of machine two. Both machines operate at most 6 hours each day. We want to maximize our profit: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; x_1 + x_2 \\
&amp;s.t.   &amp; x_1 + 2x_2 \le 6 \\
&amp;       &amp; 2x_1 + x_2 \le 6 \\
&amp;       &amp;   x_1, x_2 \ge 0
\end{aligned}
\]</span></p>
<p>It can be represented concisely in matrix form: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; c^T x \\
&amp;s.t.   &amp; A x \le b \\
&amp;       &amp; x \ge 0
\end{aligned}
\]</span></p>
<p>where <span class="math display">\[
\begin{aligned}
A = \left[ 
    \begin{matrix}
        1 &amp; 2 \\
        2 &amp;  1
    \end{matrix} 
    \right], 
\quad 
b = \left[ 
    \begin{matrix}
        6 \\
        6 
    \end{matrix} 
    \right], 
\quad 
c = \left[ 
    \begin{matrix}
        1 \\
        1 
    \end{matrix} 
    \right], 
\quad 
x = \left[ 
    \begin{matrix}
        x_1 \\
        x_2 
    \end{matrix} 
    \right],
\end{aligned}
\]</span></p>
<p><em>We call the set of points that satisfy the constraints of the optimization problem the feasible region</em> <span class="math display">\[
P = \{x : Ax \le b, x \ge 0\}
\]</span></p>
<p>It is obvious that the optimal is obtained at <span class="math inline">\((2, 2)\)</span>, which gives the objective value <span class="math inline">\(4\)</span>. It is a <em>vertex</em> of the feasible region.</p>
<h4 id="slack-form">Slack Form</h4>
<p>In general, it might not be obvious what the optimal point is. But if the elements of <span class="math inline">\(b\)</span> are non-negative, the origin is a feasible point, from which we can begin our search. To make the search easier, we are going to introduce an additional number of variables, the slack variables <span class="math inline">\(s_1, s_2\)</span> and <span class="math inline">\(z\)</span> that correspond to the constraints and objective value. <span class="math display">\[
\begin{aligned}
z =     &amp;   &amp; x_1       &amp;+x_2   \\
s_1 =   &amp;6  &amp; -x_1      &amp;-2x_2  \\
s_2 =   &amp;6  &amp; -2x_1     &amp;-x_2   \\
\end{aligned}
\]</span></p>
<p>rewrite the optimization into <em>slack form</em>: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; x_1   &amp;+x_2       &amp;       &amp;           &amp;= z\\
&amp;s.t.   &amp; x_1   &amp;+2x_2      &amp;+s_1   &amp;           &amp;= 6 \\
&amp;       &amp; 2x_1  &amp;+x_2       &amp;       &amp;+s_2       &amp;= 6 \\
&amp;       &amp; x_1,  &amp;x_2,       &amp;s_1    &amp;,s_2       &amp;\ge 0
\end{aligned}
\]</span></p>
<p>They are called slacks variables because they correspond how much slack you have in the inequalities in the original problem. For this slack form, we also call <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> basic variables, and the original variables non-basic variables.</p>
<p>Now the feasible region <span class="math inline">\(F\)</span> is considered as the intersection of the two subspaces <span class="math display">\[
F = \{x : Ax = b \} \cap \{ x \ge 0 \}
\]</span></p>
<p>We can represent the LP in <em>slack form</em> even more concisely in a table: <span class="math display">\[
\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\ 
    \hline 
    1   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  0   &amp;&amp; \mid 0 \\
\end{aligned}
\]</span></p>
<p>where the last column denotes the objective function. This example has a trivial starting point : <span class="math inline">\((0, 0, 6, 6)\)</span>. It is called a basic solution in which all non-basic variables are set to zero. It can be read from the tabular form: the identity matrix correspond to basic variables, and <span class="math inline">\((s_1, s_2) = b\)</span>.</p>
<h4 id="pivoting">Pivoting</h4>
<p>Look at the original LP: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; x_1   &amp;+x_2       &amp;       &amp;           &amp;= z\\
&amp;s.t.   &amp; x_1   &amp;+2x_2      &amp;+s_1   &amp;           &amp;= 6 \\
&amp;       &amp; 2x_1  &amp;+x_2       &amp;       &amp;+s_2       &amp;= 6 \\
&amp;       &amp; x_1,  &amp;x_2,       &amp;s_1    &amp;,s_2       &amp;\ge 0
\end{aligned}
\]</span></p>
<p>As we would like to maximize the objective, we would like to increase the value of <span class="math inline">\(x_1\)</span> or <span class="math inline">\(x_2\)</span> as much as possible. Suppose we just increase one variable at a time. For example, we can increase variable <span class="math inline">\(x_1\)</span> and keep the value of <span class="math inline">\(x_2\)</span> to be <span class="math inline">\(0\)</span>.</p>
<p>How much can we increase <span class="math inline">\(x_1\)</span>? After increasing <span class="math inline">\(x_1\)</span>, we need to maintain the inequality constraints: <span class="math display">\[
x_1 + s_1 = 6 \\
2 x_1 + s_2 = 6
\]</span></p>
<p>The values of <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> decrease as <span class="math inline">\(x_1\)</span> increases. By they can be drop below <span class="math inline">\(0\)</span>. The maximum possible increases of <span class="math inline">\(x_1\)</span> is bounded by <span class="math inline">\(6 / 2 = 3\)</span>. If we set <span class="math inline">\(x_1 = 3\)</span>, <span class="math inline">\(s_1 = 6 - x_1 = 3\)</span> and <span class="math inline">\(s_2 = 6 - 2x_1 = 0\)</span>.</p>
<p>Note that <span class="math inline">\(x_1\)</span> increases from <span class="math inline">\(0\)</span> to <span class="math inline">\(3\)</span> and <span class="math inline">\(s_2\)</span> decreases from <span class="math inline">\(6\)</span> to <span class="math inline">\(0\)</span>. This process is called <em>pivoting</em>.</p>
<p>How do we perform pivoting in the tabular form? By the replacement we have just performed -- normalizing the second row, and using it to eliminate all other elements in the first column.</p>
<p>Recall the original table: <span class="math display">\[
\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\ 
    \hline 
    1   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  0   &amp;&amp; \mid 0 \\
\end{aligned}
\]</span></p>
<p>We divide the second row by a factor of 2 to normalize it, and subtract the first and third row by the second row <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  3/2 &amp;&amp;  1   &amp;&amp; -1/2 &amp;&amp; \mid 3 \\
    1   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp;  1/2 &amp;&amp; \mid 3 \\ 
    \hline 
    0   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp; -1/2 &amp;&amp; \mid -3 \\
\end{aligned}
\]</span></p>
<p>Observe the first two rows of the tabular form: <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  3/2 &amp;&amp;  1   &amp;&amp; -1/2 &amp;&amp; \mid 3 \\
    1   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp;  1/2 &amp;&amp; \mid 3 \\ 
\end{aligned}
\]</span> If we set <span class="math inline">\(x_2 = 0\)</span> and <span class="math inline">\(s_2 = 0\)</span>, the constraints they implies becomes <span class="math display">\[
s_1 = 3 \\
x_1 = 3 
\]</span> Again the unity columns correspond to non-zero elements and we can read the solution from the tabular form.</p>
<h4 id="correctness-of-pivoting">Correctness of Pivoting</h4>
<p>Before we continue our search on the tabular form, we need to show that pivoting is correct in the sense that, after pivoting, the tabular form represents the same optimization.</p>
<p>In particular, we need to guarantee that the feasible regions are the same and the objective function are equivalent defined on the feasible regions.</p>
<p>We focus on the extended matrix that represents the constraints: <span class="math display">\[
\bar A = [A, b] = 
\left[\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\
\end{aligned}\right]
\]</span> <em>FACT 1.</em> Doing a row replacement on <span class="math inline">\(\bar A\)</span> does not change <span class="math inline">\(F\)</span>, where row replacement is defined as <span class="math display">\[
\bar A[i: ] \leftarrow \bar A[i: ] + k \cdot \bar A[j: ]
\]</span> i.e., adding <span class="math inline">\(k\)</span> times the <span class="math inline">\(j\)</span>-th row to the <span class="math inline">\(i\)</span>-th row, for <span class="math inline">\(i \neq j, k \in R\)</span>.</p>
<p>To see this, denote <span class="math inline">\(F\)</span> and <span class="math inline">\(F&#39;\)</span> the sets before and after the replacement respectively. <span class="math inline">\(F = F&#39;\)</span> implies that each point <span class="math inline">\(x\)</span> belonging to <span class="math inline">\(F\)</span> belongs to <span class="math inline">\(F&#39;\)</span>, and vice-versa. Specifically, the row replacement corresponds to left multiplying <span class="math inline">\(\bar A\)</span> by a matrix</p>
<p><span class="math display">\[
\begin{aligned}
E_{(i,j), k} \doteq 
    \left[ \begin{matrix}
        1 &amp; \\ 
          &amp; 1 \\
          &amp;     &amp; \quad...      \\
          &amp;     &amp; k     &amp; 1     \\
          &amp;     &amp;       &amp;       &amp;    ...  \\
          &amp;     &amp;       &amp;       &amp;           &amp;    1
    \end{matrix} \right]
\end{aligned}
\]</span></p>
<p>which is a diagonal matrix with an additional <span class="math inline">\(k\)</span> in the <span class="math inline">\((i, j)\)</span> position. It is invertible with inverse matrix <span class="math display">\[
\begin{aligned}
E_{(i,j), -k} \doteq 
    \left[ \begin{matrix}
        1 &amp; \\ 
          &amp; 1 \\
          &amp;     &amp; \quad...      \\
          &amp;     &amp; -k    &amp; 1     \\
          &amp;     &amp;       &amp;       &amp;    ...  \\
          &amp;     &amp;       &amp;       &amp;           &amp;    1
    \end{matrix} \right]
\end{aligned}
\]</span></p>
<p>The previous claim becomes <span class="math inline">\(\{ x : E_{(i, j), k} A x = E_{(i, j), k} b \} = \{ x : A x = b \}\)</span>.</p>
<p>But wait. Why do we subtract the last row from the second one? <em>Fact 1</em> has justified the replacement operation on <span class="math inline">\(\bar A\)</span>, but not on the objective coefficients <span class="math inline">\(c\)</span> (the last row in the tabular form). To illustrate the meaning of the operation, recall that <span class="math display">\[
x_1 + x_2 + 0 \cdot s_1 + 0 \cdot s_2 = z
\]</span> Hence <span class="math inline">\(z\)</span> is the value of the objective function. After normalizing the second row, it becomes <span class="math display">\[
x_1 + 1 / 2 x_2 +  0 s_1 + 1/2 s_2 = 3 \\
\]</span></p>
<p>The constraint holds for any feasible point of the linear programming. Therefore, <span class="math display">\[
\begin{aligned}
z - 3 
    &amp;= (x_1 + x_2 + 0 s_1 + 0 s_2)  - (x_1 + 1 / 2 x_2 +  0 s_1 + 1/2 s_2) \\ 
    &amp;= 0x_1 + 1/2x_2 + 0s_1 - 1/2 s_2 \\
    &amp;\Leftrightarrow \\
    z 
    &amp;= 0x_1 + 1/2x_2 + 0s_1 - 1/2 s_2 + 3 
\end{aligned}
\]</span></p>
<p>The last <span class="math inline">\(-3\)</span> in the table is the inverse constant in the objective functions after the replacement of variable.</p>
<h4 id="back-to-our-search">Back to our search</h4>
<p>To improve the current solution further, we note that the coefficient of <span class="math inline">\(x_2\)</span> is positive. We can increase the value of <span class="math inline">\(x_2\)</span>. The maximum increase is given by <span class="math inline">\(\min \{ 3 / (1 / 2), 3 / (3/ 2) \} = 2\)</span>, determined by the first row: <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  3/2 &amp;&amp;  1   &amp;&amp; -1/2 &amp;&amp; \mid 3 \\
    1   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp;  1/2 &amp;&amp; \mid 3 \\ 
    \hline 
    0   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp; -1/2 &amp;&amp; \mid -3 \\
\end{aligned}
\]</span> To make the change explicitly, we perform elimination by the first row <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  1   &amp;&amp;  2/3 &amp;&amp; -1/3 &amp;&amp; \mid 2 \\
    1   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp;  2/3 &amp;&amp; \mid 2 \\ 
    \hline 
    0   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp; -1/3 &amp;&amp; \mid -4 \\
\end{aligned}
\]</span> which give solution <span class="math inline">\(x = (2, 2, 0, 0)\)</span> and objective value <span class="math inline">\(4\)</span>. It is indeed optimal solution for the problem, as all coefficients of the optimization are non-positive.</p>
<h4 id="optimality-condition-and-strong-duality">Optimality Condition and Strong Duality</h4>
<p>We investigate the result deeper by viewing the initial tabular form in abbreviation as <span class="math display">\[
\left[ \begin{matrix}
A \quad I \mid b \\ c^T  \quad 0 \mid 0
\end{matrix} \right]
\]</span> The series of <span class="math inline">\(m\)</span> Gaussian elimination can be interpreted as left multiplying a matrix: <span class="math display">\[
\begin{aligned}
\left[ \begin{matrix}
    &amp; R     &amp; 0 \\
    &amp; -y^T  &amp; 1 \\
\end{matrix} \right] \doteq E_m...E_3 E_2 E_1
\end{aligned}
\]</span> where each <span class="math inline">\(E_j\)</span> ( <span class="math inline">\(1 \le j \le m\)</span>) matrix is invertible and represents a Gaussian elimination, i.e., if we left multiple a matrix by <span class="math inline">\(E_j\)</span>, it is equivalent to perform Gaussian elimination on the matrix. The matrix product <span class="math inline">\(E_m...E_3 E_2 E_1\)</span> has the form <span class="math display">\[
\begin{aligned}
\left[ \begin{matrix}
    &amp; R     &amp; 0 \\
    &amp; -y^T  &amp; 1 \\
\end{matrix} \right] 
\end{aligned}
\]</span> because we never use the last row to eliminate the other rows.</p>
<p>Now: <span class="math display">\[
\begin{aligned}
\left[ \begin{matrix}
    &amp; R     &amp; 0 \\
    &amp; -y^T  &amp; 1 \\
\end{matrix} \right]
\left[ \begin{matrix}
    &amp; A &amp; I \mid    b \\ 
    &amp; c^T  &amp; 0 \mid 0
\end{matrix} \right] 
=
\left[ \begin{matrix}
    &amp; RA            &amp; R     &amp; \mid &amp; Rb \\ 
    &amp; -y^TA+c^T     &amp; -y^T  &amp;\mid &amp; -y^T b
\end{matrix} \right]
\end{aligned}
\]</span> In our example, <span class="math display">\[
\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\ 
    \hline 
    1   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  0   &amp;&amp; \mid 0 \\
\end{aligned}
\Rightarrow ... \Rightarrow
\begin{aligned}
    0   &amp;&amp;  1   &amp;&amp;  2/3 &amp;&amp; -1/3 &amp;&amp; \mid 2 \\
    1   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp;  2/3 &amp;&amp; \mid 2 \\ 
    \hline 
    0   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp; -1/3 &amp;&amp; \mid -4 \\
\end{aligned}
\]</span> we have <span class="math display">\[
R = \begin{aligned}
\left[ \begin{matrix}
    2 / 3 &amp; -1 / 3 \\
    -1 / 3 &amp; 2 /3 
\end{matrix} \right] 
\end{aligned}
\qquad 
y^T = [ -1 / 3, -1  / 3]
\]</span></p>
<p>The optimality condition of simplex algorithm implies that <span class="math display">\[
\begin{aligned}
-y^T A + c^T &amp;\le 0 \\
y^T &amp;\le 0 \\
y^T b &amp;= c^T x
\end{aligned}
\]</span></p>
<p>This is the optimal solution for the dual program of the primal: <span class="math display">\[
\min \ b^T y, \qquad s.t.,  A^T y \ge c, \quad y^T \ge 0
\]</span></p>
<p>If simplex algorithm stops, it implies strong duality holds:</p>
<ul>
<li>The optimal value of the primal program equals to the optimal one of the dual program, assuming that both values exist and are bounded.</li>
</ul>
<h4 id="geometric-interpretation">Geometric Interpretation</h4>
<p>In the beginning we claim that the simplex algorithm move from one vertex to another. We prove it rigorously in this section. First we need a few definitions:</p>
<h5 id="vertex">Vertex</h5>
<p><em>A vertex is a point <span class="math inline">\(v \in F\)</span>, such that <span class="math inline">\(\nexists v_1, v_2 \in F, v_1 \neq v_2\)</span> and <span class="math inline">\(\lambda \in (0, 1)\)</span>, s.t., <span class="math inline">\(v = \lambda v_1 + (1 - \lambda) v_2\)</span>.</em></p>
<p>In other words, <span class="math inline">\(v\)</span> is a vertex of <span class="math inline">\(F\)</span> if it is not contained in a line segment of <span class="math inline">\(P\)</span>.</p>
<p><strong>Theorem.</strong> If LP has an optimal solution, then it has an optimal solution that is a vertex of its feasible set.</p>
<p><em>Proof.</em> Denote <span class="math inline">\(v\)</span> the optimal solution of the LP with maximum number of zero components. If <span class="math inline">\(v\)</span> is not a vertex, then <span class="math inline">\(\exists v_1, v_2 \neq v, v_1, v_2 \in F\)</span> and <span class="math inline">\(\lambda &gt; 0\)</span>, such that <span class="math inline">\(\lambda v_1 + (1 - \lambda) v_2 = v\)</span>.</p>
<p>As <span class="math inline">\(v\)</span> is optimal, we have <span class="math inline">\(c^T v \ge c^T v_1\)</span> and <span class="math inline">\(c^T v \ge c^T v_2\)</span>. By <span class="math inline">\(\lambda v_1 + (1 - \lambda) v_2 = v\)</span>, it concludes that <span class="math inline">\(c_T v = c^T v_1 = c^T v_2\)</span>.</p>
<p>Define <span class="math inline">\(I = \{ i : v[i] &gt; 0 \}\)</span>. As <span class="math inline">\(v_1 \ge 0, v_2 \ge 0\)</span> and <span class="math inline">\(\lambda v_1 + (1 - \lambda) v_2 = v\)</span> so that for <span class="math inline">\(i \neq I\)</span>, <span class="math inline">\(v_1[i] = v_2[i] = v[i] = 0\)</span>.</p>
<p>But <span class="math inline">\(A_I v_1[I] = A_I v_2[I] = b\)</span>. Let <span class="math inline">\(x = \epsilon (v_1 - v_2) + v\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(x\)</span> is feasible, for small enough <span class="math inline">\(\epsilon\)</span>.</li>
<li><span class="math inline">\(x\)</span> is optimal, since <span class="math inline">\(c^T x= \epsilon c^T (v_1 - v_2) + c^T v = c^T v\)</span>.</li>
<li><span class="math inline">\(x_i = 0\)</span> for <span class="math inline">\(i \notin I\)</span>.</li>
</ol>
<p>We can find some <span class="math inline">\(\epsilon\)</span> (either positive or false) to make <span class="math inline">\(x_i = 0\)</span> for some <span class="math inline">\(i \in I\)</span>. Now <span class="math inline">\(x\)</span> contains more zeros than <span class="math inline">\(v\)</span>, a contradiction. <span class="math inline">\(\square\)</span></p>
<h5 id="basic-solution">Basic Solution</h5>
<p><em>A solution <span class="math inline">\(x\)</span> of <span class="math inline">\(Ax = b\)</span> is called a basic solution if <span class="math inline">\(\{A_i : x_i \neq 0 \}\)</span> (columns in <span class="math inline">\(A\)</span> that correspond to non-zero components in <span class="math inline">\(x\)</span> ) are linearly independent.</em></p>
<h5 id="basic-feasible-solution">Basic Feasible Solution</h5>
<p><em>A basic solution <span class="math inline">\(x\)</span> is called basic feasible solution if <span class="math inline">\(x \ge 0\)</span>, i.e., <span class="math inline">\(x \in \{ x : Ax = b\} \cap \{ x \ge 0 \}\)</span>.</em></p>
<p>With respect to the feasible region of LP in slack form, the <em>vertices</em> have have following property:</p>
<p><em>Lemma:</em>. A point <span class="math inline">\(v \in F = \{x : Ax = b\} \cap \{ x: x \ge 0\}\)</span> is a vertex of <span class="math inline">\(F\)</span> if and only if it is a basic feasible solution. Here <span class="math inline">\(A \in R^{m \times n}, x \in R^n, b \in R^{m}\)</span> and <span class="math inline">\(rank(A) = m\)</span> (otherwise some row constraints of the <span class="math inline">\(Ax = b\)</span> are redundant).</p>
<p><em>Proof</em>: Denote <span class="math inline">\(S\)</span> the set of indices of positive components in <span class="math inline">\(v\)</span> and <span class="math inline">\(A_S\)</span> the set of</p>
<p><em>Only if:</em> If <span class="math inline">\(A_S\)</span> does not have full column rank, then <span class="math inline">\(\exists u \in R^{|S|}\)</span>, such that <span class="math inline">\(A_S u = 0\)</span>. Then for small enough <span class="math inline">\(\epsilon \in R, \epsilon &gt; 0\)</span>, both <span class="math inline">\(v + \epsilon u\)</span> and <span class="math inline">\(v - \epsilon u\)</span> are feasible point in <span class="math inline">\(F\)</span>. Now <span class="math inline">\(v = 0.5(v + \epsilon u) + 0.5(v - \epsilon u)\)</span>, a contradiction.</p>
<p><em>If:</em> As columns of <span class="math inline">\(A_S\)</span> are linearly independent, the only solution of to <span class="math inline">\(A_S x_S = 0\)</span> is given by <span class="math inline">\(x_S = 0\)</span>. Hence, <span class="math inline">\(v_S\)</span> is now the unique solution to <span class="math inline">\(A_S v_S = b\)</span>. If <span class="math inline">\(v\)</span> is not a vertex, <span class="math inline">\(\exists v_1, v_2, \lambda\)</span>, such that <span class="math inline">\(v = \lambda v_1 + (1 - \lambda) v_2\)</span>. But in this case we must have <span class="math inline">\(v_1 = v_2 = v\)</span>, as <span class="math inline">\((v_1)_S = (v_2)_S = v_S\)</span> and <span class="math inline">\((v_1)_{\bar S} = (v_2)_{\bar S} = v_{\bar S} = 0\)</span>. A contradiction.</p>
<p><span class="math inline">\(\blacksquare\)</span>.</p>
<p><em>Corollary:</em> The simplex algorithm moves from vertex to vertex.</p>
<p><em>Proof:</em> The positive components in the solution correspond to the columns in the identity matrix in the tabular form of simplex, which are linearly independent.</p>
<h4 id="correctness-of-simplex-to-finish">Correctness of Simplex (TO FINISH)</h4>
<p>Though not shown in the example, there are two issue associated with simplex algorithm.</p>
<ol type="1">
<li>How do we find an initial solution? If there are negative component in <span class="math inline">\(b\)</span>, then <span class="math inline">\(x = 0 \wedge s = b\)</span> is not a feasible solution.</li>
<li>How do we guarantee that simplex terminates?</li>
</ol>
<p>In the previous example, at each step we choose to increase the variable with largest coefficient. In some rare cases, this results in a loop of the algorithm.</p>
<p>One method for avoiding looping is called Bland's rule:</p>
<ol type="1">
<li>If there is a positive coefficient in the objective function, choose the one with largest coefficient.</li>
<li>If there are multiple rows with tight constraints, choose the one with largest coefficient.</li>
</ol>
<p><em>Theorem.</em> Blands' rule guarantees that the algorithm stops.</p>
<p><em>Proof.</em> Suppose that on the contrary, the algorithm loops. Denote <span class="math inline">\(B_1, B_2, ..., B_k\)</span> the set of basis in the loop. In each base, there is entering variable and one leaving variable. Every variable that leaves the base must enter another base later. We called these variable <em>fickle variables</em>. Denote <span class="math inline">\(x_t\)</span> the fickle variable with the largest index.</p>
<ol type="1">
<li><p><span class="math inline">\(D\)</span> the dictionary that <span class="math inline">\(x_t\)</span> leaves the base, and <span class="math inline">\(x_e\)</span> the variable that enters the base.</p></li>
<li><p><span class="math inline">\(D&#39;\)</span> the dictionary that <span class="math inline">\(x_t\)</span> enters the base again. <span class="math display">\[
D = 
\begin{aligned}
 I   &amp;&amp;  A_N &amp;&amp; \mid b \\
 \hline 
 0   &amp;&amp;  c_N &amp;&amp; \mid \alpha \\
\end{aligned}
\Rightarrow ... \Rightarrow
D&#39; = 
\begin{aligned}
 R   &amp;&amp;  R A_N   &amp;&amp; \mid R b \\
 \hline 
 y^T &amp;&amp;  y^T A_N + c_N   &amp;&amp; \mid y^T b + \alpha \\
\end{aligned}
\]</span> We have <span class="math display">\[
c_e&#39; = c_e + y^T A_N[:, e]
\]</span></p></li>
<li><p><span class="math inline">\(x_e\)</span> is the entering variable in <span class="math inline">\(D\)</span>, therefore <span class="math inline">\(c_e &gt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(x_t\)</span> is the entering variable in <span class="math inline">\(D&#39;\)</span>, and <span class="math inline">\(e &lt; t\)</span>, by the definition of <span class="math inline">\(x_t\)</span>, we have <span class="math inline">\(x_e&#39; \le 0\)</span>.</p></li>
<li><p>Hence, <span class="math inline">\(y^T A_N[:, e] = c_e&#39; - c_e &lt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(\exists i, s.t., y_i A_{i, e} &lt; 0\)</span>. Then <span class="math inline">\(y_i \neq 0\)</span>, and <span class="math inline">\(x_i\)</span> is fickle. By definition, <span class="math inline">\(i &lt; t\)</span>. But <span class="math inline">\(x_i\)</span> is not entering <span class="math inline">\(D&#39;\)</span>, we know that <span class="math inline">\(y_i &lt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(A_{i, e} &gt; 0\)</span>. But since <span class="math inline">\(x_i\)</span> is fickle, we have <span class="math inline">\(b_i = 0\)</span>. In this case, <span class="math inline">\(x_i\)</span> is chosen in preference to <span class="math inline">\(x_t\)</span>. A contradiction.</p></li>
</ol>
<h1 id="reference">Reference</h1>
<p>[1]. Dantzig, George B. "Origins of the simplex method." A history of scientific computing. 1990. 141-151.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/30/LP%20-%20Weak%20duality,%20Complementary%20Slackness,%20Strong%20Duality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/30/LP%20-%20Weak%20duality,%20Complementary%20Slackness,%20Strong%20Duality/" class="post-title-link" itemprop="url">LP - Dual Program, Weak Duality, Complementary Slackness, Strong Duality</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-30 11:47:54" itemprop="dateCreated datePublished" datetime="2020-01-30T11:47:54+11:00">2020-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-01 11:26:59" itemprop="dateModified" datetime="2020-02-01T11:26:59+11:00">2020-02-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="dual-program-and-weak-duality">Dual Program and Weak Duality</h1>
<p>Linear programming studies the optimization problems in which both its objective function and feasible region are represented by linear relationship.</p>
<p>Every linear program can be converted into Canonical form</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \max &amp; c^T x \\
&amp; s.t., &amp; Ax \le b \\
&amp;&amp; x \ge 0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(c, x \in \mathcal{R}^n\)</span>, <span class="math inline">\(A \in \mathcal{R}^{m \times n}\)</span> and <span class="math inline">\(b \in \mathcal{R}^m\)</span>.</p>
<p>Suppose we have a conic combination (<span class="math inline">\(y \in \mathcal{R}^m\)</span> and <span class="math inline">\(y \ge 0\)</span>) of the linear constraints: <span class="math display">\[
y^T (Ax) \le y^T b
\]</span></p>
<p>Such that <span class="math inline">\(y^T A \ge c^T\)</span>, then <span class="math inline">\(y^T b\)</span> gives an upper bound of the original program. We want to find a <span class="math inline">\(y\)</span>, such that the bound is as tight as possible. This gives rise to another linear program: <span class="math display">\[
\begin{aligned}
&amp; \min &amp; y^T b \\
&amp; s.t., &amp; y^T A \ge c^T \\
&amp;&amp; y \ge 0
\end{aligned}
\]</span></p>
<p>We call the original program the primal and the derived program its dual.</p>
<p>The philosophy of dual program is to use the combination of primal constraints to construct a bound of its objective function. More formally, the goal is that the <em>weak duality</em> holds:</p>
<p><span class="math display">\[
c^T x \le (y^T A) x = y^T A x = y^T (Ax) \le y^T b 
\]</span></p>
<p>Note that <span class="math inline">\(c^T x \le (A^T y)^T x\)</span> is true because <span class="math inline">\(y^T A \ge c\)</span> and <span class="math inline">\(x \ge 0\)</span>. And <span class="math inline">\(y^T (Ax) \le y^T b\)</span> follows from <span class="math inline">\(Ax \le b\)</span> and <span class="math inline">\(y^T \ge 0\)</span>.</p>
<p><em>Corollary 1:</em> If the primal is unbounded, then the dual is infeasible. If the dual is unbounded, then the primal is infeasible.</p>
<p>In general, the primal may not be given in Canonical form and can be formulated as</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \max &amp; c^T x + \bar c^T \bar x + \hat c^T \hat x \\
&amp; s.t., &amp; Ax + \bar A \bar x + \hat A \hat x \le b \\
&amp;&amp; Bx + \bar B \bar x + \hat B \hat x =  \bar b \\ 
&amp;&amp; Cx + \bar C \bar x + \hat C \hat x \ge \hat b \\
&amp;&amp; x \ge 0 \\
&amp;&amp; \hat x \le 0
\end{aligned}
\]</span></p>
<p>Note that the <span class="math inline">\(\bar x\)</span>'s are unconstrained variables. Guided by similar philosophy, we hope to derive an upper bound by using the constraints: <span class="math display">\[
\begin{aligned}
    c^T x + \bar c^T \bar x + \hat c^T \hat x 
        &amp; \le y^T (Ax + \bar A \bar x + \hat A \hat x) \\
        &amp; + \bar y^T (Bx + \bar B \bar x + \hat B \hat x) \\
        &amp; + \hat y^T (Cx + \bar C \bar x + \hat C \hat x) \\
        &amp;\le y^T b + \bar y^T \bar b + \hat y^T \hat b
\end{aligned}
\]</span></p>
<ol type="1">
<li><p>The first inequality implies that (by proper rearranging): <span class="math display">\[
 \begin{aligned}
     c^T x + \bar c^T \bar x + \hat c^T \hat x 
         &amp; \le (y^T A + \bar y^T B + \hat y^T C)x \\
         &amp; + (y^T \bar A + \bar y^T \bar B + \hat y^T \bar C) \bar x \\
         &amp; + (y^T \hat A + \bar y^T \hat B + \hat y^T \hat C) \hat x
 \end{aligned}
 \]</span></p>
<p>As <span class="math inline">\(x \ge 0\)</span>, <span class="math inline">\(\bar x\)</span> unconstrained and <span class="math inline">\(\hat x \le 0\)</span>, a sufficient condition for the inequality to become true is <span class="math display">\[
 \begin{aligned}
     c^T      &amp; \le (y^T A + \bar y^T B + \hat y^T C) \\
     \bar c^T &amp; =   (y^T \bar A + \bar y^T \bar B + \hat y^T \bar C) \\
     \hat c^T &amp; \ge (y^T \hat A + \bar y^T \hat B + \hat y^T \hat C) \\
 \end{aligned}
 \]</span></p>
<p>That gives the first set of constraints the dual variables <span class="math inline">\(y\)</span>, <span class="math inline">\(\bar y\)</span> and <span class="math inline">\(\hat y\)</span> need to satisfies. We have just seen that the sign of the primal variable determines the type of constraint in the dual.</p>
<p>Of particular interest is the inequality constraint, which we will dive into details in the next section.</p></li>
<li><p>By similar argument, the second inequality is true if <span class="math inline">\(y^T \ge 0\)</span>, <span class="math inline">\(\bar y^T\)</span> unconstrained, <span class="math inline">\(\hat y^T \le 0\)</span>. That is, the type of constraint in the primal determines the sign of variable in the dual.</p></li>
</ol>
<p>Finally, the rules of obtaining dual are summarized as follows:</p>
<p><span class="math display">\[
\begin{aligned}    
\hline
&amp;&amp;&amp; \text{Primal}       &amp;&amp;&amp;                    \max   \quad                      &amp;&amp;&amp; \qquad \qquad                      \min                    &amp;&amp;&amp;  \text{Dual}        &amp;&amp;&amp; \\
\hline
&amp;&amp;&amp; Constraints     &amp;&amp;&amp;      \begin{matrix} \le b_i \\ \ge b_i \\ = b_i  \end{matrix} \quad     &amp;&amp;&amp; \qquad \quad \begin{matrix} \ge 0 \\ \le 0 \\ \text{Both sides} \end{matrix}    &amp;&amp;&amp;  Variables      &amp;&amp;&amp; \\
\hline
&amp;&amp;&amp;  Variables      &amp;&amp;&amp;\qquad \begin{matrix} \ge 0 \\ \le 0 \\ \text{Both sides} \end{matrix}   &amp;&amp;&amp; \qquad \qquad  \begin{matrix}   \ge c_j \\ \le c_i \\ = c_j   \end{matrix}  &amp;&amp;&amp; Constraints     &amp;&amp;&amp; \\
\hline
\end{aligned}
\]</span></p>
<h1 id="complementary-slackness">Complementary Slackness</h1>
<p>The strong duality states that, if both primal and dual are feasible, then the objective function values of their optimal solutions equal.</p>
<p>If we expand the weak duality expression for the standard form, we get <span class="math display">\[
\sum_{i = 1}^n c_i x_i \le \sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j} \right) x_i =  \sum_{j = 1}^m  y_j \left( \sum_{i = 1}^n A_{i, j} x_i \right) \le \sum_{j = 1}^m y_j b_j 
\]</span></p>
<p>Then <span class="math inline">\(c^T x= y^T b\)</span> implies <span class="math display">\[
\sum_{i = 1}^n c_i x_i = \sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j}  \right) x_i \\  
\sum_{j = 1}^m  y_j \left( \sum_{i = 1}^n A_{i, j} x_i \right)  = \sum_{j = 1}^m b_j y_j
\]</span></p>
<p>i.e., <span class="math display">\[
\sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j} - c_i \right) x_i  = 0\\  
\sum_{j = 1}^m  y_j \left( b_j - \sum_{i = 1}^n A_{i, j} x_i \right) = 0
\]</span></p>
<p>As <span class="math inline">\(\sum_{j = 1}^m A_{i, j} y_j - c_i \ge 0\)</span>, <span class="math inline">\(x_i \ge 0\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(b_j - \sum_{i = 1}^n A_{i, j} x_i \ge 0\)</span> and <span class="math inline">\(y_j \ge 0\)</span> for all <span class="math inline">\(j\)</span>, we conclude that</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum_{j = 1}^m y_j A_{i, j}  - c_i = 0 &amp;\vee &amp;&amp;    x_i = 0 &amp; \forall i \\ 
&amp;b_j - \sum_{i = 1}^n A_{i, j} x_i = 0 &amp;\vee &amp;&amp;     y_j  = 0, &amp; \forall j
\end{aligned}
\]</span></p>
<p>Or more compactly <span class="math display">\[
(y^T A - c^T) x = 0 \\y^T (b - Ax) = 0
\]</span></p>
<p>If the primal is in canonical form, then the fact that <span class="math inline">\(A^T \ge c, x \ge 0, y \ge 0, b - Ax \ge 0\)</span> implies that <span class="math display">\[
\begin{aligned}
&amp;(y^T A - c^T)_i = 0  &amp;&amp;\vee &amp;&amp; x_i = 0         &amp;&amp;&amp;\forall \ i \\
&amp;y_j = 0            &amp;&amp;\vee   &amp;&amp; (b - Ax)_j = 0  &amp;&amp;&amp;\forall \ j
\end{aligned}
\]</span> where <span class="math inline">\((y^T A - c^T)\)</span> is a vector and <span class="math inline">\((y^T A - c^T)_i\)</span> is its <span class="math inline">\(i\)</span>-th coordinate.</p>
<p>This is known as complementary slackness.</p>
<h1 id="strong-duality">Strong Duality</h1>
<p>Question to ponder: what is the geometric interpretation of complementary slackness?</p>
<h2 id="special-case">Special Case</h2>
<p>To illustrate its underlying meaning, first consider the special case <span class="math display">\[
\begin{aligned}
&amp; \min &amp; y^T b \\
&amp; s.t., &amp; y^T A = c^T 
\end{aligned}
\]</span></p>
<p>whose dual form is given by <span class="math display">\[
\begin{aligned}
&amp; \max &amp; c^T x \\
&amp; s.t., &amp; Ax = b 
\end{aligned}
\]</span></p>
<p>First note that <span class="math inline">\(y^T A = c^T\)</span> means that <span class="math inline">\(c^T\)</span> is a linear combination of rows of <span class="math inline">\(A\)</span>. The primal is feasible if <span class="math inline">\(c\)</span> is in the row space of <span class="math inline">\(A\)</span>.</p>
<p>In this special case, if both primal and dual are feasible, then for any feasible solutions <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, strong duality holds, since <span class="math inline">\(c^T x = (y^T A) x = y^T (Ax) = y^T b\)</span>.</p>
<p>The dual search for some <span class="math inline">\(x\)</span> such that <span class="math display">\[
Ax = b
\]</span></p>
<p>If we can find such <span class="math inline">\(x\)</span>, it implies that <span class="math inline">\(b\)</span> lies in the column space of <span class="math inline">\(A\)</span>, denoted as <span class="math inline">\(C(A)\)</span> (the subspace in <span class="math inline">\(\mathcal{R}^n\)</span> spanned by the row vectors of <span class="math inline">\(A\)</span>). What if not? We can decompose <span class="math inline">\(b\)</span> into two part: <span class="math inline">\(b = b_1 + b_2\)</span>, such that <span class="math inline">\(b_1 \in C(A)\)</span> and <span class="math inline">\(0 \neq b_2 \in N(A^T)\)</span> (the subspace orthogonal to <span class="math inline">\(C(A)\)</span>). Here is the problem: if <span class="math inline">\(y\)</span> is a feasible solution for the primal, then we can decrease <span class="math inline">\(y\)</span> by arbitrary amount along the direction orthogonal to <span class="math inline">\(C(A)\)</span>, while maintaining its feasibility. In particular, if we add <span class="math inline">\(k \cdot b_2\)</span> to <span class="math inline">\(y\)</span> (<span class="math inline">\(k &gt; 0\)</span>), then <span class="math inline">\((y - k b_2)\)</span> is still feasible as <span class="math inline">\((y - k b_2)^T A = y^T A - k b_2^T A = c^T\)</span>. But now <span class="math inline">\((y - k \cdot b_2)^T b = y^T b - k b_2^T b_2 &lt; y^T b\)</span>, which implies that the primal is unbounded.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/StrongDualitySpecialCase.jpg" /></p>
<h2 id="general-case">General Case</h2>
<p>Now we return our discussion to the more general form: <span class="math display">\[
\begin{aligned}
&amp; \min &amp; y^T b \\
&amp; s.t., &amp; y^T A \ge c^T
\end{aligned}
\]</span></p>
<p>Its dual is given by <span class="math display">\[
\begin{aligned}
&amp; \max &amp; c^T x \\
&amp; s.t., &amp; Ax = b \\
&amp;&amp; x \ge 0
\end{aligned}
\]</span></p>
<p>Every LP can be converted into standard form.</p>
<p>The weak duality states that <span class="math display">\[
c^T x \le y^T A x = y^T b
\]</span></p>
<p>If both primal and dual are feasible, then strong duality reduce to show that the first inequality holds with equality, that is, <span class="math inline">\(\exists\)</span> feasible <span class="math inline">\(x, y\)</span>, s.t., <span class="math inline">\(c^T x = y^T A x\)</span>. In particular, complementary slackness, this is equivalent to proving that <span class="math display">\[
\begin{aligned}
&amp;(y^T A - c^T)_i = 0  &amp;&amp;\vee &amp;&amp;x_i = 0      &amp;&amp;\forall \ i \\
\end{aligned}
\]</span></p>
<p>We begin by assuming that <span class="math inline">\(y\)</span> is the optimal solution for the dual. We claim that <span class="math inline">\(y\)</span> can not be an interior point of the feasible region. Otherwise, <span class="math inline">\(y\)</span> can move along arbitrary direction by a little while maintaining feasibility. If it moves along the direction pointed by <span class="math inline">\(-b\)</span>, then the objective function value decreases, contradicting <span class="math inline">\(y\)</span> being optimal.</p>
<p>Therefore, <span class="math inline">\(y\)</span> must be on the boundary of the feasible region.</p>
<h4 id="question-to-ponder-prove-the-existence-of-optimal-solution-of-the-primal-assuming-feasibility-of-the-dual."><em>Question to ponder: prove the existence of optimal solution of the primal, assuming feasibility of the dual.</em></h4>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/MinimizeYB.jpg" /></p>
<p>The boundary <span class="math inline">\(y\)</span> touches are specified by tight constraints. The intuition behind is that, <span class="math inline">\(b\)</span> must be the positive combination of the normal vector of the tight constraints. Otherwise, we can find some direction that decreases objective function value while maintaining feasibility.</p>
<p>Denote <span class="math inline">\(S \subset [m]\)</span> the set of indexes, such that <span class="math inline">\(\forall i \in S\)</span>, <span class="math inline">\(y^T A_i = c_i\)</span>, where <span class="math inline">\(A_i\)</span> is the <span class="math inline">\(i\)</span>-th column of matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(c_i\)</span> is the <span class="math inline">\(i\)</span>-th coordinator of vector <span class="math inline">\(c\)</span>. In other words, <span class="math inline">\(S\)</span> is the set of indexes for which the inequality is tight.</p>
<p>We claim that <span class="math inline">\(b\)</span> is in the conic hull of columns of <span class="math inline">\(A_s\)</span>. That is, <span class="math inline">\(\exists x_S \in R^{|S|}\)</span>, such that 1. <span class="math inline">\(A_S x_S = b\)</span>. 2. <span class="math inline">\(x_S \ge 0\)</span>.</p>
<p>Such <span class="math inline">\(x_S\)</span> gives a feasible solution to the primal, by setting <span class="math inline">\(x_{\bar S} = 0\)</span> and <span class="math inline">\(x = x_S \cup x_{\bar S}\)</span>. Further, strong duality holds as <span class="math inline">\(c^T x = y^T A x = y^T b\)</span>.</p>
<p><em>Proof:</em></p>
<p>It suffices to show that <span class="math inline">\(b\)</span> is a conic combination of columns of <span class="math inline">\(A_S\)</span>. Suppose not. Then by <a target="_blank" rel="noopener" href="https://wuhao-wu-jiang.github.io/2019/01/17/Farkas-Lemma/">Farkas' Lemma</a>, <span class="math inline">\(\exists v \in \R^m\)</span>, such that</p>
<ol type="1">
<li><span class="math inline">\(v^T A_S \ge 0\)</span>.</li>
<li><span class="math inline">\(v^T b &lt; 0\)</span>.</li>
</ol>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/FarkasLemma3.jpg" /></p>
<p>Then we can move the vector <span class="math inline">\(y\)</span> along the direction of <span class="math inline">\(v\)</span>. But this time we need to be careful, in order not to violate the constraints.</p>
<p>For each inequality constraint <span class="math inline">\(y^T A_i &gt; c_i\)</span> (<span class="math inline">\(i \in \bar S\)</span>), <span class="math inline">\(\exists t_i &gt; 0\)</span>, such that <span class="math inline">\((y + t_i v)^T A_i &gt; c_i\)</span>. Denote <span class="math inline">\(t = \min_{i \in \bar S} t_i\)</span>.</p>
<p>As <span class="math inline">\((y + t v)^T A_S = c_S + t v^T A_S \ge c_S\)</span>, <span class="math inline">\((y + t v)\)</span> is still feasible. But <span class="math inline">\((y + t v)^T b &lt; y^T b\)</span>. A contradiction.</p>
<h4 id="question-to-ponder-relate-the-above-discussion-to-lagrange-multiplier-and-k.k.t-condition.">Question to ponder: relate the above discussion to Lagrange multiplier and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">K.K.T condition</a>.</h4>
<h1 id="reference.">Reference.</h1>
<ol type="1">
<li>David P. Williamson, ORIE 6300 Mathematical Programming I, <a target="_blank" rel="noopener" href="https://people.orie.cornell.edu/dpw/orie6300/Lectures/lec08.pdf">Lecture 8</a><br />
</li>
<li>Boyd, S. and Vandenberghe, L., 2004. Convex optimization. Cambridge university press.</li>
</ol>
<!-- 1. $A_S x_S = b$:   
   We first show $b$ in the linear hull of $A_S$, i.e., $\exists x_S$, s.t., $A_S x_S = b$. Otherwise, the case is similar to the first example we have just discussed. We decompose $b = b_1 + b_2$, where $b_2$ is orthogonal to $C(A_S)$. Then we can move the vector $y$ along the reverse direction of $b_2$. But this time we need to be careful, in order not to violate the inequality constraints. 

    For each inequality constraint $y^T A_i > c_i$ ($i \in \bar S$), $\exists t_i > 0$, such that $(y - t_i b_2)^T A_i > c_i$. Denote $t = \min_{i \in \bar S} t_i$. 
    
    As $(y - tb_2)^T A_S = c_S$, $(y - tb_2)$ is still feasible. But $(y - tb_2)^T b < y^T b$. A contradiction.

1. $x_S \ge 0$. 
   
    For simplicity, consider just the case where the columns of $A_S$ are linearly independent. If $\exists i \in S$, such that $x_i < 0$, then we can find a better solution as follows. Let $u$ be the projection of $A_i$ to $C(A_{S \setminus [i]})$, the subspace spanned by columns of $A_{S \setminus [i]}$. Define $v = A_i - u$. We have $v \neq 0$ since $A_i$ is not inside $C(A_{S \setminus [i]})$. Further $v$ is orthogonal to columns of $A_{S \setminus [i]}$ and $v^T A_i > 0$. 

    Moreover, $v^T b = v^T A_S x_S = (v^T A_i) x_i < 0$. 

    We can move $y$ along the direction of $v$, before some constraint in $\bar S$ becomes tight. But this decreases the value of the objective function, contradicting $y$ being optimal. 

    Remark: when the columns of $A_S$ are linearly dependent, there are few cases.
    1. If $A_i$ is outside $C(A_{S \setminus [i]})$, then the proof still holds. 
    2. Otherwise, $A_i$ can be expressed by a linear combination by columns in $A_{S \setminus [i]}$.  -->

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/14/Sparse-Recovery-with-Count-Sketch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/14/Sparse-Recovery-with-Count-Sketch/" class="post-title-link" itemprop="url">Sparse Recovery with Count-Sketch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-14 22:51:28" itemprop="dateCreated datePublished" datetime="2020-01-14T22:51:28+11:00">2020-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-15 23:01:24" itemprop="dateModified" datetime="2020-01-15T23:01:24+11:00">2020-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose that a stream of items comes online <span class="math inline">\(S = \{ a_1, a_2, ..., a_m \}\)</span> where <span class="math inline">\(a_i \in [n]\)</span>. Denote the frequency of <span class="math inline">\(S\)</span> as <span class="math inline">\(f = &lt;f_1, f_2, ..., f_n&gt;\)</span>, where <span class="math inline">\(f_j = |\{ a_i \in S : a_i = j \}|\)</span>. By running the count-sketch algorithm, we can obtain an estimation <span class="math inline">\(\hat f\)</span> of <span class="math inline">\(f\)</span> with space <span class="math inline">\(O(\frac{1}{\epsilon^2} \log n)\)</span>, such that with high probability, for <span class="math inline">\(i \in [n]\)</span>, <span class="math display">\[
|\hat f_i - f_i| \le \epsilon |f|_2
\]</span> where <span class="math inline">\(|f|_2 = \sqrt {\sum_{i = 1}^n f_i^2 }\)</span>.</p>
<p>In general, a sketch is a concise representation of <span class="math inline">\(f\)</span>. Suppose that we limit our space usage to <span class="math inline">\(k\)</span> words (i.e., <span class="math inline">\(|\hat f|_0 = k\)</span>), and assume that <span class="math inline">\(H\)</span> is the set of items of <span class="math inline">\(k\)</span> most frequent items, then the sketch <span class="math inline">\(\hat f\)</span> that minimize the <span class="math inline">\(l_2\)</span> error keeps all items in <span class="math inline">\(H\)</span> and the error between <span class="math inline">\(f\)</span> and <span class="math inline">\(\hat f\)</span> is <span class="math display">\[
|f - \hat f|_2 \doteq err_2^k (f) = \sqrt {\sum_{i \in H} f_i^2}
\]</span></p>
<p>In practice, however, <span class="math inline">\(S\)</span> is not given offline. As we don't know <span class="math inline">\(H\)</span>, we can only compute an <span class="math inline">\(\hat f\)</span> with <span class="math inline">\(k\)</span> words that is as good as possible. The question is, how far <span class="math inline">\(|f - \hat f|_2\)</span> can be from the minimum possible value, namely <span class="math inline">\(err_2^k(f)\)</span>?</p>
<p>One possible solution, proposed by Cormode and Muthukrishnan [1], is to construct <span class="math inline">\(\hat f\)</span> vis Count-Sketch with <span class="math inline">\(d = O(\log n)\)</span> hash tables each of which has size <span class="math inline">\(w = \frac{3k}{\epsilon^2}\)</span>.</p>
<p><em>Lemma 1.</em> For any <span class="math inline">\(i \in [n]\)</span>, we have <span class="math inline">\(|f_i - \hat f_i| \le \frac{\epsilon}{\sqrt k } err_2^k(f)\)</span> with high probability.</p>
<p><em>Proof</em><br />
Denote <span class="math inline">\(h_j(i)\)</span> the value of item <span class="math inline">\(i\)</span> in the <span class="math inline">\(j\)</span>-th hash table. Let <span class="math inline">\(A_i\)</span> be the event such that any of the <span class="math inline">\(k\)</span> items with largest frequency is hashed to position <span class="math inline">\(h_j(i)\)</span> in the <span class="math inline">\(j\)</span>-th table. Then by union bound, <span class="math display">\[
\Pr[A_i] \le \frac{k}{w} = \frac{\epsilon^2}{3}
\]</span></p>
<p>Now,</p>
<p><span class="math display">\[
\begin{aligned}
    \Pr[|f_i - h_i(j)| \ge \frac{\epsilon}{\sqrt k } err_2^k(f)    ] 
        &amp;= \Pr[|f_i - h_i(j) | \ge  \frac{\epsilon}{\sqrt k } err_2^k(f) \mid A_i ] + \Pr[|f_i - h_i(j) | \ge  \frac{\epsilon}{\sqrt k } err_2^k(f) \mid \bar A_i ] \\
        &amp;\le \Pr[A_i] + \frac{ Var[ f_i - h_i(j)  \mid \bar A_i ] }{ (\frac{\epsilon}{\sqrt k } err_2^k(f) )^2 } \\
        &amp;\le \frac{\epsilon^2}{3} + \frac{(err_2^k(f))^2}{ w (\frac{\epsilon}{\sqrt k } err_2^k(f) )^2 } \\
        &amp;\le \frac{\epsilon^2}{3} + \frac{1}{3}
\end{aligned}
\]</span></p>
<p>As long as <span class="math inline">\(\epsilon^2 / 3 &lt; 1 / 6\)</span>, this probability is smaller than <span class="math inline">\(1 / 2\)</span>. We can boost the failure probability to <span class="math inline">\(1/ n^2\)</span> by repeating the hash tables <span class="math inline">\(d = O(\log n)\)</span> times and taking the median as estimation.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>The next step is keep only set <span class="math inline">\(\hat H\)</span> of the <span class="math inline">\(k\)</span> largest items in <span class="math inline">\(\hat f\)</span>. Note that <span class="math inline">\(\hat H\)</span> might not equals to <span class="math inline">\(H\)</span>. To bound the value <span class="math inline">\(|f - \hat f|_2\)</span>, we divide <span class="math inline">\([n]\)</span> into three subsets: <span class="math inline">\(\hat H\)</span>, <span class="math inline">\(H \setminus \hat H\)</span>, and <span class="math inline">\([n]\)</span>(H H)$</p>
<p><em>Lemma 2</em>. <span class="math inline">\(\sum_{i \in \hat H} (f_i - \hat f_i)^2 \le \epsilon^2 (err_2^k(f))^2\)</span>.</p>
<p><em>Proof.</em> As <span class="math inline">\(|\hat H| = k\)</span>, this follow directly from lemma 1.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><em>Lemma 3</em>. <span class="math inline">\(\sum_{i \in H \setminus \hat H} f_i ^2 \le \sum_{j \in \hat H \setminus H} \hat f_j^2 + (2 \epsilon + \epsilon^2) ( err_2^k(f))^2\)</span></p>
<p><em>Proof.</em> <span class="math inline">\(|H| = |\hat H|\)</span>, we have <span class="math inline">\(|H \setminus \hat H| = |\hat H \setminus H|\)</span>.</p>
<p>If <span class="math inline">\(H \setminus \hat H = \emptyset\)</span>, the claim is trivial. Otherwise, we can construct a bijection between <span class="math inline">\(H \setminus \hat H\)</span> and <span class="math inline">\(\hat H \setminus H\)</span>. Then for <span class="math inline">\(i \in H \setminus \hat H\)</span>,</p>
<p><span class="math display">\[
f_i = f_i - \hat f_i + \hat f_i - \hat f_j + \hat f_j
\]</span></p>
<p>As <span class="math inline">\(\hat H\)</span> contains the largest <span class="math inline">\(k\)</span> items in <span class="math inline">\(\hat f\)</span>, it must be that <span class="math inline">\(\hat f_i - \hat f_j \le 0\)</span>. Therefore,</p>
<p><span class="math display">\[
f_i \le f_i - \hat f_i + \hat f_j
\]</span></p>
<p>By lemma 1, we have <span class="math inline">\(f_i - \hat f_i \le \frac{\epsilon}{\sqrt k } err_2^k(f)\)</span>, we have</p>
<p><span class="math display">\[
f_i \le \frac{\epsilon}{\sqrt k } err_2^k(f) + \hat f_j
\]</span></p>
<p>Summing over all possible <span class="math inline">\(i\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
    \sum_{i \in H \setminus \hat H} f_i ^2 
    &amp;\le \sum_{j \in \hat H \setminus H} (\frac{\epsilon}{\sqrt k } err_2^k(f) + \hat f_j)^2 \\
    &amp;= \sum_{j \in \hat H \setminus H} \hat f_j^2  + 2 \left( \sum_{j \in \hat H \setminus H} \hat f_j \right) \frac{\epsilon}{\sqrt k } err_2^k(f) +  \epsilon^2 ( err_2^k(f))^2 \\
    &amp;\le \sum_{j \in \hat H \setminus H} \hat f_j^2  + 2 k \sqrt{ \sum_{j \in \hat H \setminus H} \frac{1}{k} \hat f_j^2} \frac{\epsilon}{\sqrt k } err_2^k(f) +  \epsilon^2 ( err_2^k(f))^2 \\
    &amp;= \sum_{j \in \hat H \setminus H} \hat f_j^2  + 2 \epsilon (err_2^k(f)) \sqrt{ \sum_{j \in \hat H \setminus H} \hat f_j^2} +  \epsilon^2 ( err_2^k(f))^2 \\
    &amp;\le \sum_{j \in \hat H \setminus H} \hat f_j^2  + (2 \epsilon +  \epsilon^2) ( err_2^k(f))^2 
\end{aligned}
\]</span></p>
<h2 id="reference">Reference</h2>
<p>[1] G. Cormode and S. Muthukrishnan. Combinatorial algorithms for Compressed Sensing.<br />
[2] Moses Charikar, Lecture 9, CS 369G: Algorithmic Techniques for Big Data</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/12/Priority-Sampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/12/Priority-Sampling/" class="post-title-link" itemprop="url">Priority Sampling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-12 14:32:57" itemprop="dateCreated datePublished" datetime="2020-01-12T14:32:57+11:00">2020-01-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-23 15:27:41" itemprop="dateModified" datetime="2020-01-23T15:27:41+11:00">2020-01-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose there are a set of <span class="math inline">\(n\)</span> items with nonnegative weights <span class="math inline">\(w_1, w_2, ..., w_n\)</span>. We want to maintain a subset of samples <span class="math inline">\(S \subset [n]\)</span> in order to answer queries like given arbitrary <span class="math inline">\(I \subset [n]\)</span> , what is <span class="math inline">\(\sum_{i \in I} w_i\)</span>?</p>
<p>A trivial solution is to set <span class="math inline">\(S = [n]\)</span> and we can answer the query exactly. This is an overkill if our goal is to only answer the query approximately.</p>
<p>Alternatively, we can sample each item <span class="math inline">\(i \in [n]\)</span> independently with probability <span class="math inline">\(p_i\)</span> (to be determined), and maintain a random variable <span class="math display">\[
\begin{aligned}
X_i = 
    \begin{cases}
        \frac{w_i}{p_i} &amp; i \text{ is chosen} \\
        0               &amp;   \text{ else }
    \end{cases}
\end{aligned}
\]</span></p>
<p>Note that <span class="math display">\[
E \left[ X_i \right] = w_i
\]</span></p>
<p>And therefore, <span class="math display">\[
E \left[\sum_{i \in I} X_i \right] = \sum_{i \in I} E[X_i] = \sum_{i \in I} w_i
\]</span></p>
<p>One possible materialization of this sampling scheme is called <em>threshold sampling</em>. First a threshold <span class="math inline">\(T\)</span> is selected. Then each <span class="math inline">\(i \in [n]\)</span> is kept with probability with <span class="math inline">\(p_i = \min \{ 1 , \frac{w_i}{T} \}\)</span>. In particular, for each <span class="math inline">\(i \in [n]\)</span>, we sample a uniform random variable <span class="math inline">\(u_i \in [0, 1]\)</span>, and set <span class="math display">\[
\begin{aligned}
X_i = 
    \begin{cases}
        \max \{ w_i, T \} &amp; u_i &lt; \frac{w_i}{T} \\
        0               &amp;   \text{ else }
    \end{cases}
\end{aligned}
\]</span></p>
<p>Note that for <span class="math inline">\(i \in [n]\)</span> such that <span class="math inline">\(w_i &gt; T\)</span>, it always holds that <span class="math inline">\(u_i &lt; \frac{w_i}{T}\)</span> and the definition reduce to <span class="math inline">\(X_i = w_i\)</span>. For <span class="math inline">\(i \in [n]\)</span> such that <span class="math inline">\(w_i &lt; T\)</span>, <span class="math display">\[
\begin{aligned}
X_i = 
    \begin{cases}
        T = \frac{w_i}{p_i} &amp; u_i &lt; \frac{w_i}{T} = p_i \\
        0               &amp;   \text{ else }
    \end{cases}
\end{aligned}
\]</span></p>
<p>(Note that we take <span class="math inline">\(u_i &lt; \frac{w_i}{T}\)</span> instead of <span class="math inline">\(u_i \le \frac{w_i}{T}\)</span> here). Clearly, in this case we have <span class="math inline">\(E[X_i] = \frac{w_i}{T} T = w_i\)</span>.</p>
<p>We consider the space usage of <em>threshold sampling</em>. Denote <span class="math inline">\(S\)</span> the set of items with <span class="math inline">\(X_i \neq 0\)</span>. Then the expected size of <span class="math inline">\(S\)</span> is given by <span class="math display">\[
\sum_{i = 1}^n p_i = \sum_{i = 1}^n \min \left\{ 1 , \frac{w_i}{T} \right\}
\]</span></p>
<p>The problem with threshold sampling is that it is hard to control the memory usage of the sample set <span class="math inline">\(S\)</span>. If the <span class="math inline">\(n\)</span> items are known offline, and we would like <span class="math inline">\(|S| \approx k\)</span> for some integer <span class="math inline">\(k \le n\)</span>, then we can search for some <span class="math inline">\(T\)</span> such that <span class="math display">\[
\sum_{i = 1}^n p_i = k
\]</span></p>
<p>Then <span class="math inline">\(|S| = k\)</span> in expectation.</p>
<p>The question is, is it possible to force <span class="math inline">\(|S|= k\)</span> exactly? Further, what if the items come online (one by one)?</p>
<h3 id="priority-sampling">Priority Sampling</h3>
<p><em>Priority sampling</em> achieves this. First recall that in <em>threshold sampling</em> we decide whether to keep <span class="math inline">\(i\)</span> by checking whether a uniform sampled real number <span class="math inline">\(u_i\)</span> is less than <span class="math inline">\(\frac{w_i}{T}\)</span>: <span class="math display">\[
u_i &lt; \frac{w_i}{T}
\]</span></p>
<p>This is equivalent to <span class="math display">\[
T &lt; \frac{w_i}{u_i}
\]</span></p>
<p>Define the <em>priority</em> of an item <span class="math inline">\(i \in [n]\)</span> as <span class="math inline">\(q_i = \frac{w_i}{u_i}\)</span>. Note that <span class="math inline">\(q_i\)</span> is a random variable. In <em>threshold sampling</em>, item <span class="math inline">\(i\)</span> is kept with weight <span class="math inline">\(\max \{ w_i, T \}\)</span> if <span class="math display">\[
T &lt; q_i
\]</span></p>
<p>Now, instead of using a fixed value of <span class="math inline">\(T\)</span>, the <em>Priority sampling</em> sets <span class="math display">\[
T = (k + 1) \text{-th largest } \{ q_i \}
\]</span></p>
<p>By the definition of <span class="math inline">\(T\)</span>, only the <span class="math inline">\(k\)</span> items with largest priorities <span class="math inline">\(q_i\)</span>'s are kept in the sample set <span class="math inline">\(S\)</span>: <span class="math display">\[
S = \{ k \text{ items with largest } q_i \text{ &#39;s } \}
\]</span></p>
<p>It can be seen that <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> can be easily maintained by a binary heap when the items come in stream fashion.</p>
<p>The challenge is not to analyze the behaviors of the r.v.'s: <span class="math display">\[
\begin{aligned}
X_i = 
    \begin{cases}
        \max \{ w_i, T \} &amp; T &lt; \frac{w_i}{u_i} \equiv u_i &lt; \frac{w_i}{T} \\
        0               &amp;   \text{ else }
    \end{cases}
\end{aligned}
\]</span></p>
<p>since now <span class="math inline">\(T\)</span> is its self a random variable.</p>
<p>We claim that it still holds<br />
<em>Theorem 1</em> <span class="math display">\[
E[X_i] = w_i
\]</span></p>
<p>The trick is to conditioned <span class="math inline">\(X_i\)</span> on the <span class="math inline">\(k\)</span>-th and <span class="math inline">\(k+1\)</span>-th largest priority of the rest items. Define the random variables <span class="math display">\[
L = (k + 1) \text{-th largest } \{ q_j \}_{j \neq i} \\
U = (k) \text{-th largest } \{ q_j \}_{j \neq i}
\]</span></p>
<p>Consider the case <span class="math inline">\(L = a\)</span> and <span class="math inline">\(U = b\)</span>, where <span class="math inline">\(a \le b\)</span>. There two possible cases</p>
<ol type="1">
<li><p>If <span class="math inline">\(w_i &gt; b\)</span>, then <span class="math inline">\(T = b\)</span> and surely <span class="math inline">\(\frac{w_i}{u_i} &gt; T\)</span> holds. In this case <span class="math inline">\(X_i = w_i\)</span> and <span class="math display">\[
     E[X_i | U = b] = w_i
 \]</span></p></li>
<li><p>If <span class="math inline">\(w_i &lt; b\)</span>, then there are four sub-cases:</p>
<ul>
<li><p><span class="math inline">\(\frac{w_i}{u_i} \le a\)</span>, then <span class="math inline">\(T = b\)</span> and <span class="math inline">\(i\)</span> is not selected into <span class="math inline">\(S\)</span>. Therefore <span class="math inline">\(X_i = 0\)</span>.</p></li>
<li><p><span class="math inline">\(a &lt; \frac{w_i}{u_i} &lt; b\)</span>, now <span class="math inline">\(\frac{w_i}{u_i}\)</span> becomes the <span class="math inline">\(k+1\)</span>-th largest priority of all items and <span class="math inline">\(T = \frac{w_i}{u_i}\)</span>. In this case <span class="math inline">\(i\)</span> is not selected into <span class="math inline">\(S\)</span>, and <span class="math inline">\(X_i = 0\)</span> (since <span class="math inline">\(\frac{w_i}{u_i} &lt; T\)</span> does not hold).</p></li>
<li><p><span class="math inline">\(\frac{w_i}{u_i} = b\)</span>, then there is a tie and <span class="math inline">\(T = b\)</span> in this case. Depending on how the tie is broken, either <span class="math inline">\(i\)</span> is put into <span class="math inline">\(S\)</span> or not. But in both cases, <span class="math inline">\(X_i = 0\)</span> since <span class="math inline">\(\frac{w_i}{u_i} &lt; T\)</span> does not hold.</p></li>
<li><p><span class="math inline">\(\frac{w_i}{u_i} &gt; b\)</span>, then <span class="math inline">\(T = b\)</span> and <span class="math inline">\(i\)</span> is put into <span class="math inline">\(S\)</span>. As <span class="math inline">\(w_i &lt; T\)</span>, we have <span class="math inline">\(X_i = T = b\)</span>.</p></li>
</ul>
<p>Therefore, <span class="math display">\[
     E[X_i | U = b] = b \cdot \Pr\left[ \frac{w_i}{u_i} &gt; b \mid w_i &lt; b \wedge U = b \right] = b \cdot \frac{w_i}{b} = w_i
 \]</span></p></li>
</ol>
<p>In conclusion, <span class="math display">\[
E[X_i] = E[E[X_i | U]]  = w_i
\]</span></p>
<p><em>Theorem 2</em>. For <span class="math inline">\(I \subset [n]\)</span>, <span class="math display">\[
\begin{aligned}
    E\left[ \prod_{i \in I} X_i \right] =
    \begin{cases}
         \prod_{i \in I} w_i     &amp; \text{ if } |I| \le k \\
         0                      &amp; else 
    \end{cases}
\end{aligned}
\]</span></p>
<p><em>Proof:</em> Clearly, when <span class="math inline">\(|I| &gt; k\)</span>, then <span class="math inline">\(\exists i \in I\)</span>, such that <span class="math inline">\(q_i\)</span> is not one of the <span class="math inline">\(k\)</span> largest priorities. Hence <span class="math inline">\(X_i = 0\)</span> for sure.</p>
<p>If <span class="math inline">\(|I| \le k\)</span>, we show the claims holds by induction. The base case of <span class="math inline">\(|I| = 1\)</span> reduces to <em>Theorem 1</em>. Suppose that the claim hold for <span class="math inline">\(|I| \le t &lt; k\)</span>, we show it holds for <span class="math inline">\(|I| = t + 1\)</span>.</p>
<p>Similarly to <em>Theorem 1</em>, define <span class="math inline">\(U\)</span> to be the <span class="math inline">\(k + 1 - |I|\)</span>-th largest priority in <span class="math inline">\([n] \setminus I\)</span>. Conditioning on <span class="math inline">\(U = b\)</span>, then we can divide <span class="math inline">\(I\)</span> into two sets: <span class="math inline">\(I_1 = \{ i \in I : w_i &gt; b\}\)</span> and <span class="math inline">\(I_2 = I \setminus I_1\)</span>. For <span class="math inline">\(i \in I_1\)</span>, we have <span class="math inline">\(X_i = w_i\)</span> by definition. Now consider <span class="math inline">\(I_2\)</span>. Let <span class="math inline">\(j = \arg \min_{i \in I_2} q_i\)</span>. If <span class="math inline">\(q_j \le b\)</span>, then it holds that <span class="math inline">\(b \ge T \ge q_j\)</span> and <span class="math inline">\(X_j = 0\)</span>. In this case it does not contribute to <span class="math inline">\(\prod_{i \in I} X_i\)</span>. Hence <span class="math display">\[
\begin{aligned}
E \left[ \prod_{i \in I} X_i \mid U = b \right] 
    &amp;= \prod_{i \in I_1} w_i ( b^{|I_2| } \cdot \prod_{i \in I_2} \Pr \left[ \frac{w_i}{u_i} &gt; b \right] ) \\
    &amp;= \prod_{i \in I_1} w_i ( b^{|I_2| } \cdot \prod_{i \in I_2} \frac{w_i}{b} ) \\
    &amp;= \prod_{i \in I} w_i
\end{aligned}
\]</span></p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>Nick Duffield, Carsten Lund, and Mikkel Thorup. Priority sampling for estimation fo arbitrary subset sums. Journal of ACM, 2007</li>
<li>Chandra Chekuri, Lecture note, CS 598: Algorithms for Big Data</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/02/Newton-s-Method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/02/Newton-s-Method/" class="post-title-link" itemprop="url">Newton's Method</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-01-02 16:25:30 / Modified: 16:54:02" itemprop="dateCreated datePublished" datetime="2020-01-02T16:25:30+11:00">2020-01-02</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Newton's method is an algorithm for finding root of a function <span class="math inline">\(f(x)\)</span> numerically. If <span class="math inline">\(f\)</span> is differentially at a point <span class="math inline">\(x\)</span>, then we can use this local information at <span class="math inline">\(x\)</span> to approximate <span class="math inline">\(f\)</span>: <span class="math display">\[
\bar f(y) = f(x) + f&#39;(x) (y - x)
\]</span></p>
<p>This is the tangent line at <span class="math inline">\(x\)</span>. We use its intersection with the <span class="math inline">\(x\)</span>-axis as the guess of root: <span class="math display">\[
\bar f(y) = 0 = f(x) + f&#39;(x) (y - x)
\]</span></p>
<p>If <span class="math inline">\(f&#39;(x) \neq 0\)</span>, then <span class="math inline">\(y = x - \frac{f(x)}{f&#39;(x)}\)</span>. Intuitively, <span class="math inline">\(f(x)\)</span> is the height and <span class="math inline">\(f&#39;(x)\)</span> is the slope, hence <span class="math inline">\(\frac{f(x)}{f&#39;(x)} = x - y\)</span> gives the distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Note that if <span class="math inline">\(f&#39;(x) = 0\)</span>, the tangent line does not intersect with <span class="math inline">\(x\)</span>-axis.</p>
<p>With a starting point <span class="math inline">\(x_1\)</span>, the Newton's Method updates its value iteratively: <span class="math display">\[
x_{n + 1} = x_n - \frac{f(x_n)  }{ f&#39;(x_n) }
\]</span></p>
<p>Let <span class="math inline">\(z\)</span> be the root of <span class="math inline">\(f\)</span> (<span class="math inline">\(f(z) = 0\)</span>). Then under proper condition, <span class="math inline">\(x_n\)</span> converges to <span class="math inline">\(z\)</span> quadratically, that is, <span class="math inline">\(|x_n - z| \le O(2^{-n})\)</span>.</p>
<h3 id="theorem.">Theorem.</h3>
<p>Let <span class="math inline">\(f \in C^2\)</span> in some interval <span class="math inline">\([z \pm c]\)</span> and <span class="math inline">\(f&#39;(x) \neq 0\)</span>, <span class="math inline">\(f&#39;&#39;(x)\)</span> is bounded for <span class="math inline">\(x \in [z \pm c]\)</span>. If <span class="math inline">\(x_0 \in [z \pm c]\)</span>, then Newton's Method achieves quadratic convergence.</p>
<p><em>Proof:</em> By Taylor expansion, <span class="math inline">\(\forall x \in [z \pm c]\)</span>, <span class="math inline">\(\exists y\)</span> that lies between <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>, such that <span class="math display">\[
0 = f(z) = f(x) + f&#39;(x) (x - z) + \frac{f&#39;&#39;(y)}{2} (z - x)^2 
\]</span></p>
<p>Hence <span class="math display">\[
x - \frac{f(x)}{f&#39;(x)} - z = \frac{f&#39;&#39;(y)}{2 f&#39;(x)} (z - x)^2 
\]</span></p>
<p>Define <span class="math inline">\(\epsilon_n = (z - x_n)\)</span>, and replacing <span class="math inline">\(x\)</span> with <span class="math inline">\(x_n\)</span>, we have <span class="math display">\[
|\epsilon_{n + 1}| = |x_{n + 1} - z| = |x_n - \frac{f(x_n)}{f&#39;(x_n)} - z| = |\frac{f&#39;&#39;(y)}{2 f&#39;(x_n)}| (z - x_n)^2 = |\frac{f&#39;&#39;(y)}{2 f&#39;(x_n)}| \epsilon_n^2 
\]</span></p>
<p>Since <span class="math inline">\(f\)</span>'s second derivative is continuous, <span class="math inline">\(f&#39;&#39;(y)\)</span> is bounded on the closed interval <span class="math inline">\([z \pm c]\)</span>. Further, as <span class="math inline">\(f&#39;(x) \neq 0\)</span>, then <span class="math inline">\(\min_{x [z \pm c]} f&#39;(x) &gt; 0\)</span> as <span class="math inline">\(f&#39;(x)\)</span> is continuous on the closed interval <span class="math inline">\([z \pm c]\)</span>. Therefore, <span class="math display">\[
C \doteq \max |\frac{f&#39;&#39;(y)}{2 f&#39;(x_n)}|
\]</span> is bounded. Now <span class="math display">\[
|\epsilon_{n + 1}| \le |C| \epsilon_n^2
\]</span></p>
<p>If for some <span class="math inline">\(n\)</span> such that <span class="math inline">\(\epsilon_n &lt; 1\)</span>, then the number of precise digits doubles for <span class="math inline">\(\epsilon_{n + 1}\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/01/Online-Matrix-Sketch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/01/Online-Matrix-Sketch/" class="post-title-link" itemprop="url">Online Matrix Sketch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-01 21:04:12" itemprop="dateCreated datePublished" datetime="2020-01-01T21:04:12+11:00">2020-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-25 17:36:33" itemprop="dateModified" datetime="2020-12-25T17:36:33+11:00">2020-12-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given a matrix <span class="math inline">\(A \in \R^{m \times n}\)</span>, the goal of low approximation is to find a matrix <span class="math inline">\(B \in R^{k \times n}\)</span> with rank at most <span class="math inline">\(k\)</span> that approximates <span class="math inline">\(A\)</span> as well as possible. This can be measured by <span class="math inline">\(2\)</span>-norm, i.e., <span class="math display">\[
||A - B|| = \max_{x \in R^n, ||x|| = 1} ||(A - B)x||
\]</span></p>
<p>If we know the entire matrix <span class="math inline">\(A\)</span>, its best approximation is given by the singular vector decomposition, <span class="math display">\[
 B \doteq \sum_{i = 1}^k \sigma_i u_i v_i^T
\]</span> where <span class="math inline">\(\sigma_i\)</span>, <span class="math inline">\(u_i\)</span> and <span class="math inline">\(v_i\)</span> are the <span class="math inline">\(i\)</span>-th largest singular value, left singular vector and right singular vector of <span class="math inline">\(A\)</span>, respectively.</p>
<p>When some elements of <span class="math inline">\(A\)</span> are missing, finding such a matrix <span class="math inline">\(B\)</span> to approximate <span class="math inline">\(A\)</span> as well as to predict the missing entries of <span class="math inline">\(A\)</span> is known as collaborative filtering, which is used in recommendation system.</p>
<p>Another complicated scenario is to approximate <span class="math inline">\(A\)</span> in an online fashion, i.e., the rows of <span class="math inline">\(A\)</span> comes on by one (no missing values). Denote the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(A\)</span> as <span class="math inline">\(A_i\)</span>. We need to maintain a smaller matrix <span class="math inline">\(B \in \R^{k \times n}\)</span>, termed the matrix sketch of <span class="math inline">\(A\)</span>, that approximate <span class="math inline">\(A\)</span> well, in the sense that <span class="math display">\[
|| A - B || \le \frac{2 ||A||_F^2}{ k },
\]</span></p>
<p>where <span class="math inline">\(|| A ||_F \doteq \sqrt{ \sum_{i \in [n], j \in [n] } A_{i,j}^2 }\)</span> is the Frobenius norm of <span class="math inline">\(A\)</span>.</p>
<blockquote>
<p>Algorithm [1]<br />
1. <span class="math inline">\(B \leftarrow\)</span> <span class="math inline">\({k \times n}\)</span> all zero matrix. 2. For <span class="math inline">\(i \in [1:m]\)</span>: 3. <span class="math inline">\(\qquad\)</span> Insert <span class="math inline">\(A_i\)</span> into a zero row of <span class="math inline">\(B\)</span>. 4. <span class="math inline">\(\qquad\)</span> If <span class="math inline">\(B\)</span> has no zero row after insertion: 5. <span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> <span class="math inline">\(U_i, \Sigma_{i-}, V_i \leftarrow SVD(B)\)</span>, such that <span class="math inline">\(B = U_i \Sigma_{i-} V_i^T\)</span>. 6. <span class="math inline">\(\qquad\)</span><span class="math inline">\(\qquad\)</span> <span class="math inline">\(\Sigma_{i} \leftarrow \sqrt{ \max \{ \Sigma_{i-}^2 - \sigma_{i, k / 2}^2 I, 0 \} }\)</span> 7. <span class="math inline">\(\qquad\)</span><span class="math inline">\(\qquad\)</span> <span class="math inline">\(B \leftarrow \Sigma_{i} V_i^T\)</span>.</p>
</blockquote>
<p>Here <span class="math inline">\(\sigma_{i, k / 2}\)</span> is the <span class="math inline">\(k / 2\)</span>-th largest singular value of <span class="math inline">\(\Sigma_{i -}\)</span>. The line 6 is throw away all singular values that are less than <span class="math inline">\(\sigma_{i, k / 2}\)</span>, and then subtract the squares of all singular values that are greater by <span class="math inline">\(\sigma_{i, k / 2}^2\)</span>.</p>
<h4 id="properties">Properties</h4>
<h5 id="theorem-1-1.-bt-b-preccurlyeq-at-a-at-any-time.">Theorem 1 [1]. <span class="math inline">\(B^T B \preccurlyeq A^T A\)</span> at any time.</h5>
<p><em>Proof:</em> To show <span class="math inline">\(A^T A - B^T B\)</span> is semi-positive definitive, we need to show that <span class="math inline">\(\forall x \in R^n\)</span>, we have <span class="math display">\[
x^T A^T A x - x^T B^T B x \ge 0
\equiv ||Ax||^2 \ge ||Bx||^2
\]</span></p>
<p>Denote <span class="math inline">\(B_i\)</span> the matrix of <span class="math inline">\(B\)</span> after the <span class="math inline">\(i\)</span>-th iteration of the algorithm. Note that <span class="math inline">\(B_0 = 0^{k \times n}\)</span>. Then it holds that <span class="math display">\[
||B_i x||^2 + (A_i \ x)^2 \ge ||B_{i + 1} x||^2
\]</span></p>
<p>There are two possible cases. In the first case, after inserting <span class="math inline">\(A_i\)</span> into <span class="math inline">\(B_i\)</span>, <span class="math inline">\(B_i\)</span> still contains empty row, then the if-clause does not execute and the quality holds. Otherwise, if the if-clause is executed, then <span class="math display">\[
||B_{i + 1} x||^2  = ||\Sigma_i V_i^T x||^2
\]</span></p>
<p>Moreover <span class="math display">\[
||B_i x||^2 + (A_i \ x)^2 = ||U_i \Sigma_{i-} V_i^T x||^2 = ||\Sigma_{i-} V_i^T x||^2 
\]</span> Since <span class="math inline">\(U_i\)</span> is an orthogonal matrix and corresponds to a left rotation</p>
<p>By the transformation of the algorithm, we have <span class="math inline">\(\Sigma_{i-}^T \Sigma_{i-} \preccurlyeq \Sigma_{i}^T \Sigma_i\)</span>, therefore, <span class="math display">\[
 x V_i \Sigma_{i}^T \Sigma_{i} V_i^T x - x V_i \Sigma_{i-}^T \Sigma_{i-} V_i^T x \ge 0
\]</span></p>
<h5 id="theorem-2-1.-at-a---bt-b-le-2-a_f2-k">Theorem 2 [1]. <span class="math inline">\(||A^T A - B^T B|| \le 2 ||A||_F^2 / {k}\)</span></h5>
<p><em>Proof:</em> We first show that for a semi-positive definite matrix <span class="math inline">\(S\)</span>, we have <span class="math display">\[
||S|| = \max_{x \in R^n, ||x|| = 1} ||S x|| = \max_{x \in R^n, ||x|| = 1} \sqrt {x^T S^T S x} = \max_{x \in R^n, ||x|| = 1} \sqrt {x^T S^2 x} 
\]</span></p>
<p>But <span class="math inline">\(S^2\)</span> has the same eigenvectors as <span class="math inline">\(S\)</span>. Denote <span class="math inline">\(x\)</span> be <span class="math inline">\(S\)</span>'s largest unit eigenvector and <span class="math inline">\(x^T S x = x^T \lambda_1 x_1 = \lambda_1\)</span> its corresponding eigenvalues. Then <span class="math display">\[
||S|| = \sqrt {x^T S^2 x} = \lambda_1 = x^T Sx 
\]</span> and <span class="math inline">\(||S|| = \lambda_1\)</span>. Geometrically, as <span class="math inline">\(x\)</span> is eigenvector, it is colinear with <span class="math inline">\(Sx\)</span>. Further, since <span class="math inline">\(x\)</span> is a unit vector, the length of <span class="math inline">\(Sx\)</span> is given by <span class="math inline">\(x^T Sx\)</span>.</p>
<p>Now, let <span class="math inline">\(x\)</span> the largest unit eigenvector of <span class="math inline">\(A^T A - B^T B\)</span>. As <span class="math inline">\(B^T B \ll A^T A\)</span>, <span class="math display">\[
\begin{aligned}
||A^T A - B^T B|| 
    &amp;= x^TA^T A x- x^T B^T Bx \\
    &amp;= ||Ax||^2 - ||Bx||^2 \\
    &amp;= \sum_{i = 1}^n (A_i^T x)^2  + ||B_{i - 1} x||^2 - ||B_{i} x||^2 \\
    &amp;= \sum_{i = 1}^n ||\Sigma_{i-} V_i^T x||^2 - ||\Sigma_i V_i^T x||^2 \\
    &amp;\le \sum_{i = 1}^n ||V_i \Sigma_{i-}^T \Sigma_{i-} V_i^T - V_i \Sigma_i^T \Sigma_i V_i^T || \\
    &amp;= \sum_{i = 1}^n ||\Sigma_{i-}^T \Sigma_{i-}  - \Sigma_i^T \Sigma_i || \\
    &amp;= \sum_{i = 1}^n \sigma_{i, k /2 }^2
\end{aligned}
\]</span></p>
<p>It is left to bound the size of <span class="math inline">\(\sum_{i = 1}^n \sigma_{i, k /2 }^2\)</span>: <span class="math display">\[
\begin{aligned}
||A||_F - ||B||_F
    &amp;= \sum_{i = 1}^n ||A_i||^2  + ||B_{i - 1}||_F - ||B_{i}||_F \\
    &amp;= \sum_{i = 1}^n ||U_i \Sigma_{i-} V_i^T||_F - ||\Sigma_i V_i^T||_F \\
    &amp;= \sum_{i = 1}^n ||\Sigma_{i-} V_i^T||_F - ||\Sigma_i V_i^T||_F \\
    &amp;= \sum_{i = 1}^n ||\Sigma_{i-}||_F - ||\Sigma_i||_F \\
    &amp;\ge (k / 2) \sum_{i = 1}^n \sigma_{i, k /2 }^2
\end{aligned}
\]</span></p>
<p>The equality holds since left or right multiplying a orthogonal matrix does not change Frobenius norm (the sum of square distance of the row vectors to the origin). The last inequality holds by the definition of <span class="math inline">\(\Sigma_{i - }\)</span> and <span class="math inline">\(\Sigma_i\)</span>.</p>
<p>In conclusion, <span class="math display">\[
\begin{aligned}
    \frac{||A^T A - B^T B||}{||A||_F} \le \frac{||A^T A - B^T B||}{||A||_F - ||B||_F} \le \frac{2}{k}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span>.</p>
<h2 id="reference.">Reference.</h2>
<p>[1]. Liberty, Edo. "Simple and deterministic matrix sketching." In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 581-588. ACM, 2013.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/12/13/Polyhydra-and-Polytope/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/13/Polyhydra-and-Polytope/" class="post-title-link" itemprop="url">Polyhydra and Polytopes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-13 23:54:11" itemprop="dateCreated datePublished" datetime="2019-12-13T23:54:11+11:00">2019-12-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-15 22:37:11" itemprop="dateModified" datetime="2019-12-15T22:37:11+11:00">2019-12-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>When studying linear program, it necessary that we understand the structure of the feasible region. We investigate here some critical ideas of characterizing the feasible region.</p>
<ol type="1">
<li><em>Polyhedron</em>: <span class="math inline">\(P\)</span> is a polyhedron in <span class="math inline">\(R^n\)</span> if it is the intersection of a set of linear constraints: <span class="math inline">\(P = \{ x: Ax \le b \}\)</span> for some <span class="math inline">\(A \in R^{m \times n}, b \in R^n\)</span>. A polyhedron is a convex set.</li>
<li><em>Polytope</em>: <span class="math inline">\(P\)</span> is a polytope if it consists of points that are convex combination of finite number of points <span class="math inline">\(v_1, v_2, ..., v_k\)</span>, i.e., <span class="math inline">\(P = \{ x: x = \sum_{i = 1}^k \mu_i v_i , \ s.t., \ \sum_{i =1}^k \mu_i = 1 \wedge \mu_i \in [0, 1] \quad \forall i \in [k] \}\)</span>.</li>
</ol>
<p>Of interest are the vertices, or extreme points or basic feasible solution of a polyhedron <span class="math inline">\(P\)</span>:</p>
<ol type="1">
<li><em>Vertex</em>: <span class="math inline">\(v \in P\)</span> is called a vertex if <span class="math inline">\(\exists c \in R^n\)</span>, s.t., <span class="math inline">\(c^T v &gt; c^T x\)</span> <span class="math inline">\(\forall x \neq v \wedge x\in R\)</span>.</li>
<li><em>Extreme Point</em>: <span class="math inline">\(v \in R\)</span> is an extreme point of <span class="math inline">\(P\)</span> if <span class="math inline">\(\nexists x, y \neq v, x, y \in P\)</span>, s.t., <span class="math inline">\(v = \mu_1 x + \mu_2 y\)</span>, <span class="math inline">\(\mu_1, \mu_2 \in [0, 1]\)</span> and <span class="math inline">\(\mu_1 + \mu_2 = 1\)</span>.</li>
<li><em>Basic Feasible Solution:</em> <span class="math inline">\(v \in P\)</span> is a basic feasible solution of <span class="math inline">\(P = \{ x: Ax \le b \}\)</span>, if the number of tight constraints <span class="math inline">\(A v \le b\)</span> equals to <span class="math inline">\(n\)</span>. In particular, denote <span class="math inline">\(A_{i, :}\)</span> the <span class="math inline">\(i\)</span>-th row of matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(b_i\)</span> the <span class="math inline">\(i\)</span>-th element of <span class="math inline">\(b\)</span>. The constraint <span class="math inline">\(A_{i, :} v \le b_i\)</span> is called tight if equality holds. For convenience of discussion, denote <span class="math inline">\(A_{=} x = b_{=}\)</span> the set of tight constraints and <span class="math inline">\(A_{&lt;} x &lt; b_{&lt;}\)</span> the set of non-tight constraints.</li>
</ol>
<p>It turns out that the three definitions are equivalent. Here is the proof.</p>
<p><em>Proof:</em></p>
<p><span class="math inline">\(1 \rightarrow 2:\)</span> <span class="math inline">\(c^T(\mu_1 x + \mu_2 y) = \mu_1 c^T x + \mu_2 c^T y &lt; \mu_1 c^T v + \mu_2 c^T v = c^T v\)</span>, it concludes that <span class="math inline">\(\mu_1 x + \mu_2 y \neq v\)</span>.</p>
<p><span class="math inline">\(2 \rightarrow 3:\)</span> suppose that <span class="math inline">\(rank(A_{=}) \neq n\)</span>. There <span class="math inline">\(\exists y \in R^n, y \neq 0\)</span>, and <span class="math inline">\(A_{=} y = 0\)</span>. For inequality constraints, we can find small enough <span class="math inline">\(\epsilon\)</span>, such that <span class="math inline">\(A_{&lt; } (x \pm \epsilon y) &lt; b\)</span>. Hence <span class="math inline">\(x \pm \epsilon y \in P\)</span>. But <span class="math inline">\(\frac{1}{2} (x + \epsilon y) + \frac{1}{2} (x - \epsilon y) = x\)</span>, a contradiction.</p>
<p><span class="math inline">\(3 \rightarrow 1:\)</span> just define <span class="math inline">\(c^T = \sum_{i = 1}^m A_{i, :}\)</span>. If <span class="math inline">\(x \in P\)</span>, then <span class="math inline">\(\sum_{i = 1}^m A_{i, :} x \le \sum_{i = 1}^m b_i = \sum_{i = 1}^m A_{i, :} v\)</span>. As <span class="math inline">\(A_{=}\)</span> has rank <span class="math inline">\(n\)</span>, when equality holds, it implies <span class="math inline">\(x = y\)</span>.</p>
<p>The relationship between polyhedron and polytope is stated as follows:</p>
<p><em>Theorem:</em> if a polyhedron <span class="math inline">\(P\)</span> is bounded, that is, <span class="math inline">\(\exists M &gt; 0\)</span>, <span class="math inline">\(\forall x \in P\)</span>, we have <span class="math inline">\(x^T x &lt; M\)</span>, then <span class="math inline">\(P\)</span> is a polytope.</p>
<p><em>Proof:</em> It suffice to show that every <span class="math inline">\(x \in P\)</span> is a convex combination of vertices of <span class="math inline">\(P\)</span>. The proof is by induction on the rank of <span class="math inline">\(A_{=}\)</span>, given <span class="math inline">\(x \in A\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(rank(A_{=}) = n\)</span>, then by definition <span class="math inline">\(x\)</span> is a vertex.</li>
<li>Suppose the claim holds for all ranks <span class="math inline">\(\ge k + 1\)</span> for some <span class="math inline">\(k &lt; n\)</span>. We show it carries to <span class="math inline">\(x \in P\)</span> with <span class="math inline">\(rank(A_{=}) = k\)</span>.
<ul>
<li>As <span class="math inline">\(k &lt; n\)</span>, <span class="math inline">\(\exists y \in R^n, y \neq 0\)</span>, such that <span class="math inline">\(A_{=} y = 0\)</span>.</li>
<li>We extend <span class="math inline">\(x\)</span> from the directions of <span class="math inline">\(y\)</span> and <span class="math inline">\(-y\)</span> as much as possible. We find the largest <span class="math inline">\(\lambda_+ \in R, \lambda_+ &gt; 0\)</span>, such that <span class="math inline">\(x + \lambda_+ y \in P\)</span>. Since <span class="math inline">\(A_{=} (x + \lambda_+ y) = b_{=}\)</span>, these constraints will never be violated. Hence some constraints <span class="math inline">\(A_{&lt; } x &lt; b_{&lt;}\)</span> becomes tight for <span class="math inline">\(A_{&lt;} (x + \lambda_+ y)\)</span>. Which implies that the rank of <span class="math inline">\(x + \lambda_+ y\)</span> is at least <span class="math inline">\(k + 1\)</span> and thus can be expressed a convex combination of vertices. Similarly, we can find some <span class="math inline">\(x - \lambda_- y\)</span> that is a convex combination of vertices.</li>
<li><span class="math inline">\(x\)</span> can now be express a convex combination of <span class="math inline">\(x + \lambda_+ y\)</span> and <span class="math inline">\(x - \lambda_- y\)</span>, therefore a convex combination of vertices.</li>
</ul></li>
</ol>
<p>The reverse is also true:</p>
<p><em>Theorem:</em> Let <span class="math inline">\(P\)</span> be a polytope, then <span class="math inline">\(P\)</span> is a bounded polyhedron.</p>
<p>To prove this, we need an intermediate definition--<em>polar set</em>. It means something on the opposite direction.</p>
<p><em>Definition:</em> Given a set <span class="math inline">\(P\)</span>, its polar set is defined as <span class="math inline">\(\bar P = \{ y \in R^n, y^T x \le 1, \forall x \in P\}\)</span>.</p>
<p><em>Lemma</em>: If <span class="math inline">\(P\)</span> is closed, convex and <span class="math inline">\(0\)</span> is the interior point of <span class="math inline">\(P\)</span>, then <span class="math inline">\(\overline{\bar P} = P\)</span>, that is the polar set of the polar set of <span class="math inline">\(P\)</span> is <span class="math inline">\(P\)</span> itself.</p>
<p><em>Proof of the Lemma</em>:</p>
<ol type="1">
<li><span class="math inline">\(P \subset \overline{\bar P}\)</span>. If <span class="math inline">\(x \in P\)</span>, then by definition of <span class="math inline">\(\bar P\)</span>, <span class="math inline">\(\forall y \in \overline{\bar P}\)</span>, we have <span class="math inline">\(y^T x \le 1\)</span>, then <span class="math inline">\(x \in P^{oo}\)</span>.</li>
<li><span class="math inline">\((R^n - P) \cap \overline{\bar P} = \emptyset\)</span>, i.e., <span class="math inline">\(\forall x \notin P\)</span>, we have <span class="math inline">\(v \notin \overline{\bar P}\)</span>. Since <span class="math inline">\(P\)</span> is convex and closed, <span class="math inline">\(\exists y \in R^n\)</span>, such that <span class="math inline">\(y^T v &gt; k\)</span> and <span class="math inline">\(y^T x &lt; k\)</span> for all <span class="math inline">\(x \in P\)</span>. The fact that <span class="math inline">\(0 \in P\)</span> implies <span class="math inline">\(k &gt; 0\)</span>. Replace <span class="math inline">\(y\)</span> with <span class="math inline">\(\bar y = y / k\)</span>, we have <span class="math inline">\(\bar y^T x &lt; 1\)</span> for all <span class="math inline">\(x \in P\)</span>. Hence <span class="math inline">\(\bar y \in \bar P\)</span>. But now <span class="math inline">\(\bar y^T v &gt; 1\)</span>. By definition, <span class="math inline">\(v\)</span> does not belong to the polar set of <span class="math inline">\(\bar P\)</span>.</li>
</ol>
<p><em>Proof of the Theorem:</em> We assume that <span class="math inline">\(P\)</span> contains the point <span class="math inline">\(0\)</span>. Otherwise, we perform a translation <span class="math inline">\(R^n \rightarrow R^n: f(x) = x + t, \forall x \in R^n\)</span>, such that <span class="math inline">\(0 \in f(P)\)</span>. If we can show <span class="math inline">\(f(P)\)</span> is a polyhedron, then <span class="math inline">\(f(P) = \{f(x) : A f(x) \le b \}\)</span>, then we have <span class="math inline">\(P = \{ x : Ax \le b - At \}\)</span>.</p>
<p>Back to our proof. As <span class="math inline">\(P\)</span> is a polytope, then it can be written as <span class="math inline">\(P = \{ x: x = \sum_{i = 1}^k \mu_i v_i , \ s.t., \ \sum_{i =1}^k \mu_i = 1 \wedge \mu_i \in [0, 1] \quad \forall i \in [k] \}\)</span>. A point <span class="math inline">\(y\)</span> belongs to the polar set of <span class="math inline">\(P\)</span> if and only if <span class="math inline">\(y^T v_i \le 1, \forall i \in [k]\)</span>. Therefore, <span class="math inline">\(\bar P = \{y : y^T v_i \le 1, \forall i \in [k] \}\)</span>, which is a polyhedron.</p>
<p>Next, we show that <span class="math inline">\(\bar P\)</span> is bounded. This results from the assumption that <span class="math inline">\(0\)</span> is an interior point of <span class="math inline">\(P\)</span>. Therefore, <span class="math inline">\(\exists \epsilon &gt; 0\)</span>, such that <span class="math inline">\(x \in P, \forall x \in R^n, |x|_2 &lt; \epsilon\)</span>. For any <span class="math inline">\(y \in \bar P\)</span>, with proper scaling, we have <span class="math inline">\(\frac{\epsilon}{|y|_2} y \in P\)</span>. Which further implies that <span class="math inline">\(y^T \frac{\epsilon}{|y|} y = \epsilon |y|_2 \le 1\)</span>, and <span class="math inline">\(|y|_2 \le \frac{1}{\epsilon}\)</span>.</p>
<p>Now, <span class="math inline">\(\bar P\)</span> is a bounded polyhedron, hence a polytope. By previous lemma, <span class="math inline">\(P = \overline{\bar P}\)</span>, hence a polyhedron.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://people.orie.cornell.edu/dpw/orie6300/Lectures/lec03.pdf">David P. Williamson, ORIE 6300, Lecture 03, Polyhedra and polytopes.</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/12/02/Unique-Factorization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/02/Unique-Factorization/" class="post-title-link" itemprop="url">Unique Factorization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-02 21:23:35" itemprop="dateCreated datePublished" datetime="2019-12-02T21:23:35+11:00">2019-12-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-03 16:33:24" itemprop="dateModified" datetime="2019-12-03T16:33:24+11:00">2019-12-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>A prime number is an positive integer greater than 1 and whose divisor is 1 and itself. We claim that <span class="math display">\[
\forall n \in N^+, n &gt; 1,
\]</span> <span class="math inline">\(n\)</span> has a unique prime factorization, i.e., we can write <span class="math inline">\(n\)</span> as <span class="math display">\[
n = p_1 p_2 p_3 ... p_k
\]</span> where <span class="math inline">\(k \ge 1\)</span> and the <span class="math inline">\(p_i\)</span>'s are primes and <span class="math display">\[
p_1 \le p_2 \le p_3 \le ... \le p_k
\]</span> The claim uses Euclid's Lemma, which states:</p>
<ul>
<li>Lemma: If <span class="math inline">\(p\)</span> is a prime and <span class="math inline">\(p | ab\)</span>, then <span class="math inline">\(p | a\)</span> or <span class="math inline">\(p | b\)</span>.</li>
<li>Proof: if <span class="math inline">\(p\)</span> is prime, and <span class="math inline">\(p \nmid a\)</span>, then exists <span class="math inline">\(a^{-1} \in F_p\)</span>, s.t, <span class="math inline">\(a^{-1} a \equiv 1 \mod p\)</span>. It concludes that <span class="math inline">\(b \equiv a^{-1}a b \equiv 0 \mod p\)</span>. Similar results holds when <span class="math inline">\(p \nmid b\)</span>.</li>
<li>Corollary: if <span class="math inline">\(p | q_1 q_2 q_3 ... q_l\)</span>, then <span class="math inline">\(p | q_i\)</span> for some <span class="math inline">\(1 \le i \le l\)</span>.</li>
</ul>
<p>Now, suppose <span class="math inline">\(n\)</span> has another prime decomposition <span class="math display">\[
n = q_1 q_2 ... q_l
\]</span> Then by assumption, <span class="math inline">\(q_1 | q_i\)</span> for some <span class="math inline">\(1 \le i \le l\)</span>. But both <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_i\)</span> are prime. Hence <span class="math inline">\(q_1 = q_i\)</span>. By cancellation <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_i\)</span> and induct on the rest products, we get the desired result.</p>
<p><em>Corollary:</em> For integers <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>, if we write them as the prime decompositions: <span class="math display">\[
n = p_1^{d_1} p_2^{d_2} ... p_k^{d_k} \\
m = q_1^{f_1} q_2^{f_2} ... q_l^{f_l}
\]</span> such that <span class="math inline">\(p_i \neq p_j\)</span> for <span class="math inline">\(1 \le i &lt; j \le k\)</span> and <span class="math inline">\(q_i \neq q_j\)</span> for <span class="math inline">\(1 \le i &lt; j \le l\)</span>, and <span class="math display">\[
n \mid m
\]</span> Then for any <span class="math inline">\(1 \le i \le k\)</span>, <span class="math inline">\(\exists 1 \le j \le l\)</span>, such that <span class="math display">\[
p_i = q_j \\
d_i \le f_j
\]</span> <em>Proof:</em> By Euclid's Lemma, and proper cancelation, we have the desired result.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/07/Approximate-Membership-Problem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/07/Approximate-Membership-Problem/" class="post-title-link" itemprop="url">Approximate Membership Problem</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-07 16:14:36" itemprop="dateCreated datePublished" datetime="2019-11-07T16:14:36+11:00">2019-11-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 21:33:25" itemprop="dateModified" datetime="2020-04-06T21:33:25+10:00">2020-04-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the approximate membership problem, we have subset <span class="math inline">\(S \subset U\)</span>, that supports two operations:</p>
<ol type="1">
<li><em>update(<span class="math inline">\(x\)</span>)</em>: <span class="math inline">\(S \leftarrow S \cup \{x\}\)</span></li>
<li><em>query(<span class="math inline">\(x\)</span>)</em>: return True if <span class="math inline">\(x\in S\)</span> else False.</li>
</ol>
<p>where <span class="math inline">\(U\)</span> is the domain that elements <span class="math inline">\(x\)</span> belongs to. If <span class="math inline">\(U = 2^{32}\)</span>, then each representation of <span class="math inline">\(x\)</span> takes <span class="math inline">\(32\)</span> bits. If <span class="math inline">\(U = 2^{64}\)</span>, then each <span class="math inline">\(x\)</span> needs <span class="math inline">\(64\)</span> bits. If <span class="math inline">\(U\)</span> is the website address, the length of its elements might is unlimited. Suppose that the average length is about <span class="math inline">\(15\)</span> chars, and each char takes <span class="math inline">\(8\)</span> bits, then the representation could occupy most <span class="math inline">\(120\)</span> bits.</p>
<p>We can solve the membership problem exactly by dictionaries, such as balanced-binary search tree and hash set. This requires space <span class="math inline">\(O(n \log U)\)</span> bits. If we allows false positive, we can break this bound.</p>
<p>The idea is to have a bit array <span class="math inline">\(A\)</span> with length <span class="math inline">\(O(kn)\)</span> bits and a pairwise hash family <span class="math inline">\(H\)</span> that maps the elements in <span class="math inline">\(U\)</span> uniformly at random to each bit in the array. After picking a hash function uniformly at random from <span class="math inline">\(H\)</span>, the operation on <span class="math inline">\(S\)</span> is performed as follows:</p>
<ol type="1">
<li><em>update(<span class="math inline">\(x\)</span>)</em>: <span class="math inline">\(A[h(x)] \leftarrow 1\)</span></li>
<li><em>query(<span class="math inline">\(x\)</span>)</em>: return True if <span class="math inline">\(A[h(x)] = 1\)</span> else False.</li>
</ol>
<p>We can only make a mistake if an element <span class="math inline">\(y \notin S\)</span> and <span class="math inline">\(A[h(y)] = 1\)</span>. However, the probability of such event is bounded by <span class="math display">\[
\Pr[\exists x \in S, s.t., h(x) = h(y)] \le \sum_{x \in S} \Pr[h(x) = h(y)] = \frac{n}{m} = \frac{1}{k}
\]</span></p>
<p>Remark: the accurate probability is given by <span class="math display">\[
1 - \Pr[\forall x \in S, s.t., h(x) \neq h(y)] = 1 - (1 - \frac{1}{m})^n \le \frac{n}{m}
\]</span></p>
<p>To amplify the failure probability to a specified threshold <span class="math inline">\(\delta\)</span>, we can set <span class="math inline">\(k = \frac{1}{\delta}\)</span>. A more efficient way is by repetition the array <span class="math inline">\(\ln \frac{1}{\delta} /\ln k\)</span> times. Then the overall space usage is <span class="math display">\[
\frac{ n k \ln \frac{1}{\delta} }{ \ln k }
\]</span></p>
<p>Note that <span class="math inline">\(f(k) = \frac{k}{\ln k} \ge e\)</span>, since it holds that <a target="_blank" rel="noopener" href="https://www.google.com/search?q=y%3Dx-e*ln(x)"><span class="math inline">\(y = x - e\ln x\)</span></a> (click to see the plot) for <span class="math inline">\(x &gt; 0\)</span>: <span class="math display">\[
y&#39; = 1 - e / x = 0 \longrightarrow x = e.
\]</span></p>
<p>When <span class="math inline">\(k = e\)</span> the space usage is <span class="math display">\[
ne \ln \frac{1}{\delta}
\]</span></p>
<p>The compress ratio is given by <span class="math display">\[
\frac{\log U}{e \ln \frac{1}{\delta} }
\]</span></p>
<p>If we require the failure probability <span class="math inline">\(\delta = 1 / 100\)</span>, then <span class="math inline">\(\ln 1 / \delta \approx 4.6\)</span>, and <span class="math inline">\(e \ln 1/\delta \approx 12.5 \le 13\)</span>. That is, we need on <span class="math inline">\(13\)</span> bits for each element in <span class="math inline">\(S\)</span>, and the average number of operations for updating <span class="math inline">\(S\)</span> is <span class="math inline">\(5\)</span>, which is about <span class="math inline">\(1/2\)</span> space when <span class="math inline">\(U = 2^{32}\)</span>, <span class="math inline">\(1/5\)</span> space when <span class="math inline">\(U = 2^{64}\)</span>, and much less when <span class="math inline">\(U\)</span> is the set of possible web addresses.</p>
<h4 id="remark">Remark</h4>
<p>There is another implementation that use a single bit array <span class="math inline">\(A\)</span> of size <span class="math inline">\(m\)</span> but <span class="math inline">\(k\)</span> hash functions <span class="math inline">\(h_1, h_2, ..., h_k\)</span>. We initialization by setting</p>
<blockquote>
<p>For <span class="math inline">\(x \in S\)</span>:<br />
<span class="math inline">\(\qquad\)</span> set <span class="math inline">\(A[h_i(x)] = 1\)</span> for all <span class="math inline">\(i \in [k]\)</span>.</p>
</blockquote>
<p>As a result, at most <span class="math inline">\(kn\)</span> bits of <span class="math inline">\(A\)</span> are initialized to be 1.</p>
<p>Now, for an element <span class="math inline">\(x&#39; \notin S\)</span>, we accidentally report it as a member <span class="math inline">\(\in S\)</span> if <span class="math inline">\(h_i(x&#39;) = 1\)</span> for all <span class="math inline">\(i \in [k]\)</span>. But the probability that <span class="math display">\[
\Pr[h_i(x&#39;) = 1] \le \frac{kn}{m} 
\]</span></p>
<p>and <span class="math display">\[
\Pr[h_i(x&#39;) = 1, \forall i \in [k]] \le (\frac{kn}{m} )^k
\]</span></p>
<p>If we use <span class="math inline">\(k = \ln \frac{1}{\delta}\)</span> hash functions and set the size <span class="math inline">\(m = ne \ln \frac{1}{\delta}\)</span>, we can also achieve failure probability <span class="math inline">\(\delta\)</span>.</p>
<p>Question to ponder: what is <span class="math inline">\(S\)</span> is dynamic?</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/02/Frequency-Estimators/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/Frequency-Estimators/" class="post-title-link" itemprop="url">Frequency Estimators</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-02 20:47:52" itemprop="dateCreated datePublished" datetime="2019-11-02T20:47:52+11:00">2019-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-21 23:06:51" itemprop="dateModified" datetime="2021-01-21T23:06:51+11:00">2021-01-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The task of frequency estimation is to count the number of times each element appears in the multi-set <span class="math inline">\(S\)</span>. There are <span class="math inline">\(n\)</span> elements in <span class="math inline">\(S\)</span> and each element belongs to a finite domain <span class="math inline">\(U = [1, , 2, ..., u]\)</span>. If we use a balanced binary search, this can be done in <span class="math inline">\(O(n \log \min\{n, u\})\)</span> time and <span class="math inline">\(\Theta(\min\{n, u\})\)</span> space. If we use a hash table, this can be done in <span class="math inline">\(\Theta(\min\{n, u\})\)</span> time and space.</p>
<p>Here is the question: can we do it in sub-linear space?</p>
<p>If we are allowed to make some errors, this is possible. Let <span class="math inline">\(a = \left&lt; a_1, a_2, ..., a_{u} \right&gt;\)</span> be the frequency of elements in <span class="math inline">\(U\)</span> and <span class="math inline">\(b = \left&lt; b_1, ..., b_{u} \right&gt;\)</span> be our estimation. Our goal is to minimize the maximum possible error <span class="math inline">\(\max_i |b_i - a_i|\)</span>.</p>
<h1 id="count-min-sketch">Count-Min Sketch</h1>
<p>If we use array <span class="math inline">\(A\)</span> with length <span class="math inline">\(m &lt; \min\{n, u\}\)</span> to count the frequency of the elements, then some elements must share the same slot in the array. Now we apply the following strategy:</p>
<blockquote>
<ol type="1">
<li>Pick a perfect hash function <span class="math inline">\(h : U \rightarrow [m]\)</span> (this condition can be weaken, but for convenience of discussion, we assume perfectness here), that maps the elements uniformly to every bucket in the array.<br />
</li>
<li>For <span class="math inline">\(\forall s \in S\)</span>,<br />
<span class="math inline">\(\qquad A[h(s)] \leftarrow A[h(s)] + 1\)</span> .</li>
<li>Return <span class="math inline">\(b_i = A[h(i)]\)</span> as the estimation of <span class="math inline">\(a_i\)</span>.</li>
</ol>
</blockquote>
<p>Intuitively, each slot obtains <span class="math inline">\(\frac{1}{m}\)</span> fraction of the total mass <span class="math inline">\(\sum_i a_i = |a|\)</span> (where <span class="math inline">\(|\cdot |\)</span> denotes the <span class="math inline">\(l_1\)</span> norm). Conditioned on <span class="math inline">\(i\)</span>-th element is hashed to the <span class="math inline">\(h(i)\)</span> slots, <span class="math inline">\(\frac{1}{m}\)</span> fraction of the rest mass <span class="math inline">\(\sum_{j \neq i} a_j = n - a_i\)</span> is hashed to the <span class="math inline">\(h(i)\)</span>-th slot. Therefore, in expectation, <span class="math display">\[
0 \le \mathbb{E}[b_i - a_i] \le \frac{|a|}{m}
\]</span></p>
<p>An interesting property is that, if some <span class="math inline">\(a_j\)</span> (<span class="math inline">\(j \neq i\)</span>) (or a small subset of <span class="math inline">\(a_j\)</span>'s) constitutes, say <span class="math inline">\(0.99\)</span> percent of the mass <span class="math inline">\(|a|\)</span>, as long as it is not hashed to <span class="math inline">\(h(i)\)</span>, then the expected error is less than <span class="math inline">\(0.01 \frac{|a|}{m}\)</span>.</p>
<p>Formally, define the binary random variables <span class="math inline">\(X_j\)</span> for <span class="math inline">\(j \neq i\)</span> such that <span class="math display">\[
X_j = \begin{cases}
    a_j     &amp;\quad \text{if} \quad h(i) = h(j)\\
    0       &amp;\quad \text{otherwise}
\end{cases}
\]</span> Then by perfectness of <span class="math inline">\(h\)</span>, <span class="math inline">\(\Pr[X_j = a_j] = \frac{1}{m}\)</span> and <span class="math inline">\(\mathbb{E}[X_j] = \frac{a_j}{m}\)</span>. (Indeed, pairwise independence of <span class="math inline">\(h\)</span> suffices to guarantee this condition).</p>
<p>Then <span class="math display">\[
    \mathbb{E} [b_i - a_i] = \mathbb{E} \left[ \sum_{j \neq i} X_j \right] = \sum_{j \neq i} \mathbb{E}[X_j] \le \frac{|a|}{m}
\]</span></p>
<p>As <span class="math inline">\(b_i - a_i\)</span> is non-negative, by Markov inequality, <span class="math display">\[
\Pr \left[ b_i - a_i \ge \epsilon |a| \right] \le \frac{\mathbb{E}[b_i - a_i]}{\epsilon |a|}  \le \frac{1}{\epsilon m}
\]</span> If we would like to achieve failure probability <span class="math inline">\(\delta\)</span>, we can set <span class="math inline">\(m = \epsilon^{-1} \delta^{-1}\)</span>.</p>
<p>Question to ponder: can we improve this?</p>
<p>Yes, by repetition. Instead of using just one array, we set <span class="math inline">\(d\)</span> arrays (to be set latter) and choose <span class="math inline">\(d\)</span> hash functions independently. Then the probability that all estimations deviate more than <span class="math inline">\(\epsilon |a|\)</span> is at most <span class="math display">\[
\left( \frac{1}{\epsilon m} \right)^d
\]</span></p>
<p>When <span class="math inline">\(d = \ln{\delta} / \ln \frac{1}{\epsilon m} = \ln{ \frac{1}{\delta} } / \ln \epsilon m\)</span>, the failure probability decreases to <span class="math inline">\(\delta\)</span>. To minimize the space usage, we solve the following optimization problem: <span class="math display">\[
\begin{aligned}
    &amp;\min           &amp;\ln \frac{1} {\delta} \frac{m}{\ln m + \ln \epsilon} \\
    &amp;\text{s.t. }   &amp;m \ge \frac{1}{\epsilon}
\end{aligned}
\]</span></p>
<p>Solving the equation gives <span class="math inline">\(m = e \epsilon^{-1}\)</span>. The space consumption is <span class="math inline">\(\frac{e}{\epsilon} \ln \frac{1}{\delta}\)</span>.</p>
<p>Note: suppose that <span class="math inline">\(|a| = 10000\)</span> and <span class="math inline">\(\epsilon = 0.1\)</span>, <span class="math inline">\(\delta = 0.01\)</span>, then with <span class="math inline">\(10 \cdot e \ln 100 \approx 125\)</span> words, we can estimate a particular <span class="math inline">\(a_i\)</span> with the absolute error less than <span class="math inline">\(1000\)</span>. If <span class="math inline">\(a_i = 9000\)</span>, the relative error is small. However, if <span class="math inline">\(a_i = 1\)</span>, basically our estimation if rather inaccurate.</p>
<p>Question to ponder: what if we analyze this by Chernoff style inequality?</p>
<p>If we make <span class="math inline">\(d\)</span> repetition, then the confidence interval of the mean estimation is given by (Hoeffding's inequality) <span class="math display">\[
|a|\sqrt{\frac{\log \frac{2}{\delta} }{2 d} }
\]</span> If we want this interval to be smaller than <span class="math inline">\(\epsilon |a|\)</span>, then we need <span class="math inline">\(d = \frac{ \log \frac{2}{\delta} }{2 \epsilon^2}\)</span>, which is a <span class="math inline">\(\frac{1}{\epsilon }\)</span> worse than the above analysis. Intuitively, we require stronger condition for concentration of mean value than that for existence of near-min value.</p>
<h1 id="count-sketch">Count-Sketch</h1>
<p>One drawback of Count-Min Sketch is that the estimator constructed is one-sided and not unbiased. This can be fixed by introducing an additional function <span class="math inline">\(g : U \rightarrow \{ - 1, 1 \}\)</span> that gives each elements in <span class="math inline">\(U\)</span> a sign ("+" or "-") independently and uniformly at random. Now,</p>
<ol start="2" type="1">
<li>For each element <span class="math inline">\(s \in S\)</span>, we increase the counter <span class="math inline">\(h(s)\)</span> in the array by 1 if <span class="math inline">\(g(s) &gt; 0\)</span> or <span class="math inline">\(-1\)</span> if <span class="math inline">\(g(s) &lt; 0\)</span>.</li>
<li>Finally, we set <span class="math inline">\(b_i = A[h(i)] \cdot g(i)\)</span> as the estimation of <span class="math inline">\(a_i\)</span>.</li>
</ol>
<p>In expectation, each slot obtains <span class="math inline">\(\frac{1}{m}\)</span> fraction of the total mass <span class="math inline">\(\sum_i a_i = |a|\)</span>. But as each element "flips" it sign randomly, the expected value obtained is 0. Now, conditioned on <span class="math inline">\(i\)</span>-th element is hashed to the <span class="math inline">\(h(i)\)</span> slots, <span class="math inline">\(\frac{1}{m}\)</span> fraction of the rest mass <span class="math inline">\(\sum_{j \neq i} a_j = n - a_i\)</span> is hashed to the <span class="math inline">\(h(i)\)</span>-th slot. With their signs flipped randomly, they contribute 0 to the <span class="math inline">\(h(i)\)</span>-th slot. Hence, <span class="math display">\[
\mathbb{E}[b_i - a_i] = 0
\]</span> Formally, define the binary random variables <span class="math inline">\(X_j\)</span> for <span class="math inline">\(j \neq i\)</span> such that <span class="math display">\[
X_j = \begin{cases}
    a_j     &amp;\text{if} \quad h(i) = h(j) \wedge g(j) = g(i) \\
    - a_j   &amp;\text{if} \quad h(i) = h(j) \wedge g(j) \neq g(i) \\
    0       &amp;\text{otherwise}
\end{cases}
\]</span> Then by perfectness and independence of <span class="math inline">\(h\)</span> and <span class="math inline">\(g\)</span>,</p>
<p><span class="math display">\[
    \Pr[X_j = a_j] = \frac{1}{2m} \\
    \Pr[X_j = -a_j] = \frac{1}{2m} \\
    \Pr[X_j = 0] = 1 - \frac{1}{m}
\]</span></p>
<p>and <span class="math display">\[
\begin{aligned}
    \mathbb{E}[X_j] 
    &amp;= 0 \\
    \mathbb{E}[b_i - a_i] 
    &amp;= \mathbb{E}[\sum_{j \neq i} X_j] 
    = \sum_{j \neq i} \mathbb{E}[X_j] 
    = 0 \\
    \mathbb{Var} \left[ X_j \right] 
    &amp;= \mathbb{E} [X_j^2] - (\mathbb{E}[X_j])^2 
    = \frac{a_j^2}{m} \\
    \mathbb{Var} \left[ \sum_{j \neq i} X_j \right] 
    &amp;= \sum_{j \neq i} \mathbb{Var} [X_j] 
    = \sum_{j \neq i} \frac{a_j^2}{m} 
    \le \frac{|a|_2^2}{m}
    \end{aligned}
\]</span></p>
<p>where <span class="math inline">\(|a|_2^2\)</span> is the <span class="math inline">\(l_2\)</span> norm of <span class="math inline">\(a\)</span>. Note that the above analysis requires only pair independence. The random variable <span class="math inline">\(X_j\)</span> is two sided, therefore we can not use Markov Inequality for analysis.</p>
<p>Question to ponder: is it possible to analyze this directly by Chernoff style inequality?</p>
<p>The tool we resort to is Chebyshev's inequality: <span class="math display">\[
\Pr \left[ |\sum_{j \neq i} X_j - 0| \ge \epsilon |a|_2 \right] \le \frac{\mathbb{Var} [\sum_{j \neq i} X_j]}{\epsilon^2 |a|_2^2 } \le \frac{1}{m\epsilon^2 }
\]</span> we take <span class="math inline">\(m = k\epsilon^{-2}\)</span> for some integer <span class="math inline">\(k&gt; 2\)</span>.</p>
<p>How do we amplify the successful probability? We repeat the experiment <span class="math inline">\(d\)</span> times, and take the median. The only way that the median makes a mistake is when more than half of the estimations deviate more <span class="math inline">\(\epsilon |a|_2\)</span>. Denote <span class="math inline">\(Y\)</span> the number of estimation such that the error is more than <span class="math inline">\(\epsilon |a|_2\)</span>. By Chernoff, <span class="math display">\[
\begin{aligned}
    \Pr \left[ Y - \frac{d}{k}\ge \frac{d}{2} - \frac{d}{k} \right] 
    &amp;= \Pr \left[ Y - \frac{d}{k}\ge (\frac{1}{2} - \frac{1}{k})\cdot k \cdot \frac{d}{k} \right] \\
    &amp;\le \exp \big( -\frac{d}{3 k} (\frac{k}{2} - 1)^2  \big) \\
    &amp;= \delta 
\end{aligned}
\]</span></p>
<p>which solves to <span class="math display">\[
d= \frac{3k}{(k/2  -1)^2} \log \frac{1}{\delta}
\]</span> The overall space usage is therefore <span class="math display">\[
md = \frac{3k^2}{\epsilon^2 (k/2  -1)^2} \log \frac{1}{\delta} 
= \frac{3}{\epsilon^2 (1/2  - \frac{1}{k} )^2} \log \frac{1}{\delta}
\]</span> Question to ponder: the expression is wired, which implies that <span class="math inline">\(k \rightarrow \infty\)</span>, the space usage converges to <span class="math inline">\(\frac{12}{\epsilon^2} \log \frac{1}{\delta}\)</span>, which is worse than count-min hash than a <span class="math inline">\(\frac{1}{\epsilon}\)</span> factor.</p>
<h4 id="relation-between-a_1-and-a_2.">Relation between <span class="math inline">\(|a|_1\)</span> and <span class="math inline">\(|a|_2\)</span>.</h4>
<p>Their relation is given as follows (by concavity of the <span class="math inline">\(\sqrt \cdot\)</span> function) <span class="math display">\[
|a|_2 \le |a|_1 \le \sqrt{ |u| } |a|_2
\]</span> In particular, when <span class="math inline">\(|u| = 2\)</span>, the inequality becomes <span class="math display">\[
|a_1 + a_2| \le \sqrt 2 \sqrt{a_1^2 + a_2^2 }
\]</span> When the dimension is higher than <span class="math inline">\(2\)</span>, the factor in the RHS can not bounded to <span class="math inline">\(\sqrt 2\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/02/AM-GM-Inequality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/AM-GM-Inequality/" class="post-title-link" itemprop="url">AM-GM Inequality</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-02 10:48:17" itemprop="dateCreated datePublished" datetime="2019-11-02T10:48:17+11:00">2019-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-03 10:18:10" itemprop="dateModified" datetime="2019-12-03T10:18:10+11:00">2019-12-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The AM-GM inequality states that for <span class="math inline">\(x_1, x_2, ..., x_n &gt;0\)</span> and <span class="math inline">\(\lambda_1, \lambda_2, ..., \lambda_n \ge 0\)</span>, such that <span class="math inline">\(\sum_{i = 1}^n \lambda_i = 1\)</span>, we have <span class="math display">\[
\prod_{i} x_i^{\lambda_i} \le \sum_i \lambda_i x_i 
\]</span> If particular, when <span class="math inline">\(\lambda_i = \frac{1}{n}\)</span>, the inequality becomes <span class="math display">\[
 \sqrt[^n]{\prod_i x_i} \le \frac{\sum_i x_i}{n}
\]</span> Replaying <span class="math inline">\(x_i = \frac{1}{y_i}\)</span>, we obtain <span class="math display">\[
 \frac{n}{\sum_i \frac{1}{y_i}} \le \sqrt[^n]{\prod_i y_i} 
\]</span> Another famous case is <span class="math inline">\(n = 2\)</span> and <span class="math inline">\(\lambda_1 = \frac{1}{p}\)</span>, <span class="math inline">\(\lambda_2 = \frac{1}{q}\)</span> and <span class="math inline">\(x_1 = x^p\)</span>, <span class="math inline">\(x_2 = y^q\)</span>, we get the Young's inequality <span class="math display">\[
xy = (x^p)^\frac{1}{p} (y^q)^\frac{1}{q} \le \frac{x^p}{p} + \frac{y^q}{q}
\]</span> The proof of AM-GM inequality is rather simple and follows immediately from the concavity of <span class="math inline">\(\log\)</span> function: <span class="math display">\[
\sum_i \lambda_i \log x_i \le \log \sum_i \lambda_i x_i
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/31/Polynomials/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/31/Polynomials/" class="post-title-link" itemprop="url">Polynomials</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-31 21:38:36" itemprop="dateCreated datePublished" datetime="2019-10-31T21:38:36+11:00">2019-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-28 10:47:03" itemprop="dateModified" datetime="2020-11-28T10:47:03+11:00">2020-11-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Polynomials constitute of a rich class of functions. A polynomial in a single variable defined on a field <span class="math inline">\(F\)</span> is a function of the form: <span class="math display">\[
p(x) = a_d x^d + a_{d - 1} x^{d - 1} + ... + a_1 x + a_0
\]</span> where the variable <span class="math inline">\(x\)</span> and coefficients <span class="math inline">\(a_i\)</span> belongs to <span class="math inline">\(F\)</span>. The integer <span class="math inline">\(d\)</span> is called the degree of <span class="math inline">\(p(x)\)</span>.</p>
<p>Polynomials have two fundamental properties:</p>
<ol type="1">
<li>A non-zero polynomial of degree <span class="math inline">\(d\)</span> has at most <span class="math inline">\(d\)</span> distinct roots.</li>
<li>Given <span class="math inline">\(d + 1\)</span> pairs <span class="math inline">\((x_i, y_i)\)</span> (<span class="math inline">\(1 \le i \le d + 1\)</span>), such that <span class="math inline">\(x_i \neq x_j\)</span> for <span class="math inline">\(i \neq j\)</span>, there is a unique polynomial <span class="math inline">\(p(x)\)</span> with degree at most <span class="math inline">\(d\)</span> that goes through all these points.</li>
</ol>
<p>Let's prove claim 2 first. If claim 1 holds, <em>existence</em> implies <em>uniqueness</em> for claim 2. Suppose for contradiction that there is another polynomial <span class="math inline">\(q(x)\)</span> such that <span class="math inline">\(p(x) = q(x)\)</span> for all <span class="math inline">\((x_i, y_i)\)</span> pairs. Now consider the polynomial <span class="math inline">\(r(x) = p(x) - q(x)\)</span>. If <span class="math inline">\(r(x) \equiv 0\)</span>, then the claim holds trivially. Otherwise, <span class="math inline">\(r(x)\)</span> is a non-zero polynomial with degree at most <span class="math inline">\(d\)</span>. Claim 1 asserts that <span class="math inline">\(r(x)\)</span> has at most <span class="math inline">\(d\)</span> distinct roots, contradicting that <span class="math inline">\(r(x_i) = 0\)</span> for <span class="math inline">\(1 \le i \le d + 1\)</span>.</p>
<p>Question to ponder: does claim 2 implies claim 1?</p>
<h3 id="property-2">Property 2</h3>
<p>Now we return to the construction of a feasible for claim 2. The method is called Lagrange Interpolation.</p>
<p>First, we would like to construct polynomial <span class="math inline">\(\delta_i(x)\)</span>, such that</p>
<ol type="1">
<li>It has degree <span class="math inline">\(d\)</span>.</li>
<li><span class="math inline">\(\delta_i(x_j) = 0\)</span> for <span class="math inline">\(j \neq i\)</span>.</li>
<li><span class="math inline">\(\delta_i(x_i) = 1\)</span></li>
</ol>
<p>As it takes zero on <span class="math inline">\(x_j, \forall j \neq i\)</span>, we can simply try a prototype <span class="math inline">\(\delta_i&#39;\)</span> as: <span class="math display">\[
\delta_i&#39;(x) = \prod_{j \neq i} (x - x_j).
\]</span></p>
<p>Observe that it has degree <span class="math inline">\(d\)</span>. However, at point <span class="math inline">\(x_i\)</span>, <span class="math inline">\(\delta_i&#39;(x_i)\)</span> may not equal to 1. This issue is not hard to fix, we can just normalize it by <span class="math inline">\(\delta_i&#39;(x_i)\)</span>, and define</p>
<p><span class="math display">\[
\delta_i(x) = \frac{\delta_i&#39;(x)}{\delta_i&#39;(x_i)} = \frac{\prod_{j \neq i} (x - x_j) }{ \prod_{j \neq i} (x_i - x_j) } = \prod_{1 \le j \le d + 1, j \neq i} \frac{x - x_j}{x_i - x_j}. 
\]</span></p>
<p>With proper scaling and summation, <span class="math inline">\(p(x)\)</span> is given by <span class="math display">\[
p(x) = \sum_{i = 1}^{d + 1} y_i \delta_i(x)
\]</span></p>
<p><em>Remark</em>: uniqueness can also be proved by showing the following equation has unique solution: <span class="math display">\[
\left[\begin{matrix}
x_1^d &amp; x_1^{d - 1} &amp; ... &amp;  x_1 &amp; 1\\
x_2^d &amp; x_2^{d - 1} &amp; ... &amp;  x_2 &amp; 1\\
...\\
x_{d + 1}^d &amp; x_{d + 1}^{d - 1} &amp; ... &amp;  x_{d + 1} &amp; 1
\end{matrix}\right] 
\left[\begin{matrix}
a_d\\
a_{d - 1} \\
...\\
a_0
\end{matrix}\right] =
\left[\begin{matrix}
y_1 \\
y_{1} \\
...\\
y_{d + 1}
\end{matrix}\right] \\
\]</span> The left matrix is called Vandermonde Matrix, whose determinant is <span class="math display">\[
\begin{aligned}
&amp;\det 
\left[\begin{matrix}
x_1^d &amp; x_1^{d - 1} &amp; ... &amp;  x_1 &amp; 1\\
x_2^d &amp; x_2^{d - 1} &amp; ... &amp;  x_2 &amp; 1\\
...\\
x_{d + 1}^d &amp; x_{d + 1}^{d - 1} &amp; ... &amp;  x_{d + 1} &amp; 1
\end{matrix}\right]  \\
=&amp;
\det 
\left[\begin{matrix}
0&amp; 0 &amp; ... &amp;  0 &amp; 1\\
x_2^d - x_1 x_2^{d - 1} &amp; x_2^{d - 1} - x_1 x_2^{d - 2}&amp; ... &amp;  x_2 - x_1 &amp; 1\\
...\\
x_{d + 1}^d - x_1 x_{d + 1}^{d - 1} &amp;  x_{d + 1}^{d - 1} - x_1 x_{d + 1}^{d - 2}&amp; ... &amp;  x_{d + 1} - x_ 1&amp; 1
\end{matrix}\right]  \\
=&amp;\prod_{i = 1}^{d + 1} (x_i - x_1) \det 
\left[\begin{matrix}
x_2^d &amp; x_2^{d - 1} &amp; ... &amp;  x_2 &amp; 1\\
...\\
x_{d + 1}^d &amp; x_{d + 1}^{d - 1} &amp; ... &amp;  x_{d + 1} &amp; 1
\end{matrix}\right]  \\
 ...\\ 
=&amp; \prod_{i &lt; j} (x_j - x_i)
\end{aligned}
\]</span> which is non-zero when <span class="math inline">\(x_i \neq x_j\)</span>.</p>
<h3 id="property-1">Property 1</h3>
<p>Finally, we prove claim 1: 1. A non-zero polynomial of degree <span class="math inline">\(d\)</span> has at most <span class="math inline">\(d\)</span> distinct roots.</p>
<p>The prove is by induction.</p>
<ol type="1">
<li><p>The base case is <span class="math inline">\(d = 0\)</span>. As the polynomial is non-zero, it does not have any root.</p></li>
<li><p>Suppose the claim holds for <span class="math inline">\(d\)</span>. Then the claim holds for <span class="math inline">\(d + 1\)</span>. If <span class="math inline">\(p(x)\)</span> has a root <span class="math inline">\(a_{d + 1}\)</span>, by polynomial division, we can rewrite <span class="math inline">\(p(x)\)</span> as <span class="math display">\[
 p(x) = (x - a_{d + 1}) q(x)
 \]</span> and <span class="math inline">\(q(x)\)</span> has degree <span class="math inline">\(d\)</span>. If for any <span class="math inline">\(a \neq a_{d + 1}\)</span>, <span class="math inline">\(p(a) = (a - a_{d +1}) q(a) = 0\)</span>, then <span class="math inline">\(q(a) = 0\)</span>, which implies that <span class="math inline">\(a\)</span> is a root of <span class="math inline">\(q(x)\)</span>. By applying the inductive hypothesis to <span class="math inline">\(q(x)\)</span>, we see that it has at most <span class="math inline">\(d\)</span> distinct roots.</p></li>
</ol>
<h3 id="application">Application</h3>
<h4 id="pairwise-independent-hash-function.">Pairwise Independent Hash Function.</h4>
<p>Consider a set of elements <span class="math inline">\(\{0, 1, ..., u - 1\}\)</span> and the family of functions <span class="math inline">\(\mathcal{H}\)</span> defined on <span class="math inline">\([u - 1]\)</span>. Given <span class="math inline">\(i \in [u - 1]\)</span>, if we take a function <span class="math inline">\(h\)</span> uniformly at random from <span class="math inline">\(\mathcal{H}\)</span>, the value <span class="math inline">\(Y_i = h(i)\)</span> is a random variable. The <span class="math inline">\(Y_i\)</span>'s are called pairwise independent, if for <span class="math inline">\(i, j \in [u - 1], i \neq j\)</span> and <span class="math inline">\(y_i, y_j \in \cup_{h \in \mathcal{H}} h([u - 1])\)</span>, <span class="math display">\[
\Pr_{h \in \mathcal{H} } [Y_i = y_i, Y_j = y_j] = \Pr_{h \in \mathcal{H} } [Y_i = y_i] \Pr_{h \in \mathcal{H} } [Y_j = y_j]
\]</span></p>
<p>Similarly we can define <span class="math inline">\(k\)</span>-wise independence. For any <span class="math inline">\(0 \le i_1 &lt; i_2 &lt;... &lt; i_k &lt; u\)</span>, and <span class="math inline">\(y_{i_1} , y_{i_2}, ..., y_{i_k} \in \cup_{h \in \mathcal{H}} h([u - 1])\)</span>, we have <span class="math display">\[
\Pr_{h \in \mathcal{H} } [ Y_{i_1} = y_{i_1}, ... Y_{i_k} = y_{i_k} ] = \prod_{j} \Pr_{h \in \mathcal{H} } [ Y_{i_j} = y_{i_j}]
\]</span></p>
<p>Now we restrict our discussion to the finite field <span class="math inline">\(F_p = [p - 1] = \{0, 1, ..., p - 1\}\)</span>, where <span class="math inline">\(p\)</span> is a prime larger than <span class="math inline">\(u\)</span>.</p>
<p>Claim: The set of all possible functions from <span class="math inline">\([u - 1]\)</span> to <span class="math inline">\([p - 1]\)</span> is pairwise independent.</p>
<p>Pf: <span class="math display">\[
\begin{aligned}
&amp;\Pr[Y_i = a, Y_j = b] \\
= &amp;\frac{p^{u - 2}}{p^u} \\
= &amp;\frac{1}{p^2} \\
= &amp;\Pr[Y_i = a] \cdot \Pr[Y_j = b] 
\end{aligned}
\]</span></p>
<p>However, each of the functions takes <span class="math inline">\(u \log p\)</span> bits to represent. To save to space of representing the functions, we restrict to a smaller subset all possible functions: <span class="math display">\[
\mathcal{H_1} = \{ p(x) = a_1 x + a_0 \mid a_1, a_0 \in [p - 1] \}
\]</span></p>
<p>That is, all polynomials with degree at most 1 in the field <span class="math inline">\([p - 1]\)</span>. By claim 2, we conclude that for any pair <span class="math inline">\((x_1, y_1), (x_2, y_2)\)</span>, where <span class="math inline">\(x_1, x_2 \in [u - 1], x_1 \neq x_2\)</span> and <span class="math inline">\(y_1, y_2 \in [p - 1]\)</span>, there is a unique polynomial with degree at most 1 that goes through these two points. If we pick a polynomial uniformly at random from <span class="math inline">\(\mathcal{H}\)</span>, then the probability is <span class="math display">\[
\frac{1}{p^2}
\]</span></p>
<p>This already implies pairwise independence, as <span class="math display">\[
\Pr[h(x_1) = y_1 ] = \sum_{y_2 = 0}^{p - 1} \Pr[h(x_1) = y_1, h(x_2) = y_2] = \frac{1}{p}
\]</span></p>
<p>In a similar manner, we can extend the idea to <span class="math inline">\(k\)</span>-wise independent functions: <span class="math display">\[
\mathcal{H_{k - 1}} = \{ p(x) = a_{k - 1} x^{k - 1} + a_{k - 2} x^{k - 2} + ... + a_1 x_1 + a_0 \mid a_i \in [p - 1] \}
\]</span></p>
<p><span class="math inline">\(|\mathcal{H}| = p^{k}\)</span> and there is a unique polynomial that goes through any pairs <span class="math inline">\((x_1, y_1), (x_2, y_2), ..., (x_k, y_k)\)</span> for <span class="math inline">\(x_1 \neq x_2 \neq ...\neq x_k\)</span>.</p>
<p>Another application is key-sharing. Suppose that we have <span class="math inline">\(n\)</span>-candidates who knows some information about the key and we require that only when no less than <span class="math inline">\(k\)</span> of them aggragate their information can they figure the actual key. No group of less than <span class="math inline">\(k\)</span> candidates can achieve this. The way we do it is as follows:</p>
<ol type="1">
<li>Select a large prime number <span class="math inline">\(p\)</span><br />
</li>
<li>Select <span class="math inline">\(k+1\)</span> random numbers <span class="math inline">\(y_0, y_1, ..., y_k\)</span> from <span class="math inline">\([p-1]\)</span>.<br />
</li>
<li>Generate the unique polynomial <span class="math inline">\(p(x)\)</span> with degree <span class="math inline">\(k\)</span> that goes through <span class="math inline">\((0, y_0), (1, y_1), ..., (k, y_k)\)</span>.<br />
</li>
<li>Inform the value of <span class="math inline">\(p(1)\)</span> to the first person, <span class="math inline">\(p(2)\)</span> to the second one, ... and <span class="math inline">\(p(n)\)</span> to the <span class="math inline">\(n\)</span>-th person.</li>
</ol>
<p>If <span class="math inline">\(k - 1\)</span> people pool their keys together, then there are <span class="math inline">\(p\)</span> possible polynomials that goes through all their keys, each has different value at point 0. The probability of guessing the correct key is given by <span class="math inline">\(\frac{1}{p}\)</span>.</p>
<h3 id="reference">Reference:</h3>
<p>CS 70. Discrete Mathematics and Probability Theory.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description">你生之前悠悠千載已逝，未來還會有千年沉寂的期待</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
