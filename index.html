<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/12/Fourier-Transformation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/12/Fourier-Transformation/" class="post-title-link" itemprop="url">Fourier Transformation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-12 16:21:21" itemprop="dateCreated datePublished" datetime="2021-01-12T16:21:21+11:00">2021-01-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-26 23:51:21" itemprop="dateModified" datetime="2021-01-26T23:51:21+11:00">2021-01-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(f: [-\pi, \pi]: \rightarrow \mathbb{R}\)</span> be a real function. As <span class="math inline">\(f\)</span> itself can be complicated, we want to decompose it into a linear combination of simpler functions.</p>
<blockquote>
<p><strong>Example.</strong> A typical family of simple functions are polynomials: <span class="math display">\[
\{ 1, t, t^2, t^3, ... \}.
\]</span> Under proper condition, e.g., if <span class="math inline">\(f\)</span> is infinitely differentiable, it can be indeed decomposed into a linear combination of polynomials, such as: <span class="math display">\[
f = f(0) + f&#39;(0)  t + \frac{f&#39;&#39;(t)}{2!} t^2 + ...
\]</span></p>
</blockquote>
<h2 id="problem"><strong>Problem</strong></h2>
<p>Here we study the problem of approximating <span class="math inline">\(f\)</span> with trigonometric functions: <span class="math display">\[
    \mathfrak{T} = \{ 1, \sin t, \cos t, \sin 2t, \cos 2t, .... \}.
\]</span></p>
<p>Although <span class="math inline">\(f: [-\pi, \pi]: \rightarrow \mathbb{R}\)</span> is a function with output on <span class="math inline">\(\mathbb{R}\)</span>, we view it as a function <span class="math inline">\([-\pi, \pi]: \rightarrow \mathbb{C}\)</span> to take advantage of complex exponential. Now, we approximate <span class="math inline">\(f\)</span> with a set of sinusoids: <span class="math display">\[
   \Gamma = \{ 1, \exp(it), \exp(-it), \exp(i2t), \exp(-i2t), \exp(i3t), \exp(-i3t)... \}.
\]</span></p>
<p>Observe that <span class="math inline">\(\Gamma\)</span> generalizes <span class="math inline">\(\mathfrak{T}\)</span>, as for <span class="math inline">\(\forall k \in \mathbb{N}\)</span>, <span class="math display">\[
2 \cos kt =  \exp(ikt) + \exp(-ikt), \\
2 \sin kt =  \exp(ikt) - \exp(-ikt). 
\]</span></p>
<p>Define the span <span class="math inline">\(S(\Gamma)\)</span> as the set of all possible linear combinations of functions in <span class="math inline">\(\Gamma\)</span>: <span class="math display">\[
S(\Gamma) \doteq \left\{ \sum_{k \in \mathbb{Z} } c_k \exp(ikt) : \forall k \in \mathbb{Z}, c_k \in \mathbb{C} \right\}.
\]</span></p>
<p><strong><em>The goal is to find the closest function to <span class="math inline">\(f\)</span> in <span class="math inline">\(S(\Gamma)\)</span></em></strong>.</p>
<p>The key issues with the approximation are</p>
<ol type="1">
<li><p>How do we measure the closeness between two functions?</p></li>
<li><p>Under what condition, can <span class="math inline">\(f\)</span> be rewrite as a linear combination of functions in <span class="math inline">\(\Gamma\)</span>?</p></li>
<li><p>What is the approximation error, between <span class="math inline">\(f\)</span> and the linear combination?</p></li>
</ol>
<h2 id="norm"><strong>Norm</strong></h2>
<p>Despite the goal stated above, the closeness between <span class="math inline">\(f\)</span> and a function in <span class="math inline">\(S(\Gamma)\)</span> has not yet been defined. This raises the question of measuring the distance between two functions in <span class="math inline">\([-\pi, \pi] \rightarrow \mathcal{C}\)</span>.</p>
<p>In <span class="math inline">\(\mathbb{R}^n\)</span>, the Euclidean distance between two vectors <span class="math inline">\(u, v\)</span> is defined as <span class="math display">\[
    || u - v || = \sqrt{\sum_{i = 1}^n (u_i - v_i)^2 }.
\]</span></p>
<p>If we define <span class="math inline">\(w = u - v\)</span>, then the problem of measuring the distance between two vectors reduces to the one of measuring the length of a vector <span class="math display">\[
|| u - v || = || w || = \sqrt{\sum_{i = 1}^n w_i^2 }.
\]</span></p>
<!-- We call the function $||\cdot || : \mathbb{R}^n \rightarrow \mathbb{R}$ as norm and it satisfies the following three properties:

1. Positive-definite: $||w|| = 0 \rightarrow w = 0, \forall w \in \mathbb{R}^n$;
2. Absolutely homogeneous: $|| a w|| = |a| \cdot ||w||, \forall a \in \mathbb{R}, \forall w \in \mathbb{R}^n$; 
3. Triangle inequality: $||w + w'|| \le ||w|| + ||w'||, \forall w, w' \in \mathbb{R}^n$. -->
<p>In a similar manner, if we view a function <span class="math inline">\(h: [-\pi, \pi] \rightarrow \mathcal{C}\)</span> as an (uncountable) infinite dimension vector, it is natural to defined its length <span class="math display">\[
    || h || \doteq \sqrt{ \int_{ [-\pi, \pi] }   ||h(t) ||^2 } = \sqrt{ \int_{ [-\pi, \pi] }   \overline{h(t) } \cdot h(t)  }.
\]</span></p>
<blockquote>
<p><em>Remark:</em> 1. <em><span class="math inline">\(|| h ||^2\)</span> needs to be integrable.</em><br />
2. <em>Verify that <span class="math inline">\(|| \cdot ||\)</span> is a norm.</em></p>
</blockquote>
<p>As <span class="math inline">\(f - g\)</span> is also a function in <span class="math inline">\([-\pi, \pi] \rightarrow \mathcal{C}\)</span>, the distance between two functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> is given by <span class="math display">\[
\begin{aligned}
    ||f - g|| 
        &amp;= \sqrt{ \int_{ [-\pi, \pi] }   ||f(t) - g(t)||^2  } \\
        &amp;= \sqrt{ \int_{ [-\pi, \pi] }   \overline{ (f(t) - g(t) ) } \cdot (f(t) - g(t) )  } \\
        &amp;= \sqrt{ \int_{ [-\pi, \pi] }   \overline{ f(t) } \cdot f(t) + \int_{ [-\pi, \pi] }  \overline{ g(t) } \cdot g(t) - \int_{ [-\pi, \pi] }  \overline{ f(t) } \cdot g(t) - \int_{ [-\pi, \pi] }  \overline{ g(t) } \cdot f(t) \   } \\
        &amp;= \sqrt{ ||f||^2 + || g ||^2 - \int_{ [-\pi, \pi] }  \overline{ f(t) } \cdot g(t) - \int_{ [-\pi, \pi] }  \overline{ g(t) } \cdot f(t) \   }. \\
\end{aligned}
\]</span></p>
<blockquote>
<p><em>Remark:</em> 1. <span class="math inline">\(|| f - g ||^2\)</span> needs to be integrable. 2. The integration operation is required to be linear.</p>
</blockquote>
<h3 id="hermitian-product"><strong>Hermitian Product</strong></h3>
<p>To simplify the expansion of <span class="math inline">\(||f - g||\)</span>, we introduce another operation, termed Hermitian product, as follows: <span class="math display">\[
\left&lt; g, f \right&gt;_H \doteq \int_{ [-\pi, \pi] }  \overline{ g(t) } \cdot f(t).
\]</span></p>
<p>The operation is not symmetric. By definition, <span class="math display">\[
\left&lt; f, g \right&gt;_H \doteq \int_{ [-\pi, \pi] }  \overline{ f(t) } \cdot g(t).
\]</span></p>
<p><span class="math inline">\(\left&lt; g, f \right&gt;_H\)</span> is the complex conjugate of <span class="math inline">\(\left&lt; f, g \right&gt;_H\)</span>: <span class="math display">\[
\left&lt; g, f \right&gt;_H = \overline{ \left&lt; f, g \right&gt;_H }.
\]</span></p>
<p>For any <span class="math inline">\(c \in \mathbb{C}\)</span>, it holds that <span class="math display">\[
\left&lt; c \cdot g, f \right&gt;_H = \bar c \cdot \left&lt; g, f \right&gt;_H,  \\
\left&lt; g, c \cdot f \right&gt;_H = c  \cdot \left&lt; g, f \right&gt;_H. \\
\]</span></p>
<p>Now the expansion of <span class="math inline">\(||f - g||\)</span> is simplified as <span class="math display">\[
\begin{aligned}
    ||f - g|| 
        &amp;= \sqrt{ ||f||^2 + ||g||^2 - ( \overline{ \left&lt; g, f \right&gt;_H } + \left&lt; g, f \right&gt;_H ) } \\
        &amp;= \sqrt{ ||f||^2 + ||g||^2 - 2 \cdot \mathbb{Re}( \left&lt; g, f \right&gt;_H ) }. \\
\end{aligned}
\]</span></p>
<h4 id="geometric-interpretation"><strong>Geometric Interpretation</strong></h4>
<p>We have just introduced the notation of Hermitian product to simplify the expression. Its "geometric" interpretation is related to the idea of "projection". We start by reviewing the ideas of "Projection, Dot Product and Orthogonality" in <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
<blockquote>
<p><strong>Projection</strong> in <span class="math inline">\(\mathbb{R}^2\)</span>. Given <span class="math inline">\(u, v \in \mathbb{R}^2\)</span>, the projection of <span class="math inline">\(u\)</span> to <span class="math inline">\(v\)</span> is the vector in the subspace <span class="math display">\[
S(v) \doteq \{ c \cdot v : c \in \mathbb{R} \}
\]</span> that is closest to <span class="math inline">\(u\)</span>.</p>
<p><strong>Dot Product</strong> in <span class="math inline">\(\mathbb{R}^2\)</span>. Given <span class="math inline">\(u, v \in \mathbb{R}^2\)</span>, their dot product is defined as <span class="math display">\[
\left&lt; u, v \right&gt; = u_1 v_1 + u_2 v_2.
\]</span> The projection of <span class="math inline">\(u\)</span> to <span class="math inline">\(v\)</span> (closest vector to <span class="math inline">\(u\)</span> in <span class="math inline">\(S(v)\)</span>) is given by <span class="math display">\[
\frac{1}{ ||v||^2 } \left&lt; u, v \right&gt; \cdot v = \frac{ \left&lt; u, v \right&gt; }{ ||v|| }  \cdot \frac{ v }{ ||v|| }.
\]</span> The first term of the final product <span class="math inline">\(\frac{ \left&lt; u, v \right&gt; }{ ||v|| } = \frac{ ||u|| \cdot ||v|| \cdot \cos \angle (u, v) }{ ||v|| } = ||u|| \cdot \cos \angle (u, v)\)</span> gives the length of <span class="math inline">\(u\)</span>'s projection to <span class="math inline">\(v\)</span>, and the second term <span class="math inline">\(\frac{ v }{ ||v|| }\)</span> is a unit vector that shares the same direction as <span class="math inline">\(v\)</span>.</p>
<p><strong>Orthogonality</strong> in <span class="math inline">\(\mathbb{R}^2\)</span>. Given <span class="math inline">\(u, v \in \mathbb{R}^2\)</span>, they are orthogonal if the projection of <span class="math inline">\(u\)</span> to <span class="math inline">\(v\)</span> is <span class="math inline">\(\vec 0\)</span>. In this case, <span class="math display">\[
\left&lt; u, v \right&gt; = 0.
\]</span></p>
</blockquote>
<p>We can extend these ideas to <span class="math inline">\([-\pi, \pi] \rightarrow \mathbb{C}\)</span>.</p>
<h4 id="projection"><strong>Projection</strong></h4>
<p>Given <span class="math inline">\(f, g:[-\pi, \pi] \rightarrow \mathcal{C}\)</span>, the projection of <span class="math inline">\(f\)</span> to <span class="math inline">\(g\)</span> is the function in the linear span of <span class="math inline">\(g\)</span> <span class="math display">\[
    S(g) \doteq \{ c \cdot g : c \in \mathbb{C} \}
\]</span> that minimize <span class="math inline">\(||f - c \cdot g||, \forall c \in \mathbb{C}\)</span>.</p>
<h4 id="hermitian-product-1"><strong>Hermitian Product</strong></h4>
<p>Hermitian product is related to projection and arises naturally when we try to solve <span class="math display">\[
    \min_{c \in \mathbb{C} }  ||f - c \cdot g||,
\]</span></p>
<p>or equivalently, <span class="math display">\[
    \min_{c \in \mathbb{C} }  ||f - c \cdot g||^2.
\]</span></p>
<p>Expanding <span class="math inline">\(||f - c \cdot g||^2\)</span> and let <span class="math inline">\(c = x + iy\)</span>, <span class="math inline">\(\left&lt; g, f \right&gt;_H = a + i b\)</span>, we get <span class="math display">\[
\begin{aligned}
    ||f - c \cdot g||^2  
        &amp;= ||f||^2 + ||c||^2 \cdot ||g||^2 -  2 \cdot \mathbb{Re}( \left&lt; c \cdot g, f \right&gt;_H )  \\
        &amp;= ||f||^2 + ||c||^2 \cdot ||g||^2 - 2 \cdot \mathbb{Re} ( \bar c \cdot \left&lt; g, f \right&gt;_H ) \\
        &amp;= ||f||^2 + (x^2 + y^2) \cdot ||g||^2 - 2 \cdot \mathbb{Re} ( ( x- i y)  \cdot (a + i b) ) \\
        &amp;= ||f||^2 + (x^2 + y^2) \cdot ||g||^2 - 2 \cdot (a x + by) \\
\end{aligned}
\]</span></p>
<blockquote>
<p><em>Remark:</em> <em>The derivation above requires the integration operation to be linear.</em></p>
</blockquote>
<p>Taking the derivate of RHS with respect to <span class="math inline">\(x\)</span>, setting it to zero and assuming that <span class="math inline">\(||g|| \neq 0\)</span>, we see <span class="math display">\[
    2x\cdot ||g||^2 - 2 a = 0 \Rightarrow x = \frac{ a }{ ||g||^2 }.
\]</span></p>
<p>Similarly, <span class="math display">\[
    2y\cdot ||g||^2 - 2 b = 0 \Rightarrow y = \frac{ b }{ ||g||^2 }.
\]</span></p>
<p>Combined, <span class="math inline">\(c\)</span> is given by <span class="math display">\[
    c = x + iy = \frac{ \left&lt; g, f \right&gt;_H  }{ ||g||^2 } = \left&lt; \frac{g }{ ||g|| }, f \right&gt;_H \cdot \frac{ 1 }{ ||g|| }.
\]</span></p>
<p>This implies that <span class="math display">\[
    c \cdot g = \frac{ \left&lt; g, f \right&gt;_H  }{ ||g||^2 } \cdot g = \left&lt; \frac{g }{ ||g|| }, f \right&gt;_H \cdot \frac{g }{ ||g|| }.
\]</span></p>
<p>Now it is clear that the Hermitian product computes the coefficient for the projection of <span class="math inline">\(f\)</span> to <span class="math inline">\(g\)</span>, scaled by a factor of <span class="math inline">\(1/ ||g||^2\)</span>. When <span class="math inline">\(|| g ||\)</span> has unit length, <span class="math display">\[  
    c \cdot g = \left&lt; g, f \right&gt;_H \cdot g.
\]</span></p>
<p>The Hermitian product <span class="math inline">\(\in \mathbb{C}\)</span> is the just the coefficient for the projection.</p>
<h4 id="orthogonality"><strong>Orthogonality</strong></h4>
<p>Given <span class="math inline">\(f, g:[-\pi, \pi] \rightarrow \mathcal{C}\)</span>, <span class="math inline">\(f\)</span> is orthogonal to <span class="math inline">\(g\)</span> if the projection of <span class="math inline">\(f\)</span> to <span class="math inline">\(g\)</span> is the zero function, i.e., the closest function (measured by <span class="math inline">\(|| \cdot ||\)</span>) to <span class="math inline">\(f\)</span> in the linear span <span class="math inline">\(S(g)\)</span> is the constant function zero. It holds that, either <span class="math inline">\(||g||^2 = 0\)</span> or <span class="math display">\[
    \arg\min_{ c \in \mathbb{C} } \left| \left|f - c \cdot g \right| \right|^2 = \frac{ \left&lt; g, f \right&gt;_H  }{ ||g||^2 } = 0.
\]</span></p>
<p>In both cases, <span class="math display">\[
    \overline{ \left&lt; g, f \right&gt;_H } = \left&lt; f, g \right&gt; = 0.
\]</span></p>
<h2 id="properties-of-gamma"><strong>Properties of <span class="math inline">\(\Gamma\)</span></strong></h2>
<p>We are now ready to discuss the properties of <span class="math inline">\(\Gamma = \{ 1, \exp(it), \exp(-it), \exp(i2t), \exp(-i2t), ... \}\)</span>. 1. For <span class="math inline">\(k \in \mathbb{Z}\)</span>, <span class="math display">\[
   || \exp(ikt) || = \sqrt{ \int_{ [-\pi, \pi] }  \overline{\exp(ikt) } \exp(ikt) \  } = \sqrt{2 \pi }.
   \]</span></p>
<ol start="2" type="1">
<li>For <span class="math inline">\(k, l \in \mathbb{Z}\)</span>, <span class="math inline">\(k \neq l\)</span>, <span class="math inline">\(\exp(ikt)\)</span> and <span class="math inline">\(\exp(ilt)\)</span> are orthogonal. <span class="math display">\[
\begin{aligned}
    \left&lt; \exp(ikt), \exp(ilt) \right&gt;_H 
     &amp;= \int_{ [ -\pi, \pi] } \exp(-ikt) \cdot \exp(ilt)  \\
     &amp;= \int_{ [ -\pi, \pi] } \exp(i (l - k) t) \\
     &amp;= \frac{1}{ i(l - k) } \exp(i (l - k) t) \mid_{- \pi}^\pi \\
     &amp;= 0.
\end{aligned}
\]</span></li>
</ol>
<p>If we normalize the function in <span class="math inline">\(\Gamma\)</span> by <span class="math inline">\(1 / \sqrt{2 \pi}\)</span>, then each function has norm 1. We call this family an orthonormal family, and rewrite it as <span class="math display">\[
\mathfrak{F} = \{ e_0,  e_1, e_2, e_3, ..., \},
\]</span></p>
<p>where <span class="math inline">\(e_0 = 1\)</span>, and for <span class="math inline">\(k \in \mathbb{N}\)</span>,</p>
<ul>
<li><p><span class="math inline">\(e_{2k + 1} = \exp(ikt) / \sqrt{2 \pi}\)</span>;</p></li>
<li><p><span class="math inline">\(e_{2k + 2} = \exp(- ikt) / \sqrt{2 \pi}\)</span>.</p></li>
</ul>
<h2 id="approximating-f-with-mathfrakf"><strong>Approximating <span class="math inline">\(f\)</span> with <span class="math inline">\(\mathfrak{F}\)</span></strong></h2>
<p>As discuss before, we have construct an orthogonal family of <span class="math inline">\(\mathfrak{F}\)</span>, such that</p>
<ol type="1">
<li><span class="math inline">\(||e_i|| = 1\)</span> for <span class="math inline">\(e_i \in \mathfrak{F}\)</span>.</li>
<li><span class="math inline">\(\left&lt; e_i, e_j \right&gt;_H = 0\)</span> for <span class="math inline">\(e_i, e_j \in \mathfrak{F}, e_i \neq e_j\)</span>.</li>
</ol>
<!-- We continue to study the following two questions. 

2. Under what condition, can $f$ be rewrite as a linear combination of functions in $\mathfrak{F}$?
   
3. What is the approximation error, between $f$ and the linear combination?

For the second question, we provide a sufficient condition as an answer:

> $f^2$ is integrable. 

Hence,  -->
<p><strong>Definition.</strong> Define <span class="math inline">\(S(\mathfrak{F}_k)\)</span> the span of the first <span class="math inline">\(k + 1\)</span> elements in <span class="math inline">\(\mathfrak{F}\)</span>: <span class="math display">\[
    S(\mathfrak{F}_k ) \doteq \left\{ \sum_{i = 0}^k c_i \cdot e_i : \forall \ 0 \le i \le k, c_i \in \mathbb{C} \right\}. 
\]</span></p>
<blockquote>
<p><strong>Theorem.</strong> The closest function to <span class="math inline">\(f\)</span> in <span class="math inline">\(S(\mathfrak{F}_k )\)</span>, denoted as <span class="math inline">\(g_k\)</span>, is given by <span class="math display">\[
    g_k \doteq \sum_{i = 0 }^k a_i \cdot e_i.
\]</span> where <span class="math inline">\(a_i \doteq \left&lt; e_i, f \right&gt;_H, 0 \le i \le k\)</span>.</p>
</blockquote>
<blockquote>
<p><em>Remark: implicitly, we assume that <span class="math inline">\(f^2\)</span>, <span class="math inline">\(\bar e_i f\)</span> is integrable.</em></p>
</blockquote>
<p><em>Proof:</em> For <span class="math inline">\(0 \le i \le k\)</span>, it holds that <span class="math display">\[
\left&lt; e_i, g_k \right&gt;_H = \sum_{j = 0 }^k \left&lt; e_i, a_j \cdot e_j \right&gt;_H =  a_i = \left&lt; e_i, f \right&gt;_H.
\]</span></p>
<p>Hence, <span class="math display">\[
\left&lt; e_i, g_k - f\right&gt;_H = \overline{ \left&lt; g_k - f, e_i \right&gt;_H  } = 0. 
\]</span></p>
<p>Let <span class="math display">\[
    f_k = \sum_{i = 0}^k c_i \cdot e_i, 
\]</span></p>
<p>be a function in <span class="math inline">\(S(\mathfrak{F}_k )\)</span>. Then, <span class="math display">\[
\begin{aligned}
    || f - f_k||^2 
        &amp;= ||f - g_k + g_k - f_k ||^2 \\
        &amp;= ||f - g_k||^2 + ||g_k - f_k||^2 + \left&lt; f - g_k, g_k - f_k\right&gt;_H + \left&lt; g_k - f_k, f - g_k \right&gt;_H. 
\end{aligned}
\]</span></p>
<p>It holds that <span class="math display">\[
\left&lt; f - g_k, g_k - f_k\right&gt;_H = \sum_{i = 0}^k (a_i - c_i) \cdot \left&lt; f - g_k,  e_i \right&gt;_H = 0.
\]</span></p>
<p>Similarly, <span class="math inline">\(\left&lt; g_k - f_k, f - g_k \right&gt;_H = 0\)</span>. So, <span class="math display">\[
\begin{aligned}
    || f - f_k||^2 
        &amp;= ||f - g_k||^2 + ||g_k - f_k||^2,  
\end{aligned}
\]</span></p>
<p>which is minimized when <span class="math inline">\(f_k = g_k\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<!-- $$
\begin{aligned}
    ||f - f_k||^2 
        &= \left< f - f_k, f - f_k \right>_H \\
        &= \left<  f, f\right>_H + \left<  f_k, f_k \right>_H - \left<  f, f_k \right>_H - \left<  f_k , f\right>_H \\
        &= ||f||^2 + \sum_{i = 0}^k \left( \left< c_i \cdot e_i, c_i \cdot e_i \right>_H - \left<  f, c_i \cdot e_i \right>_H - \left< c_i \cdot e_i , f \right>_H \right) \\
        &= ||f||^2 + \sum_{i = 0}^k \left( \bar c_i \cdot c_i - c_i \cdot \left<  f, e_i \right>_H - \bar c_i \cdot \left< e_i , f \right>_H + \bar a_i \cdot a_i \right) - \sum_{i = 0}^k ||a_i||^2 \\
        &= ||f||^2 + \sum_{i = 0}^k \left( \bar c_i \cdot c_i - c_i \cdot \bar a_i - \bar c_i \cdot a_i + \bar a_i \cdot a_i \right) - \sum_{i = 0}^k ||a_i||^2 \\
        &= ||f||^2 + \sum_{i = 0}^k ||c_i - a_i||^2 - \sum_{i = 0}^k ||a_i||^2. \\
\end{aligned}
$$ -->
<!-- Thus, when $c_i = a_i$, the distance between $f_k$ and $f$ is minimized.  -->
<blockquote>
<p><strong>Corollary. (Bessel Ineuqality)</strong>. <span class="math display">\[
\sum_{i = 0}^\infty ||a_i||^2 \le ||f||^2.
\]</span></p>
</blockquote>
<p><em>Proof.</em> We see that <span class="math display">\[
\begin{aligned}
    || f - g_k ||^2 
        &amp;= \left&lt; f - g_k, f \right&gt;_H - \left&lt; f - g_k, g_k \right&gt;_H \\
        &amp;= \left&lt; f , f \right&gt;_H - \left&lt;  g_k, f \right&gt;_H \\
        &amp;= ||f||^2 - \sum_{i = 0}^k ||a_i||^2 \\
        &amp;\ge 0.
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math display">\[
    \sum_{i = 0}^k ||a_i||^2 \le ||f||^2.
\]</span></p>
<p>Taking the limit of <span class="math inline">\(k\)</span> we get the desired result.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>The function series <span class="math inline">\(g_1, g_2, g_3, ...\)</span> is called a Cauchy series, as <span class="math inline">\(\forall \epsilon &gt; 0\)</span>, <span class="math inline">\(\exists N \ge 0\)</span>, <span class="math inline">\(\forall N \le i &lt; j\)</span>, it holds that <span class="math display">\[
    ||g_j - g_i|| = || \sum_{t = i + 1}^j a_t \cdot e_t ||
    = \sqrt{ \sum_{t = i + 1}^j ||a_t||^2 } \le \epsilon.
\]</span></p>
<p>Suppose that we are using some kind of integration and let <span class="math inline">\(I^2([-\pi, \pi])\)</span> be the set of square integrable functions. Further, assume that <span class="math inline">\(g_1, g_2, g_3, ...\)</span> are also elements in <span class="math inline">\(I^2([-\pi, \pi])\)</span>. We are interested in</p>
<blockquote>
<p>Whether a Cauchy series in <span class="math inline">\(I^2([-\pi, \pi])\)</span> converges to a function in <span class="math inline">\(I^2([-\pi, \pi])\)</span>?</p>
</blockquote>
<p>Define <span class="math display">\[
    g = \lim_{k \rightarrow \infty} g_k = \sum_{i = 0}^\infty a_i \cdot e_i.
\]</span></p>
<p>It is important that <span class="math inline">\(g \in I^2([-\pi, \pi])\)</span>, as we need the length of <span class="math inline">\(|| g ||\)</span> to be well-defined. Further, <span class="math inline">\(f - g\)</span> should also be in <span class="math inline">\(I^2([-\pi, \pi])\)</span>.</p>
<p>However, if we are using Riemann integration, the answer is no. Let <span class="math inline">\(r_1, r_2, ....\)</span> be the rational number in the interval of <span class="math inline">\([0, 1]\)</span> and define <span class="math display">\[
    h_i(x) = \begin{cases}
        \begin{aligned}
            &amp;1,  &amp;\text{if } x \in \cup_{t = 1}^i \{ r_t \} \\
            &amp;0,  &amp;\text{otherwise}
        \end{aligned}
    \end{cases}
\]</span></p>
<p>Then <span class="math inline">\(h_i \in I^2([-\pi, \pi])\)</span> but <span class="math inline">\(\lim_{i \rightarrow \infty} h_i\)</span> is not Riemann integrable, hence not in <span class="math inline">\(I^2([-\pi, \pi])\)</span>.</p>
<p>If we are using Lebesgue integration, the answer is yes.</p>
<blockquote>
<p>Fact. If we are using Lebesgue integration, then <span class="math inline">\(I^2([-\pi, \pi])\)</span> is complete, i.e., if <span class="math inline">\(g_1, g_2, ..., \in I^2([-\pi, \pi])\)</span> is Cauchy, then <span class="math inline">\(\lim_k g_k \in I^2([-\pi, \pi])\)</span>.</p>
</blockquote>
<h2 id="approximating-error"><strong>Approximating Error</strong></h2>
<p>It is left to study the error of approximating <span class="math inline">\(f\)</span> with <span class="math inline">\(g\)</span>.</p>
<blockquote>
<p><strong>Lemma. Cauchy-Schwartz Inequality.</strong> Let <span class="math inline">\(h_1, h_2 \in I^2([-\pi, \pi])\)</span>, then <span class="math display">\[
| \left&lt; h_1, h_2 \right&gt;_H | \le ||h_1|| \cdot ||h_2 ||
\]</span></p>
</blockquote>
<p><em>Proof.</em> If <span class="math inline">\(||h_2|| = 0\)</span>, then <span class="math inline">\(h_2 = 0\)</span> almost everywhere and the case is trivial. Otherwise, <span class="math inline">\(\forall c \in \mathbb{C}\)</span>, we have <span class="math display">\[
\begin{aligned}
    || h_1 - c \cdot h_2 ||^2 
        &amp;= || h_1||^2 + ||c||^2 ||h_2||^2 -  \left&lt; h_1, c \cdot h_2 \right&gt;_H -   \left&lt; c \cdot h_2, h_1 \right&gt;_H \\
        &amp;= || h_1||^2 + ||c||^2 ||h_2||^2 - c \cdot \left&lt; h_1, h_2 \right&gt;_H - \bar c \cdot \left&lt; h_2, h_1 \right&gt;_H \\
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(c = \frac{1}{ ||h_2||^2 } \cdot \left&lt; h_2, h_1 \right&gt;_H\)</span>, then <span class="math display">\[
\begin{aligned}
    || h_1 - c \cdot h_2 ||^2 
        &amp;= || h_1||^2 - \frac{1}{ ||h_2||^2 } \cdot \left| \left&lt; h_2, h_1 \right&gt;_H \right|^2 
        &amp;\ge 0.
\end{aligned}
\]</span></p>
<!-- As  -->
<!-- $$
    || \left< h_1, h_2 \right>_H || \le ||h_1|| \cdot ||h_2 || \longleftrightarrow
    \left|\left| \left< \frac{h_1}{ ||h_1|| }, \frac{h_2}{ ||h_2 || } \right>_H \right|\right| \le  1
$$

Without lose of generality, we assume that $||h_1|| = 1$ and $||h_2|| = 1$. 

$$
\begin{aligned}
    2 \cdot \mathbb{Re} \left( \left< h_1, h_2 \right>_H \right) 
        &= \left< h_1, h_2 \right>_H  + \overline{ \left< h_2, h_1 \right>_H } \\
        &= \int_{ [ -\pi, \pi] } \bar h_1 h_2  + \bar h_2 h_1 \\
        &\le \int_{ [ -\pi, \pi] } 2 \left( ||h_1||^2 + ||h_2||^2 \right)
\end{aligned}
$$ -->
<!-- We will prove that 
$$
    || \left< h_1, h_2 \right>_H ||^2 \le 1.
$$

$$
    \begin{aligned}
        \left|\left| \int_{ [ -\pi, \pi] } \bar h_1 h_2 \right|\right|^2 
        &= \left|\left| \int_{ [ -\pi, \pi] } \mathbb{Re} (\bar h_1 h_2) + i \mathbb{Im} (\bar h_1 h_2) \right|\right|^2 \\
        &= \left( \int_{ [ -\pi, \pi] } \mathbb{Re} (\bar h_1 h_2) \right)^2 +  \left( \int_{ [ -\pi, \pi] } \mathbb{Im} (\bar h_1 h_2) \right)^2 \\
        &\le \left( \int_{ [ -\pi, \pi] } \mathbb{Re} (\bar h_1 h_2) \right)^2 +  \left( \int_{ [ -\pi, \pi] } \mathbb{Im} (\bar h_1 h_2) \right)^2
    \end{aligned}
$$ -->
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Lemma.</strong> <span class="math inline">\(\forall i \in \mathbb{N}\)</span>, <span class="math inline">\(\left&lt; e_i, g \right&gt;_H =a_i\)</span>.</p>
</blockquote>
<p><em>Proof.</em> By Cauchy-Schwartz inequality,</p>
<p><span class="math display">\[
\begin{aligned}
    \left|\left| \int_{ [ -\pi, \pi] } \bar e_i (g_k - g) \right|\right|^2
        &amp;\le \left( \int_{ [ -\pi, \pi] } ||e_i||^2 \right) \cdot \left( \int_{ [ -\pi, \pi] } ||g_k - g||^2 \right) \\
        &amp;= \int_{ [ -\pi, \pi] } ||g_k - g||^2 \\
        &amp;= \int_{ [ -\pi, \pi] } \lim_l ||g_k - g_l ||^2 
\end{aligned}
\]</span></p>
<p>By <strong>Fatou's lemma</strong>, <span class="math display">\[
    \int_{ [ -\pi, \pi] } \lim_l ||g_k - g_l ||^2 \le \liminf_l \int_{ [ -\pi, \pi] } ||g_k - g_l ||^2 
\]</span></p>
<p>Finally, <span class="math display">\[
    \lim_k  \left|\left|  \left&lt;  e_i, g_k - g \right&gt;_H \right|\right|^2 = \lim_k \left|\left|  \int_{ [ -\pi, \pi] } \bar e_i (g_k - g) \right|\right|^2 \le \lim_k \liminf_l \int_{ [ -\pi, \pi] } ||g_k - g_l ||^2  = 0.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p>Remark: the above proof involves the operation of exchanging the order of two limiting operations, namely <span class="math inline">\(\lim\)</span> and <span class="math inline">\(\int\)</span>. Exchanging order like this is dangerous for Riemann integration, but we can do it safely under Lebesgue integration, under proper condition. Here we use <strong>Fatou's lemma</strong>.</p>
</blockquote>
<p>By the above lemma, the function <span class="math inline">\(g\)</span> has an important property, such that <span class="math inline">\(\forall i \in \mathbb{N}\)</span>, <span class="math display">\[
    \left&lt; e_i, f - g \right&gt;_H = 0.
\]</span></p>
<blockquote>
<p><strong>Lemma.</strong> Let <span class="math inline">\(h \in I^2([-\pi, \pi])\)</span>. If <span class="math inline">\(\forall i \in N\)</span>, <span class="math display">\[
\left&lt; e_i, h \right&gt;_H = 0,
\]</span> then <span class="math inline">\(h = 0\)</span> almost everywhere.</p>
</blockquote>
<p><em>Proof.</em> First, we assume that <span class="math inline">\(h\)</span> is a continuous function on <span class="math inline">\([-\pi, \pi]\)</span>. If <span class="math inline">\(h \not\equiv 0\)</span>, then <span class="math inline">\(|h(x)|\)</span> achieves maximum value at some point <span class="math inline">\(x^* \in [-\pi, \pi]\)</span>. Without lose of generality, we assume that <span class="math display">\[
    h(x^*) = M &gt; 0. 
\]</span></p>
<p>By continuity of <span class="math inline">\(h\)</span>, <span class="math inline">\(\exists \delta \in (0, \pi)\)</span>, such that <span class="math inline">\(h(x) &gt; M / 2\)</span> in the interval <span class="math display">\[
    I \doteq (x^* - \delta, x^* + \delta) \cap [-\pi, \pi].
\]</span></p>
<p>We will capture the maximum value of <span class="math inline">\(f\)</span>, by taking Hermitian product of <span class="math inline">\(f\)</span> with a signal function that</p>
<ol type="1">
<li>is a linear combination of functions in <span class="math inline">\(\mathfrak{F}\)</span>;</li>
<li>is <span class="math inline">\(\ge 1\)</span> in <span class="math inline">\(I\)</span>;</li>
<li>is <span class="math inline">\(&lt; 1\)</span> in <span class="math inline">\([-\pi, \pi] \setminus I\)</span>.</li>
</ol>
<p>The signal function is given by <span class="math display">\[
    s(x) = 1 + \cos \left( \frac{1}{2} \cdot (x - x^*) \right) - \cos \frac{\delta}{2}.
\]</span></p>
<p>Observe that <span class="math inline">\(s\)</span></p>
<ol type="1">
<li>achieves maximum value <span class="math inline">\(s(x^*) = 2 - \cos \frac{\delta}{2}\)</span>;</li>
<li>is <span class="math inline">\(\ge 1\)</span> for <span class="math inline">\(x \in I\)</span>;</li>
<li>has period <span class="math inline">\(4 \pi\)</span>.</li>
</ol>
<p>Hence, it increases on the interval <span class="math inline">\([x^* - 2\pi, x^*]\)</span> and decreases on <span class="math inline">\([x^*, x^* + 2\pi]\)</span>. So</p>
<ol start="4" type="1">
<li><span class="math inline">\(s(x) &lt; 1\)</span> for <span class="math inline">\(x \in [-\pi, \pi] \setminus I\)</span>.</li>
</ol>
<p>For <span class="math inline">\(n \ge 1\)</span>, <span class="math inline">\(s^n(x)\)</span> is also a linear combination of functions in <span class="math inline">\(\mathfrak{F}\)</span>. It always holds that <span class="math inline">\(\left&lt; s^n(x), f \right&gt;_H \in \mathbb{R}\)</span> and by assumption on <span class="math inline">\(f\)</span>, it holds that <span class="math display">\[
    \left&lt; s^n(x), f \right&gt;_H = 0.
\]</span></p>
<p>But this is impossible. By monotonicity of integration, <span class="math display">\[
\int_{ [-\pi, \pi] \setminus I } f(x) s^n(x) \ dx \ge \int_{ [-\pi, \pi] \setminus I } (-M) \cdot 1 \ dx = - 2 \cdot \pi \cdot M.
\]</span></p>
<p>Define <span class="math display">\[
    I&#39; \doteq (x^* - \delta / 2, x^* + \delta / 2) \cap [-\pi, \pi].
\]</span></p>
<p>Then <span class="math inline">\(|I&#39;| \ge \delta / 2\)</span> and <span class="math inline">\(m \doteq \min_{x \in I&#39;} s(x) &gt; 1\)</span>. <span class="math display">\[
    \int_{ I } f(x) s^n(x) \ dx \ge \int_{ I&#39; } \frac{M}{2} \cdot m^n \ dx \ge \frac{M \cdot m^n \cdot \delta }{ 4 }.
\]</span></p>
<p>This implies as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math display">\[
    \left&lt; s^n(x), f \right&gt;_H \ge - 2 \cdot \pi \cdot M + \frac{M \cdot m^n \cdot \delta }{ 4 } \rightarrow \infty, 
\]</span></p>
<p>which is a contradiction.</p>
<p>Next, we consider the general case of <span class="math inline">\(h \in I^2([-\pi, \pi])\)</span>. Define <span class="math display">\[
    w(x) = \int_{-\pi}^x h(t) \ dt.
\]</span></p>
<p>The function is absolutely continuous on <span class="math inline">\([-\pi, \pi]\)</span>. Moreover, <span class="math inline">\(w(0) = 0\)</span> and by assumption, <span class="math inline">\(w(\pi) = \left&lt; e_0, h \right&gt;_H = 0\)</span>. For <span class="math inline">\(k \in \mathbb{N}\)</span>, integrating by part gives <span class="math display">\[
    \begin{aligned}
        \sqrt{2 \cdot \pi} \cdot \left&lt; e_{2k + 1}, w \right&gt;_H 
            &amp;= \int_{ [-\pi, \pi] } \exp(-ikx) w(x) \ dx \\
            &amp;= \frac{1}{ik} \exp(-ikx) w(x) \mid_{-\pi}^\pi - \frac{1}{ik}  \int_{ [-\pi, \pi] } \exp(-ikx) w&#39;(x) \ dx \\
            &amp;=  - \frac{1}{ik}  \int_{ [-\pi, \pi] } \exp(-ikx) h(x) \ dx \\
            &amp;=  - \frac{1}{ik}  \left&lt; e_{2k + 1}, h\right&gt;_H \\
            &amp;= 0.
    \end{aligned}
\]</span></p>
<p>Similarly, we can prove that <span class="math inline">\(\left&lt; e_{2k + 2}, w \right&gt;_H = 0\)</span>.</p>
<p>Hence, <span class="math inline">\(\forall k \ge 1\)</span>, <span class="math inline">\(\left&lt; e_k, w \right&gt;_H = 0\)</span>. But it is not guaranteed that <span class="math inline">\(\left&lt; e_0, w \right&gt;_H = 0\)</span>. To fix this, we construct another function <span class="math display">\[
    W(x) \doteq w(x) - B.
\]</span></p>
<p>where <span class="math inline">\(B = \left&lt; e_0, w \right&gt;_H\)</span> is a constant. It holds that</p>
<ol type="1">
<li><span class="math inline">\(\forall k \ge 1\)</span>, <span class="math inline">\(\left&lt; e_k, W \right&gt;_H = \left&lt; e_k, w \right&gt;_H - \left&lt; e_k, B \right&gt;_H = - B \cdot \left&lt; e_k, e_0 \right&gt;_H = 0\)</span>.<br />
</li>
<li><span class="math inline">\(\left&lt; e_0, W \right&gt;_H = \left&lt; e_0, w \right&gt;_H + \left&lt; e_0, B \right&gt;_H = B - B \cdot \left&lt; e_0, e_0 \right&gt;_H = 0\)</span>.</li>
</ol>
<p>By previous result, <span class="math inline">\(W(x) \equiv 0\)</span> and <span class="math inline">\(w(x) \equiv B\)</span>. This implies <span class="math display">\[
w&#39;(x) = h(x) = 0, \quad a.e.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/24/Gamma-Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/24/Gamma-Function/" class="post-title-link" itemprop="url">Gamma Function</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-12-24 21:32:57 / Modified: 21:47:33" itemprop="dateCreated datePublished" datetime="2020-12-24T21:32:57+11:00">2020-12-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Consider the integral <span class="math display">\[
    \int_0^\infty e^{- st} \ dt = \frac{1}{s} e^{- st} \mid_0^\infty  = \frac{1}{s}.
\]</span></p>
<p>Taking derivative with respect to <span class="math inline">\(s\)</span> of both side, we get <span class="math display">\[
    \int_0^\infty t e^{- t s} \ dt = \frac{1}{s^2}.
\]</span></p>
<p>Repeating this process one more step, <span class="math display">\[
    \int_0^\infty t^2 e^{- st} \ dt = \frac{2}{s^3}.
\]</span></p>
<p>By induction on <span class="math inline">\(k \in \mathbb{N}^+\)</span>, we obtain <span class="math display">\[
    \int_0^\infty t^k e^{- st} \ dt = \frac{k!}{s^{k + 1} }.
\]</span></p>
<p>Setting <span class="math inline">\(s = 1\)</span>, we have <span class="math display">\[
    k! = \int_0^\infty t^k e^{ -t } \ dt.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/19/Hermitian-Inner-Products/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/19/Hermitian-Inner-Products/" class="post-title-link" itemprop="url">Hermitian Inner Products</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-19 23:17:13" itemprop="dateCreated datePublished" datetime="2020-12-19T23:17:13+11:00">2020-12-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-23 00:41:33" itemprop="dateModified" datetime="2020-12-23T00:41:33+11:00">2020-12-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The inner product of two complex vectors <span class="math inline">\(\vec u, \vec v \in \mathbb{C}^n\)</span> is defined as <span class="math display">\[
    \vec u^H \vec v = [ \bar u_1, \bar u_2, ..., \bar u_n ] \begin{bmatrix} v_1 \\ v_2 \\ . \\ . \\ . \\ v_n \end{bmatrix} = \bar u_1 v_1 + \bar u_2 v_2 ... + \bar u_n v_n. 
\]</span></p>
<p>It is tempting to think the inner product should be <span class="math inline">\(\vec u^T \vec v\)</span>, as the one for real vectors. Why do we take the conjugate of <span class="math inline">\(\vec u\)</span>? To answer this, we need to study the geometric meaning of inner product. We begin from the inner product in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<h1 id="inner-product-in-mathbbrn">Inner Product in <span class="math inline">\(\mathbb{R}^n\)</span></h1>
<p>The inner product is closely related to the concept of "length". Generally, we express the length of a vector in term of inner product. Here we take a reverse approach. We consider the length of a vector as a more fundamental concept. Once the length of a vector is defined, it leads naturally the definition of inner product.</p>
<p>For a vector <span class="math inline">\(\vec u \in \mathbb{R}^n\)</span>, we define its length as <span class="math display">\[
||\vec u || = \sqrt{ \sum_{i \in [n] } u_i^2 }. 
\]</span></p>
<p>Given two vector <span class="math inline">\(\vec u, \vec v \in \mathbb{R}^n\)</span>, the definition of the inner product of <span class="math inline">\(\vec u^T \vec v\)</span> stems from the following problem:</p>
<blockquote>
<p>Find a vector in the subspace generated by <span class="math inline">\(\vec u\)</span>, such that it distance to <span class="math inline">\(\vec v\)</span> is minimized.</p>
</blockquote>
<p>The subspace generated by <span class="math inline">\(\vec u\)</span> is the line that passes through <span class="math inline">\(\vec u\)</span> and can be represented as <span class="math display">\[
\{ t \vec u : t \in \mathbb{R} \}. 
\]</span></p>
<p>The problem gives rise to the optimization problem: <span class="math display">\[
\min_{ t \in \mathbb{R} } ||\vec v - t \vec u||^2. 
\]</span></p>
<p>Define <span class="math display">\[
y \doteq || \vec v - t \vec u||^2 = || \vec u ||^2 t^2 - 2 ( \vec v^T \vec u) t + || \vec  v||^2. 
\]</span></p>
<p>By differentiating and setting the derivative equal to zero, we get <span class="math display">\[
y&#39; = 2 || \vec u ||^2 t - 2 (\vec  v^T  \vec  u) = 0 \implies t = \frac{ \vec  v^T \vec u }{ || \vec u ||^2 }. 
\]</span></p>
<p>The inner product appears exactly in the numerator. Its meaning is clear now: it is related to the magnitude of the closest vector in the subspace spanned by <span class="math inline">\(\vec u\)</span>. In case that <span class="math inline">\(\vec u\)</span> is a unit vector, <span class="math inline">\(t = \vec v^T \vec u\)</span> is exactly the length of this closest vector.</p>
<p>This also sheds light on the meaning of orthogonality:</p>
<blockquote>
<p>If <span class="math inline">\(\vec u\)</span> is orthogonal to <span class="math inline">\(\vec v\)</span>, then the best approximation (measured by <span class="math inline">\(|| \cdot ||\)</span>) of <span class="math inline">\(\vec v\)</span> in the subspace spanned by <span class="math inline">\(\vec u\)</span> is the zero vector.</p>
</blockquote>
<h1 id="inner-product-in-mathbbcn">Inner Product in <span class="math inline">\(\mathbb{C}^n\)</span></h1>
<p>To get the desired expression of inner product <span class="math inline">\(\mathbb{C}^n\)</span>, we first extend the definition of length in <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{C}^n\)</span>. For <span class="math inline">\(\vec u \in \mathbb{C}^n\)</span>, define <span class="math display">\[
|| \vec u || = \sqrt{ \sum_{i \in [n]}  |u_i|^2 }.
\]</span></p>
<p>Given <span class="math inline">\(\vec u, \vec v \in \mathbb{C}^n\)</span>, we consider the similar problem:</p>
<blockquote>
<p>Find a vector in the subspace generated by <span class="math inline">\(\vec u\)</span>, such that it distance to <span class="math inline">\(\vec v\)</span> is minimized.</p>
</blockquote>
<p>However, this time the subspace generated by <span class="math inline">\(\vec u\)</span> is given by <span class="math display">\[
\{ (t^r + i t^i ) \vec u: t^r, t^i \in \mathbb{R} \}. 
\]</span></p>
<p>Let <span class="math inline">\(\vec u^r \in \mathbb{R}^n\)</span> be the real part of <span class="math inline">\(\vec u\)</span> and <span class="math inline">\(\vec u^i \in \mathbb{R}^n\)</span> be its imaginary part. Then <span class="math inline">\(\vec u = \vec u^r + i \vec u^i\)</span>. Similarly, we can decompose <span class="math inline">\(\vec v\)</span> into <span class="math inline">\(\vec v = \vec v^r + i \vec v^i\)</span>.</p>
<p>We want to minimize <span class="math display">\[
\begin{aligned}
    y   &amp;\doteq ||(t^r + i t^i ) \vec u - \vec v||^2 \\
        &amp;=      ||(t^r + i t^i ) (\vec u^r + i \vec u^i) - \vec v^r - i \vec v^i ||^2 \\
        &amp;=      || t^r \vec u^r - t^i \vec u^i - \vec v^r + i ( t^i \vec u^r + t^r \vec u^i - \vec v^i ) ||^2 \\
        &amp;=      || t^r \vec u^r - t^i \vec u^i - \vec v^r ||^2 + || t^i \vec u^r + t^r \vec u^i - \vec v^i ||^2 \\
\end{aligned}
\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(t^r\)</span>, we get <span class="math display">\[
    \frac{\partial y}{ \partial t^r } = 2 ||\vec u^r||^2 t^r - 2 ( \vec u^r \cdot \vec u^i) t^i - 2 ( \vec u^r \cdot \vec v^r) + 2 ||\vec u^i||^2 t^r + 2 (\vec u^r \cdot \vec u^i ) t^i - 2 ( \vec u^i \cdot \vec v^i )
\]</span></p>
<p>where <span class="math inline">\(\vec u^r \cdot \vec u^i\)</span> denotes their dot product. Setting the derivative to zero, we have <span class="math display">\[
    t^r = \frac{ \vec u^r \cdot \vec v^r + \vec u^i \cdot \vec v^i }{ ||\vec u^r||^2 + ||\vec u^i||^2 } = \frac{ \vec u^r \cdot \vec v^r + \vec u^i \cdot \vec v^i }{ ||\vec u||^2  }.
\]</span></p>
<p>Also, <span class="math display">\[
    \frac{\partial y}{ \partial t^i } = 2 ||\vec u^i||^2 t^i - 2 ( \vec u^r \cdot \vec u^i) t^r + 2 ( \vec u^i \cdot \vec v^r) + 2 ||\vec u^r||^2 t^i + 2 (\vec u^r \cdot \vec u^i ) t^r - 2 ( \vec u^r \cdot \vec v^i ).
\]</span></p>
<p>Setting the derivative to zero, we obtain <span class="math display">\[
    t^i = \frac{ -\vec u^i \cdot \vec v^r + \vec u^r \cdot \vec v^i }{ ||\vec u^r||^2 + ||\vec u^i||^2 } = \frac{ -\vec u^i \cdot \vec v^r + \vec u^r \cdot \vec v^i }{ ||\vec u||^2  }.
\]</span></p>
<p>On the other hand, the Hermitian inner product is <span class="math display">\[
    \overline{(\vec u^r + i \vec u^i)}  \cdot (\vec v^r + i \vec v^i ) = (\vec u^r \cdot \vec v^r + \vec u^i \cdot \vec v^i) + i ( -\vec u^i \cdot \vec v^r + \vec u^r \cdot \vec v^i ).
\]</span></p>
<p>Therefore, <span class="math display">\[
    t^r + i t^i = \frac{1}{||\vec u||^2} \vec u^H \vec v.
\]</span></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/HermitainProduct.png?raw=true" /></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/18/Complex-Numbers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/18/Complex-Numbers/" class="post-title-link" itemprop="url">Complex Numbers</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-18 11:06:31" itemprop="dateCreated datePublished" datetime="2020-12-18T11:06:31+11:00">2020-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-21 00:43:06" itemprop="dateModified" datetime="2020-12-21T00:43:06+11:00">2020-12-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>I feel uncomfortable with complex numbers for a long time, partially because of the way they are introduced in the textbooks. They are considered necessary for solving equations such as <span class="math display">\[
x^2 + 1= 0. 
\]</span></p>
<p>Somehow magically, a symbol <span class="math inline">\(i\)</span> is introduced, and its square <span class="math inline">\(i^2\)</span> is manually defined as <span class="math inline">\(-1\)</span>. Following this, typically a set of operations defined on complex numbers as well as their properties are listed by the textbooks, leaving the question of why we can have such "magical symbol" unsolved.</p>
<p>In this blog, I am trying to introduce complex numbers in a more natural manner, by studying <em>rotations</em>, <em>dilations</em> and <em>additions</em> of points in <span class="math inline">\(\mathbb{R}^2\)</span>. I will show that these operations satisfy some properties, such that we can manipulate points in <span class="math inline">\(\mathbb{R}^2\)</span> in a similar way as real numbers (by addition and multiplication).</p>
<p>We first reflect on the real numbers <span class="math inline">\(\mathbb{R}\)</span>. They are concrete, since they can be represented by points on the real axis. We can perform addition (<span class="math inline">\(+\)</span>) and multiplication (<span class="math inline">\(\cdot\)</span>) on them, that satisfy the following properties (referred to as <strong>field axioms</strong>). Let <span class="math inline">\(a, b, c \in \mathbb{R}\)</span>,</p>
<ol type="1">
<li><em>Associativity of addition:</em> <span class="math inline">\(a + (b + c) = (a + b) + c\)</span>.</li>
<li><em>Commutativity of addition:</em> <span class="math inline">\(a + b = b + a\)</span>.</li>
<li><em>Additive identity:</em> there exists a 'zero' element <span class="math inline">\(0\)</span> such that <span class="math inline">\(a + 0 = a\)</span>.<br />
</li>
<li><em>Additive inverse:</em> there is an element, denoted as <span class="math inline">\(-a\)</span>, such that <span class="math inline">\(a + (-a) = 0\)</span>.<br />
</li>
<li><em>Associativity of multiplication:</em> <span class="math inline">\(a \cdot (b \cdot c) = (a \cdot b) \cdot c\)</span>.</li>
<li><em>Commutativity of addition:</em> <span class="math inline">\(a \cdot b = b \cdot a\)</span>.</li>
<li><em>Multiplicative identity:</em> there exists a 'one' element <span class="math inline">\(1\)</span> such that <span class="math inline">\(a \cdot 1 = a\)</span>.</li>
<li><em>Multiplicative inverse:</em> there exists a element <span class="math inline">\(a^{-1}\)</span> for <span class="math inline">\(a \neq 0\)</span> such that <span class="math inline">\(a \cdot a^{-1} = 1\)</span>.<br />
</li>
<li><em>Distributivity:</em> <span class="math inline">\(a \cdot (b + c) = a \cdot b + a \cdot c\)</span>.</li>
</ol>
<p>Any set with two operations that satisfy similar properties as above is called a field. In the remainder of the blog, we show how to construct a field from <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
<p>We can view <span class="math inline">\(\mathbb{R}^2\)</span> as the set of vectors that originates from the origin. Hence, each element in <span class="math inline">\(\mathbb{R}^2\)</span> is considered as both a point and a vector. We will define addition and multiplication for <span class="math inline">\(\mathbb{R}^2\)</span>. For the moment, we will use <span class="math inline">\(\oplus\)</span> and <span class="math inline">\(\odot\)</span> to denote addition and multiplications in <span class="math inline">\(\mathbb{R}^2\)</span>, to distinguish the operations <span class="math inline">\(+\)</span> and <span class="math inline">\(\cdot\)</span> for <span class="math inline">\(\mathbb{R}\)</span>. Lastly, we show that the set system <span class="math inline">\((\mathbb{R}^2, \oplus, \odot)\)</span> constitutes a field.</p>
<p>In the remainder of the blog, when we refer to a vector, we refer to the one that originate from the origin.</p>
<h1 id="addition-oplus">Addition <span class="math inline">\(\oplus\)</span></h1>
<p>Addition in <span class="math inline">\(\mathbb{R}^2\)</span> is defined the same way as vector addition. When points are specified by Cartesian coordinates, it is easy to get a closed form expression of addition. Let <span class="math inline">\(a = (a_1, a_2), b = (b_1, b_2) \in \mathbb{R}^2\)</span>, then <span class="math display">\[
a \oplus b = (a_1 + b_1, a_2 + b_2). 
\]</span></p>
<!-- The figure below shows that addition is commutative.  -->
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/addition.png?raw=true" width="400" height="340" /></p>
</div>
<h1 id="multiplication-odot">Multiplication <span class="math inline">\(\odot\)</span></h1>
<p>When multiplying two vectors, their lengths multiply and their angles add. It is an operation that involves both dilation and rotation.</p>
<p>We investigate multiplication in the polar coordinates. For <span class="math inline">\(a \in \mathbb{R}^2\)</span>, let <span class="math inline">\(|a| \in \mathbb{R}\)</span> be its length and <span class="math inline">\(\theta_a\)</span> be its angle with respect to the <span class="math inline">\(x\)</span>-axis. We can write <span class="math inline">\(a\)</span> as: <span class="math display">\[
a = |a| \angle \theta_a.
\]</span></p>
<p>For <span class="math inline">\(a, b \in \mathbb{R}^2\)</span>, <span class="math display">\[
a \odot b \doteq (|a| \cdot |b|) \angle (\theta_{v_1} + \theta_{v_2} )
\]</span></p>
<p>The result is a vector (from the origin) with length <span class="math inline">\(|a| \cdot |b|\)</span> and angle <span class="math inline">\(\theta_a + \theta_b\)</span>.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/multiplication.png?raw=true" width="400" height="340" /></p>
</div>
<p><em>Remark: we have use different coordinate systems to get closed form formulas for addition and multiplication separately. This is immaterial. We need only to make sure both addition and multiplication are functions in <span class="math inline">\(\mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}^2\)</span>. Writing the closed form of a function in a coordinate system is just a specific way to describe it. For a specific function <span class="math inline">\(f\)</span> in <span class="math inline">\(\mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}^2\)</span>, it maps each pair of <span class="math inline">\(a, b \in \mathbb{R}^2 \times \mathbb{R}^2\)</span> to a unique point <span class="math inline">\(f(a, b) \in \mathbb{R}^2\)</span>, whether this function is described in Cartesian or polar coordinates. If we want, we can even describe it by language. For example, the sentence "<span class="math inline">\(f\)</span> takes every pair of <span class="math inline">\(\forall a, b \in \mathbb{R}^2\)</span> to the origin." defines a constant function.</em></p>
<h1 id="field-verification">Field Verification</h1>
<p>We need to verify that the set system <span class="math inline">\((\mathbb{R}^2, \oplus, \odot)\)</span> is a field.</p>
<h2 id="associativity-of-addition-a-oplus-b-oplus-c-a-oplus-b-oplus-c">1. <em>Associativity of addition:</em> <span class="math inline">\(a \oplus (b \oplus c) = (a \oplus b) \oplus c\)</span></h2>
<p>The figure below shows that addition is associative.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/addition-associativity.png?raw=true" width="400" height="340" /></p>
</div>
<p>Associativity is also easy to verify in Cartesian coordinates. Let <span class="math inline">\(a = (a_1, a_2), b = (b_1, b_2), c = (c_1, c_2)\)</span>, we get <span class="math display">\[
a \oplus ( b \oplus c) = (a_1 + b_1 + c_1, a_2 + b_2 + c_2) = (a \oplus b) \oplus c.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="commutativity-of-addition-a-oplus-b-b-oplus-a">2. <em>Commutativity of addition:</em> <span class="math inline">\(a \oplus b = b \oplus a\)</span></h2>
<p>This is verified when we define addition or we can check it easily in Cartesian coordinate system.</p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="additive-identity-exists-vec-0-s.t.-a-oplus-vec-0-a">3. <em>Additive identity:</em> <span class="math inline">\(\exists\)</span> <span class="math inline">\(\vec 0\)</span>, s.t. <span class="math inline">\(a \oplus \vec 0 = a\)</span></h2>
<p><span class="math inline">\((0, 0) \in \mathbb{R}^2\)</span> serves as the zero element.</p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="additive-inverse-exists--a-in-mathbbr2-s.t.-a-oplus--a-vec-0">4. <em>Additive inverse:</em> <span class="math inline">\(\exists (-a) \in \mathbb{R}^2\)</span>, s.t. <span class="math inline">\(a \oplus (-a) = \vec 0\)</span></h2>
<p>Let <span class="math inline">\((-a)\)</span> be the vector that is symmetric to <span class="math inline">\(a\)</span> with respect to the origin. Then <span class="math inline">\(a \oplus (-a) = 0\)</span>. Indeed, if <span class="math inline">\(a = (a_1, a_2)\)</span>, <span class="math display">\[
(-a) = (-a_1, -a_2).
\]</span></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/inverse.png?raw=true" width="400" height="340" /></p>
</div>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="associativity-of-multiplication-a-odot-b-odot-c-a-odot-b-odot-c">5. <em>Associativity of multiplication:</em> <span class="math inline">\(a \odot (b \odot c) = (a \odot b) \odot c\)</span></h2>
<p>It is more convenient to check the properties involved multiplication via polar coordinates.</p>
<p><span class="math display">\[
a \odot b \odot c = ( |a| \cdot |b| \cdot |c| ) \angle (\theta_a + \theta_b + \theta_c ) = a\odot (b \odot c).
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="commutativity-of-addition-a-odot-b-b-odot-a">6. <em>Commutativity of addition:</em> <span class="math inline">\(a \odot b = b \odot a\)</span></h2>
<p>Similarly, <span class="math display">\[
a \odot b = (|a| \cdot |b|) \angle (\theta_a + \theta_b ) = (|b| \cdot |a|) \angle (\theta_a + \theta_b ) = b \odot a. 
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="multiplicative-identity-exists-vec-1-s.t.-a-odot-vec-1-a">7. <em>Multiplicative identity:</em> <span class="math inline">\(\exists \vec 1\)</span> s.t. <span class="math inline">\(a \odot \vec 1 = a\)</span></h2>
<p><span class="math inline">\((1, 0) = |1|\angle 0\)</span> is the 'one' vector.</p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="multiplicative-inverse-if-a-neq-vec-0-exists-a-1-s.t.-a-odot-a-1-vec-1">8. <em>Multiplicative inverse:</em> If <span class="math inline">\(a \neq \vec 0\)</span>, <span class="math inline">\(\exists a^{-1}\)</span>, s.t. <span class="math inline">\(a \odot a^{-1} = \vec 1\)</span></h2>
<p>The multiplicative inverse is given by <span class="math display">\[
a^{-1} = \frac{1}{|a|} \angle (-\theta_a).
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="distributivity-a-odot-b-oplus-c-a-odot-b-oplus-a-odot-c">9. <em>Distributivity:</em> <span class="math inline">\(a \odot (b \oplus c) = a \odot b \oplus a \odot c\)</span></h2>
<p>Distributivity involves both addition and multiplication. We prove it geometrically.</p>
<p>Let <span class="math inline">\(w = b \oplus c\)</span>.</p>
<p>First suppose <span class="math inline">\(|a| = 1\)</span>. In such case, <span class="math inline">\(a = |1| \angle \theta_a\)</span> and <span class="math display">\[
w \odot a = |w| \angle (\theta_w + \theta_a)
\]</span></p>
<p>Multiplying <span class="math inline">\(w\)</span> by <span class="math inline">\(a\)</span> is equivalent to rotating it by an angle of <span class="math inline">\(\theta_a\)</span>. Let <span class="math inline">\(w&#39;, b&#39;, c&#39;\)</span> be the vectors obtained by rotating <span class="math inline">\(b, c, w\)</span> by angle <span class="math inline">\(\theta_a\)</span>. Then <span class="math display">\[
w&#39; = a \odot w, \quad b&#39; = a \odot b, \quad c&#39; = a \odot c.
\]</span></p>
<p>The figure belows show that <span class="math display">\[
w&#39; = b&#39; \oplus c&#39; \implies a \odot w = a \odot b \oplus a \odot c.
\]</span></p>
<p>In case <span class="math inline">\(|a| &gt; 1\)</span>, <span class="math inline">\(|w&#39;|\)</span>, <span class="math inline">\(|b&#39;|\)</span> and <span class="math inline">\(|c&#39;|\)</span> are magnified by a factor of <span class="math inline">\(|a|\)</span> and the same conclusion apply.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/ComplexNumber/distributivity.png?raw=true" width="600" height="340" /></p>
</div>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="conclusion">Conclusion</h1>
<p>We have shown that <span class="math inline">\((\mathbb{R}^2, \oplus, \odot)\)</span> is indeed a field. For convenience, we use <span class="math inline">\(\mathbb{C}\)</span> to denote this set system. If we define <span class="math display">\[
\vec j = (1, 0), \qquad \vec i = (0, 1),
\]</span></p>
<p>Hence, <span class="math inline">\(\vec j = \vec 1\)</span> and</p>
<ol type="1">
<li><span class="math inline">\(\vec j \odot \vec j = \vec 1\)</span>,</li>
<li><span class="math inline">\(\vec i \odot \vec j = \vec j \odot \vec i = \vec i\)</span>,</li>
<li><span class="math inline">\(\vec i \odot \vec i = |1| \angle ( \pi / 2 + \pi / 2) = (-1, 0) = -\vec j\)</span>.</li>
</ol>
<p>For <span class="math inline">\(a = (a_1, a_2), b = (b_1, b_2) \in \mathbb{R}^2\)</span>, we can write <span class="math display">\[
a = a_1 \vec j \oplus a_2 \vec i, \qquad b = b_1 \vec j \oplus b_2 \vec i, 
\]</span></p>
<p>and <span class="math display">\[
    a \oplus b = (a_1 + b_1) \vec j \oplus (a_2 + b_2) \vec i.
\]</span></p>
<p>To derive a formula for multiplication, we use the <em>distributivity</em> property of the two operations <span class="math display">\[
\begin{aligned}
    a \odot b 
        &amp;= a \odot ( b_1 \vec j \oplus b_2 \vec i ) \\
        &amp;= a \odot (b_1 \vec j) \oplus a \odot (b_2 \vec i) \\
        &amp;= (a_1 \vec j \oplus a_2 \vec i) \odot (b_1 \vec j) \oplus (a_1 \vec j \oplus a_2 \vec i) \odot (b_2 \vec i) \\
        &amp;=  (a_1 \cdot b_1) (\vec j \odot \vec j)  \oplus (a_2 \cdot b_1) (\vec i \odot \vec j) \oplus (a_1 \cdot b_2) (\vec j \odot \vec i) \oplus (a_2 \cdot b_2)  (\vec i \odot \vec i) \\
        &amp;=  (a_1 \cdot b_1) \vec j  \oplus (a_2 \cdot b_1) \vec i \oplus (a_1 \cdot b_2) \vec i \oplus (a_2 \cdot b_2)  ( - \vec j ) \\
        &amp;=  (a_1 \cdot b_1 - a_2 \cdot b_2) \vec j  \oplus (a_2 \cdot b_1 + a_1 \cdot b_2) \vec i.  \\
\end{aligned}
\]</span></p>
<p>We are almost done. We now further simplify the notations. As <span class="math inline">\(\vec j\)</span> plays the role of <span class="math inline">\(\vec 1\)</span>, we can omit it. For <span class="math inline">\(a = (a_1, a_2)\)</span>, we use only <span class="math inline">\(\vec i\)</span> to distinguish its <span class="math inline">\(y\)</span>-coordinate from the <span class="math inline">\(x\)</span>-coordinate: <span class="math display">\[
a = a_1 \oplus a_2 \vec i. 
\]</span></p>
<p>Addition and multiplication can be performed correctly in this representation:</p>
<ol type="1">
<li><span class="math inline">\(a \oplus b = a_1 \oplus a_2 \vec i \oplus b_1 \oplus b_2 \vec i = (a_1 + b_1) \oplus (a_2 + b_2) \vec i\)</span>.<br />
</li>
<li><span class="math display">\[
\begin{aligned}
    a \odot b 
        &amp;= a \odot ( b_1  \oplus b_2 \vec i ) \\
        &amp;= a \odot b_1 \oplus a \odot (b_2 \vec i) \\
        &amp;= (a_1  \oplus a_2 \vec i) \odot b_1 \oplus (a_1  \oplus a_2 \vec i) \odot (b_2 \vec i) \\
        &amp;=  (a_1 \cdot b_1) \oplus (a_2 \cdot b_1) \vec i \oplus (a_1 \cdot b_2) \vec i \oplus (a_2 \cdot b_2)  (\vec i \odot \vec i) \\
        &amp;=  (a_1 \cdot b_1)   \oplus (a_2 \cdot b_1) \vec i \oplus (a_1 \cdot b_2) \vec i \oplus (- a_2 \cdot b_2) \\
        &amp;=  (a_1 \cdot b_1 - a_2 \cdot b_2)   \oplus (a_2 \cdot b_1 + a_1 \cdot b_2) \vec i.  \\
\end{aligned}
  \]</span></li>
</ol>
<p>Finally, observe that if we restrict <span class="math inline">\(\oplus\)</span> and <span class="math inline">\(\odot\)</span> to the points on the <span class="math inline">\(x\)</span>-axis, they perform similarly as <span class="math inline">\(+\)</span> and <span class="math inline">\(\cdot\)</span> do for <span class="math inline">\(\mathbb{R}\)</span>. If we do not distinguish a number <span class="math inline">\((a_1, 0) \in \mathbb{R}^2\)</span> and <span class="math inline">\(a_1 \in \mathbb{R}\)</span>, replace <span class="math inline">\(\oplus\)</span> and <span class="math inline">\(\odot\)</span> with <span class="math inline">\(+\)</span> and <span class="math inline">\(\cdot\)</span>, we get exactly the same formula for standard complex number addition and multiplication: <span class="math inline">\(\forall a, b \in \mathbb{R}^2\)</span>,</p>
<ol type="1">
<li><span class="math inline">\(a + b = (a_1 + b_1) + (a_2 + b_2) \vec i\)</span>,</li>
<li><span class="math inline">\(a \cdot b = (a_1 + a_2 \vec i) \cdot (b_1 + b_2 \vec i) = (a_1 \cdot b_1 - a_2 \cdot b_2) + (a_1 \cdot b_2 + a_2 \cdot b_1) \vec i\)</span>.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/10/Stirling-s-Approximation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/10/Stirling-s-Approximation/" class="post-title-link" itemprop="url">Stirling's Approximation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-10 10:22:35" itemprop="dateCreated datePublished" datetime="2020-12-10T10:22:35+11:00">2020-12-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-01 12:40:39" itemprop="dateModified" datetime="2021-01-01T12:40:39+11:00">2021-01-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The Stirling's approximation [1] states that <span class="math inline">\(\forall n \in \mathbb{N}^+\)</span>, <span class="math display">\[
\sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ \frac{1}{ 12n + 1 } } \le n! \le \sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ \frac{1}{ 12n } }. 
\]</span></p>
<p>As <span class="math inline">\(\forall x \in R, e^x \ge 1 + x\)</span>, and for <span class="math inline">\(0 &lt; x &lt; 0.5\)</span>, <span class="math inline">\(e^x \le 1 + 2 x\)</span>, we get <span class="math inline">\(\forall n \in \mathbb{N}^+\)</span>, <span class="math display">\[
\sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 +  \frac{1}{ 12n + 1 } \right) \le n! \le \sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 +  \frac{1}{ 6n } \right).
\]</span></p>
<p>Asymptotically, <span class="math display">\[
n! = \sqrt{2 \pi} \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 +  \Theta \left( \frac{1}{n} \right) \right). 
\]</span></p>
<p>In the remainder of the blog, we try to approach this estimation incrementally. First we have two trivial facts.</p>
<h3 id="n-le-nn.">1. <span class="math inline">\(n! \le n^n\)</span>.</h3>
<h3 id="n-ge-n-2n-2.">2. <span class="math inline">\(n! \ge (n / 2)^{n / 2}\)</span>.</h3>
<p><em>Proof:</em> Clearly this holds for even numbers. When <span class="math inline">\(n\)</span> is odd, <span class="math inline">\(\lceil n / 2 \rceil = (n + 1) / 2\)</span>. There are <span class="math inline">\((n + 1) / 2\)</span> numbers in <span class="math inline">\([n]\)</span> that are <span class="math inline">\(\ge (n + 1) / 2\)</span>. <span class="math display">\[
   n! \ge \prod_{i = (n + 1) / 2)}^n i \ge \big( (n + 1) / 2) \big)^{ (n + 1) / 2) } \ge (n / 2)^{n / 2}
   \]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p>Somewhat unexpectedly, we can combine inequality (1) and (2) to obtain the asymptotical trend of <span class="math inline">\(\log n!\)</span> up to some constant: <span class="math display">\[
(n / 2) \log_2 n - (n / 2) \le \log_2 n! \le n \log_2 n.
\]</span></p>
<p>This implies that <span class="math display">\[
\log_2 n! = \Theta( n \log n).
\]</span></p>
<p>We proceed to get even sharper bounds.</p>
<h3 id="n-ge-fracnn-expn-n-en.">3. <span class="math inline">\(n! \ge \frac{n^n}{ \exp(n) } = (n / e)^n\)</span>.</h3>
<p><em>Proof:</em> Using the Taylor expansion for <span class="math inline">\(\exp(x)\)</span> for <span class="math inline">\(\forall x \in \mathbb{R}\)</span>, we get <span class="math display">\[
    \exp(x) = 1 + x + \frac{x^2}{2!} + ... + \frac{x^n}{n!} + ... 
\]</span></p>
<p>Keeping only the <span class="math inline">\(n\)</span>-th term of the expansion, we have <span class="math display">\[
    n! \ge \frac{x^n}{\exp(x) }. 
\]</span></p>
<p>Taking <span class="math inline">\(x = n\)</span> maximizes the RHS.</p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="n-ge-e-n-en.">4. <span class="math inline">\(n! \ge e ( n / e)^n\)</span>.</h3>
<p><em>Proof:</em> Using the Taylor expansion for <span class="math inline">\(\exp(x)\)</span> for <span class="math inline">\(\forall x \in \mathbb{R}\)</span>, we get <span class="math display">\[
    \exp(x) = 1 + x + \frac{x^2}{2!} + ... + \frac{x^n}{n!} + ... 
\]</span></p>
<p>Setting <span class="math inline">\(x = n\)</span>, we get <span class="math display">\[
\exp(n) \ge \frac{n^{n - 1} }{ (n - 1)!} + \frac{ n^n }{n !} = 2 \frac{ n^n }{n !},
\]</span> which finishes our proof.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Next we resort to calculus to get finer estimation.</p>
<h3 id="n-ge-e-n-en.-1">5. <span class="math inline">\(n! \ge e ( n / e)^n\)</span>.</h3>
<p><em>Proof:</em> As the figure below shows, <span class="math inline">\(\sum_{i \in [n] } \ln i\)</span> is the sum of areas of a set of rectangles and it upper bounds the area under the curve <span class="math inline">\(y = \log t\)</span>: <span class="math display">\[
\sum_{i \in [n]} \ln i \ge \int_1^n \ln t \ dt = t \ln t \mid_1^n - \int_1^n 1 \ dt = n \ln n - n + 1. 
\]</span></p>
<p>Hence, <span class="math inline">\(n! \ge e (n / e)^n\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/StirlingApproximation/LowerBound.png?raw=true" /></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="n-le-e-sqrtn-n-en.">6. <span class="math inline">\(n! \le e \sqrt{n} (n / e)^n\)</span>.</h3>
<p><em>Proof:</em> For a rectangle, its area can be decomposed into the curvilinear area under <span class="math inline">\(y = \ln t\)</span> and a curvy triangle. To obtain an upper bound for <span class="math inline">\(\sum_{i \in [n] } \ln i\)</span>, we want to upper bound the areas of the curvy triangles.</p>
<p>How are we going to do that? Image that we put a wall at <span class="math inline">\(t = 1\)</span>, and line the curvy triangles up against the wall. By the figure, they take up at most half the area of the rectangle with width <span class="math inline">\(1\)</span> and height <span class="math inline">\(\ln n\)</span>. Then</p>
<p><span class="math display">\[
\sum_{i \in [n]} \ln i \le \int_1^n \ln t \ dt + \frac{1}{2} \ln n = n \ln n - n + 1 + \frac{1}{2} \ln n. 
\]</span></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/StirlingApproximation/UpperBound1.png?raw=true" /></p>
<p><span class="math inline">\(\square\)</span></p>
<p>By inequality (4) and (5), we get <span class="math display">\[
e ( n / e)^n \le n! \le e \sqrt{n} (n / e)^n.
\]</span></p>
<p>So <span class="math display">\[
n! = \Theta ( (n / e)^n ). 
\]</span></p>
<p>Indeed the approximation of <span class="math inline">\(e \sqrt n (n / e)^n\)</span> is quite sharp. If we compute its ratio with the near accurate estimation <span class="math inline">\(\sqrt{2 \pi} \sqrt n (n / e)^n\)</span>, we see that the ratio is very close to 1: <span class="math display">\[
\frac{ e \sqrt n (n / e)^n  }{ \sqrt{2 \pi} \sqrt n (n / e)^n } = \frac{e }{\sqrt{ 2 \pi} } \approx 1.084.
\]</span></p>
<p>Therefore, when <span class="math inline">\(n\)</span> is large, <span class="math inline">\(e \sqrt n (n / e)^n\)</span> is within <span class="math inline">\(10\%\)</span> of the real value of <span class="math inline">\(n!\)</span>.</p>
<h3 id="n-le-e-sqrtn-n-en.-1">7. <span class="math inline">\(n! \le e \sqrt{n} (n / e)^n\)</span>.</h3>
<p>The same inequality can be obtained in the following manner.</p>
<p><em>Proof:</em> We lower bound the integral by the areas of trapezoids. The trapezoid between <span class="math inline">\(t\)</span> and <span class="math inline">\(t + 1\)</span> has base heights <span class="math inline">\(\ln t\)</span> and <span class="math inline">\(\ln (t + 1)\)</span>, and area <span class="math inline">\(\frac{1}{2} (\ln t + \ln (t + 1) )\)</span>. Hence,</p>
<p><span class="math display">\[
\begin{aligned}
    \sum_{i = 1}^n \ln i &amp;= \frac{1}{2} \sum_{t = 1}^{n - 1} (\ln t + \ln (t + 1) ) + \frac{1}{2} \ln n \\
    &amp;\le \int_1^n \ln t \ dt + \frac{1}{2} \ln n \\
    &amp;= n \ln n - n + 1 + \frac{1}{2} \ln n 
\end{aligned}
\]</span> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/StirlingApproximation/UpperBound0.png?raw=true" /></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="n-in-e11-12-sqrtn-left-fracne-rightn-e-1-12n-1-e12-13-sqrtn-left-fracne-rightn-e1-12-n">8. <span class="math inline">\(n! \in [e^{11 / 12} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ 1 / (12n + 1)}, e^{12 / 13} \sqrt{n} \left( \frac{n}{e} \right)^n e^{1 / (12 n) }]\)</span></h3>
<p><em>Proof:</em> Let <span class="math display">\[
\begin{aligned}
    U_k &amp;= \int_k^{k + 1} \ln t \ dt, \\
    L_k &amp;= \frac{1}{2} \left(\ln k + \ln (k + 1) \right).
\end{aligned}
\]</span></p>
<p>The estimation error in the previous section is determined by the gap between <span class="math inline">\(U_k\)</span> and <span class="math inline">\(L_k\)</span>, for <span class="math inline">\(k \in [n - 1]\)</span>. To investigate it carefully, define <span class="math inline">\(\forall k \in [n - 1]\)</span>, <span class="math display">\[
    \epsilon_k \doteq U_k - L_k .
\]</span></p>
<p>Denote <span class="math inline">\(S = \sum_{i \in [n] } \ln i\)</span> and <span class="math inline">\(\hat S = n \ln n - n + 1 + \frac{1}{2} \ln n\)</span>. The derivation in the previous section shows <span class="math display">\[
\hat S - S = \sum_{i = 1}^{n - 1} \epsilon_k.  
\]</span></p>
<p>For a fixed <span class="math inline">\(k\)</span>, we can compute <span class="math inline">\(\epsilon_k\)</span> in closed form: <span class="math display">\[
\begin{aligned}
    \epsilon_k 
        &amp;= t \ln t \mid_k^{k + 1} - t \mid_k^{k + 1} - \frac{1}{2} \left( \ln k + \ln (k + 1) \right) \\
        &amp;= \ln \frac{ (k + 1)^{k + 1} }{ k^k } \cdot \frac{1}{ k^{\frac{1}{2} } (k + 1)^{\frac{1}{2} } } - 1 \\
        &amp;= \ln \left( \frac{k + 1}{k } \right)^{ k + \frac{1}{2} } - 1 \\
        &amp;= \left(  \frac{2k + 1}{2} \right) \cdot \ln \left( \frac{k + 1}{k } \right)- 1 \\
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(x = \frac{1}{2k + 1}\)</span>. As <span class="math inline">\(k + 1 = \frac{1}{2}( (2k + 1) + 1)\)</span>, and <span class="math inline">\(k = \frac{1}{2}( (2k + 1) - 1)\)</span>, we can write <span class="math display">\[
    \epsilon_k = \left(  \frac{2k + 1}{2} \right) \cdot \ln \left( \frac{1 + k }{k } \right)- 1 = \frac{1}{2 x} \ln \frac{1 + x}{1 - x} - 1. 
\]</span></p>
<p>Using the Taylor's expansions for <span class="math inline">\(y = \ln(1 -x)\)</span> and <span class="math inline">\(y = \ln(1 + x)\)</span>, we have <span class="math display">\[
\ln(1 - x) = -(x + \frac{x^2}{2} + \frac{x^3}{3} + ...) \\
\ln(1 + x) = -(-x + \frac{x^2}{2} - \frac{x^3}{3} + ...) \\
\]</span></p>
<p>and <span class="math display">\[
\ln \frac{1 + x}{1 - x} = 2 (x + \frac{x^3}{3} + \frac{x^5}{5} + \frac{x^7}{7} + ... ). 
\]</span></p>
<p>It follows that <span class="math display">\[
\epsilon_k = \frac{x^2}{3} + \frac{x^4}{5} + \frac{x^6}{7} + ... 
\]</span></p>
<p>We can upper bound <span class="math inline">\(\epsilon_k\)</span> by <span class="math display">\[
\begin{aligned}
    \epsilon_k 
        &amp;\le \frac{x^2}{3} ( 1 + x^2 +x^4 + ... ) \\
        &amp;= \frac{x^2}{3} \frac{1}{1 - x^2} \\
        &amp;= \frac{1}{3} \frac{1}{ (2k + 1)^2 - 1} \\
        &amp;= \frac{1}{12} \left(\frac{1}{k } - \frac{1}{k + 1} \right).
\end{aligned}
\]</span></p>
<p>Also, we can lower bound <span class="math inline">\(\epsilon_k\)</span> by <span class="math display">\[
\begin{aligned}
    \epsilon_k 
        &amp;\ge \frac{x^2}{3} ( 1 + x^2 / 3 +x^4 / 3^2 + ... ) \\
        &amp;= \frac{x^2}{3} \frac{1}{1 - x^2 / 3} \\
        &amp;= \frac{1}{ 3(2k + 1)^2 - 1} \\
        &amp;= \frac{1}{ 12(k^2 + k + \frac{2}{12} ) } \\
        &amp;\ge \frac{1}{ 12(k + 1 + \frac{1}{12}) ( k + \frac{1}{12} ) } \\
        &amp;= \frac{1}{12} \left(\frac{1}{k + \frac{1}{12} } - \frac{1}{k + 1 + \frac{1}{12} } \right). 
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(A = \sum_{k = 1}^\infty \epsilon_k\)</span> and <span class="math inline">\(R = \sum_{k = n}^\infty \epsilon_k\)</span>. Then <span class="math display">\[
\begin{aligned}
    \hat S - S &amp;= A - R \\
    \implies S &amp;= \hat S - A + R \\
    \implies S &amp;= n \ln n - n + 1 + \frac{1}{2} \ln n - A + R \\
    \implies n! &amp;= e^{1 - A} \sqrt{n} \left( \frac{n}{e} \right)^n e^R.
\end{aligned}
\]</span></p>
<p>By previous inequalities, we know <span class="math display">\[
    \begin{aligned}
        A &amp;\ge \frac{1}{12} \sum_{k = 1}^\infty \left( \frac{1}{k + \frac{1}{12} } - \frac{1}{k + 1 + \frac{1}{12} } \right) = \frac{1}{12} \frac{1}{1 + \frac{1}{12} } = \frac{1}{13} \\
        A &amp;\le \frac{1}{12} \sum_{k = 1}^\infty \left(\frac{1}{k } - \frac{1}{k + 1} \right) = \frac{1}{12}.
    \end{aligned}
\]</span></p>
<p>Similarly, we can get <span class="math inline">\(\frac{1}{12n + 1 } = \frac{1}{12} \frac{1}{n + \frac{1}{12} } \le R \le \frac{1}{12n}\)</span>. To finish our proof, we have <span class="math display">\[
n! \in [e^{11 / 12} \sqrt{n} \left( \frac{n}{e} \right)^n e^{ 1 / (12n + 1)}, e^{12 / 13} \sqrt{n} \left( \frac{n}{e} \right)^n e^{1 / (12 n) }]
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] H. Robbins, “A Remark on Stirling’s Formula,” The American Mathematical Monthly, vol. 62, no. 1, pp. 26–29, 1955</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/09/Inverting-Functions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/09/Inverting-Functions/" class="post-title-link" itemprop="url">Inverting Functions</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-12-09 10:21:46 / Modified: 11:07:15" itemprop="dateCreated datePublished" datetime="2020-12-09T10:21:46+11:00">2020-12-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We come across the problem of inverting a function often in computer science. Instead of solving it by brute force, sometimes it would be much easier to do it with the help of <span class="math inline">\(\Theta( \cdot )\)</span>, if we only care about the asymptotic trend. See two examples below.</p>
<ol type="1">
<li>If <span class="math inline">\(t \ln t = n\)</span>, then <span class="math inline">\(t = \Theta( n / \ln n )\)</span>.</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
            &amp;\ln t + \ln \ln t = \ln n \\
\implies    &amp;\ln t = \Theta( \ln n) \\
\implies    &amp;t \Theta (\ln n) = n \\
\implies    &amp;t = \Theta \left( \frac{n}{\ln n} \right)
\end{aligned}
\]</span></p>
<ol start="2" type="1">
<li>If <span class="math inline">\(t^2 \ln t = n^3\)</span>, then <span class="math inline">\(t = \Theta( n / \ln n )\)</span>. <span class="math display">\[
\begin{aligned}
         &amp;2\ln t + \ln \ln t = 3\ln n \\
\implies    &amp;\ln t = \Theta( \ln n) \\
\implies    &amp;t^2 \Theta (\ln n) = n^3 \\
\implies    &amp;t^2 = \Theta \left( \frac{n^3}{\ln n} \right) \\
\implies    &amp;t = \Theta \left( \sqrt \frac{n^3 }{\ln n} \right)
\end{aligned}
\]</span></li>
</ol>
<p><strong>Application</strong></p>
<p>Suppose there is an algorithm that has running time <span class="math inline">\(O(n^3 / t) + O(t \log t)\)</span>, for any <span class="math inline">\(t &gt; 0\)</span>. We want to choose a <span class="math inline">\(t\)</span> that minimizes the asymptotic time complexity of the algorithm.</p>
<p>As for any <span class="math inline">\(a, b &gt; 0\)</span>, <span class="math display">\[
\max \{a, b \} \le a + b \le 2 \cdot \max\{ a, b \}.
\]</span></p>
<p>It suffices to choose a <span class="math inline">\(t\)</span> that minimizes <span class="math inline">\(\max\{ n^3 / t, t \log t\}\)</span>. By the result of previous section, we can choose <span class="math display">\[
t = \Theta \left( \sqrt \frac{n^3}{\log n} \right), 
\]</span></p>
<p>and the time complexity is <span class="math display">\[
O(\sqrt{n^3 \log n}).
\]</span></p>
<h3 id="reference">Reference</h3>
<p>[1]. Ryan O'Donnell, Lecture 2, CS Theory Toolkit, CARNEGIE MELLON UNIVERSITY</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/27/The-Prime-Number-Theory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/27/The-Prime-Number-Theory/" class="post-title-link" itemprop="url">The Prime Number Theory</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-27 21:59:17" itemprop="dateCreated datePublished" datetime="2020-11-27T21:59:17+11:00">2020-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-01 23:33:18" itemprop="dateModified" datetime="2020-12-01T23:33:18+11:00">2020-12-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>For <span class="math inline">\(n \in \mathbb{N}^+\)</span>, define <span class="math inline">\(\pi(n)\)</span> be the number of prime numbers that are at most <span class="math inline">\(n\)</span>. The prime number theory states that <span class="math display">\[
\lim_{n} \pi(n) \frac{\ln n}{n} = 1. 
\]</span></p>
<p>We show here a weaker version of <span class="math display">\[
\frac{\pi(n)}{ n } = \Theta (\frac{1}{\log n}).
\]</span></p>
<p>That is, the density of the prime number in <span class="math inline">\([1, n]\)</span> is <span class="math inline">\(\Theta(1 / \log_2 n)\)</span>, which is inversely proportional to the number of bits required to represent <span class="math inline">\(n\)</span>.</p>
<blockquote>
<p>Theorem. <span class="math inline">\(n \in \mathbb{N}^+, n \ge 3\)</span>, it holds that<br />
<span class="math display">\[
\frac{1}{2 \log_2 n} \le \frac{\pi(n)}{n} \le  \frac{2}{\ln n}
\]</span></p>
</blockquote>
<p>For the first inequality, we will show that <span class="math inline">\(\forall k \in [n]\)</span>, <span class="math display">\[
    \binom{n}{ k} \le n^{ \pi(n) }. 
\]</span></p>
<p>Roughly speaking, the prime factorization of <span class="math inline">\(\binom{n}{k}\)</span> consists of at most <span class="math inline">\(\pi(n)\)</span> different primes <span class="math inline">\(\le n\)</span>. The inequality follows if we can prove that the highest power of a prime in the factorization is at most <span class="math inline">\(n\)</span>. We can choose <span class="math inline">\(k = \lfloor n / 2 \rfloor\)</span> to maximize the lower bound.</p>
<p>For the second inequality, we use <span class="math display">\[
    (k + 2)^{ \pi(2k + 1) - \pi(k + 1) } \le \binom{ 2k + 1}{ k }
\]</span> for <span class="math inline">\(k \in \mathbb{N}^+\)</span> and upper bound <span class="math inline">\(\binom{ 2k + 1}{ k }\)</span> by <span class="math inline">\(2^{2k}\)</span>. We only consider odd numbers <span class="math inline">\(2k + 1\)</span> here because <span class="math inline">\(\pi(2k + 2) = \pi(2k + 1), \forall k \in \mathbb{N}^+\)</span>.</p>
<h3 id="proof-of-the-theorem-part-1.">Proof of The Theorem, Part 1.</h3>
<p>We need a few lemmas for the proof.</p>
<p><strong><em>Definition.</em></strong> Given a prime number <span class="math inline">\(p\)</span> and <span class="math inline">\(n \in \mathbb{N}^+\)</span>, <span class="math inline">\(m_p(n)\)</span> is the number of large integer <span class="math inline">\(k\)</span> such that <span class="math inline">\(p^k\)</span> divides <span class="math inline">\(n\)</span>.</p>
<h4 id="lemma-1.">Lemma 1.</h4>
<blockquote>
<p><span class="math display">\[
m_p(n!) = \sum_{j = 1}^\infty \lfloor \frac{n}{p^j} \rfloor
\]</span></p>
</blockquote>
<p><em>Proof.</em> We count the number of time <span class="math inline">\(p\)</span> divides <span class="math inline">\(n!\)</span>. Each multiple of <span class="math inline">\(p\)</span> that is in <span class="math inline">\([n]\)</span> contributes a factor. Each multiple of <span class="math inline">\(p^2\)</span> that is in <span class="math inline">\([n]\)</span> contributes another factor, and so on.</p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="lemma-2.">Lemma 2.</h4>
<blockquote>
<p>Let <span class="math inline">\(a, b \in \mathbb{N}^+\)</span>, <span class="math inline">\(a &gt; b &gt; 0\)</span>. Given a prime <span class="math inline">\(p\)</span> and an integer <span class="math inline">\(k \in \mathbb{N}^+\)</span>, if <span class="math inline">\(p^k\)</span> divided <span class="math inline">\(\binom{a}{b}\)</span>, then <span class="math inline">\(p^k \le a\)</span>.</p>
</blockquote>
<p><em>Proof.</em> Let <span class="math inline">\(k&#39; = m_p(\binom{a}{b})\)</span>. It suffices to show that <span class="math inline">\(p^{k&#39;} \le a\)</span>. <span class="math display">\[
\begin{aligned}
    k&#39; 
        &amp;= m_p( \frac{a!}{b! (a - b)!} ) \\
        &amp;= m_p(a!) - m_p(b!) - m_p( (a - b)! ) \\
        &amp;= \sum_{j = 1}^\infty \left( \lfloor \frac{a}{p^j} \rfloor - \lfloor \frac{b}{p^j} \rfloor - \lfloor \frac{b - a}{p^j} \rfloor \right)
\end{aligned}
\]</span></p>
<p>Each summand is either 1 or 0. Further, when <span class="math inline">\(p^j &gt; a\)</span> (i.e., <span class="math inline">\(j &gt; \log_p a\)</span>), the summand is 0. It concludes that <span class="math inline">\(k&#39; \le \log_p a\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="lemma-3.">Lemma 3.</h4>
<blockquote>
<p><span class="math display">\[
\binom{n}{ \lfloor n / 2 \rfloor} \le n^{ \pi(n) }
\]</span> <em>Proof.</em> Consider the unique prime factorization of <span class="math inline">\(\binom{n }{ \lfloor n / 2 \rfloor}\)</span>, <span class="math display">\[
\binom{n}{ \lfloor n / 2 \rfloor } = p_1^{k_1} p_2^{k_2} ... p_t^{k_t},
\]</span> where <span class="math inline">\(p_i \neq p_j\)</span>, for <span class="math inline">\(1 \le i &lt; j \le t\)</span>. By Lemma 2 we have <span class="math inline">\(\forall i \in [t]\)</span>, <span class="math inline">\(p_i^{k_i} \le n\)</span> and <span class="math inline">\(p_i\)</span> is a prime number at most <span class="math inline">\(n\)</span>. Hence <span class="math inline">\(t \le \pi(n)\)</span>. It follows that <span class="math display">\[
\binom{n}{ \lfloor n / 2 \rfloor } = p_1^{k_1} p_2^{k_2} ... p_t^{k_t} \le n^t \le n^{ \pi(n) }. 
\]</span></p>
</blockquote>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="lemma-4.">Lemma 4.</h4>
<blockquote>
<p><span class="math display">\[
\frac{1}{n + 1} 2^{n} \le \binom{n}{ \lfloor n / 2 \rfloor } \le n^{\pi(n)}
\]</span></p>
</blockquote>
<p><em>Proof.</em> Given <span class="math inline">\(n \in \mathbb{N}^+\)</span>, as <span class="math inline">\(\binom{n}{k}\)</span> is maximized when <span class="math inline">\(k = \lfloor n / 2 \rfloor\)</span> for (<span class="math inline">\(k =0, 1, 2, ..., n\)</span>), it holds that <span class="math display">\[
1 = (\frac{1}{2} + \frac{1}{2})^n \le (n + 1) (\frac{1}{2})^n \binom{n}{ \lfloor n / 2 \rfloor }. 
\]</span></p>
<p>This establishes the lower bound.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>By Lemma 4, <span class="math display">\[
\pi(n) \ge \frac{n  - \log (n + 1) }{\log n} \ge \frac{1}{2} \frac{n}{\log n}.
\]</span></p>
<p>The final inequality holds when <span class="math inline">\(\log (n + 1) \le 0.5 n\)</span>. It suffices to take <span class="math inline">\(n \ge 7\)</span>, as <span class="math inline">\(\log (7 + 1) = 3 &lt; 3.5\)</span> but <span class="math inline">\(\log (3 + 1) = 2 &gt; 1.5\)</span>. When <span class="math inline">\(n &lt; 7\)</span>, we can check manually that <span class="math inline">\(\pi(n) / n \ge 0.5 / \log n\)</span> holds.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="proof-of-the-theorem-part-2.">Proof of The Theorem, Part 2.</h3>
<h4 id="lemma-5.-for-k-in-mathbbn">Lemma 5. For <span class="math inline">\(k \in \mathbb{N}^+\)</span>,</h4>
<blockquote>
<p><span class="math display">\[
\binom{2k + 1}{ k } \ge \prod_{k + 2 \le p \le 2k + 1 \wedge p \text{ is prime}} p
\]</span></p>
</blockquote>
<p><em>Proof.</em> First, <span class="math display">\[
\binom{2k + 1}{k} = \frac{(2k + 1) (2k) (2k - 1) ... (k + 2)}{ k (k - 1) ... 1}.
\]</span></p>
<p>For a prime <span class="math inline">\(p \in [k + 2, 2k + 1]\)</span>, it divides the numerator but not the denominator.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>We prove that <span class="math inline">\(\pi(n) \le 2 n / \ln n\)</span> by induction. When <span class="math inline">\(n &gt; 2\)</span> is even, <span class="math inline">\(\pi(n) = \pi(n - 1)\)</span>. As <span class="math inline">\(2 n / \ln n\)</span> increases with <span class="math inline">\(n\)</span>, it suffices to prove this for odd numbers of <span class="math inline">\(n\)</span>. Suppose that <span class="math inline">\(n = 2k + 1\)</span> for some <span class="math inline">\(k \in \mathbb{N}^+\)</span>, then</p>
<p><span class="math display">\[
 (k + 2)^{\pi(2k + 1) - \pi(k + 1) }  \le \prod_{k + 2 \le p \le 2k + 1 \wedge p \text{ is prime}} p \le \binom{2k + 1}{k}. 
\]</span></p>
<!-- Observe that 
$$
\binom{2k + 1}{k} = \binom{2k + 1}{k - 1} \frac{k + 2}{k} = \binom{2k + 1}{k - 2} \frac{(k + 2) (k + 3)}{k(k - 1)}.
$$

When $k \ge 2$, $(k + 2) / k \le 2$. Solving 
$$
\frac{(k + 2) (k + 3)}{k(k - 1)} \le 2
$$
gives 
$$
k^2 + 5k + 6 \le 2k^2 - 2k \longleftrightarrow k^2 - 7k - 6 \ge 0.
$$
It suffices to take $k \ge 8$. 

So when $k \ge 8$, 
$$
\begin{aligned}
    \binom{2k + 1}{k} 
        &\le \frac{1}{4} \left( \binom{2k + 1}{k - 2}+ \binom{2k + 1}{k - 1} + \binom{2k + 1}{k} + \binom{2k + 1}{k + 1} + \binom{2k + 1}{k + 2} + \binom{2k + 1}{k + 3} \right) \\
        &\le \frac{1}{4} \sum_{j = 0}^{2k + 1} \binom{2k + 1}{j}\\
        &\le \frac{1}{4} \cdot 2^{2k + 1} = 2^{2k - 1}
\end{aligned}
$$ -->
<p>Further <span class="math display">\[
\binom{2k + 1}{k} = \frac{1}{2} \left( \binom{2k + 1}{k} + \binom{2k + 1}{k + 1}\right) \le \frac{1}{2} \cdot 2^{2k + 1} = 2^{2k}.
\]</span></p>
<p>Therefore, <span class="math display">\[
\pi(2k + 1) - \pi(k + 1) \le \frac{ 2k \cdot \ln 2 }{ \ln (k + 2)}.
\]</span></p>
<p>By induction on <span class="math inline">\(\pi(k + 1)\)</span>, we arrive at <span class="math display">\[
\pi(2k + 1) \le \frac{ 2k \cdot \ln 2 }{ \ln (k + 2)} + 2 \frac{ k + 1}{\ln (k + 1)} &lt; \frac{ (n - 1) \cdot \ln 2 + n + 1 }{\ln (k + 1) } &lt; \frac{  (1 + \ln 2) \cdot n + 1}{\ln (n / 2) }.
\]</span></p>
<p>In order that <span class="math display">\[
\frac{  (1 + \ln 2) \cdot n + 1}{\ln (n / 2) } \le \frac{2 n }{ \ln n},
\]</span></p>
<p>we need <span class="math display">\[
(1 + \ln 2) \cdot n \cdot \ln n + \ln n \le 2 n \cdot \ln n - 2 n \cdot \ln 2 \\
\Longleftrightarrow (2 \ln 2) \cdot n + \ln n \le (1 - \ln 2) \cdot n \cdot \ln n. 
\]</span></p>
<p>This holds when <span class="math inline">\(n \ge 107\)</span>. It is left to verify manually that <span class="math inline">\(\pi \le 2 n / \ln n\)</span> holds for all <span class="math inline">\(n &lt; 107\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/The-Prime-Number-Theory.PNG?raw=true" /></p>
<h3 id="reference">Reference</h3>
<p>[1] Jan-Hendrik, "Chapter 1: Introduction to prime number theory", ANALYTIC NUMBER THEORY</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/25/SmallDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/25/SmallDB/" class="post-title-link" itemprop="url">SmallDB</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-25 22:34:48" itemprop="dateCreated datePublished" datetime="2020-11-25T22:34:48+11:00">2020-11-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-26 11:53:10" itemprop="dateModified" datetime="2020-11-26T11:53:10+11:00">2020-11-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(U = \{x_1, x_2, ... , x_n \} \subset \mathcal{X}^n\)</span> be a set of <span class="math inline">\(n\)</span> points from a domain <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(Q = \{ f_1, f_2, ... \mid f_i : \mathcal{X} \rightarrow [0, 1] \}\)</span> be a finite set of queries (functions). For any <span class="math inline">\(f \in Q\)</span> and finite set <span class="math inline">\(S\)</span> with domain <span class="math inline">\(\mathcal{X}\)</span>, define <span class="math display">\[
f(S) \doteq \frac{1}{ |S| }\sum_{x \in S} f(x)
\]</span></p>
<p>as the average over the values <span class="math inline">\(\{ f(x) : x \in S \}\)</span>. Our goal is to find a subset <span class="math inline">\(S\)</span> with size <span class="math display">\[
O( \frac{\log |Q| }{\lambda^2})
\]</span></p>
<p>in a <span class="math inline">\((2 \epsilon, 0)\)</span> differentially private manner, such that with probability at least <span class="math inline">\(1 - \delta\)</span>, it holds simultaneously <span class="math inline">\(\forall f \in Q\)</span>, <span class="math display">\[
|f(S) - f(U) | \le \lambda + \frac{\frac{ }{} \log |Q| \log |\mathcal{X}|  } { \lambda^2 \epsilon n } + \frac{ \log (1 / \delta) }{ \epsilon n  }
\]</span></p>
<h3 id="existence-of-small-representative">Existence of Small Representative</h3>
<p>Let <span class="math inline">\(X\)</span> be a random element sampled uniformly from <span class="math inline">\(U\)</span>. Then for a fixed <span class="math inline">\(f \in Q\)</span>, <span class="math display">\[
\mathbb{E}[ f(X) ] = f(U)
\]</span></p>
<p>By Hoeffding, if we take <span class="math inline">\(m = \frac{\ln |Q|}{\lambda^2}\)</span> independent copies of <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(X_1, X_2, ..., X_m\)</span>, then <span class="math display">\[
\Pr[ |\frac{1}{m} \sum_{i \in [m]} X_i - f(U) | \ge \lambda ] \le 2 \exp(- 2m \lambda^2 ) = \frac{2}{e^{2 \ln |Q|}} = \frac{2}{|Q|^2} &lt; \frac{1}{ |Q| }.
\]</span></p>
<p>By union bound, this happens for some <span class="math inline">\(f \in Q\)</span> with probability less than 1. Therefore, if we enumerate over all sets <span class="math inline">\(S \subset \mathcal{X}^m\)</span> (which takes exponential time), then it is guaranteed that we can find one such that <span class="math display">\[
|f(S) - f(U) | \le \lambda. 
\]</span></p>
<p>This is tighter than the bound we would like to prove. The additional error is due to the privacy protection mechanism.</p>
<h3 id="release-representative-differential-privately">Release Representative Differential-Privately</h3>
<p>We can just release the <span class="math inline">\(S\)</span> blatantly.</p>
<h4 id="counter-example">Counter Example</h4>
<p>Suppose <span class="math inline">\(\mathcal{X} = \{0, 1\}\)</span>, and <span class="math inline">\(U = \{0, 0, ..., 0\} \subset \mathcal{X}^n\)</span>. Further, the set <span class="math inline">\(Q\)</span> contains two copies of the function <span class="math inline">\(f\)</span>, defined as <span class="math display">\[
f(0) = 0, f(1) = 1.
\]</span></p>
<p>Assume that <span class="math inline">\(\lambda = \sqrt{\frac{ 2 \ln 2 }{ n } }\)</span>. Then <span class="math inline">\(m = \frac{n \ln 2}{2 \ln 2} = n / 2\)</span>. Any <span class="math inline">\(S \subset \mathcal{X}^m\)</span> that contains no more than <span class="math display">\[
\lfloor m \sqrt{\frac{ 2 \ln 2 }{ n } } \rfloor = \lfloor \sqrt{\frac{ 2 \ln 2 }{ n } } \frac{n}{2} \rfloor = \lfloor \sqrt{\frac{ n \ln 2 }{ 2 } } \rfloor
\]</span> ones, will satisfies <span class="math display">\[
|f(S) - f(U) | \le \lambda. 
\]</span></p>
<p>Now, consider a neighboring set <span class="math inline">\(U&#39; = \{0, 0, ..., 1\}\)</span>, obtained by replacing the last zero with one in <span class="math inline">\(U\)</span>. Any <span class="math inline">\(S\)</span> with no more than <span class="math inline">\(\lfloor \sqrt{\frac{ n \ln 2 }{ 2 } } + 1 \rfloor\)</span> ones satisfies <span class="math inline">\(|f(S) - f(U&#39;) | \le \lambda\)</span>.</p>
<p>Consider the scenario where we know the underlying set is either <span class="math inline">\(U\)</span> or <span class="math inline">\(U&#39;\)</span>. If a set in <span class="math inline">\(\mathcal{X}^m\)</span> with exactly <span class="math inline">\(\lfloor \sqrt{\frac{ n \ln 2 }{ 2 } } + 1 \rfloor\)</span> is released as an representative of the underlying set, we can infer immediately that the underlying set is <span class="math inline">\(U&#39;\)</span>.</p>
<h4 id="exponential-mechanism">Exponential Mechanism</h4>
<p>One remedy for the above problem is that, for a set <span class="math inline">\(S \subset \mathcal{X}^m\)</span>, even if <span class="math inline">\(\exists f \in Q\)</span>, s.t., <span class="math inline">\(|f(S) - f(U)| &gt; \lambda\)</span>, we output <span class="math inline">\(S\)</span> as the representative of <span class="math inline">\(U\)</span> with some probability, because <span class="math inline">\(S\)</span> might be a feasible solution for <span class="math inline">\(U\)</span>'s neighbors. This motivates the <em>exponential mechanism</em>. In particular,</p>
<ol type="1">
<li>It outputs the <span class="math inline">\(S \in \mathcal{X}^m\)</span> that minimizes <span class="math inline">\(\max_{f \in Q} |f(S) - f(U)|\)</span> with highest probability.</li>
<li>It outputs other <span class="math inline">\(S \in \mathcal{X}^m\)</span> with probability exponentially and inversely proportional to its error <span class="math inline">\(\max_{f \in Q} | f(S) - f(U) |\)</span>.</li>
</ol>
<p>The mechanism, termed <span class="math inline">\(M\)</span>, is as follows.</p>
<blockquote>
<ol type="1">
<li>For <span class="math inline">\(S \in \mathcal{X}^m\)</span>, define weight <span class="math inline">\(w(U, S) \doteq \exp(- \epsilon n \max_{f \in Q} | f(S) - f(U) | )\)</span>.<br />
</li>
<li>Sample and output an <span class="math inline">\(S \in \mathcal{X}^m\)</span> with probability <span class="math display">\[
\frac{ w(U, S) }{ \sum_{T \in \mathcal{X}^m} w(U, T)}
\]</span></li>
</ol>
</blockquote>
<h5 id="privacy"><strong><em>Privacy</em></strong></h5>
<p>The mechanism <span class="math inline">\(M\)</span> is <span class="math inline">\((2\epsilon, 0)\)</span> differentially private.</p>
<p><em>Proof.</em> Let <span class="math inline">\(U&#39; \in \mathcal{X}^n\)</span> be obtained by replacing exactly one element in <span class="math inline">\(U\)</span>. For any <span class="math inline">\(T \in \mathcal{X}^m\)</span>, by definition, <span class="math display">\[
\begin{aligned}
    \max_{f \in Q} |f(U) - f(T)| - \max_{f \in Q} |f(T) - f(U&#39;) |
        &amp;\le \max_{f \in Q} \big( |f(U) - f(T)| - |f(T) - f(U&#39;) | \big) \\
        &amp;\le \max_{f \in Q} \big( |f(U) - f(T) + f(T) - f(U&#39;) | \big) \\
        &amp;\le \max_{f \in Q} \big( |f(U) - f(U&#39;) | \big) \\
        &amp;\le 1 / n \\
\end{aligned}
\]</span></p>
<p>By symmetry, <span class="math inline">\(\max_{f \in Q} |f(U) - f(T)| - \max_{f \in Q} |f(T) - f(U&#39;) | \in [- 1 / n, 1 / n ]\)</span> and <span class="math display">\[
\frac{w(U, T)}{ w(U&#39;, T)} \in [e^{-\epsilon}, e^\epsilon]
\]</span></p>
<p>For an <span class="math inline">\(S \in \mathcal{X}^m\)</span>, it holds that <span class="math display">\[
\begin{aligned}
    \frac{\Pr[M(U) = S]}{ \Pr[M(U&#39;) = S]} 
        &amp;= \frac{ w(U, S) }{ w(U, S&#39;) } \frac{ \sum_{T \in \mathcal{X}^m} w(U&#39;, T) }{ \sum_{T \in \mathcal{X}^m} w(U, T) } \\
        &amp;\le e^\epsilon \cdot \frac{ \sum_{T \in \mathcal{X}^m} e^\epsilon \cdot w(U, T) }{ \sum_{T \in \mathcal{X}^m} w(U, T) } \\
        &amp;= e^{2\epsilon}
\end{aligned}
\]</span></p>
<h5 id="error"><strong><em>Error</em></strong></h5>
<p>For a set <span class="math inline">\(U\)</span>, although we might assign probabilities to bad <span class="math inline">\(S \in \mathcal{X}^m\)</span>, as long as the probability mass assigned is small, we are unlikely to get a bad representative.</p>
<blockquote>
<p>Theorem. With probability at most <span class="math inline">\(\delta\)</span>, <span class="math inline">\(M\)</span> returns an <span class="math inline">\(S\)</span>, such that <span class="math display">\[
\max_{f \in Q} | f(S) - f(U) | \ge \lambda + \frac{m \log |\mathcal{X}| + \log (1 / \delta) } {\epsilon n }
\]</span></p>
</blockquote>
<p><em>Proof.</em> Let <span class="math inline">\(S^* \in \mathcal{X}^m\)</span> be the set that minimizes <span class="math inline">\(\max_{f \in Q} |f(S) - f(U)|\)</span>.</p>
<p>For a <span class="math inline">\(S \in \mathcal{X}^m\)</span>, if <span class="math inline">\(\max_{f \in Q} |f(S) - f(U)| \ge \lambda + t\)</span> for some <span class="math inline">\(t &gt; 0\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[M(U) = S] 
        &amp;= \frac{ w(U, S) }{ \sum_{T \in \mathcal{X}^m} w(U, T)} \\
        &amp;\le \frac{ w(U, S) }{ w(U, S^*) } \\
        &amp;\le \exp( - \epsilon n (\max_{f \in Q} |f(S) - f(U)| - \max_{f \in Q} |f(S^*) - f(U)|)) \\
        &amp;\le \exp( - \epsilon n t)
\end{aligned}
\]</span></p>
<p>By union bound, <span class="math display">\[
\begin{aligned}
    \Pr[ M(U) \in \{ S : \max_{f \in Q} |f(S) - f(U)| \ge \lambda + t \} ] 
        &amp;\le |\{ S : \max_{f \in Q} |f(S) - f(U)| \ge \lambda + t \} | \exp( - \epsilon n t) \\
        &amp;\le |\mathcal{X}|^m \exp( - \epsilon n t)
\end{aligned}
\]</span></p>
<p>By requiring this probability to be bounded by some <span class="math inline">\(\delta &gt; 0\)</span>, we get <span class="math display">\[
t = \frac{m \log |\mathcal{X}| + \log (1 / \delta) } {\epsilon n }
\]</span></p>
<p><span class="math inline">\(\square\)</span>.</p>
<p>Substituting <span class="math inline">\(m\)</span> with <span class="math inline">\(\frac{\log |Q| }{\lambda^2}\)</span>, the error is then <span class="math display">\[
\lambda + \frac{\frac{\log |Q| }{\lambda^2} \log |\mathcal{X}| + \log (1 / \delta) } {\epsilon n }
\]</span></p>
<p>When <span class="math inline">\(\lambda = (\frac{\log |Q| \log |\mathcal{X}| }{ \epsilon n})^{1 / 3}\)</span>, the error is given by <span class="math display">\[
2(\frac{\log |Q| \log |\mathcal{X}| }{ \epsilon n})^{1 / 3} + \frac{\log (1 / \delta) }{ \epsilon n }
\]</span></p>
<h3 id="reference">Reference</h3>
<p>[1] S. Vadhan, “The Complexity of Differential Privacy,” in Tutorials on the Foundations of Cryptography, Y. Lindell, Ed. Cham: Springer International Publishing, 2017, pp. 347–450.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/25/Releasing-Histogram-Privately/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/25/Releasing-Histogram-Privately/" class="post-title-link" itemprop="url">Releasing Histogram Privately</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-11-25 10:19:09 / Modified: 22:11:21" itemprop="dateCreated datePublished" datetime="2020-11-25T10:19:09+11:00">2020-11-25</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given a set of <span class="math inline">\(n\)</span> elements <span class="math inline">\(S\)</span> that belongs to some finite domain <span class="math inline">\(\mathcal{X}= \{ x_1, x_2, ..., x_m \}\)</span>, we want to release the (normalized) histogram in a differentially private manner. Define <span class="math inline">\(\forall x_i \in \mathcal{X}\)</span>:</p>
<ol type="1">
<li><span class="math inline">\(F_S(x_i) \doteq\)</span> the number of times <span class="math inline">\(x_i\)</span> appears in <span class="math inline">\(S\)</span>;<br />
</li>
<li><span class="math inline">\(f_S(x_i) \doteq \frac{F_S(x_i) }{ |S| } = \frac{F_S(x_i) }{ n }\)</span>, the frequency of <span class="math inline">\(x_i\)</span> in <span class="math inline">\(S\)</span>.</li>
</ol>
<p>The (normalized) histogram of <span class="math inline">\(h_S\)</span> is a frequency vector: <span class="math display">\[
h_S \doteq \left&lt; f_S(x_1), f_S(x_2), ..., f_S(x_m) \right&gt; \in \mathbb{R}^m
\]</span></p>
<p>Consider another set <span class="math inline">\(S&#39;\)</span> obtained by replacing an element <span class="math inline">\(x_k\)</span> in <span class="math inline">\(S\)</span> to another element <span class="math inline">\(x_l\)</span>, such that</p>
<ol type="1">
<li><span class="math inline">\(F_{S&#39;} (x_k) = F_{S} (x_k) - 1\)</span>;</li>
<li><span class="math inline">\(F_{S&#39;} (x_l) = F_{S} (x_l) + 1\)</span>;</li>
<li><span class="math inline">\(F_{S&#39;} (x_i) = F_{S} (x_i), \forall i \notin \{k, l\}\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(h_{S&#39;}\)</span> be the histogram of <span class="math inline">\(S&#39;\)</span>. Our goal is to design some mechanism <span class="math inline">\(M\)</span> that adds noise to the normalized so that the output distribution of <span class="math inline">\(M(h_S)\)</span> and <span class="math inline">\(M(h_{S&#39;})\)</span> are indistinguishable.</p>
<h3 id="naive-laplacian">Naive Laplacian</h3>
<p>One way is to add Laplacian noise directly to each dimension of the histogram. For any vector <span class="math inline">\(v \in \mathbb{R}^m\)</span>, <span class="math inline">\(M\)</span> outputs a new perturbed vector: <span class="math display">\[
M(v) \doteq v + L
\]</span></p>
<p>where <span class="math inline">\(L \doteq \left&lt; l_1, l_2, ..., l_m\right&gt;\)</span> and each <span class="math inline">\(l_i\)</span> is sampled independently from <span class="math inline">\(Lap( \frac{2 }{ n \epsilon } )\)</span>.</p>
<h4 id="privacy">Privacy</h4>
<p>We claim this mechanism is <span class="math inline">\((\epsilon, 0)\)</span>-differentially private. For <span class="math inline">\(u \in \mathbb{R}^n\)</span>, we have <span class="math display">\[
\begin{aligned}
    \frac{ p(M(h_S) = u) }{ p(M(h_{S&#39;}) = u) } 
        &amp;= \frac{ \prod_{i \in [m] } p( f_S (x_i) + l_i = u_i) }{ \prod_{i \in [m] } p( f_{S&#39;} (x_i) + l_i = u_i) } \\
        &amp;= \frac{ \prod_{i \in \{k, l\} } p( f_S (x_i) + l_i = u_i) }{ \prod_{i \in \{k, l\} } p( f_{S&#39;} (x_i) + l_i = u_i) } \\
        &amp;\le \exp( - (|u_k - f_S(x_k)| - |u_k - f_{S&#39;} (x_k) |) \frac{n\epsilon}{2} ) \\
        &amp;\quad \cdot \exp( - (|u_l - f_S(x_l)| - |u_l - f_{S&#39;} (x_l) |) \frac{n\epsilon}{2} ) \\
        &amp;\le \exp( |f_S(x_k) - f_{S&#39;} (x_k) | \frac{n\epsilon}{2} ) \cdot \exp( |f_S(x_l) - f_{S&#39;} (x_l) | \frac{n\epsilon}{2} ) \\
        &amp;= \exp( \frac{1}{n} \frac{ n \epsilon}{2} ) \cdot \exp( \frac{1}{n} \frac{n\epsilon}{2} ) \\
        &amp;= \exp( \epsilon).
\end{aligned}
\]</span></p>
<h4 id="accuracy">Accuracy</h4>
<p>We prove that <span class="math inline">\(|M(h_S) - h_S |_\infty = O (\frac{1}{ n \epsilon } \log \frac{m}{\delta})\)</span> with probability at least <span class="math inline">\(1 - \delta\)</span>.</p>
<p>Let <span class="math inline">\(b = \frac{2 }{ n \epsilon }\)</span>. For a failure probability <span class="math inline">\(\delta\)</span>, <span class="math display">\[
\Pr[ |f_S(x_i) + l_i - f_S(x_i) | \ge b \log \frac{m}{\delta} ] = \int_{ b \log \frac{m}{\delta} }^\infty \frac{1}{b} \exp( - x / b ) \ dx = \exp(- \log \frac{m}{\delta} ) = \frac{\delta}{m}
\]</span></p>
<p>By union bound, it holds that <span class="math display">\[
\Pr[ |M(h_S) - h_S |_\infty \ge b \log \frac{m}{\delta} ] \le \delta. 
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="rounding-small-values">Rounding Small Values</h3>
<p>When <span class="math inline">\(|\mathcal{X}| = m \gg n\)</span>, we could improve the release mechanism. As shown in the previous section, we would like to achieve error <span class="math inline">\(O(\frac{1}{n})\)</span>. This inspire to round the small frequencies (roughly <span class="math inline">\(O(\frac{1}{n})\)</span>) to zero. Of course, we should carry out this in a differentially private manner.</p>
<blockquote>
<p>Mechanism <span class="math inline">\(M\)</span><br />
<span class="math inline">\(===============================\)</span><br />
INPUT: a set <span class="math inline">\(S\)</span> of <span class="math inline">\(n\)</span> elements.<br />
OUTPUT: a histogram of <span class="math inline">\(S\)</span> in differentially private manner.<br />
<span class="math inline">\(------------------------\)</span><br />
For <span class="math inline">\(i \in [m]\)</span> do<br />
<span class="math inline">\(\qquad\)</span> If <span class="math inline">\(f_S(x_i) = 0\)</span>, then <span class="math inline">\(M(h_S)_i \leftarrow 0\)</span><br />
<span class="math inline">\(\qquad\)</span> Else<br />
<span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> <span class="math inline">\(M(h_S)_i \leftarrow f_S(x_i) + l_i\)</span>, where <span class="math inline">\(l_i \sim Lap(\frac{2}{n \epsilon} )\)</span>.<br />
<span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> If <span class="math inline">\(M(h_S)_i &lt; \frac{1}{n} +\frac{2 \log 2 / \delta }{ \epsilon n}\)</span>, then <span class="math inline">\(M(h_S)_i \leftarrow 0\)</span>.<br />
Release <span class="math inline">\(M(h_S) \doteq \left&lt; M(h_S)_1, M(h_S)_2, ... , M(h_S)_n \right&gt;\)</span><br />
<span class="math inline">\(------------------------\)</span></p>
</blockquote>
<h4 id="privacy-1">Privacy</h4>
<p>We claim this mechanism is <span class="math inline">\((\epsilon, \delta)\)</span>-differentially private.</p>
<ul>
<li><p>Case 1. If <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> have the same set of supports, i.e., <span class="math inline">\(f_S(x_i) &gt; 0 \leftrightarrow f_{S&#39;}(x_i) &gt; 0\)</span>, then <span class="math inline">\(M\)</span> is <span class="math inline">\((\epsilon, 0)\)</span> differentially private.</p></li>
<li><p>Case 2.</p>
<ol type="1">
<li><span class="math inline">\(F_{S} (x_k) = 1\)</span>, then <span class="math inline">\(F_{S&#39;} (x_k) = F_{S} (x_k) - 1 = 0\)</span>. For <span class="math inline">\(S&#39;\)</span>, it is guaranteed that <span class="math inline">\(M(h_{S&#39;})_k = 0\)</span>. For <span class="math inline">\(S\)</span>, as <span class="math inline">\(f_S(x_k) = \frac{1}{n}\)</span>, <span class="math display">\[
     \Pr[ M(h_S)_k \neq 0 ] = \Pr[ M(h_S)_k \neq M(h_{S&#39;})_k ] = \frac{\delta}{2}
 \]</span></li>
<li><span class="math inline">\(F_{S} (x_l) = 0\)</span>, then <span class="math inline">\(F_{S&#39;} (x_l) = F_{S} (x_l) + 1 = 1\)</span>. For <span class="math inline">\(S\)</span>, it is guaranteed that <span class="math inline">\(M(h_S)_k = 0\)</span>. For <span class="math inline">\(S&#39;\)</span>, by similar argument, we have <span class="math inline">\(\Pr[ M(h_{S&#39;})_k \neq 0 ] = \Pr[ M(h_S)_k \neq M(h_{S&#39;})_k ] = \frac{\delta}{2}\)</span>.</li>
</ol>
<p>There are three combinations such that either <span class="math inline">\(F_S(x_k) = 1\)</span> or <span class="math inline">\(F_{S} (x_l) = 0\)</span> holds, namely</p>
<ol type="1">
<li><span class="math inline">\(F_S(x_k) = 1, F_{S} (x_l) = 0\)</span>,</li>
<li><span class="math inline">\(F_S(x_k) = 1, F_{S} (x_l) &gt; 0\)</span>,</li>
<li><span class="math inline">\(F_S(x_k) &gt; 1, F_{S} (x_l) = 0\)</span>.</li>
</ol>
<p>We give a proof for the first case. The proofs for the other two are similar. Now, for <span class="math inline">\(\forall\)</span> measurable <span class="math inline">\(E \subset \mathbb{R}^n\)</span>, <span class="math display">\[
  \begin{aligned}
      \Pr[ M(h_S) \in E] 
          &amp;= \Pr[ M(h_S) \in E \mid M(h_S)_k = 0, M(h_{S&#39;})_l = 0] \\
          &amp;\ + \Pr[ M(h_S) \in E \mid M(h_S)_k \neq 0 \vee M(h_{S&#39;})_l \neq 0] \\
          &amp;\le \Pr[ M(h_S) \in E \mid M(h_S)_k = 0, M(h_{S&#39;})_l = 0] \\
          &amp;\ + \Pr[M(h_S)_k \neq 0 ] + \Pr[ M(h_{S&#39;})_l \neq 0] \\
          &amp;= \Pr[ M(h_{S&#39;}) \in E \mid M(h_S)_k = 0, M(h_{S&#39;})_l = 0] + \delta \\
          &amp;\le \Pr[ M(h_{S&#39;}) \in E] + \delta
  \end{aligned}
  \]</span></p></li>
</ul>
<h4 id="accuracy-1">Accuracy</h4>
<p>We prove that <span class="math inline">\(|M(h_S) - h_S |_\infty = O (\frac{1}{ n \epsilon } \log \frac{1}{\delta})\)</span> with probability at least <span class="math inline">\(1 - \delta\)</span>, if <span class="math inline">\(\delta \le \frac{1}{n}\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(f_S(x_i) = 0\)</span>, then there is no error.<br />
</li>
<li>Otherwise, if <span class="math inline">\(M(h_S)_i\)</span> is not truncated, with probability at most <span class="math inline">\(\frac{\delta}{n}\)</span>, it <span class="math inline">\(|M(h_S)_i - f_S(x_i)|\le \frac{2\log 1 / \delta}{n\epsilon}\)</span>. If it is truncated, this will introduce an additional error of at most <span class="math inline">\(\frac{1}{n} +\frac{2 \log 2 / \delta }{ \epsilon n}\)</span>. The overall error is at most <span class="math inline">\(\frac{1}{n} +\frac{4 \log 2 / \delta }{ \epsilon n}\)</span>, which is <span class="math inline">\(O( \frac{ \log 1 / \delta }{ \epsilon n})\)</span> if <span class="math inline">\(\epsilon \le \log n\)</span>.</li>
</ol>
<!-- Let $b = \frac{2 }{ n \epsilon }$. For a failure probability $\delta$, 
$$
\Pr[ |f_S(x_i) + l_i - f_S(x_i) | \ge b \log \frac{m}{\delta} ] = \int_{ b \log \frac{m}{\delta} }^\infty \frac{1}{b} \exp( - x / b ) \ dx = \exp(- \log \frac{m}{\delta} ) = \frac{\delta}{m}
$$

By union bound, it holds that 
$$
\Pr[ |M(h_S) - h_S |_\infty \ge b \log \frac{m}{\delta} ] \le \delta. 
$$ -->
<p><span class="math inline">\(\square\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] S. Vadhan, “The Complexity of Differential Privacy,” in Tutorials on the Foundations of Cryptography, Y. Lindell, Ed. Cham: Springer International Publishing, 2017, pp. 347–450.</p>
<!-- Now, for $\forall$ measurable $E \subset \mathbb{R}^n$, define  
$$
    E_1 \doteq \{ v \in E : v_k = 0, v_l = 0 \}
$$
be the set of points with the $k$-th and $l$-th dimensions equal to 0. Let $E_2 = E \setminus E_1$. 
$$
\begin{aligned}
    \Pr[ M(h_S) \in E] &= \Pr[ M(h_S) \in E_1] + \Pr[ M(h_S) \in E_2] \\
    &= \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot \Pr[M(h_S)_k = 0] + \Pr[ M(h_S) \in E_2] \\
    &= \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot (1 - \frac{\delta}{2}) + \Pr[ M(h_S) \in E_2] \\
    &\le \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot (1 - \frac{\delta}{2}) + \Pr[ M(h_S)_k = 0]  \\
    &= \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0] \cdot (1 - \frac{\delta}{2}) + \frac{\delta}{2}  \\
    &= \Pr[ M(h_{S'}) \in E_1 \mid M(h_{S'})_l = 0] \cdot (1 - \frac{\delta}{2}) + \frac{\delta}{2}  \\
    &\le \Pr[M(h_{S'}) \in E ] + \frac{\delta}{2}
\end{aligned}
$$

The last inequality follows from $\Pr[ M(h_{S'}) \in E_1 \mid M(h_{S'})_l = 0] = \Pr[ M(h_S) \in E_1 \mid M(h_S)_k = 0]$.  -->

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/23/Propose-Test-Release/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/23/Propose-Test-Release/" class="post-title-link" itemprop="url">Propose-Test-Release</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-23 21:53:14" itemprop="dateCreated datePublished" datetime="2020-11-23T21:53:14+11:00">2020-11-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-18 10:32:42" itemprop="dateModified" datetime="2020-12-18T10:32:42+11:00">2020-12-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\((\mathcal{X}, d)\)</span> be a metric space and <span class="math inline">\(q : \mathcal{X} \rightarrow \mathbb{R}\)</span> be a function defined on <span class="math inline">\(\mathcal{X}\)</span>. The metric <span class="math inline">\(d: \mathcal{X} \rightarrow \mathbb{N}\)</span> takes only integer values. For an <span class="math inline">\(x \in \mathcal{X}\)</span>, it neighbors an <span class="math inline">\(x&#39; \in \mathcal{X}\)</span>, termed <span class="math inline">\(x \sim x&#39;\)</span>, if <span class="math inline">\(d(x, x&#39;) = 1\)</span>.</p>
<p>Further, define the global sensitivity <span class="math inline">\(\Delta\)</span> of <span class="math inline">\(q\)</span> as: <span class="math display">\[
    \Delta \doteq \max_{x \sim x&#39; } | q(x) - q(x&#39;) |
\]</span></p>
<p>The Laplace mechanism <span class="math inline">\(M\)</span> of <span class="math inline">\(q\)</span> adds random noise <span class="math inline">\(N \sim Lap(\frac{\Delta}{\epsilon})\)</span> to the output of <span class="math inline">\(q\)</span>: <span class="math display">\[
M(x) \doteq q(x) + N
\]</span></p>
<p>where <span class="math inline">\(Lap(\frac{\Delta}{\epsilon})\)</span> is the Laplacian distribution with parameter <span class="math inline">\(\frac{\Delta}{\epsilon}\)</span>. It guarantees that if <span class="math inline">\(x \sim x&#39;\)</span>, the output distributions of <span class="math inline">\(M(x)\)</span> and <span class="math inline">\(M(x&#39;)\)</span> are indistinguishable in the sense that <span class="math inline">\(\forall S \subset \mathbb{R}\)</span>, if <span class="math inline">\(S\)</span> is measurable, <span class="math display">\[
\Pr[ M(x) \in S] \in [\exp(-\epsilon), \exp(\epsilon) ] \cdot \Pr[ M(x&#39;) \in S]
\]</span></p>
<p>The value of <span class="math inline">\(\Delta\)</span> could be large and is independent of an <span class="math inline">\(x \in \mathcal{X}\)</span>. We would like to investigate the possibility of adding a smaller noise than <span class="math inline">\(Lap(\frac{\Delta}{\epsilon})\)</span>.</p>
<p>For a fixed <span class="math inline">\(x\)</span>, we can define the local sensitivity with respect to <span class="math inline">\(x\)</span> as <span class="math display">\[
    \delta(x) \doteq \max_{x&#39; \sim x} | q(x) - q(x&#39;) |. 
\]</span></p>
<p>Here the maximum is over the neighbors of a fixed <span class="math inline">\(x\)</span>. The value between <span class="math inline">\(\Delta\)</span> and <span class="math inline">\(\delta(x)\)</span> is quite different.</p>
<blockquote>
<p>Example. Let * <span class="math inline">\(\mathcal{X} \doteq [0, 100]^3\)</span>.<br />
* <span class="math inline">\(d(x, x&#39;)\doteq \#\text{ different values between } x \text{ and } x&#39;\)</span>. * <span class="math inline">\(q(x) \doteq \min_{1 \le i &lt; j \le 3} |x_i - x_j|\)</span>, where <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>-dimension of <span class="math inline">\(x\)</span>.</p>
<p>Then for the point <span class="math inline">\(x = (0, 0, 0) \in [0, 100]^3\)</span>, if <span class="math inline">\(x&#39; \sim x\)</span>, <span class="math inline">\(x&#39;\)</span> contains at least two zeros in some of its dimensions. Therefore, <span class="math inline">\(q(x&#39;) = 0\)</span> and <span class="math inline">\(\delta(x) = 0\)</span>. But for <span class="math inline">\(x = (0, 0, 100)\)</span>, it has a neighbor <span class="math inline">\(x&#39; = (0, 50, 100)\)</span>. Now <span class="math inline">\(q(x) = 0\)</span> but <span class="math inline">\(q(x&#39;) = 50\)</span>. So <span class="math inline">\(\Delta \ge \delta(x) \ge 50\)</span>.</p>
</blockquote>
<p>We are interested in whether adding smaller noise <span class="math inline">\(N \sim Lap(\frac{\delta(x) }{\epsilon})\)</span> to the output of <span class="math inline">\(q(x)\)</span> would preserve privacy: <span class="math display">\[
M(x) = q(x) + N
\]</span></p>
<p>It is not. In the previous example, <span class="math inline">\((0, 0, 0)\)</span> and <span class="math inline">\((0, 0, 100)\)</span> are neighbors. But for the former we add noise <span class="math inline">\(N \sim Lap(0)\)</span> and for the latter we add noise <span class="math inline">\(N \sim Lap(\frac{50}{\epsilon})\)</span>. So the output distributions of them are very different and this would lead to information leakage.</p>
<p>Perhaps, we should propose some fixed parameter <span class="math inline">\(\beta &gt; 0\)</span> (which could be much smaller than <span class="math inline">\(\Delta\)</span>), and add a noise <span class="math inline">\(N \sim Lap(\frac{\beta}{\epsilon})\)</span> for whatever <span class="math inline">\(x \in \mathcal{X}\)</span>. For a pair of neighboring <span class="math inline">\(x, x&#39;\)</span>, when <span class="math inline">\(|q(x) - q(x&#39;)| \le \beta\)</span>, then the distributions of <span class="math inline">\(q(x) + N\)</span> and <span class="math inline">\(q(x&#39;) + N\)</span> are indeed <span class="math inline">\((\epsilon, 0)\)</span> indistinguishable. The only concern is for the case where <span class="math inline">\(|q(x) - q(x&#39;)| &gt; \beta\)</span> and adding a noise <span class="math inline">\(N \sim Lap(\frac{\beta}{\epsilon})\)</span> fails to protect the privacy.</p>
<p>One simple remedy is simply to refuse to output <span class="math inline">\(q(x) + N\)</span> (or <span class="math inline">\(q(x) + N&#39;\)</span>). This should be done in a differentially private manner. Let <span class="math display">\[
\{ y \in \mathcal{X} : \delta(y) &gt; \beta \}
\]</span></p>
<p>be the set of points with local sensitivity greater than <span class="math inline">\(\beta\)</span>. For a fixed <span class="math inline">\(x\)</span>, define <span class="math inline">\(d(x, \{ y : \delta(y) &gt; \beta \})\)</span> be its distance to the set. It follows that for <span class="math inline">\(x \sim x&#39;\)</span>, <span class="math display">\[
|d(x, \{ y \in \mathcal{X} : \delta(y) &gt; \beta \}) - d(x&#39;, \{ y \in \mathcal{X} : \delta(y) &gt; \beta \})| \le 1.
\]</span></p>
<p>This motivates the following <em>propose-test-release</em> algorithm.</p>
<blockquote>
<ol type="1">
<li>Propose a bound <span class="math inline">\(\beta\)</span> on local sensitivity.<br />
</li>
<li>Compute <span class="math inline">\(\hat d \doteq d(x, \{ y \in \mathcal{X} : \delta(y) &gt; \beta \}) + Lap(1 / \epsilon)\)</span>.<br />
</li>
<li>If <span class="math inline">\(\hat d \le \frac{1}{\epsilon} \ln \frac{1}{\gamma}\)</span> then<br />
<span class="math inline">\(\qquad\)</span> output <span class="math inline">\(\bot\)</span>.</li>
<li>Else<br />
<span class="math inline">\(\qquad\)</span> output <span class="math inline">\(q(x) + Lap(\beta / \epsilon)\)</span>.</li>
</ol>
</blockquote>
<p><strong>Theorem.</strong> The algorithm is <span class="math inline">\((2\epsilon, \gamma / 2)\)</span> differentially private.</p>
<p><em>Proof.</em></p>
<p>First observe that for two neighboring <span class="math inline">\(x, x&#39;\)</span>, their probabilities of outputting <span class="math inline">\(\bot\)</span> are similar. <span class="math display">\[
 \Pr[ M(x) = \bot ] \le e^{ \epsilon} \cdot \Pr[ M(x&#39;) = \bot ] \\
 \Pr[ M(x) \neq \bot ] \le e^{ \epsilon} \cdot \Pr[ M(x&#39;) \neq \bot ]
\]</span></p>
<p>Now consider a measurable set <span class="math inline">\(S \subset \mathbb{R}\)</span>. <span class="math inline">\(M(x) \in S\)</span> is only possible when <span class="math inline">\(\hat d &gt; \frac{1}{\epsilon} \ln \frac{1}{\gamma}\)</span>:<br />
<span class="math display">\[
    \begin{aligned}
        \Pr[M(x) \in S] 
            &amp;= \Pr[ M(x) \in S \mid M(x) \neq \bot ] \cdot \Pr[ M(x) \neq \bot]
    \end{aligned}
  \]</span></p>
<p>We consider two cases.</p>
<ul>
<li><p>Case 1. When <span class="math inline">\(\delta(x) &gt; \beta\)</span>, then <span class="math inline">\(d(x, \{ y \in \mathcal{X} : \delta(y) &gt; \beta \}) = 0\)</span>. It follows that <span class="math display">\[
  \Pr[M(x) \in S] \le \Pr[M(x) \neq \bot ]  \le \frac{\gamma}{2}
\]</span></p></li>
<li><p>Case 2. When <span class="math inline">\(\delta(x) \le \beta\)</span>, it holds that <span class="math inline">\(|q(x) - q(x&#39;) |\le \beta\)</span> and <span class="math display">\[
      \Pr[ M(x) \in S \mid M(x) \neq \bot ] \le e^\epsilon \cdot \Pr[ M(x&#39;) \in S \mid M(x&#39;) \neq \bot ]
  \]</span> Therefore, <span class="math display">\[
  \begin{aligned}
      \Pr[M(x) \in S] 
          &amp;= \Pr[ M(x) \in S \mid M(x) \neq \bot ] \cdot \Pr[ M(x) \neq \bot] \\
          &amp;\le \Pr[ M(x) \in S \mid M(x) \neq \bot ] \cdot e^\epsilon \cdot \Pr[ M(x&#39;) \neq \bot] \\
          &amp;\le e^\epsilon \cdot \Pr[ M(x&#39;) \in S \mid M(x&#39;) \neq \bot ] \cdot e^\epsilon \cdot \Pr[ M(x&#39;) \neq \bot] \\
          &amp;= e^{2\epsilon} \Pr[ M(x&#39;) \in S].
  \end{aligned}
  \]</span></p></li>
</ul>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] S. Vadhan, “The Complexity of Differential Privacy,” in Tutorials on the Foundations of Cryptography, Y. Lindell, Ed. Cham: Springer International Publishing, 2017, pp. 347–450.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/19/PAC-Learning-From-Countable-Hypothesis-Family/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/19/PAC-Learning-From-Countable-Hypothesis-Family/" class="post-title-link" itemprop="url">PAC Learning From Countable Hypothesis Family</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-19 15:17:27" itemprop="dateCreated datePublished" datetime="2020-11-19T15:17:27+11:00">2020-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-21 20:31:33" itemprop="dateModified" datetime="2020-11-21T20:31:33+11:00">2020-11-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We generalize the PAC learning problem of finding an hypothesis from a finite hypothesis family to the one from a countable hypothesis family.</p>
<h2 id="problem-setting">Problem Setting</h2>
<ol type="1">
<li><p><span class="math inline">\(\mathcal{X}:\)</span> the input space.</p></li>
<li><p><span class="math inline">\(\mathcal{Y} \doteq \{-1, 1\}:\)</span> the output space.</p></li>
<li><p><span class="math inline">\(\mathcal{Z} \doteq \mathcal{X} \times \mathcal{Y}:\)</span> the product space of <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal{D}:\)</span> an unknown distribution defined on <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal{H}:\)</span> an countable family of hypothesis, s.t., each <span class="math inline">\(h \in \mathcal{H}\)</span> is a function from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathcal{Y}\)</span>.</p></li>
<li><p><span class="math inline">\(\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow [0, 1]\)</span>, a loss function that takes two points in <span class="math inline">\(\mathcal{Y}\)</span> and outputs a value between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p></li>
<li><p><span class="math inline">\(L_\mathcal{D} (h):\)</span> the expected loss of a hypothesis <span class="math inline">\(h\)</span> is defined as <span class="math display">\[
 L_\mathcal{D} (h) = \mathbb{E}_{ (x, y) \sim \mathcal{D} } [ \ell( h(x), y) ]
 \]</span></p>
<p>where <span class="math inline">\((x, y) \sim \mathcal{D}\)</span> implies that the pair <span class="math inline">\((x, y) \in \mathcal{Z}\)</span> is sampled according to the distribution <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p><span class="math inline">\(h^* \doteq \arg\min_{h \in \mathcal{H} } L_\mathcal{D} (h):\)</span> the hypothesis that minimize the expected loss.</p></li>
<li><p><span class="math inline">\(\mu_h:\)</span> an alias for <span class="math inline">\(L_\mathcal{D} (h)\)</span>, when the distribution <span class="math inline">\(\mathcal{D}\)</span> discussed in the context is unique.</p></li>
<li><p><span class="math inline">\(B(\mu_h, \epsilon) \doteq \{ r \in \mathbb{R} : |r - \mu_h | &lt; \epsilon \}:\)</span> an open ball, which is an open interval in <span class="math inline">\(\mathbb{R}\)</span> centered at <span class="math inline">\(\mu_h\)</span> with length <span class="math inline">\(2 \epsilon\)</span>, where <span class="math inline">\(\epsilon &gt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(S \sim \mathcal{D}^n:\)</span> a set of <span class="math inline">\(n\)</span> i.i.d samples drawn from <span class="math inline">\(\mathcal{D}\)</span>, where <span class="math inline">\(n \in \mathbb{N}^+\)</span> is some positive integer. In particular, when <span class="math inline">\(S \sim \mathcal{D}^n\)</span>, it can be represented as <span class="math display">\[
 S = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) : (x_i, y_i) \sim \mathcal  {D}, \forall i \in [n] \}.
 \]</span> We also view <span class="math inline">\(S\)</span> as a point in <span class="math inline">\(\mathcal{Z}^n\)</span>.</p></li>
<li><p><span class="math inline">\(L_{S}(h) \doteq \frac{1}{ |S| } \sum_{ (x, y) \in S } \ell( h(x) , y ) :\)</span> the empirical loss of a hypothesis <span class="math inline">\(h\)</span> on a sample set <span class="math inline">\(S\)</span>.</p></li>
<li><p>Given a <span class="math inline">\(h \in \mathcal{H}\)</span>, its restriction on <span class="math inline">\(\mathcal{C} \subset \mathcal{X}\)</span> is a function <span class="math inline">\(h_S\)</span> defined on <span class="math inline">\(S\)</span>, such that <span class="math display">\[
h_\mathcal{C} (x) = h(x), \forall x \in \mathcal{C} 
\]</span></p></li>
<li><p>The restriction of <span class="math inline">\(\mathcal{H}\)</span> on <span class="math inline">\(\mathcal{C}\)</span> is the set of possible restriction of a function in <span class="math inline">\(\mathcal{H}\)</span> to <span class="math inline">\(\mathcal{C}\)</span> <span class="math display">\[
        \mathcal{H}_\mathcal{C}  = \{ h_\mathcal{C}  : \mathcal{C}  \rightarrow \mathcal{Y} : h \in \mathcal{H} \}
\]</span></p>
<p>As <span class="math inline">\(\mathcal{Y} = \{ -1, +1 \}\)</span>, the set of possible functions defined on <span class="math inline">\(\mathcal{C}\)</span> is bounded by <span class="math inline">\(2^{n}\)</span>. Hence, <span class="math display">\[
    |\mathcal{H}_\mathcal{C} | \le 2^{n}.
\]</span></p></li>
<li><p>The growth function <span class="math inline">\(\Pi_{\mathcal{H} } (n): \mathbb{N}^+ \rightarrow \mathbb{N}^+\)</span> of <span class="math inline">\(\mathcal{H}\)</span> is defined as <span class="math display">\[
   \Pi_{\mathcal{H} } (n) = \max_{\mathcal{C} \subset \mathcal{X}, |C| = n} | \mathcal{H}_\mathcal{C} |
   \]</span></p></li>
</ol>
<p>Ideally, we would like to find <span class="math inline">\(h^*\)</span>. The problem is difficult, as</p>
<ul>
<li>The space <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span> could be infinite.</li>
<li>The distribution <span class="math inline">\(\mathcal{D}\)</span> is unknown.</li>
</ul>
<p>To deal with the possibly infinite space <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span> and the unknown distribution <span class="math inline">\(\mathcal{D}\)</span>, we investigate <span class="math inline">\(\mathcal{H}\)</span> on a finite sample set <span class="math inline">\(S\)</span>. Due to the randomness inherited in sampling <span class="math inline">\(S\)</span>, we allow our solution to be approximate and to make error. For a given pair of parameters of <span class="math inline">\(\epsilon &gt; 0\)</span> and <span class="math inline">\(\delta &gt; 0\)</span>, we relax the goal to designing an <span class="math inline">\((\epsilon, \delta)\)</span>-learning algorithm <span class="math inline">\(A\)</span> that returns an <span class="math inline">\(h&#39;\)</span>, that is</p>
<blockquote>
<ul>
<li><span class="math inline">\(\epsilon\)</span>-approximate: <span class="math inline">\(\mu_{h&#39;} \le \mu_{h^*} + \epsilon\)</span>,<br />
</li>
<li>probably correct: <span class="math inline">\(A\)</span> return an <span class="math inline">\(h&#39;\)</span> that does not satisfies the above condition with probability at most <span class="math inline">\(\delta\)</span>.</li>
</ul>
</blockquote>
<p>Combined, <span class="math inline">\(A\)</span> should return an <span class="math inline">\(\epsilon\)</span>-approximate solution <span class="math inline">\(h&#39;\)</span> with probability at least <span class="math inline">\(1 - \delta\)</span>.</p>
<h2 id="the-algorithm">The Algorithm</h2>
<blockquote>
<p>An <span class="math inline">\((\epsilon, \delta)\)</span> approximate algorithm <span class="math inline">\(A\)</span><br />
1. Draw a set <span class="math inline">\(S\)</span> of <span class="math inline">\(n =\)</span> samples independently from <span class="math inline">\(\mathcal{D}\)</span>.<br />
2. Return an <span class="math inline">\(h&#39;\)</span> such that <span class="math display">\[
h&#39; = \arg\min_{h \in \mathcal{H} } L_{S} (h)
\]</span></p>
</blockquote>
<p>A key result of the algorithm states that <span class="math inline">\(L_S(h)\)</span> is a good approximation of <span class="math inline">\(\mu_h\)</span> for all <span class="math inline">\(h \in \mathcal{H}\)</span> simultaneously.</p>
<blockquote>
<p>Theorem. If <span class="math inline">\(n \ge\)</span>, then <span class="math display">\[
\Pr_{S \sim \mathcal{D}^n } [ \exists h \in \mathcal{H} : L_S(h) \notin B(\mu_h, \epsilon / 2 )  ] \le \delta
\]</span></p>
</blockquote>
<p>An immediate corollary is that <span class="math inline">\(h&#39;\)</span> is <span class="math inline">\(\epsilon\)</span>-approximate with probability at least <span class="math inline">\(1 - \delta\)</span>: <span class="math display">\[
\mu_{h&#39;} \le L_S(h&#39;) + \frac{\epsilon}{2} \le L_S(h^*) + \frac{\epsilon}{2} \le \mu_{h^*} + \epsilon.
\]</span></p>
<h2 id="proof-of-the-theorem">Proof of The Theorem</h2>
<p>The proof relies on a technique called double sampling. Alongside with <span class="math inline">\(S\)</span>, we create another sample set <span class="math inline">\(S&#39; \sim \mathcal{D}^n\)</span> independently, termed the "ghost sample". Let <span class="math display">\[
S = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \}
\]</span> and <span class="math display">\[
S&#39; = \{ (x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;), ..., (x_n&#39;, y_n&#39;) \}.
\]</span></p>
<p>Then, we define</p>
<ol start="16" type="1">
<li><span class="math inline">\(\sigma:\)</span> a random swap that exchanges the <span class="math inline">\(i\)</span>-th (<span class="math inline">\(i \in [n]\)</span>) element of <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> independently with probability 0.5. Call the resulting sample sets <span class="math inline">\(\sigma S\)</span> and <span class="math inline">\(\sigma S&#39;\)</span>. Let <span class="math inline">\(\sigma S[i]\)</span> (<span class="math inline">\(\sigma S&#39;[i]\)</span>) be the <span class="math inline">\(i\)</span>-th element of <span class="math inline">\(\sigma S[i]\)</span> (<span class="math inline">\(\sigma S&#39;[i]\)</span>). So <span class="math display">\[
\Pr \left[ 
    \sigma S [i] = (x_i,  y_i) \wedge
    \sigma S&#39;[i] = (x_i&#39;, y_i&#39;)
 \right] = 0.5 \\
 \Pr \left[ 
    \sigma S [i] = (x_i&#39;, y_i&#39;) \wedge
    \sigma S&#39;[i] = (x_i,  y_i)
 \right] = 0.5
\]</span></li>
</ol>
<p>We use the gap of <span class="math inline">\(|L_{\sigma S} (h) - L_{\sigma S&#39;} (h)|\)</span> as a proxy of <span class="math inline">\(|L_S(h) - \mu_h|\)</span>. This enables us to focus on finite sets <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> without worrying about the infinite size of <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>For convenience, we use <span class="math inline">\(\underset{S}{\Pr}[\cdot]\)</span> (<span class="math inline">\(\underset{S&#39;}{\Pr}[\cdot]\)</span>) as shorthand for <span class="math inline">\(\underset{S \sim \mathcal{D}^n}{\Pr}[\cdot]\)</span> (<span class="math inline">\(\underset{S&#39; \sim \mathcal{D}^n}{\Pr}[\cdot]\)</span>). The road map of our proof is as follows.</p>
<blockquote>
<p>Lemma 1.<br />
<span class="math display">\[
\Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \le 2 \max_{S, S&#39; \in \mathcal{Z}^n } \Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ]
\]</span></p>
</blockquote>
<p>On the left hand side of the inequality, the probability measures the event of a random set <span class="math inline">\(S\)</span> sampling from <span class="math inline">\(\mathcal{D}\)</span>. On the right hand side, we can pick a fixed pair of <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> that maximize <span class="math display">\[
\Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ],
\]</span></p>
<p>and the probability measures the event of the random swap <span class="math inline">\(\sigma\)</span>. Fixing <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> significantly simplifies the structure of <span class="math inline">\(\mathcal{H}\)</span>. By Hoeffding inequality and union bound, we will prove that</p>
<blockquote>
<p>Lemma 2. For any fixed pair of <span class="math inline">\(S, S&#39; \in \mathcal{Z}^n\)</span>, <span class="math display">\[
\Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ] \le 2 \Pi_\mathcal{H} (2n) \exp( - \frac{n^2 \epsilon^2 }{2} )
\]</span></p>
</blockquote>
<h3 id="proof-of-lemma-1."><strong><em>Proof of Lemma 1.</em></strong></h3>
<p>The proof of Lemma 1 consists of three steps.</p>
<h4 id="step-1"><strong>S<em>tep 1</em></strong></h4>
<p>We will prove that <span class="math display">\[
    \Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \le 2 \Pr_{S , S&#39;} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2)  ].
\]</span></p>
<p><em>Proof of Step 1.</em><br />
By Hoeffding inequality, for a fixed <span class="math inline">\(h \in \mathcal{H}\)</span>, when <span class="math inline">\(n \ge \frac{2}{\epsilon^2} \ln 4\)</span> <span class="math display">\[
    \Pr_{S&#39;} [ L_{S&#39;} (h) \notin B(\mu_h, \epsilon / 2) ] \le 2 \exp(- 2 n (\frac{\epsilon }{ 2 })^2 ) = 2 \exp( - \frac{n \epsilon^2}{2} ) \le \frac{1}{2}
\]</span></p>
<p>It follows that <span class="math display">\[
    \Pr_{S , S&#39;} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2)  ] 
        \ge 
    \frac{1}{2} \Pr_{S}[ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ]
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="step-2"><strong><em>Step 2</em></strong></h4>
<p>Observe that <span class="math display">\[
    \{  \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2)  \} 
        \subset 
    \{  \exists h : |L_S (h) - L_{S&#39;} (h) | \ge \frac{\epsilon}{2} \} 
\]</span></p>
<p>By monotonicity of probability, we get <span class="math display">\[
    \Pr_{S} [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge  L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2) ] 
        \le 
    \Pr_{S, S&#39;} [ \exists h : |L_S (h) - L_{S&#39;} (h) | \ge \frac{\epsilon}{2} ]
\]</span></p>
<h4 id="step-3"><strong><em>Step 3</em></strong></h4>
<p>Finally, <span class="math display">\[
    \Pr_{S, S&#39; } \{  \exists h : |L_S (h) - L_{S&#39;} (h) | \ge \frac{\epsilon}{2} \} 
        \le 
    \max_{S, S&#39; \in \mathcal{Z}^n } \Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma S&#39; } (h) | \ge \frac{\epsilon}{2} ]
\]</span></p>
<p><em>Proof of Step 3.</em><br />
We claim that <span class="math inline">\(\sigma S\)</span> and <span class="math inline">\(\sigma S&#39;\)</span> have the same joint distribution as <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> (if <span class="math inline">\(S, S&#39; \sim \mathcal{D}^n\)</span>). For points <span class="math inline">\(\forall Z, Z&#39; \in \mathcal{Z}^n\)</span>, by symmetry, it holds that <span class="math display">\[
\Pr_{S, S&#39;} [ S = Z, S&#39; = Z&#39;] = \Pr_{S, S&#39;, \sigma} [ \sigma S = Z, \sigma S&#39; = Z&#39;]
\]</span></p>
<p>The right hand side can be viewed as the successful probability of the following experiment:</p>
<ul>
<li>Sample independently <span class="math inline">\(S \sim \mathcal{D}^n\)</span> and <span class="math inline">\(S&#39; \sim \mathcal{D}^n\)</span>.<br />
</li>
<li>For each <span class="math inline">\(i \in [n]\)</span>, exchange the <span class="math inline">\(i\)</span>-th elements of <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> independently with probability <span class="math inline">\(0.5\)</span>.</li>
<li>After the random swap, <span class="math inline">\(\sigma S = Z\)</span> and <span class="math inline">\(\sigma S&#39; = Z&#39;\)</span>.</li>
</ul>
<p>Now, <span class="math inline">\(\forall \mathcal{E} \subset \mathcal{Z}^{2n}\)</span>, it holds that <span class="math display">\[
\begin{aligned}
    \Pr_{S, S&#39;} [ (S, S&#39;) \in \mathcal{E} ] 
        &amp;= \Pr_{S, S&#39;, \sigma} [ (\sigma S, \sigma S&#39;) \in \mathcal{E} ] \\
        &amp;= \mathbb{E}_{S, S&#39;} [ \Pr_\sigma [(\sigma S, \sigma S&#39;) \in \mathcal{E} ] \mid S, S&#39; ] \\
        &amp;\le \max_{S, S&#39; \in \mathcal{Z}^n } \Pr_\sigma [(\sigma S, \sigma S&#39;) \in \mathcal{E} ]
\end{aligned}
\]</span></p>
<p>Replacing <span class="math inline">\(\mathcal{E}\)</span> with the set <span class="math inline">\(\{Z, Z&#39; \in \mathcal{Z}^n : \exists h : |L_Z (h) - L_{Z&#39;} (h) | \ge \frac{\epsilon}{2} \}\)</span> finishes the proof.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><em>Remark of the proof of step 1.</em> If we define <span class="math display">\[
\mathcal{H}(S, \epsilon) \doteq \{ h \in \mathcal{H} :L_S(h) \notin B(\mu_h, \epsilon ) \}.
\]</span> <em>which is the set of hypothesis whose empirical loss on <span class="math inline">\(S\)</span> is <span class="math inline">\(\epsilon\)</span> more than its expectation. Then,</em> <span class="math display">\[
\begin{aligned}
    \Pr_{S} [ \exists h : L_S(h) \notin B(\mu_h, \epsilon ) ] \cdot \frac{1}{2} 
    &amp;= \Pr_{S } [ \mathcal{H}(S, \epsilon) \neq \emptyset ] \cdot \frac{1}{2} \\
    &amp;\le \Pr_{S } [ \mathcal{H}(S, \epsilon) \neq \emptyset] \\
    &amp; \ \ \cdot \Pr_{S, S&#39; } [ \exists h \in \mathcal{H}(S, \epsilon) : L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2 ) \mid \mathcal{H}(S, \epsilon) \neq \emptyset ] \\
    &amp;= \Pr_{S, S&#39; } [ \exists h : L_S (h) \notin B(\mu_h, \epsilon )  \wedge L_{S&#39;} (h) \in B(\mu_h, \epsilon / 2 )  ]
\end{aligned}
\]</span></p>
<p><em>Note that the event <span class="math inline">\(\mathcal{H}(S, \epsilon) \neq \emptyset\)</span> is equivalent to the one</em> <span class="math display">\[
\cup_{h \in \mathcal{H} } \{ L_S(h) \notin B(\mu_h, \epsilon ) \} 
\]</span></p>
<p><em>For a fixed <span class="math inline">\(h \in \mathcal{H}\)</span>, the event <span class="math inline">\(\{ L_S(h) \notin B(\mu_h, \epsilon ) \}\)</span> is measurable when <span class="math inline">\(S\)</span> consists of a finite number of i.i.d samples from <span class="math inline">\(\mathcal{D}\)</span>. If <span class="math inline">\(\mathcal{H}\)</span> consists of countable number of <span class="math inline">\(h\)</span>, then <span class="math inline">\(\cup_{h \in \mathcal{H} } \{ L_S(h) \notin B(\mu_h, \epsilon ) \}\)</span> is a countable union and should be measurable.</em></p>
<!-- *Definition*.

 1. $\Gamma_n:$ the set of permutations on $\{1, 2, ..., 2n \}$ that swaps only $i$ and $i + n$ for $\forall i \in [n]$,  s.t., $\forall \sigma \in \Gamma_n$,  $\forall i \in [n]$, 
    $$
    \begin{aligned}
        \text{ either }
        \begin{cases}
            \sigma(i) = i  \\
            \sigma(i + n) = i + n
        \end{cases}
        \text{ or }
        \begin{cases}
            \sigma(i) = i + n \\
            \sigma(i) = i 
        \end{cases}.
    \end{aligned}   
    $$

As $\sigma$ is an one-to-one mapping, its inversion $\sigma^{-1}$ exists. Further, $\sigma^{-1}$ is also a permutation in $\Gamma_n$. 

Let $S \sim \mathcal{D}^{2n}$ and denote it as 
$$
S \doteq \{ (x_1, y_1), ..., (x_i, y_i), ..., (x_{2n}, y_{2n} ) \}.
$$

Applying a permutation $\sigma$ on $S$, we obtain
$$
\sigma S \doteq \{ (x_{ \sigma^{-1}(1) }, y_{ \sigma^{-1}(1) } ), ..., (x_{ \sigma^{-1}(i) }, y_{ \sigma^{-1}(i) } ), ...., (x_{ \sigma^{-1}(2n) }, y_{ \sigma^{-1}(2n) } )\}
$$

Observe that $\forall i \in [n]$, $(x_{ \sigma^{-1}(i) }, y_{ \sigma^{-1}(i) } )$ is sampled independently from $\mathcal{D}$. It follows that $\sigma S$ has the same distribution as $S$. 

***Definition.***

1. $\forall \mathcal{E} \subset \mathcal{Z}^{2n}, \forall S \in \mathcal{Z}^{2n}$, define $\mathbb{1}_\mathcal{E} (S)$ the indicator function of whether $S$ belongs to $\mathcal{E}$. 

> Corollary. $\forall \sigma \in \Gamma_n, \forall \mathcal{E} \subset \mathcal{Z}^{2n}$, it holds that 
> $$
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ] = \Pr_{S \sim \mathcal{D}^{2n}} [ \sigma S \in \mathcal{E} ] = \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ]
> $$

If we choose an $\sigma$ uniformly from $\Gamma_n$, then 
$$
\begin{aligned}
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ]  
        &= \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &= \frac{1}{ |\Gamma_n| } \sum_{\sigma \in \Gamma_n } \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &=  \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \frac{1}{ |\Gamma_n| } \sum_{\sigma \in \Gamma_n } \mathbb{1}_\mathcal{E} (\sigma S) ] \\
        &=  \mathbb{E}_{S \sim \mathcal{D}^{2n}} [ \Pr_\sigma [\sigma S \in \mathcal{E} ] ]. \\
\end{aligned}
$$

> Corollary 2. $\forall \mathcal{E} \subset \mathcal{Z}^{2n}$, it holds that 
> $$
    \Pr_{S \sim \mathcal{D}^{2n} } [ S \in \mathcal{E} ] \le \max_{S \in \mathcal{Z}^{2n} } \Pr_\sigma [\sigma S \in \mathcal{E} ]
> $$

***Definition.*** For $S \in \mathcal{Z}^{2n}$, define 
$$
S_1 \doteq \{ (x_1, y_1), ..., (x_n, y_n) \}
$$
as the first half of $S$ and 
$$
S_2 \doteq \{ (x_{n + 1}, y_{n + 1}), ..., (x_{ 2n } , y_{ 2n }) \}
$$
as the second half. In a similar manner we define $\sigma S_1$ and $\sigma S_2$ as the first and second half of $\sigma S$ respectively. 

It follows from Corollary 2 that 

The corollary allows us to upper bond the failure probability on a fixed $S$ that maximizes $\Pr_\sigma [ \exists h : |L_{ \sigma S_1} (h) - L_{ \sigma S_2 } (h) | \ge \frac{\epsilon}{2} ]$, and now the randomness comes only from the uniform choice of $\sigma$. This is important as there are finitely many of functions defined on finite set $S$, on which we could apply union bound.  -->
<h3 id="proof-of-lemma-2."><strong><em>Proof of Lemma 2.</em></strong></h3>
<p>Let <span class="math display">\[
S = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \}
\]</span> and <span class="math display">\[
S&#39; = \{ (x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;), ..., (x_n&#39;, y_n&#39;) \}.
\]</span> be a pair of fixed sample sets. Define the set <span class="math inline">\(\mathcal{C} = \{ x_1, x_2, ..., x_n, x_1&#39;, x_2&#39;, ..., x_n&#39; \} \subset \mathcal{X}\)</span> (with duplicates removed). The size of <span class="math inline">\(\mathcal{C}\)</span> is bounded by <span class="math inline">\(2n\)</span> and therefore the size of restriction of <span class="math inline">\(\mathcal{H}\)</span> on <span class="math inline">\(\mathcal{C}\)</span> is bounded by <span class="math inline">\(\Pi_\mathcal{H} (2n)\)</span>.</p>
<p>First, fix a hypothesis <span class="math inline">\(h\)</span>. For all <span class="math inline">\(i \in [n]\)</span>, define <span class="math inline">\(a_i = |\ell( h( x_i ), y_i) - \ell( h( x_i&#39; ), y_i&#39; ) | \le 1\)</span> and let <span class="math inline">\(V_i \in \{-1, 1\}\)</span> be a random variable with equal probability: <span class="math display">\[
\Pr[ V_i = -1 ] = \Pr[ V_i = 1] = 0.5
\]</span></p>
<p>If we apply a random swap <span class="math inline">\(\sigma\)</span> to <span class="math inline">\((S, S&#39;)\)</span>, then <span class="math inline">\(L_{ \sigma S} (h_S) - L_{ \sigma S&#39; } (h_S)\)</span> has the same distribution as <span class="math inline">\(\sum_{i \in [n] } \frac{1}{n} a_i V_i\)</span>. Since <span class="math display">\[
\mathbb{E} [\sum_{i \in [n] } \frac{1}{n} a_i V_i] = \sum_{i \in [n] } \frac{1}{n} a_i \mathbb{E}[ V_i ] = 0,
\]</span></p>
<p>by Hoeffding inequality, <span class="math display">\[
\begin{aligned}
    \Pr_\sigma [ |L_{ \sigma S} (h) - L_{ \sigma  S&#39; } (h) | \ge \frac{\epsilon}{2} ] 
        &amp;= \Pr[ | \sum_{i \in [n] } \frac{1}{n} a_i V_i - 0 | \ge \frac{\epsilon}{2} ] \\
        &amp;\le 2 \exp( - \frac{ 2 }{  \sum_{i \in [n] } a_i^2 } (\frac{n \epsilon}{2})^2 )  \\
        &amp;\le 2 \exp( - \frac{n^2 \epsilon^2 }{2} )
\end{aligned}
\]</span></p>
<p>The last inequality follows from that <span class="math inline">\(\sum_{i \in [n] } a_i^2 \le n\)</span>.</p>
<p>Since for each <span class="math inline">\(h \in \mathcal{H}\)</span>, it has the same behavior on <span class="math inline">\(\mathcal{C}\)</span> as some function in <span class="math inline">\(\mathcal{H}_\mathcal{C}\)</span>. We can apply union bound on <span class="math inline">\(\mathcal{H}_\mathcal{C}\)</span>. <span class="math display">\[
\Pr_\sigma [ \exists h : |L_{ \sigma S} (h) - L_{ \sigma  S&#39; } (h) | \ge \frac{\epsilon}{2} ] \le 2 |\mathcal{H}_\mathcal{C} | \exp( - \frac{n^2 \epsilon^2 }{2} )\le 2 \Pi_\mathcal{H} (2n) \exp( - \frac{n^2 \epsilon^2 }{2} )
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] Ethan Fetaya, "Lecture 02 - Introduction to Statistical Learning Theory", Weizmann Institute of Science, 2016<br />
[2].R. Schapire and D. Bieber, “Lecture 05 - COS 511: Theoretical Machine Learning,”, Princeton University, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/13/Advanced-Composition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/13/Advanced-Composition/" class="post-title-link" itemprop="url">Advanced Composition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-13 22:26:07" itemprop="dateCreated datePublished" datetime="2020-11-13T22:26:07+11:00">2020-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-15 22:41:53" itemprop="dateModified" datetime="2020-11-15T22:41:53+11:00">2020-11-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Once we have designed some basic differentially private algorithms, it is a natural idea to combine them and analysis the privacy loss. We begin with an illustrative example that sets up the mathematical model step by step.</p>
<p>Image yourself in front of the door of a safe vault protected by a password lock. To open the door, you need the correct password <span class="math inline">\(P\)</span>. If tried with the wrong password, the lock would destroy itself automatically.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/door.png?raw=true" width="400" height="340" /></p>
</div>
<p>Luckily, you know two candidate passwords <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span>, with one of them being the correct one. Further, you notice that the designer of the lock left a collection of boxes near the door, which contain information on how they decide the correct passwords. Obtaining complete information of anyone of them gives you the correct password.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box1.png?raw=true" width="400" height="340" /></p>
</div>
<p>But the boxes are also protected and you don't have legally access to them. However, you can hack into the boxes. Hacking into the box won't give you all its information, but a random message. In particular, each box <span class="math inline">\(B\)</span> is associated with a set <span class="math inline">\(\mathcal{R}_B\)</span>, which is a finite collection of messages (in English). When <span class="math inline">\(B\)</span> is hacked, it returns a message <span class="math inline">\(Y_B\)</span> generated randomly from <span class="math inline">\(\mathcal{R}_B\)</span>, whose distribution depends on the correct password <span class="math inline">\(P\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> be the distribution of <span class="math inline">\(Y_B\)</span> if the correct password is <span class="math inline">\(P_1\)</span>, and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> be the one if the correct password is <span class="math inline">\(P_2\)</span>. If there is a huge difference between <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>, then you might be able to guess the correct password.</p>
<p>E.g., suppose <span class="math inline">\(\mathcal{R}_B =\)</span> { "Dog bites.", "Cat scratches." } and the distributions are given as</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_1}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">"Dog bites."</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="even">
<td style="text-align: center;">"Cat scratches."</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.99</td>
</tr>
</tbody>
</table>
<p>Hence, when you get <span class="math inline">\(Y_B=\)</span> "Dog bites.", you prefer <span class="math inline">\(P_1\)</span> over <span class="math inline">\(P_2\)</span> and vice versa.</p>
<p>Anticipating such potential information leakage, the designer of the lock equips the boxes with a defense mechanism, called <span class="math inline">\((\epsilon, 0)\)</span>-mechanism. It guarantees the distributions <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are similar so that it is hard for you to infer the correct password from the message. In particular, <span class="math inline">\(\forall S \subset \mathcal{R}_B\)</span>, if <span class="math inline">\(S\)</span> is measurable, it holds that <span class="math display">\[
\Pr[ Y_B \in S \ | \ P = P_1 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_2 ] \\
\Pr[ Y_B \in S \ | \ P = P_2 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_1 ] \\
\]</span></p>
<p>When these inequalities are satisfied, we say that <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close. The inequalities require <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> to have the same support on <span class="math inline">\(\mathcal{R}_B\)</span>. Therefore, throughout our discussion below, we assume that both <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> assign positive probability to each element in <span class="math inline">\(\mathcal{R}_B\)</span>. Otherwise, we can just replace <span class="math inline">\(\mathcal{R}_B\)</span> with the support of <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> (which is also the support of <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>).</p>
<p>Now, hacking one box is unlikely to help you to guess the correct password. You want to hack more boxes, with the hope that the information combined will assist you. Due to resource limit, you can't hack all boxes but only a finite number of them. You choose the first box randomly. Then you choose every new box based on the information obtained from the hacked boxes. The figure below shows an example of hacking five boxes.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box2.png?raw=true" width="400" height="340" /></p>
</div>
<p>Suppose that your resource enables you to hack <span class="math inline">\(n\)</span> boxes and let <span class="math inline">\(\vec Y_n = (Y_1, Y_2, ..., Y_n)\)</span> be the output messages you obtained. Similar to the situation of hacking one box, if the distribution of <span class="math inline">\(\vec Y_n\)</span>, conditioned on <span class="math inline">\(P = P_1\)</span>, is utterly distant from that conditioned on <span class="math inline">\(P = P_2\)</span>, then there could be some cases when you can confidently infer the true password. Conversely, to prevent severe information leakage, the designer needs to ensure there isn't such case, i.e., the two distributions should be similar.</p>
<p>What makes things even more complicated is that, the distribution of <span class="math inline">\(\vec Y_n\)</span> depends not only on the output distributions of the boxes, but also on your strategy of choosing the boxes to hack. Let's use symbol <span class="math inline">\(A\)</span> to denote your strategy. Whatever <span class="math inline">\(A\)</span> is, the designer need to guarantee that the distribution of <span class="math inline">\(\vec Y_n\)</span> conditioned <span class="math inline">\(P = P_1\)</span> should be similar to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Surprisingly, this is in some sense achievable, as long as for each box, its output distribution conditioned on <span class="math inline">\(P = P_1\)</span> is <span class="math inline">\((\epsilon, 0)\)</span> close to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{R}\)</span> be the set of possible possible messages of all boxes.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> <span class="math inline">\(\forall A\)</span>, <span class="math inline">\(\forall S \subset \mathcal{R}^n\)</span>, it holds that <span class="math inline">\(\forall \delta&#39; \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ \vec Y_n \in S \ | \ P = P_1, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; \\
\Pr[ \vec Y_n \in S \ | \ P = P_2, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_1, A] + \delta&#39;
\]</span> where <span class="math inline">\(\epsilon&#39; = k\epsilon(e^\epsilon - 1) + \epsilon \sqrt{2 n \log \frac{1}{\delta&#39;} }\)</span>.</p>
</blockquote>
<p>If we view <span class="math inline">\(\epsilon\)</span> as the privacy loss of a single box, then the theorem states that the privacy loss grows to <span class="math inline">\(O(\sqrt {n} \epsilon )\)</span> is <span class="math inline">\(n\)</span> boxes are hacked.</p>
<p>To prove the theorem, we need a rigorous model for the problem.</p>
<p><strong><em>Definitions.</em></strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathcal{I}\)</span>: the index set.</p></li>
<li><p><span class="math inline">\(\{ B_\alpha : \alpha \in \mathcal{I} \}\)</span>: the collection of boxes.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{R}_{\alpha} : \alpha \in \mathcal{I} \}\)</span>: the ranges of the outputs of the boxes.</p></li>
<li><p><span class="math inline">\(\mathcal{R} \doteq \cup_{\alpha \in \mathcal{I} } R_\alpha\)</span>: the range of any possible output by any box.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{D}_{\alpha, P_1} : \alpha \in \mathcal{I} \}\)</span> (<span class="math inline">\(\{ \mathcal{D}_{\alpha, P_2} : \alpha \in \mathcal{I} \}\)</span>): the set of output distribution when the correct password is <span class="math inline">\(P_1\)</span> (<span class="math inline">\(P_2\)</span>). Without loss of generality, we assume that for a fixed <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, the distribution <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> (<span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span>) assigns positive probability to each element in <span class="math inline">\(\mathcal{R}_\alpha\)</span>. Moreover, <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close.</p></li>
<li><p><span class="math inline">\(n\)</span>: the number of boxes you can hack.</p></li>
<li><p><span class="math inline">\(\vec i_k \doteq (i_1, i_2, ..., i_k)\)</span>: the index sequence of boxes you have chosen to hack up to time <span class="math inline">\(k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, <span class="math inline">\(\vec i_0\)</span> corresponds to a empty sequence.</p></li>
<li><p><span class="math inline">\(\vec Y_k = (Y_1, Y_2, ..., Y_k):\)</span> the random variables that represent messages outputted by the chosen boxes up to time <span class="math inline">\(k \in [1, n]\)</span>, where <span class="math inline">\(Y_t \in \mathcal{R}_{i_t} \subset \mathcal{R}\)</span> for <span class="math inline">\(t \in [1, k]\)</span>. Therefore, <span class="math inline">\(\vec Y_k\)</span> can be view as random vector in <span class="math inline">\(\mathcal{R}^k\)</span>.</p></li>
<li><p><span class="math inline">\(\vec x_k = (x_1, x_2, ..., x_k) \in \mathcal{R}^k:\)</span> a point in <span class="math inline">\(\mathcal{R}^k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, we define <span class="math inline">\(\vec x_0 = \emptyset\)</span>.</p></li>
<li><p><span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt; = \left&lt; (i_1, i_2, i_3, ..., i_k), (x_1, x_2, ..., x_k) \right&gt;:\)</span> the history up to <span class="math inline">\(k \in [0, n]\)</span>, which consists of the chosen indexes and observations up to time <span class="math inline">\(k\)</span>. We use <span class="math inline">\(\vec h_0\)</span> to denote the empty history.</p></li>
<li><p><span class="math inline">\(A:\)</span> your strategy (policy) for choosing boxes. It works as follows: for any fixed history <span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt;\)</span>, A is associated with a fixed distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span> over <span class="math inline">\(\mathcal{I} \setminus \vec i_k\)</span>, the set of indexes of the unchosen boxes. When inputted with <span class="math inline">\(\vec h_k\)</span>, <span class="math inline">\(A\)</span> returns a random variable <span class="math inline">\(A(\vec h_k)\)</span> that follows the distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span>.</p></li>
</ol>
<p>We will show that, no matter what strategy you use, it is unlikely that you distinguish via the output <span class="math inline">\(\vec Y_n\)</span> whether the correct password is <span class="math inline">\(P_1\)</span> or <span class="math inline">\(P_2\)</span>. This is because whether <span class="math inline">\(P = P_1\)</span> or not, the output distributions of <span class="math inline">\(\vec Y_n\)</span> are similar.</p>
<p><strong><em>Proof of the theorem.</em></strong> We will just prove the first inequality, and the second one follows from symmetry. We consider a bad set in <span class="math inline">\(\mathcal{R}^n\)</span>: <span class="math display">\[
\mathcal{W} \doteq \{ \vec x_n \in \mathcal{R}^n : \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] \ge e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] \}
\]</span></p>
<p>and will use the following lemma.</p>
<blockquote>
<p><strong>Lemma.</strong> <span class="math inline">\(\Pr[ \vec Y_n \in \mathcal{W}\ | \ P = P_1, A] \le \delta&#39;\)</span>.</p>
</blockquote>
<p>Hence, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n \in S \ | \ P = P_1, A] 
        &amp;=  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in S \cap \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \delta&#39; \\
        &amp;= \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] + \delta&#39; \\
        &amp;&lt; \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } e^{\epsilon&#39;} \cdot  \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] + \delta&#39; \\
        &amp;=  e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; 
\end{aligned}
\]</span></p>
<p>The first inequality follows from monotonicity of probability, the second one from the lemma, and the final one from the definition of <span class="math inline">\(\mathcal{W}\)</span>.</p>
<p><em>Proof of the lemma.</em> To prove the lemma, we need only to consider those point <span class="math inline">\(\vec x_n\)</span> with <span class="math display">\[
\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0. 
\]</span> Otherwise, <span class="math inline">\(\vec x_n\)</span> contributes to 0 probability to the set <span class="math inline">\(\mathcal{W}\)</span> (whether it belongs to <span class="math inline">\(\mathcal{W}\)</span> or not).</p>
<p>Now, we expand the probability <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A]\)</span>. To obtain the <span class="math inline">\(k\)</span>-th output, there are two steps</p>
<ol type="1">
<li><span class="math inline">\(A\)</span> generates a index <span class="math inline">\(i_k\)</span> of a box based on the known history <span class="math inline">\(\vec h_{k - 1}\)</span>.<br />
</li>
<li>The box <span class="math inline">\(B_{i_k}\)</span> is hacked, and output a random message <span class="math inline">\(x_k\)</span> sampled from its distribution <span class="math inline">\(D_{i_k, P_1}\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(\vec H_k\)</span> be a random variable that represents the history up to <span class="math inline">\(k\)</span>. By chain rule, we have <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A  ]
\end{aligned}
\]</span></p>
<p>By <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0\)</span>, each term in the expansion are positive. Similarly, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A  ]
\end{aligned}
\]</span></p>
<p>As both <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> assign positive probability to each point in <span class="math inline">\(\mathcal{R}_{i_k}\)</span>, we know <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] &gt; 0\)</span>.</p>
<p>We are ready to consider the ratio: <span class="math display">\[
\begin{aligned}
    \ln \frac{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] }{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] } 
    = \sum_{k = 1}^n \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }. 
\end{aligned}
\]</span></p>
<p>If we replace <span class="math inline">\(x_k\)</span> by a random variable <span class="math inline">\(X_k \sim \mathcal{D}_{i_k, P_1}\)</span>, then <span class="math display">\[
C_k \doteq \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }
\]</span></p>
<p>is a random variable. As <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close, we have <span class="math display">\[
C_k \le \epsilon.
\]</span></p>
<p>Further, we have</p>
<blockquote>
<p><strong>Fact 1.</strong> For any <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, it holds that <span class="math display">\[
\begin{aligned}
\mathbb{E}_{ X \sim \mathcal{D}_{\alpha, P_1} } \left[ \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = X]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = X]  }  \right] 
&amp;= \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;+ 
\sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } \\
&amp;= \sum_{x \in \mathcal{R}_\alpha } \left[ \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  - \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x] \right] \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le (e^\epsilon - 1) \epsilon
\end{aligned}
\]</span></p>
</blockquote>
<p>Therefore, <span class="math inline">\(\mathbb{E} [C_k] \le (e^\epsilon - 1) \epsilon\)</span>.</p>
<blockquote>
<p><strong>Fact 2.</strong> (<strong>Azuma Inequality</strong>). Let <span class="math inline">\(C_1, ...., C_n\)</span> be random variables such that <span class="math inline">\(\forall k \in [n]\)</span>, <span class="math inline">\(\Pr[ |C_k| \le \epsilon ] = 1\)</span>, and for every <span class="math inline">\((c_1, ..., c_{k -1} ) \in \text{Supp} (C_1, ..., C_{k - 1} )\)</span>, we have <span class="math display">\[
\mathbb{E}[ C_i \mid C_1 = c_1, ..., C_{k -1} = c_{k - 1} ] \le \beta,
\]</span> Then for every <span class="math inline">\(z &gt; 0\)</span>, we have <span class="math display">\[
\Pr[ \sum_{k = 1}^n C_i -  n \beta &gt;  z] \le \exp(- \frac{z^2}{ 2 n\epsilon^2 } )
\]</span></p>
</blockquote>
<p>Finally, applying <em>Azuma Inequality</em> with <span class="math inline">\(z = \sqrt{ 2n \log \frac{1}{\delta&#39;} }\)</span> and <span class="math inline">\(\beta = (e^\epsilon - 1) \epsilon\)</span>, we get the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference.">Reference.</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/12/PAC-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/12/PAC-learning/" class="post-title-link" itemprop="url">PAC Learning Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-12 16:06:55" itemprop="dateCreated datePublished" datetime="2020-11-12T16:06:55+11:00">2020-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-19 15:16:12" itemprop="dateModified" datetime="2020-11-19T15:16:12+11:00">2020-11-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss a toy model of probably approximately correct (PAC) learning [1]. The setting focuses on a <span class="math inline">\(d\)</span>-dimension Boolean hypercube <span class="math inline">\(\{0, 1\}^d\)</span> and a set of functions <span class="math inline">\(\mathcal{H} = \{ h : \{0, 1\}^d \rightarrow \{0, 1 \} \}\)</span>. A function <span class="math inline">\(h \in \mathcal{H}\)</span> is called a hypothesis or a concept, which assigns each vertex in the hypercube a label 0 or 1.</p>
<p>The goal of PAC learning is to learn an unknown hypothesis, denoted as <span class="math inline">\(h^* \in \mathcal{H}\)</span>, from a dataset, which consists of <span class="math inline">\(n\)</span> points <span class="math inline">\(X_1, X_2, ..., X_n \in \{0, 1\}^n\)</span> sampled independently from an unknown distribution <span class="math inline">\(D\)</span> (over the hypercube), as well as their labels <span class="math inline">\(Y_1 = h^*(X_1), Y_2 = h^*(X_2), ..., Y_n = h^*(X_n)\)</span>.</p>
<p>We can state the algorithm for PAC learning in just one sentence.</p>
<blockquote>
<p>Output any hypothesis <span class="math inline">\(h&#39; \in \mathcal{H}\)</span> that is consistent with the sampled dataset, i.e., <span class="math display">\[
h&#39;(X_i) = Y_i, \qquad \forall i \in [n]
\]</span></p>
</blockquote>
<p>Such an hypothesis always exists, as <span class="math inline">\(h^*\)</span> satisfies this constraint. We have the classic PAC learning theorem.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The PAC learning theorem states that the probability that <span class="math inline">\(h&#39;\)</span> mislabels a point is bounded by <span class="math inline">\(\sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}\)</span>.</p>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\mathbb{E}[Z_i]\)</span> is an unbiased estimator of <span class="math inline">\(\mu_h\)</span>. Further, denote <span class="math inline">\(\hat \mu_h\)</span> the empirical mean of <span class="math inline">\(Z_i\)</span>'s <span class="math display">\[
\hat \mu_h \doteq \frac{1}{n} \sum_{i \in [n] } Z_i
\]</span></p>
<p>Note that for the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(\hat \mu_{h&#39;} = 0\)</span>.</p>
<p>By Hoeffding inequality, for any <span class="math inline">\(\delta&#39; &gt; 0\)</span>, <span class="math display">\[
\Pr[ \hat \mu_h \le \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} } ] \le \delta&#39;. 
\]</span></p>
<p>By union bound, the probability that <span class="math display">\[
\exists h \in \mathcal{H}, \hat \mu_h \ge \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} }
\]</span></p>
<p>is bounded by <span class="math inline">\(| \mathcal{H} | \delta&#39;\)</span>. By setting <span class="math inline">\(\delta&#39; = \frac{\delta}{ | \mathcal{H} | }\)</span>, it is guaranteed that <span class="math display">\[
0 = \hat \mu_{h&#39; } \le \mu_{h&#39;} -  \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span></p>
<p>with probability at most <span class="math inline">\(\delta\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>The previous theorem can be improved with the <span class="math inline">\(\sqrt{ \cdot }\)</span> removed. The key here is that the selected <span class="math inline">\(h&#39;\)</span> is error-free on the samples.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>We will prove that if <span class="math inline">\(\mu_h \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}\)</span>, then <span class="math inline">\(h\)</span>'s probability of being selected is very low.</p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\Pr[Z_i = 1] = \mu_h\)</span>. Therefore, <span class="math display">\[
    \begin{aligned}
        \Pr[ Z_1 = 0 \wedge Z_2 = 0 \wedge ... \wedge Z_n = 0] = (1 - \mu_h)^n \le \exp(- \mu_h n) = \frac{\delta}{ |\mathcal{H} | }
    \end{aligned}
\]</span></p>
<p>Taking union bound over all possible <span class="math inline">\(h\)</span> gives the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Caveat.</em></strong> <em>In both theorem, we see a <span class="math inline">\(\log \mathcal{H}\)</span> term in the failure probability, which is due to the use of union bound and can be roughly considered as the complexity of the hypothesis family. It is tempting to think that this could be avoid as follows.</em></p>
<blockquote>
<p>For the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(Z_1, ..., Z_n\)</span> are i.i.d samples. Therefore, the probability of all of them being <span class="math inline">\(0\)</span> is give by<br />
<span class="math display">\[
(1 - \mu_{h&#39;} )^n \le \exp( - \mu_{h&#39;} n). 
  \]</span><br />
Bounding this probability by <span class="math inline">\(\delta\)</span> gives <span class="math inline">\(\mu_{h&#39;} \le \frac{ \log \frac{1}{\delta} } {n}\)</span>.</p>
</blockquote>
<p><em>The argument is wrong. For the outputted <span class="math inline">\(h&#39;\)</span>, the <span class="math inline">\(Z_1, ..., Z_n\)</span> are not i.i.d samples, as <span class="math inline">\(h&#39;\)</span> is selected according to <span class="math inline">\(Z_1, ..., Z_n\)</span> and depends on them.</em></p>
<p><em>To make it more concrete, suppose that there is an <span class="math inline">\(h\)</span>, such that <span class="math inline">\(\mu_h = \frac{ \log \frac{1}{\delta} } {n}\)</span>. Consider the following experiment</em></p>
<blockquote>
<ol type="1">
<li>Sample <span class="math inline">\(n\)</span> points independently the distribution <span class="math inline">\(D\)</span>.<br />
</li>
<li>Pick an <span class="math inline">\(h&#39;\)</span> that makes no prediction error on the samples.</li>
</ol>
</blockquote>
<p><em>If we repeat the experiment <span class="math inline">\(N\)</span> times for some positive integer <span class="math inline">\(N\)</span>, then the fixed <span class="math inline">\(h\)</span> makes no mistake in roughly <span class="math inline">\((1 - \delta) N\)</span> of the experiments and constitutes an candidate of <span class="math inline">\(h&#39;\)</span>. However, on the other roughly <span class="math inline">\(\delta N\)</span> experiments, where <span class="math inline">\(h\)</span> makes prediction error, it is no longer considered as the candidate of <span class="math inline">\(h&#39;\)</span>. The selection procedure of <span class="math inline">\(h&#39;\)</span> ignores bad event of <span class="math inline">\(h\)</span> automatically.</em></p>
<p><em>We could also investigate the following example. Let <span class="math inline">\(h^* = h_0\)</span>. There are also other two hypothesis <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span>. Let <span class="math inline">\(S\)</span> be the sample space. It holds that</em> <span class="math display">\[
|h_1(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_1} \\
    0,\ \       \forall x \in S_{h_1}
\end{cases}
\]</span> <span class="math display">\[
|h_2(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_2} \\
    0,\ \       \forall x \in S_{h_2}
\end{cases}
\]</span> <em>Moreover, <span class="math inline">\(\Pr[S_{h_1}] = \Pr[S_{h_2} ] = \delta\)</span>. How, if we get a sample in <span class="math inline">\(S_{h_1}\)</span>, then we can output <span class="math inline">\(h&#39;\)</span> as <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_1\)</span>, since both of them make no error when <span class="math inline">\(x \in S_{h_1}\)</span>. Similarly, if <span class="math inline">\(x \in S_{h_2}\)</span>, we can output either <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_2\)</span>. We call <span class="math inline">\(S_{h_1}\)</span> and <span class="math inline">\(S_{h_2}\)</span> the bad areas. When <span class="math inline">\(x\)</span> is in the bad area of any hypothesis, we can't distinguish this hypothesis from the true hypothesis. That is why we need to use union bound to bound the total probabilities of these bad areas.</em></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/PAC-Learning.png?raw=true" width="400" height="340" /></p>
</div>
<h3 id="reference">Reference</h3>
<p>[1] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. 6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/Gaussian-Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/06/Gaussian-Mechanism/" class="post-title-link" itemprop="url">Gaussian Mechanism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-06 23:00:19" itemprop="dateCreated datePublished" datetime="2020-11-06T23:00:19+11:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-10 20:56:26" itemprop="dateModified" datetime="2020-11-10T20:56:26+11:00">2020-11-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="gaussian-distribution">Gaussian Distribution</h2>
<p>We illustrate how Gaussian distribution (a.k.a. normal distribution) is constructed and some of its basic properties.</p>
<p>Suppose we would like to have a distribution in <span class="math inline">\(\mathbb{R}\)</span>, such that</p>
<ol type="1">
<li>It is centered at the origin.<br />
</li>
<li>Its density function decreases exponentially with respect to the square distance to the origin.</li>
</ol>
<p><em>Remark: if we replace 2 with "a density function decreases exponentially with the distance to the origin", we obtain Laplace distribution.</em></p>
<p>By definition, the density function <span class="math inline">\(p(x)\)</span> should be inversely and exponentially proportional to <span class="math inline">\(x^2\)</span>:</p>
<p><span class="math display">\[
p(x) \propto \exp(-x^2 )
\]</span></p>
<p>If we define <span class="math inline">\(M\)</span> to be the value of<br />
<span class="math display">\[
M \doteq \int_{-\infty}^\infty \exp(-x^2) \ dx, 
\]</span></p>
<p>then we can give the precise formula of <span class="math inline">\(p(x)\)</span>: <span class="math display">\[
p(x)  = \frac{1}{M} \exp(- x^2)
\]</span></p>
<p>It is easy to see that <span class="math inline">\(p(x)\)</span> is a density function such that <span class="math inline">\(\int_{-\infty}^\infty p(x) \ dx = 1\)</span>.</p>
<h3 id="closed-form">Closed Form</h3>
<p>In general, it is not hard to construct a distribution. Let <span class="math inline">\(h(x)\)</span> be any integrable function on a domain <span class="math inline">\(\mathcal{D}\)</span> such that <span class="math display">\[
M = \int_\mathcal{D} h(x) \ dx &lt; \infty
\]</span></p>
<p>Then we can view <span class="math inline">\(h(x)\)</span> as almost an density function of some distribution. The only obstacle is that <span class="math inline">\(M\)</span> might not be <span class="math inline">\(1\)</span>. However this is easy to overcome. We just scale the value of <span class="math inline">\(h(x)\)</span> by a constant factor at each point on <span class="math inline">\(\mathbb{R}\)</span> and set <span class="math display">\[
p(x) = \frac{1}{M} h(x)
\]</span></p>
<p>then we have immediately a density function. We don't even need to know the exact value of <span class="math inline">\(M\)</span>. What we need to guarantee is just that</p>
<blockquote>
<p>The value of <span class="math inline">\(M\)</span> exists and is finite.</p>
</blockquote>
<p>And we can safely use <span class="math inline">\(M\)</span> to denote this value. For example, we know that <span class="math display">\[
\int_0^\infty x^{0.2} e^{-x} \ dx \le  \int_0^1 e^{-x} \ dx  + \int_1^\infty x e^{-x} \ dx &lt; \infty.
\]</span></p>
<p>We can construct a density function as above. It turns out that this is a special case of <em>Gamma distribution</em>.</p>
<p>For Gaussian distribution, we are lucky as we can compute a closed form of <span class="math inline">\(M\)</span>. This is done by a trick of computing its square: <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \big( \int_{-\infty}^\infty \exp( -x^2 ) \  dx \big)^2 \\
        &amp;= \int_{-\infty}^\infty \exp( -x^2 ) \  dx \ \cdot \int_{-\infty}^\infty \exp( -y^2 ) \  dy \\
        &amp;= \int_{-\infty}^\infty \int_{-\infty}^\infty \exp( -(x + y) ^2 ) \  dx  dy. 
\end{aligned}
\]</span></p>
<p>Denote <span class="math inline">\(r = \sqrt{ x^2 + y^2}\)</span> the distance of <span class="math inline">\((x, y)\)</span> to <span class="math inline">\((0, 0)\)</span>. We are integrating the function <span class="math inline">\(\exp( - r^2)\)</span> over the plane <span class="math inline">\(\mathbb{R}^2\)</span>. It is natural to switch to polar coordinate system <span class="math inline">\((r, \theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the angle of a point. Here is an informal but intuitive explanation. As the picture demonstrated below, if <span class="math inline">\(r\)</span> increases by <span class="math inline">\(dr\)</span> and <span class="math inline">\(\theta\)</span> increases by <span class="math inline">\(d\theta\)</span>, the new area spanned by <span class="math inline">\(dr\)</span> and <span class="math inline">\(d \theta\)</span> is roughly <span class="math inline">\(r dr d \theta\)</span>. Or we can calculate it algebraically <span class="math display">\[
\frac{1}{2} [(r + dr)^2 - r^2] d\theta = r dr d \theta + \frac{1}{2} (dr)^2 d \theta \approx r dr d \theta.
\]</span></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration1.png?raw=true" width="400" height="340" /></p>
</div>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    M^2 &amp;= \int_{0}^{ 2 \pi} \int_{0}^\infty \exp( -r^2 ) r d r d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2}  \int_{0}^\infty \exp( -r^2 ) d r^2 d \theta \\
        &amp;= \int_{0}^{ 2 \pi} \frac{1}{2} d \theta \\
        &amp;= \pi.
\end{aligned}
\]</span></p>
<p>Hence, <span class="math display">\[
p(x) = \frac{1}{ \sqrt \pi} \exp( - x^2 ).
\]</span></p>
<h3 id="basic-properties">Basic Properties</h3>
<ol type="1">
<li><blockquote>
<p><em>Expectation.</em> As the distribution is symmetric to <span class="math inline">\(0\)</span>, it has expectation 0.</p>
</blockquote></li>
<li><blockquote>
<p><em>Variance.</em> As its expectation is 0, the variance is given by</p>
</blockquote>
<span class="math display">\[
 \begin{aligned}
     \int_{-\infty}^\infty  \frac{ x^2 }{ \sqrt \pi } \exp( -x^2 ) \ dx 
         &amp;= -\frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty x \ d \exp( -x^2 ) \\
         &amp;= \frac{1}{2 \sqrt \pi } \int_{-\infty}^\infty \exp( -x^2 ) d x\\
         &amp;= \frac{1}{2 \sqrt \pi } \sqrt \pi \\
         &amp;= \frac{1}{2}
 \end{aligned}
 \]</span></li>
</ol>
<h3 id="general-gaussian-distribution">General Gaussian Distribution</h3>
<p>We have proved that if <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(p(x) = \frac{1}{\sqrt \pi} \exp( -x^2)\)</span>, then <span class="math display">\[
\mathbb{Var}[X] = \frac{1}{2}
\]</span></p>
<p>Consider another random variable <span class="math inline">\(Y = \sqrt{2} X\)</span>. It has variance <span class="math display">\[
\mathbb{Var}[X] = 2 \mathbb{Var}[X] = 1
\]</span></p>
<p>To obtain its density function of <span class="math inline">\(Y\)</span>, we first scale the function <span class="math inline">\(p(x)\)</span> horizontally by a factor of <span class="math inline">\(\sqrt 2\)</span> to get <span class="math display">\[
\frac{1}{ \sqrt{ \pi } } \exp( -\frac{x^2}{2} ).
\]</span></p>
<p>Now the area under the curve is <span class="math inline">\(\sqrt 2\)</span>. We normalize this area to 1 and obtain <span class="math inline">\(Y\)</span>'s density function as <span class="math display">\[
p(y) = \frac{1}{ \sqrt{2 \pi } } \exp( -\frac{y^2}{2} )
\]</span></p>
<p>From now on, we use <span class="math inline">\(N(0, 1)\)</span> to denote a Gaussian distribution with mean 0 and variance <span class="math inline">\(1\)</span>. We can also scale <span class="math inline">\(X\)</span> by a factor of <span class="math inline">\(\sqrt 2 \sigma\)</span> for any <span class="math inline">\(\sigma &gt; 0\)</span>, to get <span class="math inline">\(Y = \sqrt 2 \sigma X\)</span>. It have variance <span class="math inline">\(\sigma^2\)</span> and density function <span class="math display">\[
p(y) = \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( - (\frac{y}{ \sqrt{2} \sigma })^2 ) = \frac{1}{ \sqrt{2 \pi \sigma^2 } } \exp( -\frac{y^2}{ 2 \sigma^2 } )
\]</span></p>
<p>Finally, we can shift the center of <span class="math inline">\(0\)</span> to any real number <span class="math inline">\(\mu \in \mathbb{R}\)</span>, and obtain a density function <span class="math display">\[
p(y) = \frac{1}{ \sqrt{ \pi\cdot 2  \sigma^2 } } \exp( -\frac{ (y - \mu)^2 }{ 2 \sigma^2 } )
\]</span></p>
<p>The corresponding distribution is denoted as <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<em>The picture below shows a few Gaussian distributions.</em>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution.png?raw=true" width="700" height="430" /></p>
</div>
<h3 id="further-properties">Further Properties</h3>
<ol type="1">
<li><blockquote>
<p>If <span class="math inline">\(X \sim N(0, a^2)\)</span> and <span class="math inline">\(Y \sim N(0, b^2)\)</span>, then <span class="math inline">\(X + Y \sim N(0, a^2 + b^2)\)</span>.</p>
</blockquote>
<p>The demonstration here follows from [1]. The key observation is that the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is rotation invariant with the origin.</p>
<p>As an example, we plot below the joint distribution of <span class="math inline">\(X, Y \sim N(0, 1)\)</span>.</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-0.png?raw=true" width="500" height="330" />
</div>
<p>The figure below shows the same distribution. Observe again that the probability mass concentrates at a small region centered at origin.</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-3D-1.png?raw=true" width="500" height="330" />
</div>
<p>Therefore, for any <span class="math inline">\(t \in \mathbb{R}\)</span>, the set <span class="math display">\[
 \{ (x, y) \in \mathbb{R}^2 : ax + by \le t \}
 \]</span> has the same probability measure as the one <span class="math display">\[
 \{ (x, y) \in \mathbb{R}^2 : x \le \frac{ t}{ \sqrt{a^2 + b^2} } \}.
 \]</span></p>
<p>Observe that the latter can be obtained by rotating the former with respect to the origin (see the figure below).</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration2.png?raw=true" width="800" height="350" />
</div>
<p>Now, the probability of the latter set is given by <span class="math display">\[
 \Pr[ X \le \frac{ t}{ \sqrt{a^2 + b^2} } ] = \Pr[  \sqrt{a^2 + b^2} X \le t]
 \]</span></p>
<p>It concludes that <span class="math inline">\(X + Y\)</span> has the same distribution as <span class="math inline">\(\sqrt{a^2 + b^2} X \sim N(0, a^2 + b^2)\)</span>.</p>
<p><em>Remark:if we want, we can verify this algebraically:</em><br />
<span class="math display">\[
 \Pr[X + Y \le t] = \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} }\exp(-\frac{x^2}{2} ) \ dx 
 \]</span></p>
<p>hence <span class="math display">\[
 \begin{aligned}
     \frac{ \partial }{ \partial t} \Pr[X + Y \le t]
         &amp;= \frac{ \partial }{ \partial t} \int_{-\infty}^{ \frac{ t}{ \sqrt{a^2 + b^2} }  } \frac{1}{ \sqrt{2 \pi} }\exp(-\frac{x^2}{2} ) \ dx \\
         &amp;= \frac{1}{ \sqrt{2 \pi (a^2 + b^2) } }\exp(-\frac{x^2}{2 (a^2 + b^2) } )
 \end{aligned}
 \]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p></li>
<li><blockquote>
<p>If <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>, then <span class="math inline">\(\forall t &gt; 0\)</span>, <span class="math display">\[
\Pr[ |X| \ge t] \le \exp( -\frac{t^2}{ 2\sigma^2} )
\]</span></p>
</blockquote>
<p><em>Remark: it is possible to get a tighter bound by using more advanced techniques.</em></p>
<p>In previous figures, it seems Gaussian distributions have a shape bump around its mean. This is kind of mis-leading, because the x-axis and y-axis are scaled. In the figure below, we plot <span class="math inline">\(N(0, 1)\)</span>, with ratio between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes being approximate <span class="math inline">\(1:1\)</span>. We see only a small bump around its mean. Although the distribution span the entire range of <span class="math inline">\(\mathbb{R}\)</span>, the probability mass is centered tightly around the origin.</p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Concentration.png?raw=true" width="800" height="550" />
</div>
<p>Let <span class="math inline">\(T = \Pr[ |X| \ge t]\)</span>. Then <span class="math display">\[
 T^2 = \int_{ |x| \ge t, |y| \ge t} \frac{1}{ 2 \pi \sigma^2 } \exp( - \frac{ x^2 + y^2}{2 \sigma^2} ) 
 \]</span></p>
<p>As <span class="math display">\[
 \{ (x, y) \in \mathbb{R}^2: |x| \ge t \wedge |y| \ge t \} \subset
 \{ (x, y) \in \mathbb{R}^2: x^2 + y^2 \ge 2 t^2 \} 
 \]</span></p>
<div style="text-align:center">
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Normal-Distribution-Integration3.png?raw=true" width="600" height="600" />
</div>
<p>It follows that <span class="math display">\[
 \begin{aligned}
     T^2 
         &amp;\le \int_{ \sqrt 2 t}^\infty \int_{0}^{2 \pi } \frac{1}{ 2 \pi \sigma^2 } \exp( - \frac{ r^2 }{2 \sigma^2} ) r \ d\theta dr\\
         &amp;\le \int_{ \sqrt 2 t }^\infty \exp( - \frac{ r^2 }{2 \sigma^2} ) \ d \frac{r^2}{ 2\sigma^2} \\
         &amp;\le \int_{ \frac{ t^2 }{  \sigma^2 } }^\infty \exp( - z ) \ d z \\
         &amp;= \exp( -\frac{t^2}{ \sigma^2} )
 \end{aligned}
 \]</span></p>
<p>Hence <span class="math inline">\(T \le \exp( -\frac{t^2}{ 2\sigma^2} )\)</span>.</p>
<p><em>Remark: we briefly discuss the algebraic approach here.</em> <span class="math display">\[
 \begin{aligned}
     \Pr[X &gt; t] = \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( - \frac{ x^2 }{ 2 \sigma^2 } ) \ dx
 \end{aligned}
 \]</span></p>
<p><em>When <span class="math inline">\(x \ge t\)</span>, <span class="math inline">\(\frac{x}{t} \ge 1\)</span>, therefore,</em> <span class="math display">\[
 \begin{aligned}
     \Pr[X &gt; t] 
         &amp;\le \int_{t}^\infty \frac{1}{ \sqrt{2 \pi \sigma^2} } \frac{x}{t} \exp( - \frac{ x^2 }{ 2  \sigma^2 } ) \ dx \\
         &amp;\le  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \int_{t}^\infty  \exp( - \frac{ x^2 }{ 2  \sigma^2 } ) \ d \frac{ x^2 }{ 2 \sigma^2 } \\
         &amp;=  - \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp( - \frac{ x^2 }{ 2  \sigma^2 } ) \mid_{t}^\infty \\
         &amp;=  \frac{ \sigma }{ \sqrt{2 \pi }  t }  \exp( - \frac{ t^2 }{ 2 \sigma^2 } )  \\
 \end{aligned}
 \]</span></p>
<p><em>This bound is more useful only when we know that <span class="math inline">\(t \ge \frac{ \sigma }{ \sqrt{2 \pi } }\)</span></em>. <!-- ![](https://www.mathworks.com/help/examples/stats/win64/ComputeTheMultivariateNormalPdfExample_01.png) --></p>
<p><span class="math inline">\(\blacksquare\)</span></p></li>
</ol>
<h2 id="gaussian-mechanism">Gaussian Mechanism</h2>
<p>Our discussion in this section focus on a metric space <span class="math inline">\((\mathcal{X}, d)\)</span> and a function <span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}^n\)</span>. The Gaussian mechanism adds Gaussian noise to the output of <span class="math inline">\(f\)</span>, such that for any neighboring pairs <span class="math inline">\(x, y \in \mathcal{X}\)</span> with <span class="math inline">\(d(x, y) = 1\)</span>, it is hard to distinguish the outputs, i.e., their outputs have similar distribution after adding noises.</p>
<blockquote>
<p><strong>Definition.</strong> The sensitivity <span class="math inline">\(\Delta\)</span> of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
\Delta = \max_{x, y \in \mathcal{X}, d(x, y) = 1 } ||f(x) - f(y) ||
\]</span> where <span class="math inline">\(|| \cdot ||\)</span> is the <span class="math inline">\(\ell_2\)</span>-norm of a vector in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Given <span class="math inline">\(f\)</span>, and a variance parameter <span class="math inline">\(\sigma^2\)</span>, the Gaussian mechanism is a random variable defines as <span class="math display">\[
g(x) = f(x) + X
  \]</span> where <span class="math inline">\(X = (X_1, X_2, ..., X_n) \sim N(0, \sigma^2 I)\)</span>.</p>
</blockquote>
<p>We are ready to state the main theorem of this section.</p>
<blockquote>
<p><strong>Theorem.</strong> For <span class="math inline">\(\delta, \epsilon \in (0, 1)\)</span>, if <span class="math inline">\(\sigma = \frac{ \Delta \log \frac{1}{\delta} }{ \epsilon }\)</span>, then <span class="math inline">\(\forall x, y \in \mathcal{X}\)</span>, s.t., <span class="math inline">\(d(x, y) = 1\)</span>, and for all measurable subset <span class="math inline">\(S \subset \mathbb{R}^n\)</span>, it holds</p>
<p><span class="math display">\[
\Pr[ g(x) \in S] \le e^\epsilon \cdot \Pr[ g(y) \in S] + \delta 
\]</span></p>
</blockquote>
<p>It is obvious that if we set <span class="math inline">\(\sigma \rightarrow \infty\)</span>, then the Gaussian distribution tends to be a uniform one over <span class="math inline">\(\mathbb{R}^n\)</span>. So we are interested in how small <span class="math inline">\(\sigma\)</span> can be.</p>
<p>We claim that <span class="math inline">\(\sigma = O(\frac{\Delta}{\epsilon } )\)</span> suffices. This is based on three observations: 1) Most of the probability mass of a Gaussian distribution concentrates on a ball centered at its mean with radius <span class="math inline">\(O(\sigma)\)</span>; 2) the density ratio of any two points within the ball is bounded by a constant; 3) the distance between the distribution center of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> is bounded by <span class="math inline">\(\Delta\)</span>.</p>
<p><strong><em>Proof.</em></strong> For convenience of discussion, we write <span class="math inline">\(g(y) = f(y) + Y\)</span> to distinguish it from <span class="math inline">\(g(x)\)</span>. By definition, <span class="math inline">\(g(x)\)</span> is a random variable that follows <span class="math inline">\(N(f(x), \sigma^2 I)\)</span>. Similarly, <span class="math inline">\(g(y) \sim N( f(y), \sigma^2 I)\)</span>.</p>
<p>Further, we define <span class="math inline">\(p_x\)</span> and <span class="math inline">\(p_y\)</span> the density function of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> respectively. The sketch of the proof is as follows: we partition <span class="math inline">\(\mathbb{R}^n\)</span> into two parts: <span class="math display">\[
\mathcal{Y}_1 = \{ t \in \mathcal{R}^n : p_x(t) \le e^\epsilon \cdot p_y(t) \} \\
\mathcal{Y}_2 = \{ t \in \mathcal{R}^n : p_x(t) &gt; e^\epsilon \cdot p_y(t) \}.
\]</span></p>
<p>If <span class="math inline">\(\Pr[ g(x) \in \mathcal{Y}_2 ] \le \delta\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[ g(x) \in S] 
        &amp;= \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[ g(x) \in S \cap \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[  g(x) \in \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \delta \\
        &amp;= \int_{S \cap \mathcal{Y}_1 } p_x(t) \ d t + \delta \\
        &amp;\le \int_{S \cap \mathcal{Y}_1 } e^\epsilon \cdot p_y(t) \ d t + \delta \\
        &amp;= e^\epsilon \cdot \Pr[ g(y) \in S \cap \mathcal{Y}_1] + \delta. \\
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(p_y(t) &gt; 0\)</span> for all <span class="math inline">\(t \in \mathbb{R}^n\)</span>, we can rewrite <span class="math display">\[
\mathcal{Y}_1 = \{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } \le \epsilon  \} \\
\mathcal{Y}_2 = \{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } &gt; \epsilon  \}.
\]</span></p>
<p>In literature, the ratio <span class="math inline">\(\ln \frac{p_x(t) }{ p_y(t) }\)</span> is know as <em>privacy loss</em>. Substituting <span class="math inline">\(p_x(t)\)</span> and <span class="math inline">\(p_y(t)\)</span> with their definitions, we get <span class="math display">\[
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &amp;= \ln \frac{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( -\frac{ ||t - f(x)||^2 }{ 2 \sigma^2 } ) }{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp( -\frac{ ||t - f(y) ||^2 }{ 2 \sigma^2 } )  } \\
        &amp;=  -\frac{ || t - f(x) ||^2 - || t - f(x) + f(x) - f(y) ||^2 }{ 2 \sigma^2 } 
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(t \sim N( f(x), \sigma^2 I)\)</span>, it holds that <span class="math inline">\(t - f(x) \sim N( 0, \sigma^2 I)\)</span>. Let <span class="math inline">\(t&#39; = t - f(x)\)</span> and <span class="math inline">\(v = f(x) - f(y)\)</span>, then <span class="math display">\[
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &amp;=  -\frac{ || t&#39; ||^2 - || t&#39; + v ||^2 }{ 2 \sigma^2 }    \\
        &amp;= \frac{ 2 v^T t&#39; + || v ||^2 }{ 2 \sigma^2 }    \\
\end{aligned}
\]</span></p>
<p>But <span class="math inline">\(v^T t&#39; \sim N(0, || v ||^2 \sigma^2)\)</span>. Let <span class="math inline">\(Z \sim N(0, 1)\)</span>, then <span class="math inline">\(\ln \frac{ p_x( t) }{ p_y(t)}\)</span> has the same distribution as <span class="math display">\[
\frac{ 2 ||v|| \sigma Z + ||v||^2 }{ 2 \sigma^2  } = \frac{ ||v||^2 }{ 2 \sigma^2  } + \frac{ ||v|| }{  \sigma  } Z \sim N(\frac{ ||v||^2 }{ 2 \sigma^2  },  \frac{ ||v||^2 }{  \sigma^2  } ).   \\
\]</span></p>
<p>Now, <span class="math display">\[
\frac{ ||v||^2 }{ 2 \sigma^2  } + \frac{ ||v|| }{  \sigma  } Z  \ge \epsilon \\
\longleftrightarrow  Z  \ge \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  }
\]</span></p>
<p>As <span class="math inline">\(Z \sim N(0, 1)\)</span>, as shown in the previous section, <span class="math inline">\(\forall z \in \mathbb{R}\)</span>, <span class="math display">\[
\Pr[ Z \ge z] \le \exp( -\frac{ z^2 }{2} )
\]</span></p>
<p>Replacing <span class="math inline">\(z\)</span> with <span class="math inline">\(\frac{ \sigma }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma }\)</span>, <span class="math display">\[
\Pr[ z \ge t ] \le  \exp( -\frac{1}{2} (\frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } )^2 ).
\]</span></p>
<p>We would like to bound this probability by some <span class="math inline">\(\delta \in (0, 1)\)</span>, then <span class="math display">\[
\begin{aligned}
    &amp;\exp( -\frac{1}{2} ( \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } )^2 ) \le \delta \\
    &amp;\Longrightarrow \frac{  \sigma  }{ ||v|| } \epsilon - \frac{ ||v|| }{ 2 \sigma  } \ge \sqrt{2 \ln \frac{1}{\delta} } \\
    &amp;\Longrightarrow \frac{\epsilon }{||v|| } \sigma^2 - \sqrt{2 \ln \frac{1}{\delta} } \sigma - \frac{ ||v|| }{2} \ge 0
\end{aligned}
\]</span></p>
<p>Finally, we get <span class="math display">\[
\sigma \ge \frac{ \sqrt{ 2 \ln \frac{1}{\delta} } + \sqrt{ 2 \ln \frac{1}{\delta} + 2 \epsilon  }  }{ 2 \frac{\epsilon }{||v|| } }
\]</span></p>
<p>By concavity of the square root function, it suffices to take <span class="math display">\[
\sigma = \frac{ ||v|| }{ \epsilon } \sqrt{ 0.5 * 2 \ln \frac{1}{\delta} + 0.5 * (2 \ln \frac{1}{\delta} + 2\epsilon) } = \frac{ ||v|| }{ \epsilon } \sqrt{  2 \ln \frac{1}{\delta} + \epsilon },
\]</span></p>
<p>i.e., <span class="math display">\[
\sigma^2 = \frac{ ||v||^2 }{ \epsilon^2 } (2 \ln \frac{1}{\delta} + \epsilon) 
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] B. Eisenberg and R. Sullivan, “Why Is the Sum of Independent Normal Random Variables Normal?,” Mathematics Magazine, vol. 81, no. 5, pp. 362–366, Dec. 2008<br />
[2] G. Kamath, “Lecture 5 — Approximate Diﬀerential Privacy”, CS 860 - Algorithms for Private Data Analysis.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/03/Laplace-Distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/03/Laplace-Distribution/" class="post-title-link" itemprop="url">Laplace Distribution</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-03 10:22:41" itemprop="dateCreated datePublished" datetime="2020-11-03T10:22:41+11:00">2020-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-05 16:03:07" itemprop="dateModified" datetime="2020-11-05T16:03:07+11:00">2020-11-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The density function of a Laplace distribution (denoted as Laplace(<span class="math inline">\(\mu, b\)</span>) ) with location and scale parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(b\)</span> is given by <span class="math display">\[
p(x) = \frac{1}{2 b} \exp( -\frac{ | x - \mu |  }{ b } )
\]</span></p>
<p>The density function of the distribution is symmetric with respect to <span class="math inline">\(\mu\)</span>, where it achieves peak value <span class="math inline">\(\frac{1}{2b}\)</span>. We plot below a few Laplace distributions with different parameters of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(b\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Laplace-Distribution.png?raw=true" /></p>
<!-- <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Laplace-Distribution.png?raw=true" style="zoom: 67%;" /> -->
<h3 id="properties.">Properties.</h3>
<p>We list and prove a few properties of the distribution. Let <span class="math inline">\(X\)</span> be a random variable that follows Laplace(<span class="math inline">\(\mu, b\)</span>).</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbb{E}[X] = \mu\)</span>.</p>
<p><em>Proof:</em> <span class="math display">\[
 \begin{aligned}
     \frac{1}{2} \int_{\mu }^\infty \frac{1}{b} \exp( -\frac{x - \mu}{b} ) \ dx
     = \frac{1}{2} \int_{0 }^\infty \frac{1}{b} \exp( -\frac{x}{b} ) \ dx = \frac{1}{2}
 \end{aligned}
 \]</span> The proof follows from symmetry of Laplace(<span class="math inline">\(\mu, b\)</span>) at <span class="math inline">\(\mu\)</span>.</p></li>
<li><p><span class="math inline">\(\forall t \ge 0, \Pr[X - \mu \ge t] \le \frac{1}{2} \exp(-t)\)</span>.</p>
<p><em>Proof:</em> <span class="math display">\[
 \begin{aligned}
     \Pr[X - \mu \ge t] = \frac{1}{2} \int_{\mu + t }^\infty \frac{1}{b} \exp( -\frac{x - \mu}{b} ) \ dx
     = \frac{1}{2} \int_{t }^\infty \exp( -x ) \ dx = \frac{1}{2} \exp(-t)
 \end{aligned}
 \]</span> <span class="math inline">\(\square\)</span></p></li>
<li><p><span class="math inline">\(\forall t \ge 0, \Pr[|X - \mu| \ge t] \le \exp(-t)\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{Var}[X] = 2b^2\)</span>.</p>
<p><em>Proof:</em> First, for an integer <span class="math inline">\(n \ge 0\)</span>, we define <span class="math display">\[
 \Gamma(n) = \int_{0 }^\infty x^n \exp( -x ) \ dx
 \]</span></p>
<p>Therefore, <span class="math inline">\(\Gamma(0) = 1\)</span>. Suppose <span class="math inline">\(n \ge 1\)</span>, we have <span class="math display">\[
 \begin{aligned}
     \Gamma(n) 
         &amp;= -\int_{0 }^\infty x^n \ d \exp( -x ) \\
         &amp;= - x^n \exp(-x) \mid_0^\infty + \int_{0 }^\infty \exp( -x ) \ d x^n \\
         &amp;= n\int_{0 }^\infty x^{n - 1} \exp( -x ) \ dx \\
         &amp;= n \Gamma(n - 1)
 \end{aligned}
 \]</span></p>
<p>By induction, we conclude <span class="math inline">\(\Gamma(n) = n!\)</span>. Now, consider</p>
<p><span class="math display">\[
 \begin{aligned}
     \frac{1}{2} \int_{\mu }^\infty \frac{1}{b} (x - \mu)^2 \exp( -\frac{x - \mu}{b} ) \ dx
     &amp;= \frac{1}{2} \int_{0 }^\infty \frac{1}{b} x^2 \exp( -\frac{x}{b} ) \ dx \\
     &amp;= \frac{b^2}{2} \int_{0 }^\infty x^2 \exp( -x ) \ dx \\
     &amp;= \frac{b^2}{2} \Gamma(2) \\
     &amp;= b^2
 \end{aligned}
 \]</span> The proof follows from symmetry of Laplace(<span class="math inline">\(\mu, b\)</span>) at <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X&#39; \sim Laplace(\mu, b)\)</span> and define another random variable <span class="math inline">\(Y = X&#39; + b\)</span>, then the density function of <span class="math inline">\(Y\)</span> shares the same shape with that of <span class="math inline">\(X&#39;\)</span>, with its center shifted to the right by distance <span class="math inline">\(b\)</span>. To distinguish their density function, denote <span class="math inline">\(p_{X&#39;} (\cdot)\)</span> the one for random variable <span class="math inline">\(X&#39;\)</span> and <span class="math inline">\(p_Y(\cdot)\)</span> the one for <span class="math inline">\(Y\)</span>. For any <span class="math inline">\(t \in \mathbb{R}\)</span>,</p>
<p><span class="math display">\[
 \frac{ p_{X&#39;} (t) }{ p_Y(t) } = \exp( - \frac{ |t - \mu | - | t - ( \mu + b ) | }{ b  }) \in [ \exp(-1), \exp(1) \ ]
 \]</span></p></li>
<li><p>If <span class="math inline">\(X, X&#39; \sim Laplace(\mu, \frac{b}{\epsilon} )\)</span> be a pair of independent random variables, and <span class="math inline">\(Y = X&#39; + b\)</span>, then</p>
<p><span class="math display">\[
 \frac{ p_X(t) }{ p_Y(t) } = \exp( - \epsilon \frac{ |t - \mu | - | t - ( \mu + b ) | }{ b  }) \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
 \]</span></p></li>
<li><p>Continued. Let <span class="math inline">\(S\)</span> be any interval in <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p><span class="math display">\[
 \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } = \frac{ \int_{t \in S} p_X(t) \ dt }{ \int_{t \in S} p_Y(t) \ dt } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
 \]</span></p></li>
<li><p>Continued. Denote <span class="math inline">\(P_X\)</span> and <span class="math inline">\(P_Y\)</span> the probability functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> respectively. Let <span class="math inline">\(S\)</span> be any Borel set in <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p><span class="math display">\[
 \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } = \frac{ \int_S 1 \ d P_X }{ \int_S 1 \ d P_Y } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
 \]</span></p></li>
<li><p>Let <span class="math inline">\(X = (X_1, X_2, ..., X_n), X&#39; = (X_1&#39;, X_2&#39;, ..., X_n&#39;)\)</span> be independent random vectors with dimension <span class="math inline">\(n\)</span>, where <span class="math inline">\(X_i, X_i&#39; \sim Laplace(0, \frac{\Delta}{\epsilon} )\)</span> (for <span class="math inline">\(1 \le i \le n\)</span>) are independent random variables. Let <span class="math inline">\(Y \doteq X&#39; + \mu = (Y_1, Y_2, ..., Y_n)\)</span> be another random vector, such that <span class="math inline">\(Y_i = X_i&#39; + \mu_i\)</span> for <span class="math inline">\(1 \le i \le n\)</span> and <span class="math inline">\(|\mu| = \sum_{i \in [n] } \mu_i \le \Delta\)</span>. For any <span class="math inline">\(t \in \mathbb{R}^n\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
    \frac{ p_X(t) }{ p_Y(t) } &amp;= \frac{ \exp( - \frac{\epsilon}{\Delta} \sum_{i \in [n] } |t_i| ) }{ \exp( - \frac{\epsilon}{\Delta} \sum_{i \in [n] } |t_i - \mu_i| ) } \\
    &amp;= \exp( - \epsilon \frac{ |t | - | t - \mu| }{ \Delta  } ) \\
    &amp;\in [\exp( - \epsilon \frac{ |\mu| }{ \Delta  } ), \exp( \epsilon \frac{ |\mu| }{ \Delta  } )] \\
    &amp;\in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
\end{aligned}
\]</span></p></li>
<li><p>Continued. Let <span class="math inline">\(S \subset \mathbb{R}^n\)</span> be any Borel set. Then</p>
<p><span class="math display">\[
    \frac{ \Pr[X \in S] }{ \Pr[Y \in S] } \in [ \exp( -\epsilon ), \exp( \epsilon ) \ ]
\]</span></p></li>
</ol>
<h3 id="laplacian-mechanism.">Laplacian Mechanism.</h3>
<p>Let <span class="math inline">\((\mathcal{X}, d)\)</span> be a metric space and <span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}^n\)</span> a function defined on <span class="math inline">\(\mathcal{X}\)</span>.</p>
<p><strong><em>Definition.</em></strong> <span class="math inline">\(x, x&#39; \in \mathcal{X}\)</span> are called neighboring points if <span class="math inline">\(d(x, x&#39;) = 1\)</span>.</p>
<p><strong><em>Definition.</em></strong> The sensitivity <span class="math inline">\(\Delta\)</span> of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
\Delta \doteq \max_{x, x&#39; \in \mathcal{X}, d(x, x&#39;) = 1} | f(x) - f(x&#39;) |
\]</span></p>
<p>where <span class="math inline">\(| \cdot |\)</span> is the <span class="math inline">\(\ell_1\)</span> distance. In other words, <span class="math inline">\(\Delta\)</span> is the maximum <span class="math inline">\(\ell_1\)</span> distance between the images of two neighboring points.</p>
<p>We now construct a new randomized function, called <em>Laplacian mechanism</em> from <span class="math inline">\(f\)</span>, as follows.</p>
<p><strong><em>Definition.</em></strong> The Laplacian mechanism of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
g(x) = f(x) + Y
\]</span></p>
<p>where <span class="math inline">\(Y = (Y_1, Y_2, ..., Y_n)\)</span> is a random vector with independent random variables <span class="math inline">\(Y_i \sim Laplace(\Delta / \epsilon)\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="theorem">Theorem</h4>
<p><em><span class="math inline">\(g\)</span> is <span class="math inline">\((\epsilon, 0)\)</span>-differentially private.</em></p>
<p><em>Proof.</em> Let <span class="math inline">\(S \subset \mathbb{R}^n\)</span> be an arbitrary Borel set. We need to prove for any pair of neighboring <span class="math inline">\(x, x&#39; \in \mathcal{X}\)</span>, <span class="math display">\[
    \frac{ \Pr[ g(x) \in S]}{ \Pr[ g(x&#39;) \in S] } \le \exp(\epsilon)
\]</span></p>
<p>Denote <span class="math inline">\(p_x\)</span> and <span class="math inline">\(p_{x&#39;}\)</span> the density functions for <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(x&#39;)\)</span> respectively. It reduces to prove the following property of the density function <span class="math display">\[
    \frac{ p_x(t) }{ p_{x&#39;} (t)  } \le \exp(\epsilon), \qquad \forall t \in \mathbb{R}
\]</span></p>
<p>But <span class="math display">\[
\begin{aligned}
    \frac{ p_x(t) }{ p_{x&#39;} (t)  } 
        &amp;= \frac{ \exp( - \frac{\epsilon |t - f(x)| }{\Delta} ) }{ \exp( - \frac{\epsilon |t - f(x&#39;)| }{\Delta} )  } \\
        &amp;= \exp\big( - \frac{\epsilon }{\Delta} (|t - f(x)| - |t - f(x&#39;) | )  \big) \\
        &amp;\le \exp \big( \frac{\epsilon }{\Delta}  |f(x) - f(x&#39;) |  \big) \\
        &amp;= \exp( \epsilon )
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/02/Randomized-Response/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/02/Randomized-Response/" class="post-title-link" itemprop="url">Randomized Response</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-02 10:41:39" itemprop="dateCreated datePublished" datetime="2020-11-02T10:41:39+11:00">2020-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-09 17:26:31" itemprop="dateModified" datetime="2020-11-09T17:26:31+11:00">2020-11-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the setting of randomized response, there is a coordinator and a set of <span class="math inline">\(n\)</span> players each having a secrete bit <span class="math inline">\(x_i \in \{0, 1\}\)</span>. Instead of sending <span class="math inline">\(x_i\)</span> to the coordinator directly, the player sends a randomly perturbed version <span class="math inline">\(X_i\)</span>, such that <span class="math display">\[
X_i = 
\begin{cases}
    x_i, \qquad \text{with probability } 0.5 + \epsilon \\
    \bar{ x}_i, \qquad \text{with probability } 0.5 - \epsilon \\
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\epsilon \in [0, 0.5]\)</span> and <span class="math inline">\(\bar{x}_i = 1 - x_i\)</span>. If <span class="math inline">\(\epsilon = 0\)</span>, then the response from play <span class="math inline">\(i\)</span> is totally random. On the other hand, if <span class="math inline">\(\epsilon = 0.5\)</span>, there isn't noise in <span class="math inline">\(X_i\)</span>.</p>
<p>The coordinator would like to estimate the ratio of <span class="math inline">\(1\)</span>'s among the <span class="math inline">\(x_i\)</span>'s. Let <span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i \in [n] } X_i. 
\]</span></p>
<p>Denote <span class="math inline">\(\mu = \frac{1}{n} \sum_{i \in [n] } x_i\)</span> the true ratio. Now <span class="math display">\[
\begin{aligned}
    \mathbb{E} [ \bar{X} ] &amp;= \frac{1}{n} \sum_{i \in [n] } \mathbb{E}[ X_i ] \\
                            &amp;= \frac{1}{n} \sum_{i \in [n] } [ (0.5 + \epsilon) x_i + (0.5 - \epsilon )\bar{x}_i ] \\
                            &amp;= \frac{1}{n} \sum_{i \in [n] } [ 2 \epsilon x_i  + (0.5 - \epsilon) (x_i + \bar{x}_i) ] \\
                            &amp;= \frac{1}{n} \sum_{i \in [n] } [ 2 \epsilon x_i  + (0.5 - \epsilon)  ] \\
                            &amp;= \frac{1 - 2\epsilon }{2n}  + 2\epsilon \mu
\end{aligned}
\]</span></p>
<p>and <span class="math display">\[
\begin{aligned}
    \mathbb{Var} [ \bar{X} ] = \frac{1}{n} \sum_{i \in [n] } \mathbb{Var}[ X_i ] 
                            \le \frac{1}{4n} 
\end{aligned}
\]</span></p>
<p>Hence, <span class="math display">\[
\mu = \mathbb{E}[ \frac{1}{2\epsilon} (\bar X + \frac{2\epsilon - 1}{2n}) ]
\]</span></p>
<p>and <span class="math inline">\(\hat \mu \doteq \frac{1}{2\epsilon} (\bar X + \frac{2\epsilon - 1}{2n})\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span>. As <span class="math display">\[
\mathbb{Var}[ \hat \mu ] = \frac{1}{4 \epsilon^2 } \mathbb{Var}[\bar{X} ] \le \frac{1}{16 n \epsilon^2}
\]</span></p>
<p>By Chebyshev's inequality, <span class="math display">\[
\Pr[ |\hat \mu - \mu | \ge \sqrt{2} \cdot \frac{1 }{4\epsilon \sqrt{n} } ] \le \frac{ \mathbb{Var}[ \hat \mu ] }{ ( \sqrt{2} \cdot \frac{1 }{4 \epsilon \sqrt{n} } )^2 } = \frac{1}{2}
\]</span></p>
<p>Or we can apply Hoeffding inequality to show that, with probability at least <span class="math inline">\(1 - \delta\)</span>, <span class="math display">\[
|\bar{X} - \mathbb{E} [ \bar{X} ] | \le \sqrt{ \frac{\log \frac{2}{\delta} }{2n} }
\]</span></p>
<p>i.e., <span class="math display">\[
|\hat \mu - \mu | = \frac{1}{2 \epsilon} |\bar{X} - \mathbb{E} [ \bar{X} ] | \le \frac{1}{ 2 \epsilon } \sqrt{ \frac{\log \frac{2}{\delta} }{2n} }
\]</span></p>
<p>which is much tighter than Chebyshev inequality.</p>
<p>Finally, observe that to get a meaningful estimate <span class="math inline">\(\hat \mu\)</span>, we require that <span class="math inline">\(\frac{ 1 }{\epsilon \sqrt{n} } \le 1\)</span>, i.e., <span class="math inline">\(n \in \Omega(\frac{1}{\epsilon^2})\)</span> or <span class="math inline">\(\epsilon \in \Omega( \frac{1}{ \sqrt{n} } )\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/27/Twin-Drive-or-Not/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/27/Twin-Drive-or-Not/" class="post-title-link" itemprop="url">Twin Drive or Not?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-27 16:15:03" itemprop="dateCreated datePublished" datetime="2020-10-27T16:15:03+11:00">2020-10-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-06 11:08:17" itemprop="dateModified" datetime="2020-11-06T11:08:17+11:00">2020-11-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this article, we introduce differential privacy. We start with a story of information leakage.</p>
<h3 id="twin-drive-or-not">Twin Drive or Not</h3>
<p>Celestial Being is a private military organization with superior technology. It has four advanced machines. The most powerful one is the Gundam 00.</p>
<p>The Gundam 00 is about to engage the enemy.</p>
<div style="text-align:center">
<p><img src="https://lh5.googleusercontent.com/-9AOc-O5bdRk/S_mZxjf1DWI/AAAAAAAACvs/uXI0SvAMqWc/s640/5.jpg" width="500" height="250" /></p>
</div>
<p>Due to maintenance, Gundam 00 is not always equipped with two engines (known as <strong><em>twin drive system</em></strong>). With probability <strong><em>0.5</em></strong>, it uses one drive. We use <span class="math inline">\(T = (1, 1)\)</span> to denote the status of equipping with the twin drive system, and <span class="math inline">\(S = (1, 0)\)</span> (or <span class="math inline">\((0, 1)\)</span>) for status of single drive. Let <span class="math inline">\(X\)</span> be a random variable that indicates drive status. Therefore, <span class="math display">\[
\Pr[ X = T ] = 0.5, \\
\Pr[ X = S ] = 0.5.  
\]</span></p>
<div style="text-align:center">
<p><img src="https://knolly.files.wordpress.com/2009/03/00gundamdrive1.jpg" width="500" height="250" /></p>
</div>
<p>Raiser sword is one of Gundam 00 most powerful weapon. The energy level of the Raiser sword, denote as <span class="math inline">\(Y\)</span>, is a random variable in <span class="math inline">\([0, 100]\)</span>, whose distribution, denoted as <span class="math inline">\(\Pr[\cdot \mid X]\)</span>, depends on the drive status. Thus, <span class="math inline">\(\Pr[Y = y \mid X = T]\)</span> (or <span class="math inline">\(\Pr[Y = y \mid X = S]\)</span>) is the probability that <span class="math inline">\(Y\)</span> equals to <span class="math inline">\(y\)</span> conditioned on <span class="math inline">\(X = T\)</span> (<span class="math inline">\(X = S\)</span>). Specifically, <span class="math display">\[
\Pr[\cdot \mid T] \sim B(100, 0.81) \\
\Pr[\cdot \mid S] \sim B(100, 0.09)
\]</span></p>
<p>where <span class="math inline">\(B(n, p)\)</span> denotes a binomial distribution. The expected energy level is <span class="math inline">\(81\)</span> with <em>twin drive system</em>, compared to only <span class="math inline">\(9\)</span> with single drive. This is called "<strong><em>squaring</em></strong>" phenomenon of <em>twin drive</em>.</p>
<div style="text-align:center">
<p><img src="https://vignette.wikia.nocookie.net/gundam/images/8/84/Raiser_sword.png/revision/latest?cb=20101124151429" width="500" height="250" /></p>
</div>
<p>Now suppose that you're the enemy pilot of a mobile suit with just average performance. Before you start dog fighting with Gundam 00, you will be attacked by the 00's long-range raiser sword (you have not seen 00 yet). Luckily, you survive the raiser sword attack. Now you need to make a decision. If 00 is equipped with twin drive, there is zero chance that you can win the dog fight. The best choice is to leave the battle. Otherwise, you can outperform 00 and you would like to engage it.</p>
<div style="text-align:center">
<p><img src="https://blogimg.goo.ne.jp/user_image/79/da/c198b8cf829b5f7f7dd507d40bb39ba5.jpg" width="500" height="250" /></p>
</div>
<p>Since you have just been attacked by the raiser sword, you have an observation of its energy level <span class="math inline">\(y\)</span>. By Bayes' theorem, <span class="math display">\[
\begin{aligned}
\Pr[ X = T \mid Y = y] = \frac{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] }{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] + \Pr[Y = y \mid X = S] \cdot \Pr[X = S] } 
\\
\\
\Pr[ X = S \mid Y = y] = \frac{ \Pr[Y = y \mid X = S] \cdot \Pr[X = S] }{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] + \Pr[Y = y \mid X = S] \cdot \Pr[X = S] }
\end{aligned}
\]</span></p>
<p>Substituting with <span class="math inline">\(\Pr[X = S] = \Pr[X = T] = 0.5\)</span>, we get <span class="math display">\[
\begin{aligned}
\Pr[ X = T \mid Y = y] = \frac{ \Pr[Y = y \mid X = T]  }{ \Pr[Y = y \mid X = T]  + \Pr[Y = y \mid X = S] } 
\\
\\
\Pr[ X = S \mid Y = y] = \frac{ \Pr[Y = y \mid X = S]  }{ \Pr[Y = y \mid X = T]  + \Pr[Y = y \mid X = S] }
\end{aligned}
\]</span></p>
<p>The distributions <span class="math inline">\(B(100, 0.09)\)</span> and <span class="math inline">\(B(100, 0.81)\)</span> are plotted together below.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/B-100-0.09-vs-B-100-0.81.png?raw=true" /></p>
<p>Immediately, you can draw some conclusions based on single value of <span class="math inline">\(y\)</span>, such as</p>
<ol type="1">
<li>If <span class="math inline">\(y = 10\)</span>, it is likely 00 has only one drive.<br />
</li>
<li>If <span class="math inline">\(y = 75\)</span>, it is likely 00 has twin drive.</li>
</ol>
<p>Or you can conclude based on the range of <span class="math inline">\(y\)</span>, such as</p>
<ol type="1">
<li>If <span class="math inline">\(y \in [0, 20]\)</span>, it is likely 00 has single drive.</li>
<li>If <span class="math inline">\(y \in [70, 90]\)</span>, it is likely 00 has twin drive.</li>
</ol>
<p>Before the observation of <span class="math inline">\(y\)</span>, as <span class="math inline">\(\Pr[X = S] = \Pr[X = T] = 0.5\)</span>, you have only random guess over the engine status of 00. After the observation, you might be much more confident about your guess, even though the energy level of the raiser sword is a random variable.</p>
<p>Why does this happen? Because the two distributions are well-separated. They are far from each other. Further, the prior probabilities <span class="math inline">\(\Pr[X = S]\)</span> and <span class="math inline">\(\Pr[X = T]\)</span> play important roles. If you know <span class="math inline">\(\Pr[X = S] = 10^{-10}\)</span>, even if you observe an energy level of <span class="math inline">\(y = 10\)</span>, you had better not engage Gundam 00. You know it could be just a trap!</p>
<h3 id="formal-definition">Formal Definition</h3>
<p>Database managers face similar scenarios in privacy protection. We know define the setting for differential privacy. We view a dataset <span class="math inline">\(D\)</span> as a table of <span class="math inline">\(n\)</span> rows, each of which comes from a domain <span class="math inline">\(\mathcal{X}\)</span>. Hence <span class="math inline">\(D \in \mathcal{X}^n\)</span>. Instead of releasing the dataset directly, the manager runs a randomized algorithm <span class="math inline">\(A: \mathcal{X}^n \rightarrow \mathcal{Y}\)</span> on <span class="math inline">\(D\)</span>, and outputs <span class="math inline">\(Y = A(D) \in \mathcal{Y}\)</span>. Here <span class="math inline">\(\mathcal{Y}\)</span> is called the co-domain of <span class="math inline">\(A\)</span> and does not necessarily equal to <span class="math inline">\(\mathcal{X}^n\)</span>. Note that the output distribution of <span class="math inline">\(A\)</span> may depend on the input <span class="math inline">\(D\)</span>. Further, for simplicity of discussion, we believe a uniform prior distribution over datasets in <span class="math inline">\(\mathcal{X}^n\)</span>.</p>
<p>In the Gundam 00's story, <span class="math inline">\(\mathcal{X} = \{0, 1\}^2\)</span>, and <span class="math inline">\(\mathcal{Y} = [0, 100]\)</span>.</p>
<p>Suppose there is another dataset <span class="math inline">\(D&#39;\)</span> that differs only one row from <span class="math inline">\(D\)</span>. The algorithm <span class="math inline">\(A\)</span> is said to be differentially private if a malicious user is unlikely to distinguish the input to <span class="math inline">\(A\)</span> between <span class="math inline">\(D\)</span> and <span class="math inline">\(D&#39;\)</span>, by merely observing <span class="math inline">\(A\)</span>'s output. Simply put, the conditional distributions of <span class="math inline">\(\Pr[\cdot \mid X = D]\)</span> and <span class="math inline">\(\Pr[\cdot \mid X = D&#39;]\)</span> should be similar, where <span class="math inline">\(X\)</span> is a variable that denotes the input dataset.</p>
<p>We observe in the previous section that, if the distributions are well separated, then we can infer the underlying input with high confidence when the output value takes specific values or lies in certain ranges.</p>
<p>There are many ways to characterize the closeness of two distributions. We introduce the one proposed in [1].</p>
<blockquote>
<p>Algorithm <span class="math inline">\(A\)</span> is called <span class="math inline">\((\epsilon, \delta)\)</span> differentially private, if for any pair of neighboring datasets <span class="math inline">\(D\)</span> and <span class="math inline">\(D&#39;\)</span> (the ones that differ in only one row), and for any (measurable) subset <span class="math inline">\(\mathcal{R} \subset \mathcal{Y}\)</span>, it holds that <span class="math display">\[
\Pr[Y \in \mathcal{R} \mid X = D ] \le \exp(\epsilon) \cdot \Pr[Y \in \mathcal{R} \mid X = D&#39; ]+ \delta
\]</span></p>
</blockquote>
<p>In other words, we can't find a subset <span class="math inline">\(\mathcal{R}\)</span>, with which we can distinguish <span class="math inline">\(D\)</span> and <span class="math inline">\(D&#39;\)</span> with high confidence. In the previous example, such <span class="math inline">\(\mathcal{R}\)</span> exists. E.g., <span class="math inline">\(R = [70, 90]\)</span>. If <span class="math inline">\(Y\)</span> lies in <span class="math inline">\(\mathcal{R}\)</span>, we can infer that Gundam 00 is likely to have twin drive.</p>
<h3 id="hypothesis-testing">Hypothesis Testing</h3>
<p>There is alternative view of <span class="math inline">\((\epsilon, 0)\)</span> differential privacy as hypothesis testing. Suppose that we know the underlying dataset is either <span class="math inline">\(D\)</span> or <span class="math inline">\(D&#39;\)</span> with equal probability. After observing the output of <span class="math inline">\(A\)</span>, we need to decide which hypothesis of the following holds: <span class="math display">\[
H_0: \text{ the dataset if } D \\
H_1: \text{ the dataset if } D&#39;
\]</span></p>
<p>Assume that we adopt a fixed strategy:</p>
<ol type="1">
<li>First we choose a fixed subset <span class="math inline">\(\mathcal{R} \subset \mathcal{Y}\)</span>,</li>
<li>If <span class="math inline">\(Y \in \mathcal{R}\)</span>, we choose to accept <span class="math inline">\(H_0\)</span>,</li>
<li>Otherwise, we accept <span class="math inline">\(H_1\)</span>.</li>
</ol>
<p>There are two kinds of errors we can make. The type I error is the one when the true hypothesis is <span class="math inline">\(H_0\)</span> and we accept <span class="math inline">\(H_1\)</span>. Conversely, type II error is the one when the true hypothesis is <span class="math inline">\(H_1\)</span> and we accept <span class="math inline">\(H_0\)</span>. Let <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> be the probabilities we make type I and II errors respectively. The following theorem holds</p>
<blockquote>
<p>Algorithm A is <span class="math inline">\((\epsilon, 0)\)</span> differentially private then</p>
<ol type="1">
<li><span class="math inline">\(\frac{1}{1 + \exp(\epsilon) } \le q \le \frac{1}{1 + \exp(-\epsilon) }\)</span><br />
</li>
<li><span class="math inline">\(\frac{1}{1 + \exp(\epsilon) } \le p \le \frac{1}{1 + \exp(-\epsilon) }\)</span></li>
</ol>
</blockquote>
<p><em>Proof.</em> By definition, <span class="math display">\[
\begin{aligned}
    q   &amp;= \Pr[ X = D \mid  Y \in \bar{\mathcal{R}} ] \\
        &amp;= \frac{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \Pr[X = D ] }{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \Pr[X = D ] + \Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ] \Pr[X = D&#39; ] } \\
        &amp;= \frac{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]  }{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]  + \Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ]  } 
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(A\)</span> is <span class="math inline">\((\epsilon, 0)\)</span> differentially private, it holds that <span class="math display">\[
\Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ] \le \exp(\epsilon) \cdot \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]
\]</span> and <span class="math display">\[
\Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \le \exp(\epsilon) \cdot \Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ]
\]</span></p>
<p>therefore, <span class="math display">\[
\frac{1}{1 + \exp(\epsilon) } \le q \le \frac{1}{1 + \exp(-\epsilon) }
\]</span></p>
<p>By symmetry, we also have <span class="math display">\[
\frac{1}{1 + \exp(\epsilon) } \le p \le \frac{1}{1 + \exp(-\epsilon) }
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013<br />
[2] L. Wasserman and S. Zhou, “A statistical framework for differential privacy,” arXiv:0811.2501 [math, stat], Oct. 2009, Accessed: Oct. 27, 2020.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/26/Norm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/26/Norm/" class="post-title-link" itemprop="url">Norm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-26 22:17:18" itemprop="dateCreated datePublished" datetime="2020-10-26T22:17:18+11:00">2020-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-01 00:49:31" itemprop="dateModified" datetime="2020-11-01T00:49:31+11:00">2020-11-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>A norm <span class="math inline">\(| \cdot |:\mathbb{R}^n \rightarrow \mathbb{R}\)</span> is a function that satisfies the following properties:</p>
<ol type="1">
<li>Positive definite: <span class="math inline">\(|x| = 0 \rightarrow x = \vec 0\)</span>,</li>
<li>Nonnegative:<span class="math inline">\(|x| \ge 0\)</span> for any <span class="math inline">\(x \in \mathbb{R}^n\)</span>,</li>
<li>Absolutely homogeneous: <span class="math inline">\(|k x| = |k| |x|\)</span>, for any <span class="math inline">\(x \in \mathbb{R}^n, k \in \mathbb{R}\)</span>,</li>
<li>Subadditive (triangle inequality): <span class="math inline">\(|x + y| \le |x| + |y|\)</span>, for any <span class="math inline">\(x \in \mathbb{R}^n, y \in \mathbb{R}^n\)</span>.</li>
</ol>
<p>Given properties 1-3, we claim that property 4 is equivalent to</p>
<ol start="5" type="1">
<li>The region <span class="math inline">\(\{ x \in \mathbb{R}^n: |x| \le 1\}\)</span> is convex.</li>
</ol>
<p><em>Proof:</em> The proof is straightforward.</p>
<p><span class="math inline">\(5 \rightarrow 4:\)</span> If <span class="math inline">\(x = \vec 0\)</span> or <span class="math inline">\(\vec y = 0\)</span>, then 4 holds trivially. Otherwise, suppose that <span class="math inline">\(x, y \neq \vec 0\)</span>. Then <span class="math display">\[
\begin{aligned}
    &amp;|x + y| \le |x| + |y| \\
    \leftrightarrow 
    &amp;| \frac{ |x| }{ |x| + |y| } \frac{ x }{ |x| } + \frac{ |x| }{ |x| + |y| } \frac{ y }{ |y| } | \le 1
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(|\frac{ x }{ |x| } | =1\)</span>, <span class="math inline">\(|\frac{ y }{ |y| }| = 1\)</span> and <span class="math inline">\(\frac{ |x| }{ |x| + |y| } + \frac{ |y| }{ |x| + |y| } = 1\)</span>, the second inequality follows exactly from condition 5.</p>
<p><span class="math inline">\(4 \rightarrow 5:\)</span> Let <span class="math inline">\(x, y \in \mathbb{R}^n, |x| \le 1, |y| \le 1\)</span> and <span class="math inline">\(a, b \ge 0\)</span>, <span class="math inline">\(a + b = 1\)</span>. Then <span class="math display">\[
|ax + by| \le |ax| + |by| = a|x| + b|y| = 1
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="induced-norm"><strong>Induced Norm</strong></h3>
<p>To illustrate a deeper connection between property 4 and 5, we first show how a norm can be induced by a convex and symmetric region centered at the origin <span class="math inline">\(O\)</span>. Let the boundary of the region as <span class="math inline">\(E\)</span>. We are going to induce a norm by <span class="math inline">\(E\)</span>, denoted as <span class="math inline">\(|\cdot |_E\)</span>.</p>
<p>First, we define</p>
<ol type="1">
<li><span class="math inline">\(| x |_E = 1, \forall x \in E\)</span>.</li>
</ol>
<p>For any other vector <span class="math inline">\(x \notin E\)</span>, consider the ray initiated from the origin and with the same direction as <span class="math inline">\(x\)</span>. Denote its intersection point with <span class="math inline">\(E\)</span> as <span class="math inline">\(x_E\)</span>. Suppose that <span class="math inline">\(x = a \cdot x_E\)</span> for some <span class="math inline">\(a \in \mathbb{R}_+\)</span>. Then we define</p>
<ol start="2" type="1">
<li><span class="math inline">\(| x |_E = a |x_E|_E = a\)</span>.</li>
</ol>
<p>Indeed, if we write the <span class="math inline">\(|\cdot|_2\)</span> as the <span class="math inline">\(\ell_2\)</span> norm, then the value of <span class="math inline">\(a\)</span> is given by <span class="math inline">\(\frac{ |x|_2 } { |x_E|_2 }\)</span>. Clearly, by definition, <span class="math inline">\(|\cdot|_2\)</span> is positive definite and non-negative. To prove that it is absolutely homogeneous, <span class="math inline">\(\forall k \in \mathbb{R}\)</span>, <span class="math display">\[
|k x|_E = |k ax_E|_E
\]</span></p>
<p>If <span class="math inline">\(k \ge 0\)</span>, we have <span class="math inline">\(|k ax_E|_E = ka |x_E|_E = ka\)</span>. Otherwise, if <span class="math inline">\(k &lt; 0\)</span>, <span class="math display">\[
|k x|_E = |k ax_E|_E = |-k a (-x_E)|_E = -k a |(-x_E)|_E
\]</span></p>
<p>By symmetry of <span class="math inline">\(E\)</span> (with respect to the origin <span class="math inline">\(O\)</span>), <span class="math inline">\(|(-x_E)|_E = |x_E|_E = 1\)</span>. Therefore, <span class="math display">\[
|k x|_E = |k|a 
\]</span></p>
<p>Finally, we have a graphical verification of triangle inequality. Let <span class="math inline">\(v, u \in \mathbb{R}^n\)</span>, which intersect with <span class="math inline">\(E\)</span> at <span class="math inline">\(v_E\)</span> and <span class="math inline">\(u_E\)</span> respectively. If <span class="math inline">\(|v|_E &lt; 1\)</span> (<span class="math inline">\(|u|_E &lt; 1\)</span>), then we can extend it along its direction to get its intersection with <span class="math inline">\(E\)</span>. Let <span class="math inline">\(c = |v|_E\)</span> and <span class="math inline">\(d = |u|_E\)</span>, then</p>
<ol type="1">
<li><span class="math inline">\(v = c \cdot v_E\)</span>,</li>
<li><span class="math inline">\(u = d \cdot u_E\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(w = v + u = c \cdot v_E + d \cdot u_E\)</span>. Let <span class="math inline">\(p\)</span> be the intersection between <span class="math inline">\(w\)</span> and the line segment between <span class="math inline">\(v\)</span> and <span class="math inline">\(u\)</span>. It is easy to verify that <span class="math display">\[
p = \frac{c}{c + d} v_E + \frac{d}{ c + d} u_E
\]</span></p>
<p>By convexity of the region bounded by <span class="math inline">\(E\)</span>, <span class="math display">\[
|p|_E \le 1
\]</span></p>
<p>It concludes that <span class="math inline">\(|w|_E = |c v_E + d u_E |_E \le c + d = |v|_E + |u|_E\)</span>.</p>
<!-- <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Induced-Norm.png?raw=true" style="zoom: 67%;" /> -->
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Induced-Norm.png?raw=true" /></p>
<h3 id="application"><strong>Application</strong></h3>
<p>The idea discussed above significantly simplifies the proof of Minkowski inequality.</p>
<p>The <span class="math inline">\(p\)</span>-norm (<span class="math inline">\(p \ge 1\)</span>) on <span class="math inline">\(\mathbb{R}^n\)</span> is given as <span class="math display">\[
| x |_p = \big( \sum_{i = 1}^n |x_i|^p \big)^\frac{1}{p}
\]</span></p>
<p>Clearly, <span class="math inline">\(p\)</span>-norm is positive definite, nonnegative, absolutely homogeneous. It is left to verify triangle inequality, which is known as Minkowski inequality for the case of <span class="math inline">\(p\)</span>-norm. This is equivalent to show that <span class="math display">\[
\{ x \in \mathbb{R}^n : |x |_p \le 1 \}
\]</span></p>
<p>is convex. The trick here is that <span class="math inline">\(|x|_p \le 1\)</span> is equivalent to <span class="math inline">\((|x|_p)^p \le 1\)</span>. Hence, <span class="math display">\[
\{ x \in \mathbb{R}^n : |x |_p \le 1 \} = \{ x \in \mathbb{R}^n : \sum_{i = 1}^n |x_i|^p  \le 1 \}
\]</span></p>
<p>The convexity of this set follows directly from the point-wise convexity of the function <span class="math inline">\(|\cdot |^p\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong><em>Remark:</em></strong> To appreciate how concise the above proof is, we also give one traditional proof here, which consists of three steps.</p>
<h4 id="youngs-inequality">Young's Inequality</h4>
<p>For <span class="math inline">\(a, b &gt; 0\)</span>, <span class="math inline">\(p, q &gt; 0\)</span>, s.t., <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>, it holds that <span class="math display">\[
ab \le \frac{a^p}{p} + \frac{b^q}{q}
\]</span></p>
<p><em>Proof:</em> The inequality is nothing more than convexity of the function <span class="math inline">\(x \rightarrow e^x\)</span>: <span class="math display">\[
ab = \exp(\frac{ p \ln a }{p} + \frac{ q \ln b }{q}) \le \frac{ \exp( \ln a^p ) }{p} + \frac{ \exp( \ln b^q ) }{q} = \frac{a^p}{p} + \frac{b^q}{q} 
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="holders-inequality">Holder's Inequality</h4>
<p>For <span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, <span class="math inline">\(p, q &gt; 0\)</span>, s.t., <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>, it holds that <span class="math display">\[
\sum_{i = 1}^n |x_i| |y_i| \le |x|_p |y|_q
\]</span></p>
<p><em>Proof:</em> The inequality is trivial if <span class="math inline">\(x = \vec 0\)</span> or <span class="math inline">\(y = \vec 0\)</span>. Otherwise, let <span class="math inline">\(u = \frac{x}{ |x|_p }\)</span> and <span class="math inline">\(v = \frac{ y }{ |y|_q }\)</span>. It remains to prove that <span class="math display">\[
\sum_{i = 1}^n |u_i| |v_i| \le 1
\]</span></p>
<p>for <span class="math inline">\(|u|_p = 1\)</span> and <span class="math inline">\(|v|_q = 1\)</span>. WLOG, we assume <span class="math inline">\(u &gt; 0\)</span> and <span class="math inline">\(v &gt; 0\)</span>. Applying Young's Inequality, we have <span class="math display">\[
u_i v_i \le \frac{ u_i^p }{p}  + \frac{v_i^q}{q}.
\]</span> Hence, <span class="math display">\[
u \cdot v = \sum_{i = 1}^n \big( \frac{ u_i^p }{p}  + \frac{v_i^q}{q} \big) = \frac{1}{p} + \frac{1}{q} = 1 .
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="minkowski-inequality">Minkowski Inequality</h4>
<p>For <span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, <span class="math inline">\(p \ge 1\)</span>, it holds that <span class="math display">\[
|x + y|_p \le |x|_p + |y|_p
\]</span></p>
<p><em>Proof:</em> The inequality is trivial if <span class="math inline">\(x = \vec 0\)</span> or <span class="math inline">\(y = \vec 0\)</span>. Otherwise, assume that <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(y &gt; 0\)</span>. Now</p>
<p><span class="math display">\[
|x + y|_p^p = \sum_{i = 1}^n |x_i + y_i|^p = \sum_{i = 1}^n [ x_i (x_i + y_i)^{p - 1} + y_i (x_i + y_i)^{p - 1} ]
\]</span></p>
<p>Let <span class="math inline">\(q = \frac{p}{p - 1}\)</span>. Then <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>. By Holder's inequality, <span class="math display">\[
\begin{aligned}
    \sum_{i = 1}^n x_i (x_i + y_i)^{p - 1} 
    &amp;\le |x|_p ( \sum_{i = 1}^n (x_i + y_i)^{(p - 1)q} )^{ \frac{1}{q} } \\
    &amp;= |x|_p ( \sum_{i = 1}^n (x_i + y_i)^{ p } )^{ \frac{p - 1}{ p } } \\
    &amp;= |x|_p (|x + y|_p)^{p - 1}
\end{aligned}
\]</span></p>
<p>Similarly, we have <span class="math display">\[
\sum_{i = 1}^n y_i (x_i + y_i)^{p - 1} \le |y|_p (|x + y|_p)^{p - 1}
\]</span></p>
<p>Putting the inequalities together, <span class="math display">\[
|x + y|_p^p \le |x|_p (|x + y|_p)^{p - 1} + |y|_p (|x + y|_p)^{p - 1}
\]</span></p>
<p>We finish the proof by dividing both side with <span class="math inline">\((|x + y|_p)^{p - 1}\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/25/Bit-Guessing-Game/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/25/Bit-Guessing-Game/" class="post-title-link" itemprop="url">Bit Guessing Game</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-25 15:33:16" itemprop="dateCreated datePublished" datetime="2020-10-25T15:33:16+11:00">2020-10-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-03 10:23:55" itemprop="dateModified" datetime="2020-11-03T10:23:55+11:00">2020-11-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The bit guessing game models two players, namely Alice and Bob, where Alice has a secret vector <span class="math inline">\(\vec x \in \{0, 1 \}^n\)</span> which Bob tries to learn. Bob is only allowed to ask Alice the value of <span class="math display">\[
\vec c \cdot \vec x
\]</span></p>
<p>for bit vector <span class="math inline">\(\vec c \in \{0, 1\}^n\)</span> he comes up with. Under this setting, it is trivial for Bob to learn each bit of <span class="math inline">\(x\)</span>: he needs to just query Alice for <span class="math inline">\(\vec e_i \cdot \vec x\)</span>, where <span class="math inline">\(\vec e_i\)</span> is the indicator vector with its <span class="math inline">\(i\)</span>-th dimension equal to 1 and other dimensions equal to 0.</p>
<p>One way for Alice to prevent information leakage is to add some bounded noise <span class="math inline">\(Z\)</span> to the query result and returns <span class="math display">\[
r(\vec c) \doteq \vec c \cdot \vec x + Z
\]</span></p>
<p>For example, <span class="math inline">\(Z\)</span> could be a bounded random variable. Is is still a chance for Bob to learn the vector <span class="math inline">\(\vec x\)</span>?</p>
<p><strong><em>Theorem 1.</em></strong> If Bob is allowed to query <span class="math inline">\(r(\vec c)\)</span> for all <span class="math inline">\(\vec c \in \{0, 1\}^n\)</span>, and suppose that <span class="math inline">\(Z\)</span> is bounded by some real number <span class="math inline">\(R\)</span>, then Bob can have an estimation <span class="math inline">\(\vec y\)</span> such that <span class="math display">\[
|\vec x - \vec y| = \sum_{i = 1}^n | x_i - y_i | \le 4R
\]</span></p>
<p><strong>Remark:</strong> <em>In this case Bob queries for <span class="math inline">\(2^n\)</span> vectors. This result implies that is Bob is able to ask exponential number of questions, then it is hard to prevent information leakage. E.g., suppose <span class="math inline">\(R = n / 400\)</span>, which is a relatively large number when <span class="math inline">\(n\)</span> is large, the theorem says that Bob can recover <span class="math inline">\(99\%\)</span> of bits in <span class="math inline">\(\vec x\)</span>.</em></p>
<p><em>Proof:</em> We say a vector <span class="math inline">\(\vec y\)</span> is consistent with a query result <span class="math inline">\(r(\vec c)\)</span> if <span class="math display">\[
| r(\vec c) - \vec c \cdot \vec y| \le R.
\]</span></p>
<p>The strategy of Bob is to choose any <span class="math inline">\(\vec y\)</span> that is consistent with all <span class="math inline">\(\vec c \in \{0, 1\}^n\)</span>. Such <span class="math inline">\(\vec y\)</span> exits, a special of which is given by <span class="math inline">\(\vec y = \vec x\)</span>. It remains to prove that such an <span class="math inline">\(\vec y\)</span> satisfies the constraint.</p>
<p>Consider the special case of <span class="math inline">\(\vec c = \vec x\)</span>, <span class="math display">\[
|r(\vec x) - \vec x \cdot \vec y| \le R \\
|r(\vec x) - \vec x \cdot \vec x| \le R \\
\]</span></p>
<p>By triangle inequality, it holds <span class="math display">\[
|\vec x \cdot \vec y - \vec x \cdot \vec x| \le 2R.
\]</span></p>
<p>Observe that <span class="math inline">\(\vec x \cdot \vec x \ge \vec x \cdot \vec y\)</span>, which implies <span class="math display">\[ 
|\vec x \cdot \vec y - \vec x \cdot \vec x| = \vec x \cdot \vec x - \vec x \cdot \vec y \le 2R
\]</span></p>
<p>Hence, <span class="math inline">\(\vec y\)</span> and <span class="math inline">\(\vec x\)</span> differ by at most <span class="math inline">\(2R\)</span> bits, on the non-zero dimensions of <span class="math inline">\(\vec x\)</span>.</p>
<p>Let <span class="math inline">\(\bar x\)</span> be the bit-wise complement of <span class="math inline">\(\vec x\)</span>. By similar arguments, we get <span class="math display">\[
|\bar x \cdot \vec y - \bar x \cdot \vec x|\le 2R.
\]</span></p>
<p>and <span class="math inline">\(\vec x\)</span> and <span class="math inline">\(\vec y\)</span> differ by at most <span class="math inline">\(2R\)</span> bits, on the zero dimensions of <span class="math inline">\(\vec x\)</span>. It concludes that <span class="math inline">\(|\vec x - \vec y | \le 4R\)</span>.<br />
<!-- Define $S$ to be the set of non-zero dimensions of $\vec x$: 
$$
S \doteq \{ i \in [n]: x_i > 0 \}
$$

Let $\vec e_S \in \{0, 1\}^n$ ($\vec e_{\bar S} \in \{0, 1\}^n$) be the indicator vector that takes $1$ on dimensions belonging to $S$ ($\bar S$).  It follows that 
$$
|r(\vec e_S) - \vec e_S \cdot \vec y| \le R \\
|r(\vec e_S) - \vec e_S \cdot \vec x| \le R \\
$$ --></p>
<!-- By triangle inequality, it holds 
$$
|\vec e_S \cdot \vec y - \vec e_S \cdot \vec x|\le 2R.
$$   -->
<!-- Similarly, $|\vec e_{\bar S} \cdot \vec y - \vec e_{\bar S} \cdot \vec x| \le 2R$. As $S$ and $\bar S$ is a partition of $[n]$, 
$$
| \vec x - \vec y| = |\vec e_S \cdot \vec y - \vec e_S \cdot \vec x| + |\vec e_{\bar S} \cdot \vec y - \vec e_{\bar S} \cdot \vec x| \le 4R
$$ -->
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>What if Bob asks <span class="math inline">\(O(n)\)</span> questions? The following theorem states that if <span class="math inline">\(Z\)</span> is bounded by <span class="math inline">\(O(\sqrt n)\)</span>, then Bob is able to have a good estimation of <span class="math inline">\(\vec x\)</span>.</p>
<p><strong><em>Theorem 2.</em></strong> If Bob is able to ask <span class="math inline">\(\frac{n \log 2 + \ln n}{\log \frac{3}{2} - \log \lambda }\)</span> queries, and <span class="math inline">\(Z\)</span> is bounded by <span class="math inline">\(\alpha \sqrt n\)</span>, then Bob can obtains an estimation <span class="math inline">\(\vec y\)</span>, such that <span class="math display">\[
|\vec x - \vec y| \le (3e \frac{\alpha}{\lambda } )^2 \cdot n
\]</span></p>
<p><em>Proof:</em> As before, a vector <span class="math inline">\(\vec y\)</span> is said to be consistent with a query result <span class="math inline">\(r(\vec c)\)</span> if <span class="math display">\[
| r(\vec c) - \vec c \cdot \vec y| \le \alpha \sqrt n.
\]</span></p>
<p>It suffices to find a sequence of vectors <span class="math inline">\(\vec c_1, \vec c_2, ..., \vec c_k\)</span> (<span class="math inline">\(k\)</span> is a value to be determined), such that</p>
<ol type="1">
<li><span class="math inline">\(k = O(n)\)</span>,</li>
<li>For any <span class="math inline">\(\vec y\)</span> with <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span>, there exists at least one <span class="math inline">\(\vec c_i\)</span>, such that <span class="math inline">\(\vec y\)</span> is not consistent with <span class="math inline">\(r(\vec c_i)\)</span>, with high probability.</li>
</ol>
<p>If Bob can find a sequence of query vectors with the specified properties, he can rule out any <span class="math inline">\(\vec y\)</span> such that <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span>. Note that not every vector in <span class="math inline">\(\{0, 1\}^n\)</span> will be ruled out. At least <span class="math inline">\(\vec x\)</span> will remain. Bob can output any <span class="math inline">\(\vec y\)</span> that is not ruled out as his guess of <span class="math inline">\(\vec x\)</span>.</p>
<p>It turns out that fining <span class="math inline">\(\vec c_i\)</span>'s is simple: each <span class="math inline">\(\vec c_i\)</span> is sampled independently and uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>. It remains to prove that any <span class="math inline">\(\vec y\)</span> with <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span> will be ruled out with high probability.</p>
<p>For a fix <span class="math inline">\(i\)</span>, consider the probability of <span class="math display">\[
\Pr[ | r(\vec c_i) - \vec c_i \cdot \vec y| &gt; \alpha \sqrt n ] = \Pr[ |\vec c_i \cdot (\vec x - \vec y) | &gt; \alpha \sqrt n ].
\]</span></p>
<p>Lower bounding this probability is equivalent to upper bounding the probability <span class="math display">\[
\Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ].
\]</span></p>
<p><em>Lemma.</em> <strong>For a value <span class="math inline">\(l \in \mathbb{N}\)</span></strong>, <span class="math display">\[
\Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] \le \frac{e }{\sqrt {|\vec x - \vec y| } }
\]</span></p>
<p>We leave the proof of the lemma at the end of our discussion. It following by union bound that <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ] 
        &amp;= \sum_{l = -\alpha \sqrt n}^{\alpha \sqrt n} \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] \\
        &amp;\le \frac{2 e \cdot \alpha \sqrt n}{\sqrt {|\vec x - \vec y| } }
\end{aligned}
\]</span></p>
<p>Conditioning on that <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span>, this probability is at most <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ] 
        &amp;\le \frac{2}{3} \lambda
\end{aligned}
\]</span></p>
<p>The probability that this holds simultaneously for all <span class="math inline">\(\vec c_i\)</span> is therefore <span class="math inline">\((\frac{2\lambda }{3})^k\)</span>. Applying union bound over all <span class="math inline">\(\vec y\)</span> such that <span class="math inline">\(|\vec x - \vec y| &gt; (3e\alpha)^2 \cdot n\)</span>, we have an upper bound of failure probability <span class="math display">\[
(\frac{2 \lambda }{3})^k 2^n = \exp( k \log \frac{2 \lambda }{3} + n \log 2)
\]</span></p>
<p>If we take <span class="math inline">\(k = \frac{n \log 2 + \ln n}{\log \frac{3}{2} - \log \lambda }\)</span>, this probability becomes <span class="math inline">\(\frac{1}{n}\)</span>.</p>
<p><em>Proof of the lemma:</em> Without lose of generality, suppose that <span class="math inline">\(l \ge 0\)</span>. Observe that <span class="math inline">\(\vec x - \vec y\)</span> is a vector in <span class="math inline">\(\{-1, 0, 1 \}^n\)</span>. Denote <span class="math inline">\(v = |\vec x - \vec y|\)</span> be the number of non-zero dimensions in <span class="math inline">\(\vec x - \vec y\)</span>. Further, let <span class="math inline">\(v_+\)</span> (<span class="math inline">\(v_-\)</span>) be the number of positive (negative) dimensions in <span class="math inline">\(\vec x - \vec y\)</span>. Hence, <span class="math inline">\(v = v_+ + v_-\)</span>. As <span class="math inline">\(\vec c_i\)</span> is sampled uniformly from <span class="math inline">\(\{0, 1\}^n\)</span>, we have <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] 
    &amp;= \frac{1}{2^{v} } \sum_{j = 0}^{ \min \{ v_+ - l, v_- \} } \binom{v_+}{l + j} \binom{v_-}{j} \\
    &amp;= \frac{1}{2^{v} } \sum_{j = 0}^{ \min \{ v_+ - l, v_- \} } \binom{v_+}{l + j} \binom{v_-}{v_- - j} \\
    &amp;\le \frac{1}{2^{v} } \binom{v_+ + v_- }{l + v_-} \\
    &amp;\le \frac{1}{2^{v} } \binom{v }{ \lfloor v / 2 \rfloor } \\
\end{aligned}
\]</span></p>
<p>By Stirling's approximation, <span class="math display">\[
v! \le e \sqrt v (\frac{v}{e} )^v  \\
\lfloor v / 2 \rfloor! \ge \sqrt{2 \pi v} (\frac{ \lfloor v / 2 \rfloor }{e} )^{\lfloor v / 2 \rfloor }  \\
\lceil v / 2 \rceil ! \ge \sqrt{2 \pi v} ( \frac{ \lceil v / 2 \rceil }{ e } )^{\lceil v / 2 \rceil }   \\
\]</span></p>
<p>Assuming that <span class="math inline">\(\lfloor v / 2\rfloor \ge 1\)</span>, we get <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] 
    &amp;\le \frac{1}{2^{v} } \binom{v }{ \lfloor v / 2 \rfloor } \\
    &amp;\le \frac{1}{2^{v} } \frac{e}{2 \pi} \frac{v^{ v + 0.5 } }{ (v / 2)^{ v + 1 } } (\frac{ v / 2 }{ \lfloor v / 2 \rfloor })^{ \lfloor v / 2 \rfloor + 0.5 } (\frac{ v / 2 }{ \lceil v / 2 \rceil })^{ \lceil v / 2 \rceil + 0.5 } \\
    &amp;\le \frac{ 1 }{ \sqrt v } ( \frac{ v / 2 }{ \lfloor v / 2 \rfloor })^{ \lfloor v / 2 \rfloor + 0.5 } \\
    &amp;\le \frac{ 1 }{ \sqrt v } \exp( \frac{ 1 }{ 2 \lfloor v / 2 \rfloor } ( \lfloor v / 2 \rfloor + 0.5 )  ) \\
    &amp;\le \frac{ e }{ \sqrt v }  \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p>The idea behind Theorem 2 is more obvious if we study the scenario where <span class="math inline">\(\vec x \in \{-1, 1\}^n\)</span>. For any estimation <span class="math inline">\(\vec y \in \{-1, 1\}^n\)</span>, if <span class="math inline">\(\vec c\)</span> is sampled uniformly at random from <span class="math inline">\(\{-1, 1\}^n\)</span>, then <span class="math display">\[
\mathbb{E}[ \vec c (\vec x - \vec y)] = 0
\]</span></p>
<p>If <span class="math inline">\(|\vec x - \vec y| = O(\alpha^2 n)\)</span>, by anti-concentration property, the probability <span class="math display">\[
\Pr[ |\vec c (\vec x - \vec y) | \ge \alpha \sqrt n] 
\]</span></p>
<p>equals to some constant. Equivalently, <span class="math inline">\(\Pr[ |\vec c (\vec x - \vec y) | &lt; \alpha \sqrt n]\)</span> is some constant. By sampling <span class="math inline">\(\vec c\)</span> repeatedly, the probability that <span class="math inline">\(|\vec c (\vec x - \vec y) | &lt; \alpha \sqrt n\)</span> holds for all sampled <span class="math inline">\(\vec c\)</span> decreases exponentially. That is why we can rule out any <span class="math inline">\(\vec y\)</span> such that <span class="math inline">\(|\vec x - \vec y| = O(\alpha^2 n)\)</span> with high probability.</p>
<!-- First, observe that 
$$
\mathbb{E}[ \vec c_i \cdot ( \vec x - \vec y) ] = \mathbb{E}[ \vec c_i \cdot ( \vec x - \vec y) ] \ge 0.
$$ -->
<!-- We re-write the event of $\vec c_i$ not consistent with $\vec y$ as 
$$
\begin{array}{ll}
\{ | r(\vec c_i) - \vec c_i \cdot \vec y| > \alpha \sqrt n \} \\
= \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] + \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n \} \\
= \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | + |\mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n \} \\
\supset \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n  \} 
\end{array}
$$

and 
$$
\Pr[ \vec c \cdot (\vec x - \vec y) - \mathbb{E}[ \vec c \cdot ( \vec x - \vec y) ] \ge \sqrt n] ] \le \binom{\alpha n}{ \alpha n / 2} \frac{1}{2^{\alpha n} } 
$$ -->
<h3 id="reference">Reference</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/24/Littlewood%E2%80%93Offord-Problem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/24/Littlewood%E2%80%93Offord-Problem/" class="post-title-link" itemprop="url">Littlewood–Offord Problem</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-24 23:25:49" itemprop="dateCreated datePublished" datetime="2020-10-24T23:25:49+11:00">2020-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-31 23:07:05" itemprop="dateModified" datetime="2020-10-31T23:07:05+11:00">2020-10-31</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(a = (a_1, a_2, ..., a_n) \in \mathbb{R}^n\)</span> with <span class="math inline">\(|a_i| \ge 1\)</span>. Let <span class="math inline">\(p(a)\)</span> be the number of vectors <span class="math inline">\(c = (c_1, c_2, ..., c_n) \in \mathbb{R}^n\)</span> where <span class="math inline">\(c_i = \pm 1, \forall i \in [n]\)</span>, such that <span class="math display">\[
-1 \le \left&lt; a , c \right&gt; = \sum_{i = 1}^n c_i a_i &lt; 1
\]</span></p>
<ol type="1">
<li><p>Prove that <span class="math inline">\(p(a) \le \binom{n}{\lfloor n / 2 \rfloor }\)</span>.</p></li>
<li><p>Find an <span class="math inline">\(a \in \mathbb{R}^n\)</span> with <span class="math inline">\(p(a) = \binom{n}{\lfloor n / 2 \rfloor }\)</span>.</p></li>
</ol>
<p><em>Proof.</em> By symmetry, we can assume that <span class="math inline">\(a &gt; 0\)</span> without lose of generality. Define <span class="math display">\[
C = \{ c = (c_1, c_2, ..., c_n) \in \mathbb{R}^n \mid c_i = \pm 1, i \in [n] \}. 
\]</span></p>
<p>Let <span class="math inline">\([n]\)</span> be the set of integers from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span>, and <span class="math inline">\(\mathscr{P}([n])\)</span> be the power set of <span class="math inline">\([n]\)</span>, i.e., the set of all subsets of <span class="math inline">\([n]\)</span>. For any subset <span class="math inline">\(s \subset [n]\)</span>, we can associate to <span class="math inline">\(s\)</span> a vector in <span class="math inline">\(C\)</span>, which is denoted as <span class="math display">\[
c(s) \doteq (c(s)_1, c(s)_2, ..., c(s)_n), 
\]</span></p>
<p>where <span class="math display">\[
\begin{cases}
    c(s)_i = 1, &amp; \forall i \in s \\
    c(s)_i = -1,&amp; \forall i \notin s
\end{cases}
\]</span></p>
<p>It remains to show the set <span class="math display">\[
S(a) \doteq \{ s \subset [n] : -1 &lt; \left&lt; a, c(s) \right&gt; &lt; 1 \}
\]</span></p>
<p>has size at most <span class="math inline">\(\binom{n}{\lfloor n / 2 \rfloor }\)</span>. The key observation is that <span class="math display">\[
\forall s, s&#39; \in S(a) \Rightarrow s \nsubseteq s&#39;. 
\]</span></p>
<p>We call <span class="math inline">\(S(a)\)</span> an antichain. To show this, suppose thta <span class="math inline">\(s&#39;\)</span> contains at least one element that is not in <span class="math inline">\(s\)</span>. Denote this element as <span class="math inline">\(j\)</span>. It follows that <span class="math inline">\(c(s&#39;)_j = 1\)</span> and <span class="math inline">\(c(s&#39;)_j = -1\)</span> and that <span class="math display">\[
\left&lt; a, c(s&#39;) \right&gt; - \left&lt; a, c(s) \right&gt; \ge c(s&#39;)_j a_j - c(s)_j a_j = 2 a_j \ge 2
\]</span></p>
<p>We conclude that <span class="math inline">\(\left&lt; a, c(s&#39;) \right&gt; \ge \left&lt; a, c(s) \right&gt; + 2 \ge 1\)</span>, a contradiction.</p>
<p>Now, consider an increasing sequence of <span class="math inline">\(n\)</span> subsets of <span class="math inline">\([n]\)</span> of the form: <span class="math display">\[
\emptyset \subset \{ x_1 \} \subset \{ x_1, x_2 \} \subset \{ x_1, x_2, x_3 \} \subset ... \subset \{ x_1, x_2, ..., x_n \}
\]</span></p>
<p>where <span class="math inline">\(x_1, ..., x_n\)</span> are different integers in <span class="math inline">\([n]\)</span>. Each such sequence induces a permutation of integers in <span class="math inline">\([n]\)</span>. As a result, the total number of such sequence equals <span class="math inline">\(n!\)</span>. The relation between <span class="math inline">\(S(a)\)</span> and such sequence is established as follows:</p>
<ol type="1">
<li><p>For each such sequence, there can be at most one set in <span class="math inline">\(S(a)\)</span>, because <span class="math inline">\(S(a)\)</span> is a antichain.</p></li>
<li><p>For each <span class="math inline">\(s = \{x_1, x_2, ..., x_k \} \in S(a)\)</span>, where <span class="math inline">\(k = |s| \ge 1\)</span> is the size of <span class="math inline">\(s\)</span>, the number of sequences it appears in is given by</p>
<p><span class="math display">\[
 |s| ! (n - |s|)! \ge \lfloor n / 2 \rfloor ! (n - \lfloor n / 2 \rfloor)!
 \]</span></p></li>
</ol>
<p>We obtain <span class="math display">\[
|S(a) | \cdot \lfloor n / 2 \rfloor ! (n - \lfloor n / 2 \rfloor)! \le\sum_{s \in S(a) } |s| ! (n - |s|) ! \le n!
\]</span></p>
<p>and <span class="math display">\[
p(a) = |S(a) | = \frac{n!}{ \lfloor n / 2 \rfloor ! (n - \lfloor n / 2 \rfloor)! } = \binom{n}{\lfloor n / 2 \rfloor }
\]</span></p>
<p>This finishes the proof for problem 1.</p>
<p>Finally, <span class="math inline">\(a_i = 1, \forall i \in [n]\)</span> gives a tight example to problem 2.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] J. Matoušek and J. Nešetřil, Invitation to discrete mathematics, 2nd ed. Oxford ; New York: Oxford University Press, 2009.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description">你生之前悠悠千載已逝，未來還會有千年沉寂的期待</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
