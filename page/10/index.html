<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/10/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/18/Entropy-and-Message-Transmission/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/Entropy-and-Message-Transmission/" class="post-title-link" itemprop="url">Entropy and Message Transmission</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-18 20:26:03" itemprop="dateCreated datePublished" datetime="2020-06-18T20:26:03+10:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-01 11:06:27" itemprop="dateModified" datetime="2020-07-01T11:06:27+10:00">2020-07-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss another application that sheds light on meaning of entropy, the message transmission problem. In this model, a sender transmits a message to the receiver through a channel. In real world, the channel could be an optical fiber, a wireless channel, a hard disk etc. In the final application, the computer that writes and reads information from the hard disk is both the sender and the receiver.</p>
<p>In real world the channel is not perfect and the message transmitted is distorted by noise. It is natural to ask whether the message can be transmitted accurately under the noise, i.e., whether the receiver can recover the noisy message.</p>
<p>To study the problem, we need a mathematical model for both the channel and the noise. We focus on the simplest binary channel with bits of 0 and 1. The noise causes the bits to flip and is modelled by the distribution on the bit-flips. We study the simplest one that flips each bit independently with some identical probability <span class="math inline">\(p &lt; 0.5\)</span>. For a message with length <span class="math inline">\(n\)</span>, the number of bit flips follows a Binomial distribution <span class="math inline">\(B(n, p)\)</span>. We call such channel a <em>binary symmetric channel</em>.</p>
<p>To protect the message against noise, a naïve way is to send the each bit multiple times. For example, if the sender wants to send a bit 1, it sends <span class="math inline">\(10\)</span> copies of <span class="math inline">\(1\)</span> as <span class="math inline">\(1111111111\)</span>. The receiver decides that the bit sent is 1, if the majority of the <span class="math inline">\(10\)</span> bits it receives is 1.</p>
<p>Could it be improved? Repeating each bit may not be the mot efficient way against the noise. In general, if the sender wants to send a message of <span class="math inline">\(k\)</span> bits, it can convert it into a new message of <span class="math inline">\(n\)</span> bits and sends the new one. The receiver considers the <span class="math inline">\(n\)</span> bits received as a whole, and try to recover the <span class="math inline">\(k\)</span>-bit message the sender wants to send.</p>
<p>We call the method used by the sender to convert the original message the <em>encoding function</em>, and the one used by the receiver to recover the message the <em>decoding function.</em></p>
<p><strong><em>Definition.</em></strong> A <span class="math inline">\((k, n)\)</span> encoding function <span class="math inline">\(\text{Enc}: \{0, 1\}^k \rightarrow \{0, 1\}^n\)</span> and a <span class="math inline">\((k, n)\)</span> decoding function <span class="math inline">\(\text{Dec}: \{0, 1\}^n \rightarrow \{0, 1\}^k\)</span>, where <span class="math inline">\(n \ge k\)</span>.</p>
<p>Since the noise flips the bits randomly, it is impossible to have an encoding function and a decoding function without error (unless <span class="math inline">\(p = 0\)</span>). Instead, we aim to control the probability of error within some specified threshold <span class="math inline">\(\delta\)</span>. To achieve this, we add redundancy to the message and encode a <span class="math inline">\(k\)</span>-bit one into an <span class="math inline">\(n\)</span>-bit one. Now, <span class="math inline">\(n - k\)</span> is the amount of redundancy introduced. We would like to make <span class="math inline">\(n - k\)</span> as small as possible. On the other hand, the value of <span class="math inline">\(n-k\)</span> should positively related to <span class="math inline">\(p\)</span>. The larger <span class="math inline">\(p\)</span> is, the noisier the channel is and the larger <span class="math inline">\(n - k\)</span> should be.</p>
<p>For fixed <span class="math inline">\(\delta\)</span> and <span class="math inline">\(p\)</span>, the <em>Shannon's Theorem</em> says that the smallest possible value of <span class="math inline">\(n - k\)</span> we can achieve is roughly <span class="math inline">\(n\mathbf{H}(p)\)</span>. Intuitively, <span class="math inline">\(\mathbf{H}(p)\)</span> is the amount of randomness the noise exerts on each bit. Therefore <span class="math inline">\(1 - \mathbf{H}(p)\)</span> is the maximum amount of information we can transmit by one bit.</p>
<p>Before we proceed, we prove the following lemma.</p>
<p><strong><em>Lemma 1</em></strong><br />
For <span class="math inline">\(q \in \mathbb{R}, q \le 1 / 2\)</span> and <span class="math inline">\(n \in \mathbb{N}\)</span>, it hold that <span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor q n \rfloor} \binom{n}{i} 
    &amp;\le 2^{ n \mathbf{H} ( q)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p><strong><em>Proof.</em></strong><br />
<span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor q n \rfloor} \binom{n}{i} 
    &amp;\le \frac{n + 1}{2} \binom{n }{\lfloor q n \rfloor }  \\
    &amp;\le \frac{n + 1}{2} 2^{ n \mathbf{H} \left( { \lfloor q n \rfloor } / { n } \right) } \\
    &amp;\le 2^{ n \mathbf{H} ( q)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p>The first inequality follows from that <span class="math inline">\(\binom{n}{i}\)</span> is increasing for <span class="math inline">\(i \le n / 2\)</span> and that the summation is over at most <span class="math inline">\((n + 1) /2\)</span> terms. The last one follows from that <span class="math inline">\(\mathbf{H}(x)\)</span> is increasing for <span class="math inline">\(x \le 1 / 2\)</span> and that <span class="math inline">\({ \lfloor q n \rfloor } / { n } \le { q n } / { n } = q &lt; 1/2\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Shannon's Theorem</em></strong><br />
Given a binary symmetric channel with flip probability <span class="math inline">\(p &lt; 1 / 2\)</span> and for any any <span class="math inline">\(\epsilon, \delta &gt; 0\)</span>, when <span class="math inline">\(n\)</span> is large enough,</p>
<ol type="1">
<li>if <span class="math inline">\(k \le n (1 - \mathbf{H}(p) - \epsilon)\)</span>, there exist <span class="math inline">\((k, n)\)</span> encoding and decoding functions such that the receiver fails to obtain the correct message with probability at most <span class="math inline">\(2\delta\)</span>.<br />
</li>
<li><span class="math inline">\(\nexists (k, n)\)</span> encoding and decoding functions with <span class="math inline">\(k \ge n (1 - \mathbf{H}(p) + \epsilon)\)</span> such that the decoding is correctly is at most <span class="math inline">\(\delta\)</span> for a <span class="math inline">\(k\)</span>-bit input message chosen uniformly at random.</li>
</ol>
<p><strong><em>Proof of 1.</em></strong></p>
<p><strong>Intuition.</strong> When an <span class="math inline">\(n\)</span>-bit message <span class="math inline">\(s\)</span> is transmitted through the channel, the number of flipped bits is roughly <span class="math inline">\(np\)</span>. As <span class="math inline">\(p &lt; {1} / {2}\)</span>, we can find a <span class="math inline">\(\lambda &gt; 0\)</span> such that <span class="math inline">\(p + \lambda &lt; {1} / {2}\)</span>. Let <span class="math inline">\(R\)</span> be the received <span class="math inline">\(n\)</span>-bit message and define <span class="math inline">\(d(s, R)\)</span> the number of different bits between <span class="math inline">\(s\)</span> and <span class="math inline">\(R\)</span>, i.e., the Hamming distance between <span class="math inline">\(s\)</span> and <span class="math inline">\(R\)</span>. Given <span class="math inline">\(s\)</span> and <span class="math inline">\(p\)</span>, let <span class="math inline">\(\mathfrak{D}(s, p)\)</span> be the distribution of <span class="math inline">\(R\)</span> on <span class="math inline">\(\{0, 1\}^n\)</span>.</p>
<p>By Hoeffding's inequality, it holds that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s, p) } [ |d(s, R) - pn | \ge \lambda n] \le \exp(- 2 \lambda^2 n).
\]</span></p>
<p>Define the <span class="math inline">\((p + \lambda)n\)</span> ball centered <span class="math inline">\(s\)</span> as <span class="math display">\[
B_{s, (p + \lambda)n} \doteq \{ r \in \{0, 1\}^n : d(s, r) &lt; (p + \lambda)n \}.
\]</span></p>
<p>The the Hoeffding's inequality implies that for large enough <span class="math inline">\(n\)</span>, we have <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s, p) } [ R \notin B_{s, (p + \lambda)n}] \le \delta.
\]</span></p>
<p>That is, with probability at most <span class="math inline">\(\delta\)</span>, the received message fall outside the ball <span class="math inline">\(B_{s, (p + \lambda)n}\)</span>. This motivates to decode each message in <span class="math inline">\(B_{s, (p + \lambda)n}\)</span> as the original message the sender wants to send.</p>
<p>Further, by Lemma 1, the size of <span class="math inline">\(B_{s, (p + \lambda) n}\)</span> is bounded by <span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor (p + \lambda) n \rfloor} \binom{n}{i} 
    &amp;\le 2^{ n \mathbf{H} ( p + \lambda)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p>Therefore, the maximum number of non-overlapped balls we can pack into the message space <span class="math inline">\(\{0, 1\}^n\)</span> is<br />
<span class="math display">\[
\frac{2^n}{ 2^{ n \mathbf{H} ( p + \lambda)  + \log \frac{n + 1}{2} }  } = 2^{ n (1 - \mathbf{H} ( p + \lambda) - \frac{1}{n} \log \frac{n + 1}{2})   }
\]</span></p>
<p>When <span class="math inline">\(n\)</span> is large enough, this is at least <span class="math inline">\(2^{ n (1 - \mathbf{H} ( p ) - \epsilon) }\)</span>.</p>
<p><em>Remark: Hoeffding's inequality is an overkill. Chebyshev's inequality suffices for this proof.</em></p>
<p><strong>Designing Encoding and Decoding Functions.</strong></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Balls.png" /></p>
<p>This motivates one to design of encoding and decoding function as follows: find a set of messages <span class="math inline">\(s_1, s_2, ..., s_{2^k} \in \{0, 1\}^n\)</span>, such that <span class="math display">\[
B_{s_1, (p + \lambda)n}, B_{s_2, (p + \lambda)n}, ..., B_{s_{2^k} , (p + \lambda)n}
\]</span></p>
<p>are disjoint balls, and assign the messages in <span class="math inline">\(\{0, 1\}^k\)</span> to the ones in <span class="math inline">\(\{ s_i \}&#39;s\)</span> one by one. If the sending want to send <span class="math inline">\(i\)</span>, it sends <span class="math inline">\(s_i\)</span>. On receiving a message <span class="math inline">\(r\)</span>, the receiver determines which ball <span class="math inline">\(r\)</span> belongs to. The probability of decoding error is at most <span class="math inline">\(\delta\)</span>.</p>
<p><em>Question to ponder: can we find such a set efficiently, such that</em><br />
<span class="math display">\[
B_{s_1, (p + \lambda)n}, B_{s_2, (p + \lambda)n}, ..., B_{s_{2^k} , (p + \lambda)n}
\]</span> <em>are disjoint?</em></p>
<p><strong>Finding the Codewords.</strong></p>
<p>It is left to find a set of satisfactory <span class="math inline">\(s_1, s_2, ..., s_{2^k} \in \{0, 1\}^n\)</span>. The method we show here does not find a set of non-overlapped balls. Instead it aims to find a set such that</p>
<blockquote>
<p>if <span class="math inline">\(s_i\)</span> is transmitted, then the probability received message <span class="math inline">\(r\)</span> falls into the another ball, i.e., <span class="math inline">\(r \in B_{s_{j} , (p + \lambda)n}\)</span> (<span class="math inline">\(j \neq i\)</span>), is less than <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The property implies that the overlap between <span class="math inline">\(B_{s_{i} , (p + \lambda)n}\)</span> and <span class="math inline">\(\cup_{j \neq i} B_{s_{j} , (p + \lambda)n}\)</span> is less than <span class="math inline">\(\delta\)</span> (measured by probability). To formalize the statement, define the random variable <span class="math inline">\(S\)</span> to be the message sent and <span class="math inline">\(R\)</span> to be the one received. Given that <span class="math inline">\(S = s_i\)</span> is sent, the conditional probability of receiving <span class="math inline">\(R = r\)</span> is <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] = p^{d(s_i,r)} (1 - p)^{1 - d(s_i, r)}
\]</span></p>
<p>To goal is to prove that it holds for all <span class="math inline">\(i\)</span>, <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] = \sum_{r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n} } p_r \le \delta
\]</span></p>
<p>Then , by union bound, it holds that <span class="math display">\[
\begin{aligned} 
\Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \text{ is not recovered correctly} \mid S = s_i] 
    &amp;= \Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \notin B_{s_i, (p + \lambda)n}  \vee  R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  \mid S = s_i] \\
    &amp;\le \Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \notin B_{s_i, (p + \lambda)n}   \mid S = s_i]  + \Pr[ R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  \mid S = s_i] \\
    &amp;\le 2 \delta
\end{aligned}
\]</span> That is, <span class="math inline">\(s_i\)</span> is transmitted and decoded correctly with probability at least <span class="math inline">\(1 - 2 \delta\)</span>.</p>
<p>In what follows, we will prove the following claim:</p>
<p><strong>Claim 1</strong>. <em>If we generate <span class="math inline">\(2^{k + 1}\)</span> codewords uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>, then in expectation</em> <span class="math display">\[
\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} }  \left[ \sum_{i = 1}^{2^{k + 1} }  \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \le  2^k \delta
\]</span></p>
<p>Claim 1 implies that there exits a set of codewords <span class="math inline">\(s_1, s_2, ..., s_{2^{k + 1} }\)</span> with <span class="math display">\[
\sum_{i = 1}^{2^{k + 1} } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]    \le 2^k \delta
\]</span></p>
<p>If we send each <span class="math inline">\(s_i\)</span> with probability <span class="math inline">\(1 / 2^k\)</span>, then the expected error probability is already <span class="math inline">\(\delta\)</span>. But we can have a stronger result: we can find a set of <span class="math inline">\(2^k\)</span> codewords, such that for each <span class="math inline">\(i\)</span>, <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \le 2 \delta
\]</span></p>
<p>W.L.O.G., suppose that <span class="math inline">\(s_1, s_2, ..., s_{2^k}\)</span> are the ones with the smallest <span class="math inline">\(\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]\)</span> 's, it must be that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]  \le \frac{2^k \delta }{2^k } = \delta
\]</span></p>
<p><strong><em>Proof of Claim 1</em></strong>. By Lemma 1, for a fixed <span class="math inline">\(r\)</span>, the number of strings <span class="math inline">\(s \in \{0, 1\}^n\)</span> such that <span class="math inline">\(d(s, r) &lt; (p + \lambda) n\)</span> is bounded by <span class="math display">\[
\begin{aligned}
    \sum_{k = 0}^{\lfloor (p + \lambda) n \rfloor} \binom{n}{k} 
        &amp;\le \frac{n + 1}{2} 2^{ n \mathbf{H} ( p + \lambda) }
\end{aligned}
\]</span></p>
<p>If a message <span class="math inline">\(s\)</span> is picked uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>, the probability such that <span class="math display">\[
\Pr_{s \sim \mathbf{U}(\{0, 1\}^n) } [d(s, r) &lt; (p + \lambda) n ] \le \frac{n + 1}{2^{n + 1} } 2^{ n \mathbf{H} \left(  p + \lambda \right) }
\]</span></p>
<p>By union bound,<br />
<span class="math display">\[
\begin{aligned}
    \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] 
    &amp;&lt; 2^k \frac{n + 1}{2^{n + 1} } 2^{ n \mathbf{H}( p + \lambda) }  \\
    &amp;= 2^{ k + \log (n + 1) +  n \mathbf{H} ( p + \lambda) - n - 1 } 
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(k \le n(1 - \mathbf{H}(p) - \epsilon)\)</span>, when <span class="math inline">\(n\)</span> is sufficient large, it follows<br />
<span class="math display">\[
\begin{aligned}
     \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] 
        &lt; 2^{ \log \frac{n + 1}{2} +  n ( \mathbf{H} ( p + \lambda) - \mathbf{H} ( p) - \delta) } 
        \le \frac{\delta}{2}
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    &amp;\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} } \left[ \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \\
    &amp;= \sum_{r \in \{0, 1\}^n }     \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] \cdot  \Pr_{R \sim \mathfrak{D}(s_i, p) } [ R = r \mid S = s_i] \\
    &amp;\le \frac{\delta}{2}
\end{aligned}
\]</span></p>
<p>and <span class="math display">\[
\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} }  \left[ \sum_{i = 1}^{2^{k + 1} }  \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \le 2^{k + 1} \frac{\delta}{2} = 2^k \delta
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Proof of 2.</em></strong></p>
<p>The difficulty here is to prove that the claim holds for arbitrary <span class="math inline">\((k, n)\)</span> encoding and decoding functions. The proof relies on an important property:</p>
<blockquote>
<p>the decoding function <span class="math inline">\(\text{Dec}\)</span> is a function on <span class="math inline">\(\{0, 1\}^n\)</span></p>
</blockquote>
<p>For any <span class="math inline">\(r \in \{0, 1\}^n\)</span>, there is a unique output <span class="math inline">\(\text{Dec}(r) \in \{0, 1\}^k\)</span>. The decoding function needs to decide the unique message <span class="math inline">\(\{0, 1\}^n\)</span> the sender want to send.</p>
<p>Now, define <span class="math display">\[
S_i \doteq \{ r \in \{0, 1\}^n :  r \text{ is decoded correctly if } s_i \text{ is sent}   \}
\]</span></p>
<p>The <span class="math inline">\(S_i\)</span>'s are non-overlapped and we have, <span class="math display">\[
\cup_{i = 1}^{2^k} S_i \subset \{0, 1 \}^n
\]</span></p>
<p>If <span class="math inline">\(s_i\)</span> is sent, then by Hoeffding inequality, with probability at least <span class="math inline">\(1 - \exp(- 2\lambda^2 n)\)</span>, the received message <span class="math inline">\(R\)</span> is likely to fall into a ring centered at <span class="math inline">\(s_i\)</span>: <span class="math display">\[
\text{ring} (s_i) \doteq \{ r \in \{0, 1\}^n : |d(r, s_i) - pn |&lt; \lambda n \}
\]</span></p>
<p>Finally, if the messages <span class="math inline">\(s_1, s_2, ..., s_{2^k}\)</span> are sent uniformly at random, i.e., <span class="math inline">\(i \sim \mathbf{U}[1, 2^k]\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[ R \text{ is decoded correctly}]   &amp;=  \sum_{i = 1}^{2^k} \sum_{r \in S_i } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \Pr_{i \sim \mathbf{U}[1, 2^k]} [ S = s_i] \\ 
        &amp;=  \frac{1}{2^k} \sum_{i = 1}^{2^k} \sum_{r \in S_i } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] 
\end{aligned}
\]</span></p>
<p>Observed that <span class="math inline">\(S_i \subset \text{ring} (s_i) \cup (S_i \cap \text{ring} (s_i) )\)</span>, we get</p>
<p><span class="math display">\[
\begin{aligned}
 &amp;\Pr[ R \text{ is decoded correctly}] \\
        &amp;\le \frac{1}{2^k}  \sum_{i = 1}^{2^k} \left( \sum_{r \notin \text{ring} (s_i)  } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i]  +\sum_{r \in S_i \cap \text{ring} (s_i) } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \right) \\ 
        &amp;\le \frac{1}{2^k}  \sum_{i = 1}^{2^k} \left( \exp( -2\lambda^2n) +\sum_{r \in S_i \cap \text{ring} (s_i) } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \right) 
\end{aligned}
\]</span></p>
<p>Note that for <span class="math inline">\(r \in S_i \cap \text{ring} (s_i)\)</span>, as <span class="math inline">\(p &lt; 1 / 2\)</span>, it holds that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \le p^{ \lceil(p - \lambda) n \rceil} (1 - p)^{ n - \lceil(p - \lambda) n \rceil }
\]</span></p>
<p>We obtain</p>
<p><span class="math display">\[
\begin{aligned}
        &amp;\Pr[ R \text{ is decoded correctly}] \\
        &amp;\le \exp( -2\lambda^2n) +  \frac{1}{2^k}  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } p^{ \lceil(p - \lambda) n \rceil} (1 - p)^{ n - \lceil(p - \lambda) n \rceil } \\
        &amp;\le \exp( -2\lambda^2n) +  \frac{1}{2^k}  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } p^{ (p - \lambda) n } (1 - p)^{ n - (p - \lambda) n  } \\
        &amp;= \exp( -2\lambda^2n) +  p^{ (p - \lambda) n } (1 - p)^{ n - (p - \lambda) n  }  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } \frac{1}{2^k}  \\
        &amp;= \exp( -2\lambda^2n) +   p^{ pn } (1 - p)^{ n - pn } \left(\frac{ 1 - p}{ p} \right)^{\lambda n} 2^{n - k}\\
        &amp;= \exp( -2\lambda^2n) +  2^{n - k - n \mathbf{H}(p)}  \left(\frac{ 1 - p}{ p} \right)^{\lambda n}
\end{aligned}
\]</span></p>
<p>Conditioning on that <span class="math inline">\(k \ge n (1 - \mathbf{H}(p) + \delta)\)</span>, <span class="math display">\[
\Pr[ R \text{ is decoded correctly}] \le \exp( -2\lambda^2n) +  2^{- n (\delta + \lambda \log \frac{ 1 - p}{ p} ) }  
\]</span></p>
<p>By choosing sufficient small <span class="math inline">\(\lambda\)</span> and large enough <span class="math inline">\(n\)</span>, <span class="math inline">\(\Pr[ R \text{ is decoded correctly}]\)</span> can be arbitrary small.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h2 id="reference">Reference</h2>
<p>[1] M. Mitzenmacher and E. Upfal, Probability and computing: an introduction to randomized algorithms and probabilistic analysis. New York: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/07/Entropy-and-Compression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/07/Entropy-and-Compression/" class="post-title-link" itemprop="url">Entropy and Compression</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-07 12:24:22" itemprop="dateCreated datePublished" datetime="2020-06-07T12:24:22+10:00">2020-06-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-13 23:57:00" itemprop="dateModified" datetime="2020-07-13T23:57:00+10:00">2020-07-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>One interpretation of entropy of a random variable is as a measure of how many unbiased, independent random bits in expectation we can extract from the random variable. Another one comes from compression of a sequence.</p>
<p>Assume that the sequence studied (denoted as <span class="math inline">\(s\)</span>) is a binary one. It consists of a concatenation of the outcomes of <span class="math inline">\(n\)</span> independent Bernoulli random variables. Without lose of generality, we assume that each bit comes up <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p \le { 1 / 2 }\)</span>. This is one of the simplest models of a sequence.</p>
<p>The fact that sequence could be a biased one makes it possible to represent it by a new sequence with shorter length in expectation. For example, suppose that <span class="math inline">\(p = \frac{1}{4}\)</span>, then for a pair of consecutive bits, it have</p>
<ol type="1">
<li>probability <span class="math inline">\(\frac{1}{16}\)</span> of being <span class="math inline">\(11\)</span>,<br />
</li>
<li>probability <span class="math inline">\(\frac{3}{16}\)</span> of being <span class="math inline">\(01\)</span>,</li>
<li>probability <span class="math inline">\(\frac{3}{16}\)</span> of being <span class="math inline">\(10\)</span>,</li>
<li>probability <span class="math inline">\(\frac{9}{16}\)</span> of being <span class="math inline">\(00\)</span>.</li>
</ol>
<p>If we use <span class="math inline">\(111\)</span> to represent <span class="math inline">\(11\)</span>, <span class="math inline">\(110\)</span> to represent <span class="math inline">\(01\)</span>, <span class="math inline">\(10\)</span> to represent <span class="math inline">\(10\)</span>, and <span class="math inline">\(0\)</span> to represent <span class="math inline">\(00\)</span>, then the expected number of representation bits per pair is <span class="math display">\[
\begin{array}{r}
    3 \cdot \frac{1}{16} + 3 \cdot \frac{3}{16} + 2 \cdot  \frac{3}{16} + 1 \cdot  \frac{9}{16} 
    = \frac{27}{16} 
    &lt; 2
\end{array}
\]</span></p>
<p>In general, compression is not limited to the way described above. We call the rule with which we compress a string <em>a compression function</em>.</p>
<h3 id="compression-function"><strong><em>Compression Function</em></strong></h3>
<p><em>Let <span class="math inline">\(S = \{0, 1\}^n\)</span> be the set of binary sequence of length <span class="math inline">\(n\)</span>, and <span class="math inline">\(T = \{ 0, 1\}^+\)</span> the set of binary sequence of any positive length. A compression function <span class="math inline">\(h: S \rightarrow T\)</span> is an injective (one-to-one) function from <span class="math inline">\(S\)</span> to <span class="math inline">\(T\)</span>.</em></p>
<p>In other words, each <span class="math inline">\(s \in S\)</span> is assigned a unique non-empty sequence (of arbitrary length) by <span class="math inline">\(h\)</span>.</p>
<p>Observe that the size of <span class="math inline">\(S\)</span> is <span class="math inline">\(2^n\)</span>. On the other hand, the number of sequences with length less than <span class="math inline">\(n\)</span> is <span class="math inline">\(\sum_{i = 1}^{n - 1} 2^{i} = 2^n - 1 &lt; 2^n\)</span>. For any <span class="math inline">\(h\)</span>, it is impossible for <span class="math inline">\(h\)</span> to map every <span class="math inline">\(s \in S\)</span> to a sequence with length less than <span class="math inline">\(n\)</span>. Under adversarial input, <span class="math inline">\(h\)</span> can not compress at all.</p>
<p>The hope is that when there is a distribution <span class="math inline">\(\mathfrak{D}\)</span> on <span class="math inline">\(S\)</span>, the expected length of the compressed sequences would be small: <span class="math display">\[
\mathbf{E} [ |h(s)| ] = \sum_{s \in S} p_s \cdot |h(s)|
\]</span></p>
<p>To illustrate the meaning of entropy, we study a simple case where <span class="math inline">\(\mathfrak{D}\)</span> is the joint distribution of <span class="math inline">\(n\)</span> <em>i.i.d.</em> Bernoulli random variables that come up <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p \le { 1 / 2 }\)</span> (by symmetry, the claim holds for the case of <span class="math inline">\(p &gt; { 1 / 2 }\)</span>).</p>
<p>The following theorem formalizes how good a compression function we can find.</p>
<h3 id="entropy-as-lower-bound-and-upper-bound"><strong><em>Entropy as Lower Bound and Upper Bound</em></strong></h3>
<p><em>Theorem.</em></p>
<ol type="1">
<li><span class="math inline">\(\forall \delta &gt; 0\)</span>, <span class="math inline">\(\exists\)</span> a compression <span class="math inline">\(h\)</span> and integer <span class="math inline">\(N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n \ge N\)</span>, it holds that <span class="math display">\[
\mathbf{E}[ |h(s)| ] \le (1 + \delta) n \mathbf{H}(p)
\]</span></li>
<li><span class="math inline">\(\forall \delta &gt; 0\)</span>, <span class="math inline">\(\exists\)</span> integer <span class="math inline">\(N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n \ge N\)</span>, it holds that for any compression <span class="math inline">\(h\)</span>, <span class="math display">\[
     \mathbf{E}[ |h(s)| ] \ge (1 - \delta) n \mathbf{H}(p)
 \]</span></li>
</ol>
<p><em>Intuitively, the entropy <span class="math inline">\(\mathbf{H}(p)\)</span> is the measure of the average number of bits in expectation we need after compression to represent a Bernoulli random variable that comes up <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p\)</span>.</em></p>
<p><em>The high level idea of the proof is that, with high probability, the number of ones in a <span class="math inline">\(s \in S\)</span> is roughly <span class="math inline">\(np\)</span>. There are roughly <span class="math inline">\(2^{n\mathbf{H}(p)}\)</span> such sequences. Therefore, we can use <span class="math inline">\(n \mathbf{H}(p)\)</span> bits to represent each sequence.</em></p>
<p><strong><em>Proof.</em></strong> The claim is trivially true if <span class="math inline">\(p = { 1 / 2 }\)</span>. Otherwise, <span class="math inline">\(p &lt; { 1 / 2 }\)</span>. We can pick some <span class="math inline">\(\epsilon &gt; 0\)</span>, such that <span class="math inline">\(p + \epsilon &lt; { 1 / 2 }\)</span>. Further, if <span class="math inline">\(n\)</span> is large enough, <span class="math inline">\(np + n\epsilon \le n / 2 - 1\)</span>. Hence, we may assume that <span class="math inline">\(\lceil np + n\epsilon \rceil \le n / 2\)</span> .</p>
<p>Denote <span class="math inline">\(Z\)</span> be the number of ones in the string <span class="math inline">\(s\)</span>. By Hoeffding Inequality, it holds that <span class="math display">\[
\Pr[ |Z - np| \ge n\epsilon ] \le \exp(-2n \epsilon^2)
\]</span></p>
<p>By the fact that <span class="math inline">\(Z\)</span> takes only integer values, we have <span class="math display">\[
\Pr[Z \ge \lceil np + n\epsilon \rceil] \le \exp(-2 n \epsilon^2 )
\]</span></p>
<p>We use the first bit output of <span class="math inline">\(h\)</span> as a flag. For a string <span class="math inline">\(s\)</span> with <span class="math inline">\(Z \ge \lceil np + n\epsilon \rceil\)</span>, we set the first bit to <span class="math inline">\(0\)</span> and then output the same sequence, i.e., <span class="math inline">\(h(s) = 0s\)</span>. In such case we do not compress the string at all and use <span class="math inline">\(n + 1\)</span> bits to represent it.</p>
<p>We set the first bit output to <span class="math inline">\(1\)</span> if <span class="math inline">\(Z &lt; \lceil np + n \epsilon \rceil \le n / 2\)</span>. The number of such sequences is bounded by <span class="math display">\[
\sum_{k = 0}^{\lceil np + n\epsilon \rceil - 1} \binom{n}{k} \le  \frac{n}{2} \binom{n}{\lceil np + n\epsilon \rceil - 1 } \le \frac{n}{2} 2^{ n \cdot \mathbf{H} \left( \frac{ \lceil np + n\epsilon \rceil - 1 }{n} \right) } \le \frac{n}{2} 2^{ n \mathbf{H} \left( p + \epsilon  \right) }
\]</span></p>
<p>The first inequality holds because <span class="math inline">\(\binom{n}{k}\)</span> increases for <span class="math inline">\(k &lt; n / 2\)</span> and that the summation is over <span class="math inline">\(\lceil np + n \epsilon \rceil \le n / 2\)</span> terms. The last inequality holds since <span class="math inline">\((\lceil np + n\epsilon \rceil - 1) / n \le ( np + n\epsilon ) / n = p + \epsilon &lt; 1 / 2\)</span> and <span class="math inline">\(\mathbf{H}( \cdot )\)</span> is a increasing function in the range <span class="math inline">\([0, 1/2]\)</span>.</p>
<p>We compress these sequences by representing each of them with a unique sequence of at most<br />
<span class="math display">\[
\begin{array}{rl}
    \log \left[ (n / 2) \cdot  2^{ n \cdot\mathbf{H} \left( p + \epsilon  \right) } \right]
    = n \cdot\mathbf{H} \left( p + \epsilon  \right)  + \log n -1
\end{array}
\]</span></p>
<p>bits. Considering the leading flag bit 1 (to indicate the sequence is a compressed one), we output at most <span class="math inline">\(n \mathbf{H} \left( p + \epsilon \right) + \log n\)</span> bits for sequences with <span class="math inline">\(Z &lt; \lceil np + n \epsilon \rceil\)</span>. This can be written as <span class="math inline">\(\left[ \mathbf{H} \left( p + \epsilon \right) + (1 / n) \cdot (\log n - 1) \right] n\)</span> bits and is smaller than <span class="math display">\[
(1 + \delta /{2} ) \cdot \mathbf{H} (p) \cdot n
\]</span></p>
<p>is <span class="math inline">\(\epsilon\)</span> is smaller enough and <span class="math inline">\(n\)</span> is large enough.</p>
<p>We are almost done with the proof. It is left to consider sequences with <span class="math inline">\(Z \ge \lceil np + n \epsilon \rceil\)</span>. For those sequence, we don't compress them and and set the leading flag bit to 0. The outputs for those sequences consist of at most <span class="math inline">\(1 + n\)</span> bits. However, by Hoeffding inequality, such sequences do not appear frequently and the expected output length can be made arbitrary small.</p>
<p>Specifically, <span class="math inline">\(\forall \delta &gt; 0\)</span>, we can find large enough <span class="math inline">\(N\)</span>, such that <span class="math inline">\(\forall n &gt; N\)</span>, it holds</p>
<p><span class="math display">\[
(n + 1 ) \cdot \exp(-2 n \epsilon^2 ) \le n \cdot (\delta / 2)  \cdot  \mathbf{H}(p)
\]</span></p>
<p>Then, in expectation, the bits outputted by <span class="math inline">\(h\)</span> is at most <span class="math display">\[
\begin{aligned}
    &amp;(n + 1) \cdot \exp(-2 n \epsilon^2 ) + [n \cdot \mathbf{H} \left( p + \epsilon  \right)   + \log n ] \cdot [ 1 - \exp(-2 n \epsilon^2 ) ] \\
    \le&amp; n \cdot (\delta / 2)  \cdot  \mathbf{H}(p) + [ n \cdot \mathbf{H} \left( p + \epsilon  \right)   + \log n ]\\
    \le&amp; n \cdot (\delta / 2)  \cdot  \mathbf{H}(p)  +  (1 + \delta /{2} ) \cdot \mathbf{H} (p) \cdot n\\
    \le&amp; \left(  1 + \delta \right) \cdot \mathbf{H}(p) \cdot n
\end{aligned}
\]</span></p>
<p>To prove the lower bound, first we need the following lemma.</p>
<p><strong><em>Lemma.</em></strong> <em>for <span class="math inline">\(s_1, s_2 \in S\)</span>, if <span class="math inline">\(s_1\)</span> has more ones than <span class="math inline">\(s_2\)</span>, i.e., <span class="math inline">\(\Pr(s_1) \ge \Pr(s_2)\)</span>, then the <span class="math inline">\(h\)</span> that minimizes the expected output length <span class="math inline">\(\mathbf{E}[ |h(s)| ]\)</span> should assign <span class="math inline">\(s_1\)</span> a sequence at most as long as <span class="math inline">\(s_2\)</span>, i.e., <span class="math inline">\(|h(s_1)| \le |h(s_2)|\)</span>.</em></p>
<p><em>Proof</em>. Otherwise, if we swap the output sequences of <span class="math inline">\(h(s_1)\)</span> and <span class="math inline">\(h(s_2)\)</span>, we lower value of <span class="math inline">\(\mathbf{E}[ |h(s)| ]\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Further, consider the number of <span class="math inline">\(s \in S\)</span> with <span class="math inline">\(\lfloor np - n \epsilon \rfloor\)</span> ones, <span class="math display">\[
\begin{aligned}
\binom{n}{\lfloor np - n\epsilon \rfloor } 
    &amp;\ge \frac{1}{n + 1} 2^{ n \cdot \mathbf{H} \left( { \lfloor np - n\epsilon \rfloor } / { n} \right) } \\
    &amp;\ge \frac{1}{n + 1} 2^{ n \cdot \mathbf{H}( { (np - n\epsilon - 1) } / {n} )} \\
    &amp;\ge 2^{ \lfloor n \cdot \mathbf{H}( p - \epsilon -1 / n) -  \log (n + 1)  \rfloor }
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(k = \lfloor n \cdot \mathbf{H}( p - \epsilon -1 / n) - \log (n + 1) \rfloor\)</span>. For large enough <span class="math inline">\(n\)</span>, it holds that <span class="math display">\[
k \ge (1 - \delta / 2) \cdot n \cdot \mathbf{H}(p)
\]</span></p>
<p>Further, since <span class="math display">\[
\sum_{i = 1}^{  k - 1  } 2^i \le \sum_{i = 0}^{  k - 1  } 2^i = 2^{  k } - 1 &lt; 2^k,
\]</span></p>
<p>there are strictly less than <span class="math inline">\(2^k\)</span> distinct binary sequences with length at most <span class="math inline">\(k - 1\)</span>.</p>
<p>Therefore, for the set of <span class="math inline">\(s \in S\)</span> with <span class="math inline">\(\lfloor np - n\epsilon \rfloor\)</span> ones, at least one of them has output length at least <span class="math inline">\(k\)</span>.</p>
<p>By the previous lemma, all sequences with more than <span class="math inline">\(\lfloor np - n\epsilon \rfloor\)</span> ones has length at least <span class="math inline">\(k\)</span>.</p>
<p>Denote <span class="math inline">\(Z\)</span> be the number of ones in the string <span class="math inline">\(s\)</span>. By Hoeffding Inequality, it holds that <span class="math display">\[
\Pr[ |Z - np| \ge n\epsilon ] \le \exp(-2n \epsilon^2)
\]</span></p>
<p>By the fact that <span class="math inline">\(Z\)</span> takes only integer values, we have <span class="math display">\[
\Pr[Z \le \lfloor np - n\epsilon \rfloor] \le \exp(-2 n \epsilon^2 )
\]</span></p>
<p>The expected output length of the sequences with more than <span class="math inline">\(\lfloor np - n\epsilon \rfloor\)</span> ones is at least <span class="math display">\[
\begin{aligned}
    &amp;[ 1 - \exp(-2n\epsilon^2) ] \cdot k \\
    =&amp;[ 1 - \exp(-2n\epsilon^2) ] \cdot  (1 - \delta / 2) \cdot n \cdot \mathbf{H}(p)
\end{aligned}
\]</span></p>
<p>This is at least <span class="math inline">\((1 - \delta) \cdot n \cdot \mathbf{H}(p)\)</span> when <span class="math inline">\(n\)</span> is large enough.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="huffman-code"><strong><em>Huffman Code</em></strong></h3>
<p>In this section, we show that the upper bound can be achieved by Huffman code.</p>
<blockquote>
<p><em><span class="math inline">\(\forall \delta &gt; 0\)</span>, <span class="math inline">\(\exists\)</span> a compression <span class="math inline">\(h\)</span> and integer <span class="math inline">\(N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n \ge N\)</span>, it holds that</em> <span class="math display">\[
\mathbf{E}[ |h(s)| ] \le (1 + \delta) n \mathbf{H}(p)
\]</span></p>
</blockquote>
<p>We begin with an important property of Huffman Code. Suppose that we have an alphabet <span class="math inline">\(\mathbb{U}\)</span> such that probability associated with each element in <span class="math inline">\(\mathbb{U}\)</span> is <span class="math inline">\(2^{-l}\)</span> for some integer <span class="math inline">\(l\)</span>. Then <span class="math inline">\(\exists h\)</span>, such that <span class="math display">\[
   \mathbf{E}_{X \in \mathbb{U} } [ |h(X)| ] =  n \mathbf{H}(X)
\]</span></p>
<p>For example, if <span class="math inline">\(\mathbb{U} = \{0, 1, 2, 3 \}\)</span> and the distribution <span class="math inline">\(p = \{ 0.5, 0.25, 0.125, 0.125 \}\)</span>, then we can have the following encoding</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Encoding.png" /></p>
<p>The expected coding length is <span class="math display">\[
0.5 \cdot 1 + 0.25 \cdot 2 + 2 \cdot 0.125 \cdot 3 = 1.75
\]</span></p>
<p>which is exactly the entropy of the random variable.</p>
<p>In general, if we have a random variable <span class="math inline">\(X\)</span>, we can round down its probability to the nearest integer negative power of <span class="math inline">\(2\)</span>. The expected code length is given by <span class="math display">\[
\sum_{i } p_i \left\lceil \log \frac{1}{p_i} \right\rceil \le \mathbf{H}(p) + 1
\]</span></p>
<p>Intuitively, the <span class="math inline">\(\left\lceil \log \frac{1}{p_i} \right\rceil\)</span> has enough slot to accommodate all elements with probabilities <span class="math inline">\(p_i\)</span>'s.</p>
<p>In particular, we view <span class="math inline">\(\{0, 1\}^n\)</span> as a large alphabet <span class="math inline">\(\mathbf{\Sigma }\)</span>. The alphabet has entropy <span class="math inline">\(n \cdot \mathbf{H}(p)\)</span>. We can have a encoding such that the expected output length is at most <span class="math inline">\(n \cdot \mathbf{H}(p) + 1\)</span>. For large enough <span class="math inline">\(n\)</span>, this is at most <span class="math inline">\((1 + \delta) n \cdot \mathbf{H}(p)\)</span>.</p>
<h3 id="relative-entropy-and-mutual-information"><strong><em>Relative Entropy and Mutual Information</em></strong></h3>
<h4 id="relative-entropy">Relative Entropy</h4>
<p>Given a distribution <span class="math inline">\(p\)</span>, we mistaken it as a distribution <span class="math inline">\(q\)</span>, the expected coding length is roughly <span class="math display">\[
\sum_i p_i \log \frac{1}{q_i}
\]</span></p>
<p>The discrepancy between the optimal coding is given by <span class="math display">\[
D(p || q) = \sum_i p_i \log \frac{1}{q_i} - \sum_i p_i \log \frac{1}{p_i} = \sum_i p_i \log \frac{p_i}{q_i}
\]</span></p>
<p>By definition, we know that this gap is always non-negative. We can prove it rigorously algebraically. We show two different approaches here.</p>
<ol type="1">
<li>By that <span class="math inline">\(f(x) = x \log x\)</span> is convex for <span class="math inline">\(x \ge 0\)</span>, we have <span class="math display">\[
\begin{aligned}
    D(p || q) &amp;= \sum_i p_i \log \frac{p_i}{q_i} \\
    &amp;= \sum_i q_i \cdot \frac{p_i}{q_i} \log \frac{p_i}{q_i} \\
    &amp;\ge \left( \sum_i q_i \cdot \frac{p_i}{q_i} \right) \log \left( \sum_i q_i \cdot \frac{p_i}{q_i} \right) \\
    &amp;= 0
\end{aligned}
\]</span></li>
<li>By that <span class="math inline">\(f(x) = -\log x\)</span> is convex (for <span class="math inline">\(x \ge 0\)</span> ), we have <span class="math display">\[
\begin{aligned}
    D(p || q) &amp;= \sum_i - p_i \log \frac{q_i}{p_i} \\
    &amp;\ge  -\log \left( \sum_i p_i \cdot \frac{q_i}{p_i} \right) \\
    &amp;= 0
\end{aligned}
\]</span></li>
</ol>
<h4 id="mutual-information">Mutual Information</h4>
<p>Given two random variable, the mutual information between them is defined as <span class="math display">\[
I(X; Y) = I(Y; X) = \mathbb{E}_{p(x, y) } \left[ \log \frac{ p(x, y)}{p(x) p(y) } \right]
\]</span></p>
<p>If we know <span class="math inline">\(p(x,y)\)</span> we use in expectation <span class="math inline">\(\mathbb{E}_{p(x, y) } \left[ \log \frac{1} { p(x, y)} \right]\)</span> to describe them. If we know only <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(p(y)\)</span>, the mutual information measured the bits we waste.</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;\sum p(x, y) \log \frac{ p(x, y)}{p(x) p(y) } \\
    =&amp;  \sum p(x, y) \log \frac{ 1 }{p(x)} \\
    &amp;+  \sum p(x, y) \log \frac{ 1 }{p(y)} \\
    &amp;-  \sum p(x, y) \log \frac{ 1 }{p(x, y)} \\
    =&amp; \mathbf{H} ( X ) + \mathbf{H} ( Y ) - \mathbf{H} ( X, Y ) \\
    =&amp;\sum p(x, y) \log \frac{ p(x | y)}{p(x)  } \\
    =&amp; \mathbf{H} ( X ) - \mathbf{H} ( X \mid Y) \\
    =&amp;\sum p(x, y) \log \frac{ p(x | y)}{ p(x) }  \\
    =&amp; \mathbf{H} ( Y ) - \mathbf{H} ( X \mid Y) \\
\end{aligned}
\]</span></p>
<p><strong><em>Lemma.</em></strong> <span class="math display">\[
\mathbf{I}(X; Y) \ge 0
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\mathbf{I}(X; Y) = D( p(x, y) \mid p(x) p(y) ) \ge 0
\]</span></p>
<p>Or <span class="math display">\[
\begin{aligned}
    \sum p(x, y) \log \frac{ p(x, y)}{p(x) p(y) } 
    &amp;= -\sum p(x, y) \log \frac{p(x) p(y) }{ p(x, y)} \\
    &amp;\ge - \log \sum p(x, y) \frac{p(x) p(y) }{ p(x, y)} \\
    &amp;=0   
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Corollary</em></strong> <span class="math display">\[
\mathbf{H} ( Y ) - \mathbf{H} ( X \mid Y) = \mathbf{H} ( X ) - \mathbf{H} ( X \mid Y) \ge 0
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Definition</em></strong> Conditional Relative Entropy <span class="math display">\[
\begin{aligned}
    \mathbf{D}( p(y \mid x) || q(y \mid x) ) &amp;\doteq  \mathbb{E}_{p(x, y) } \left[ \log \frac{ p(Y \mid X) }{ q(Y \mid X) }  \right]\\
    &amp;=\sum_{x}  \sum_{ y } p(y \mid x) \log \frac{ p(y \mid x) }{ q(y \mid x) }
\end{aligned}
\]</span></p>
<p><strong><em>Lemma.</em></strong> Relative entropy <span class="math inline">\(\mathbf{D}( p || q )\)</span> is convex in the pair <span class="math inline">\((p, q)\)</span>: if there are two distribution pairs <span class="math inline">\((p_1, q_1)\)</span> and <span class="math inline">\((p_2, q_2)\)</span>, then . <span class="math display">\[
\mathbf{D}( \lambda p_1 + (1 - \lambda) p_2 || \lambda q_1 + (1 - \lambda_1) q_2 ) \le \lambda \mathbf{D}( p_1 ||  q_1 ) + (1 - \lambda) \mathbf{D}( p_2 ||  q_2 )
\]</span></p>
<p><strong><em>Proof.</em></strong> For any fix value <span class="math inline">\(x\)</span>, it holds that <span class="math display">\[
\begin{aligned}
    &amp;[ \lambda p_1(x) + (1 - \lambda) p_2(x) ] \log \frac{ \lambda p_1(x) + (1 - \lambda) p_2(x) }{ \lambda q_1(x) + (1 - \lambda) q_2(x) } \\
    &amp;=-[ \lambda p_1(x) + (1 - \lambda) p_2(x) ] \log \frac { \lambda q_1(x) + (1 - \lambda) q_2(x) } { \lambda p_1(x) + (1 - \lambda) p_2(x) } \\
    &amp;= -[ \lambda p_1(x) + (1 - \lambda) p_2(x) ] \log \left( \frac { \lambda p_1(x)  } { \lambda p_1(x) + (1 - \lambda) p_2(x) } \frac { \lambda q_1(x) } { \lambda p_1(x)  } + \frac { (1 - \lambda) p_2(x)  } { \lambda p_1(x) + (1 - \lambda) p_2(x) } \frac { (1 - \lambda) q_2(x) } { (1 - \lambda) p_2(x)  } \right) \\
    &amp;\le -\lambda p_1(x) \log   \frac { \lambda q_1(x) } { \lambda p_1(x)  } - (1 - \lambda) p_2(x) \log \frac { (1 - \lambda) q_2(x) } { (1 - \lambda) p_2(x)  }
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="chain-rules"><strong><em>Chain Rules</em></strong></h3>
<p>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be random variables whose density function is <span class="math inline">\(p(x_1, x_2, ..., x_n)\)</span>, then</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbf{H}(X_1, X_2, ..., X_n) = \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1})\)</span><br />
<strong><em>Proof.</em></strong><br />
<span class="math display">\[
 \begin{aligned}
     \mathbf{H}(X_1, X_2, ..., X_n) 
         &amp;= \sum_{x_1, x_2, ..., x_n} p(x_1, x_2, ..., x_n)  \log \frac{1}{\prod_{i = 1}^n p(x_i \mid x_1, ..., x_{i - 1})  }  \\
         &amp;= \sum_{x_1, x_2, ..., x_n} p(x_1, x_2, ..., x_n) \sum_{i = 1}^n \log \frac{1}{ p(x_i \mid x_1, ..., x_{i - 1})  }  \\
         &amp;=\sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1}) \\
 \end{aligned}
 \]</span></p>
<p><strong><em>Corollary</em></strong> <span class="math display">\[
 \mathbf{H} ( X_1, X_2, ..., X_n) = \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1}) \le \sum_{i = 1}^n \mathbf{H}(X_i)
 \]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{I}(X_1, X_2, ..., X_n; Y) = \sum_{i = 1}^n \mathbf{I}(X_i ; Y\mid X_1, ..., X_{i - 1})\)</span><br />
<strong><em>Proof.</em></strong><br />
<span class="math display">\[
 \begin{aligned}
     \mathbf{I}(X_1, X_2, ..., X_n; Y)
     &amp;=  \mathbf{H}(X_1, X_2, ..., X_n) -  \mathbf{H}(X_1, X_2, ..., X_n \mid Y) \\
     &amp;= \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1})  - \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1}, Y) \\
     &amp;= \sum_{i = 1}^n \mathbf{I}(X_i ; Y\mid X_1, ..., X_{i - 1}) \\
 \end{aligned}
 \]</span><br />
</p></li>
<li><p><span class="math inline">\(\mathbf{D}( p(x, y) || q(x, y) ) = \mathbf{D}( p(x) || q(x) ) + \mathbf{D}( p(y \mid x) || q(y \mid x) )\)</span><br />
<strong><em>Proof.</em></strong> <span class="math display">\[
 \begin{aligned}
     \mathbf{D}( p(x, y) || q(x, y) ) 
     &amp;= \sum_{x, y} p(x, y) \log \frac{ p(x) p(y \mid x) }{q(x) q (y \mid x) }
     \\
     &amp;= \sum_{x, y} p(x, y) \log \frac{ p(x)  }{q(x) } + \sum_{x, y} p(x, y) \log \frac{  p(y \mid x) }{  q (y \mid x) }
     \\
     &amp;= \mathbf{D}( p(x) || q(x) ) + \mathbf{D}( p(y \mid x) || q(y \mid x) )
 \end{aligned}
 \]</span></p></li>
</ol>
<h3 id="reference.">Reference.</h3>
<p>[1]M. Mitzenmacher and E. Upfal, Probability and computing: an introduction to randomized algorithms and probabilistic analysis. New York: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/01/Entropy-and-Random-Bits/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/01/Entropy-and-Random-Bits/" class="post-title-link" itemprop="url">Entropy and Random Bits</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-01 00:16:09" itemprop="dateCreated datePublished" datetime="2020-06-01T00:16:09+10:00">2020-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-14 00:15:12" itemprop="dateModified" datetime="2020-07-14T00:15:12+10:00">2020-07-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="entropy">Entropy</h4>
<p>The entropy of a random variable <span class="math inline">\(X\)</span> is defined as <span class="math display">\[
\mathbf{H}[X] = \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right]
\]</span></p>
<p>Here we adopt the convention that <span class="math inline">\(0 \log 0 = 0\)</span>, as <span class="math inline">\(\lim_{x \rightarrow 0^+} x \log x = \lim_{x \rightarrow \infty} \frac{1 }{ x } \log \frac{1 }{ x } = 0\)</span>.</p>
<p>The entropy is oblivious to the specific values <span class="math inline">\(X\)</span> takes and is only sensitive to the probabilities with which <span class="math inline">\(X\)</span> takes these values.</p>
<blockquote>
<p>Example.<br />
1. <span class="math inline">\(X_1 = \begin{cases} e^{1000}, \text{with probability 0.5} \\ 0, \text{with probability 0.5} \end{cases}\)</span><br />
2. <span class="math inline">\(X_2 = \begin{cases} 1, \text{with probability 0.5} \\ -1, \text{with probability 0.5} \end{cases}\)</span></p>
</blockquote>
<p>By definition, both <span class="math display">\[
\mathbf{H}[X_1] = \mathbf{H}[X_2] = 0.5 \log_2 2 + 0.5 \log_2 2 = 1
\]</span></p>
<p>We also notice that the entropy of a random variable may not reflect the whether a random variable is concentrated at its mean or not.</p>
<p>The binary entropy function <span class="math inline">\(\mathbf{H}(p)\)</span> of a Bernoulli random variable <span class="math inline">\(X\)</span> that takes value <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p\)</span> is <span class="math display">\[
\begin{aligned}
    \mathbf{H}(p) = &amp;p \log_2 \frac{1}{p} + (1 - p) \log_2 \frac{1}{1 - p} \\
    = &amp;-p \log_2 p - (1 - p) \log_2 (1 - p)    
\end{aligned}
\]</span></p>
<p>By concavity of <span class="math inline">\(\log_2(\cdot )\)</span>, we know that <span class="math display">\[
\begin{aligned}
    \mathbf{H}(p)
    &amp;\le \log_2 \left( p \frac{1}{p} + (1 - p) \frac{1}{1 - p} \right) 
    &amp;= 1
\end{aligned}
\]</span></p>
<p>The equality holds when <span class="math inline">\(p = 0.5\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Entropy.png" /></p>
<p>Some interesting values:</p>
<ul>
<li><span class="math inline">\(p = 0.5\)</span>, <span class="math inline">\(\mathbf{H}(p) = 1\)</span></li>
<li><span class="math inline">\(p = 0.2/0.8\)</span>, <span class="math inline">\(\mathbf{H}(p) \approx 0.7\)</span></li>
<li><span class="math inline">\(p = 0.1/0.9\)</span>, <span class="math inline">\(\mathbf{H}(p) \approx 0.5\)</span></li>
</ul>
<p><strong><em>Lemma.</em></strong> Given a discrete range <span class="math inline">\(\mathcal{U}\)</span>, the entropy of a random variable <span class="math inline">\(X\)</span> defined on <span class="math inline">\(\mathcal{U}\)</span> is maximized when <span class="math inline">\(X\)</span> has uniform distribution, i.e., <span class="math display">\[
\mathbf{H} (X) \le \log |\mathcal{U}|
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\begin{aligned}
    \mathbf{H} (X) 
        &amp;= \sum_{x} p(x) \log \frac{1}{p(x) } \\ 
        &amp;\le  \log \sum_{x} p(x) \frac{1}{p(x) } \\
        &amp;= \log |\mathcal{U}|
\end{aligned}
\]</span></p>
<p>The inequality holds with equality only when <span class="math inline">\(p(x) = 1/ |\mathcal{U}|\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Lemma.</em></strong> Let <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> be two distributions and <span class="math inline">\(\lambda \in [0, 1]\)</span>, then <span class="math display">\[
\mathbf{H} (\lambda p(x) + (1 - \lambda) q(x) ) \ge \lambda \mathbf{H}(p(x) ) + (1 - \lambda) \mathbf{H} (q(x) )
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\begin{aligned}
    \lambda \mathbf{H}(p(x) ) + (1 - \lambda) \mathbf{H} (q(x) ) 
        &amp;= \sum_{x} \left( \lambda p(x) \log \frac{1}{p(x)} + (1 - \lambda) q(x) \log \frac{1}{q(x) }  \right) \\ 
        &amp;= (  \lambda p(x) + (1 - \lambda) q(x) )\sum_{x} \left( \frac{ \lambda p(x) }{  \lambda p(x) + (1 - \lambda) q(x) } \log \frac{1}{p(x)} + \frac{(1 - \lambda) q(x) }{ \lambda p(x) + (1 - \lambda) q(x) } \log \frac{1}{q(x) }  \right) \\
        &amp;\le \sum_{x} ( \lambda p(x) + (1 - \lambda) q(x) ) \log \frac{1}{ \lambda p(x) + (1 - \lambda) q(x) } \\ 
        &amp;= \mathbf{H} ( \lambda p(x) + (1 - \lambda) q(x) ) 
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Lemma.</em></strong> For two independent random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we have <span class="math display">\[
\mathbb{E} \left[ \log_2 \frac{1}{\Pr[X + Y] } \right] = \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right] + \mathbb{E} \left[ \log_2 \frac{1}{\Pr[Y] } \right]
\]</span></p>
<p>i.e., <span class="math display">\[
\mathbf{H}[X + Y] = \mathbf{H}[X] + \mathbf{H}[Y]
\]</span></p>
<p><strong><em>Proof.</em></strong> We prove it for the case of discrete random variables. <span class="math display">\[
\begin{array}{rrl}
    &amp;\mathbb{E} \left[ \log_2 \frac{1}{\Pr[X + Y] }  \right] 
    &amp;= \sum_{x, y} p(x, y) \log_2 \frac{1}{p(x, y)} \\
    &amp;&amp;= \sum_{x, y} p(x) p(y) \left( \log_2 \frac{1}{p(x)} + \log_2 \frac{1}{p(y) } \right) \\
    &amp;&amp;= \sum_{x} p(x) \sum_{y} p(y) \log_2 \frac{1}{p(x)} + \sum_{y} p(y) \sum_{x} p(x) \log_2 \frac{1}{p(y) } \\
    &amp;&amp;= \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right] + \mathbb{E} \left[ \log_2 \frac{1}{\Pr[Y] } \right]    
\end{array}
\]</span> <span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Definition.</em></strong> Given random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the joint entropy <span class="math inline">\(\mathbf{H}(X, Y)\)</span> with a distribution <span class="math inline">\(p(x, y)\)</span> is defined as <span class="math display">\[
    \mathbf{H}(X,Y) = \mathbb{E}\left[ \log \frac{1}{p(x, y)} \right]
   \]</span> When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables, it is equivalent to <span class="math display">\[
    \mathbf{H}(X,Y) = \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(x, y) }.
   \]</span></p>
<p><strong><em>Definition.</em></strong> The conditional entropy <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> with a distribution <span class="math inline">\(p(x, y)\)</span> is defined as <span class="math display">\[
   \begin{aligned}
       \mathbf{H}(Y \mid X) 
        &amp;= \sum_{x \in \mathcal{X} } p(x) \cdot \mathbf{H} (Y \mid X = x) \\
        &amp;= \sum_{x \in \mathcal{X} } p(x) \cdot \sum_{y \in \mathcal{Y} } p(y \mid x) \log \frac{1}{p(y \mid x) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) } \\
        &amp;= \mathbb{E} \left[  \log \frac{1}{p(y \mid x) } \right]
   \end{aligned}
   \]</span></p>
<ul>
<li><strong>Remark.</strong> <em>Conditioned on <span class="math inline">\(X = x\)</span>, <span class="math inline">\(Y\)</span> is a random variable and therefore we can define its Entropy as <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span>. The conditional entropy <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> is the average of the <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span> over the distribution of <span class="math inline">\(X\)</span>. Both <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> and <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span> represent a value. This is different from the conditional expectation: <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span> is a random variable and <span class="math inline">\(\mathbb{E}[Y \mid X = x]\)</span> is a value.</em></li>
</ul>
<p><strong><em>Chain Rule.</em></strong> For random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, it holds that <span class="math display">\[
   \begin{aligned}
       \mathbf{H}(X, Y) 
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(x, y) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) p(x) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) } + \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{ p(x) } \\
        &amp;= \mathbf{H}(Y \mid X) + \mathbf{H}(X)
   \end{aligned}
   \]</span></p>
<p><em>Corollary of</em> <strong><em>Chain Rule.</em></strong> <span class="math display">\[
   \mathbf{H}(X) + \mathbf{H}(Y \mid X) =  
   \mathbf{H}(Y) + \mathbf{H}(X \mid Y)
   \]</span></p>
<p>Or <span class="math display">\[
   \mathbf{H}(X) - \mathbf{H}(X \mid Y)=  
   \mathbf{H}(Y) - \mathbf{H}(Y \mid X)
   \]</span></p>
<h3 id="binomial-distribution">Binomial Distribution</h3>
<p>We study the relationship of binomial distribution <span class="math inline">\(B(n, p)\)</span> and entropy. As a rule of thumb, for reasonably large <span class="math inline">\(n\)</span> (say, <span class="math inline">\(n \ge 100\)</span>) and moderate size of <span class="math inline">\(p\)</span> (e.g., <span class="math inline">\(0.1 \le p \le 0.9\)</span>), the probability is concentrated on the interval <span class="math inline">\([np - \sqrt n, np + \sqrt n]\)</span>.</p>
<p><span class="math inline">\(N = 100, p = 0.5\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.5.png" /></p>
<p><span class="math inline">\(N = 100, p = 0.3\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.3.png" /></p>
<p><span class="math inline">\(N = 100, p = 0.1\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.1.png" /></p>
<p><span class="math inline">\(N = 1000, p = 0.1\)</span>. Note that <span class="math display">\[
\Pr[ 100 - \sqrt{1000} \le X \le 100 + \sqrt{1000} ] \approx 0.9993
\]</span> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=1000_P=0.1.png" /></p>
<p><em><strong>Lemma</strong>.</em> For <span class="math inline">\(n\in \mathbf{Z}^+\)</span>, <span class="math inline">\(k \in [n]\)</span>, it holds that <span class="math display">\[
\binom{n}{k} \ge \frac{1}{n + 1} 2^{n \mathbf{H}(\frac{k}{n} ) }
\]</span></p>
<p><em><strong>Proof</strong>.</em> Define <span class="math inline">\(q = \frac{k}{n}\)</span>. Then <span class="math display">\[
\begin{aligned}
    (q + (1 - q))^n &amp;= \sum_{i = 0}^n \binom{n}{i} q^i (1 - q)^{n - i}
\end{aligned}
\]</span></p>
<p>The summation consists of <span class="math inline">\((n + 1)\)</span> terms. We claim that <span class="math display">\[
\binom{n}{k} q^k (1 - q)^{n - k}
\]</span></p>
<p>is the largest one. For <span class="math inline">\(i \in [n]\)</span>, <span class="math display">\[
\frac{ \binom{n}{i + 1} q^{i + 1} (1 - q)^{n - i - 1} }{ \binom{n}{i } q^{i} (1 - q)^{n - i } } = \frac{n - i }{i + 1} \frac{ q}{1 - q}
\]</span></p>
<p>The ratio is at least <span class="math inline">\(1\)</span> when <span class="math inline">\(\frac{n - i}{i + 1} \ge \frac{1 - q}{ q }\)</span>. We see that when <span class="math inline">\(i = nq - 1 = k - 1\)</span>, it holds that <span class="math display">\[
\frac{n - i}{i + 1} = \frac{n - nq + 1}{ n q} \ge \frac{1 -  q}{ q }
\]</span> and when <span class="math inline">\(i = k\)</span>, <span class="math display">\[
\frac{n - i}{i + 1} = \frac{n - nq}{ n q + 1} &lt; \frac{1 -  q}{ q }
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    \binom{n}{k} q^k (1 - q)^{n - k} 
        &amp;= \binom{n}{k} 2^{k \log_2 q} 2^{ (n - k )\log_2 (1 - q)} \\
        &amp;= \binom{n}{k} 2^{ n [ q \log_2 q +  (1 - q )\log_2 (1 - q) ]} \\
        &amp;\ge \frac{1}{n + 1}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="extracting-random-bits">Extracting Random Bits</h4>
<p>Consider a uniform random variable <span class="math inline">\(X\)</span> at <span class="math inline">\([0, m - 1]\)</span>. By definition we know that <span class="math display">\[
\mathbf{H}[X] = \sum_{i = 0}^{m - 1} \frac{1}{m} \log_2 \frac{1}{\frac{1}{m} } = \log_2 m
\]</span></p>
<p>Given <span class="math inline">\(X\)</span> as an input, the following algorithm outputs a binary string <span class="math inline">\(s\)</span>, such that each bit of <span class="math inline">\(s\)</span> can be interpreted as the outcome of independently Bernoulli random variables with probability <span class="math inline">\(0.5\)</span>.</p>
<p>On the high level, we divide <span class="math inline">\(m\)</span> into intervals of powers of 2. When <span class="math inline">\(X\)</span> falls into an interval, we return a binary number that is within the range of that interval. Specifically, we rewrite <span class="math display">\[
m = 2^{d_k} + 2^{d_{k - 1} } + ... + 2^{d_1}
\]</span></p>
<p>where <span class="math inline">\(\lfloor \mathbf{H}[X] \rfloor = \lfloor \log_2 m \rfloor = d_k &gt; d_{k - 1} &gt; ... &gt; d_1 \ge 0\)</span> are the indexes of non-zero bits in the binary representation of <span class="math inline">\(m\)</span>. The extraction is as follows:</p>
<blockquote>
<p><strong><em>Algorithm 1.</em></strong></p>
<ol type="1">
<li>for <span class="math inline">\(i \leftarrow k\)</span> <em>down-to</em> <span class="math inline">\(1\)</span> <em>do</em>:<br />
</li>
<li><span class="math inline">\(\quad\)</span> <em>if</em> <span class="math inline">\(X &lt; 2^{d_i}\)</span> <em>then</em>:<br />
</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(\quad\)</span> <em>return a <span class="math inline">\(d_i\)</span>-bit binary representation of</em> <span class="math inline">\(X\)</span>;<br />
</li>
<li><span class="math inline">\(\quad\)</span> <em>else:</em><br />
</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(\quad\)</span> <span class="math inline">\(X \leftarrow X - 2^{d_i}\)</span>;</li>
</ol>
</blockquote>
<p><strong>Example (<span class="math inline">\(m = 13\)</span>).</strong> <em>It is interesting to note that when <span class="math inline">\(X = 12\)</span>, our proposed extraction method return no random bit, as <span class="math inline">\(0 &lt; 2^0\)</span>. In this case our proposed method has wasted some randomness.</em></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/ExtractingRandomBits.png" /></p>
<p><em>Question to ponder: Come up with some method that uses <span class="math inline">\(n\)</span> i.i.d. <span class="math inline">\(X\)</span>'s to extract binary string with expected length close to <span class="math inline">\(n \mathbf{H}[X]\)</span>.</em></p>
<p>Let <span class="math inline">\(s\)</span> be the binary string returned by Algorithm 1.</p>
<p><strong><em>Theorem 1</em></strong>. <span class="math display">\[
\lfloor \log_2 m \rfloor - 1 \le \mathbf{E}[|s|] \le \log_2 m
\]</span></p>
<p><em>Proof.</em> It is easy to see that <span class="math display">\[
  \mathbf{E}[|s| ] = \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } 
\]</span> which depends on <span class="math inline">\(m\)</span>. To show the upper bound, <span class="math display">\[
\begin{array}{lll}
    \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } - \log_2 m
    = \sum_{i = 1}^k \frac{2^{d_i} }{m } \log_2 \frac{2^{d_i} }{m } \le 0
\end{array}
\]</span> The inequality follows from <span class="math inline">\(\log_2 \frac{2^{d_i} }{m } \le 0\)</span> for <span class="math inline">\(1 \le i \le k\)</span>.</p>
<p>To show the lower bound, define the function <span class="math display">\[
f(m) = \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } 
\]</span></p>
<p>We prove by induction on <span class="math inline">\(m\)</span> that <span class="math inline">\(f(m) \ge \lfloor \log_2 m \rfloor - 1 = d_k - 1\)</span>. The cases holds trivially when <span class="math inline">\(m = 1\)</span>. Now suppose it holds for all integer less than <span class="math inline">\(m\)</span>. We prove that it holds for <span class="math inline">\(m\)</span>.</p>
<p>Case 1. <span class="math inline">\(m = 2^{d_k}\)</span>. Then <span class="math inline">\(f(m) = d_k = \log_2 m\)</span>.</p>
<p>Case 2. <span class="math inline">\(m &gt; 2^{d_k}\)</span>. Now<br />
<span class="math display">\[
\begin{aligned}
    f(m) &amp;= \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } \\
    &amp;= d_k \frac{2^{d_k} }{m} + \left( \sum_{i = 1}^{k - 1} d_i \frac{2^{d_i} }{m - 2^{d_k}  }  \right) \frac{m - 2^{d_k} }{m} \\
\end{aligned}
\]</span></p>
<p>But <span class="math inline">\(\sum_{i = 1}^{k - 1} 2^{d_i} = m - 2^{d_k}\)</span>. It follows by induction <span class="math display">\[
\sum_{i = 1}^{k - 1} d_i \frac{2^{d_i} }{m - 2^{d_k}  } = f(m - 2^{d_k}) \ge d_{k - 1} - 1
\]</span></p>
<p>Finally, <span class="math display">\[
\begin{aligned}
    f(m)
    &amp;\ge d_k \frac{2^{d_k} }{m} + \left( d_{k - 1} -1 \right) \frac{m - 2^{d_k} }{m} \\
    &amp;= d_k +  \left( d_{k - 1}- d_k -1 \right)  \frac{m - 2^{d_k} }{m} \\
    &amp;= d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( 1 - \frac{ 2^{d_k} }{m} \right)\\
    &amp;\ge  d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( 1 - \frac{ 2^{d_k} }{2^{d_k} + 2^{d_{k - 1} } } \right)\\
    &amp;=  d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( \frac{ 2^{d_{k - 1} } }{2^{d_k} + 2^{d_{k - 1} } } \right)\\
    &amp;=  d_k -  \left( \frac{ d_k  - d_{k - 1} + 1  }{2^{d_k - d_{k - 1} } + 1 } \right)\\     
    &amp;\ge  d_k -  \left( \frac{ d_k  - d_{k - 1} + 1  }{2^{d_k - d_{k - 1} } } \right)\\
    &amp;\ge d_k - 1
\end{aligned}
\]</span></p>
<p>The last inequality holds since <span class="math inline">\(d_{k - 1} \le d_k - 1\)</span> and the function <span class="math inline">\(f(x) = \frac{x + 1}{2^x}\)</span> decreases when <span class="math inline">\(x \ge 1\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h5 id="binary-strings">Binary Strings</h5>
<p>To illustrate the idea of entropy, we study another example of random bit extraction.</p>
<p><strong><em>Problem.</em></strong> <em>Let <span class="math inline">\(t\)</span> be an <span class="math inline">\(n\)</span>-bit binary string each bit of which is interpreted as the outcome of independent bias coin flip that comes up head probability <span class="math inline">\(p \le 0.5\)</span>. Given <span class="math inline">\(t\)</span> as input, we want to output a binary string <span class="math inline">\(s\)</span> (not necessary of length <span class="math inline">\(n\)</span>) of independent unbiased random bits.</em></p>
<p><strong><em>Theorem.</em></strong> There is an extraction algorithm, such that,<br />
1. <span class="math inline">\(\forall \delta \in (0, 1)\)</span>, <span class="math inline">\(\exists N &gt; 0\)</span>, <span class="math inline">\(s.t.\)</span>, <span class="math inline">\(\forall n \ge N\)</span>, <span class="math inline">\(\mathbf{E}[|s|] \ge (1 - \delta) n\mathbf{H}(p)\)</span>.<br />
2. <span class="math inline">\(\forall n &gt; 0\)</span>, <span class="math inline">\(\mathbf{E}[|s|] \le n\mathbf{H}(p)\)</span>.</p>
<p>The theorem states that the expected length of <span class="math inline">\(s\)</span> approximates <span class="math inline">\(n\mathbf{H}(p)\)</span>. Equivalently, <span class="math display">\[
\lim_{n \rightarrow \infty} \frac{\mathbf{E}[|s|]}{n} = \mathbf{H}(p)
\]</span></p>
<p><strong><em>Proof.</em></strong> Define the random variable <span class="math inline">\(Z\)</span> to be the number of ones in <span class="math inline">\(t\)</span>. For any integer <span class="math inline">\(k \ge 0\)</span>, there are <span class="math inline">\(\binom{n}{k}\)</span> strings with <span class="math inline">\(k\)</span> ones. We can map these strings to <span class="math inline">\(\{0, 1, ..., \binom{n}{k} - 1 \}\)</span>. Conditioned on <span class="math inline">\(Z = k\)</span>, the mapping of <span class="math inline">\(t\)</span> is uniform on <span class="math inline">\(\{0, 1, ..., \binom{n}{k} - 1 \}\)</span>. We can use Algorithm 1 to return a binary string <span class="math inline">\(s\)</span>. Now its expected length is <span class="math display">\[
\mathbf{E}[|s|] = \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k]
\]</span></p>
<p>Further, by Hoeffding's inequality, it holds that for <span class="math inline">\(0 &lt; \epsilon &lt; p\)</span>, <span class="math display">\[
\Pr[ |Z - np | \ge n\epsilon] \le \exp\left(- 2 n \epsilon^2 \right)
\]</span></p>
<p>By setting the failure probability <span class="math inline">\(\exp(-2 n \epsilon^2) = \delta / 2\)</span>, we get <span class="math inline">\(\epsilon = \sqrt{\frac{\ln \frac{2}{\delta} }{2n} }\)</span>.</p>
<p>Let <span class="math inline">\(LB = \lceil np - n\epsilon \rceil\)</span> and <span class="math inline">\(UB = \lfloor np + n\epsilon \rfloor\)</span>. Observing that <span class="math inline">\(Z\)</span> is an integer, we have <span class="math display">\[
\begin{array}{rl}  
    \Pr[LB \le Z \le UB] &amp;= \Pr[np - n\epsilon \le Z \le np + n\epsilon] \\
    &amp;\ge 1 - \delta/2
\end{array}
\]</span></p>
<p>As <span class="math inline">\(p \le 0.5\)</span>, it holds that <span class="math display">\[
n = \lceil n/2 - n\epsilon \rceil + \lfloor n/2 + n\epsilon \rfloor \ge \lceil np - n\epsilon \rceil + \lfloor np + n\epsilon \rfloor
\]</span></p>
<p>Therefore, <span class="math display">\[
LB \le UB \le n - LB \\
\]</span></p>
<p>Since <span class="math inline">\(\binom{n}{k}\)</span> increase for <span class="math inline">\(k &lt; n / 2\)</span> and decrease <span class="math inline">\(k &gt; n / 2\)</span>, it holds that <span class="math display">\[
\binom{n}{LB} \le \binom{n}{UB}
\]</span></p>
<p>For <span class="math inline">\(LB \le k \le UB\)</span>, we have <span class="math display">\[
\binom{n}{LB} \le \binom{n}{k} 
\]</span></p>
<p>Define <span class="math inline">\(q = \frac{ LB }{n}\)</span>, <span class="math display">\[
\binom{n}{LB}  \ge \frac{1}{n + 1} 2^{n \mathbf{H}( \frac{ LB }{n} ) } \ge \frac{1}{n + 1} 2^{n\mathbf{H}(p - \epsilon)} 
\]</span></p>
<p>By <strong><em>Theorem 1</em></strong>, we have <span class="math display">\[
\begin{aligned}
    \mathbf{E}[|s|] &amp;= \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
    &amp;\ge \sum_{k = LB}^{UB} \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
    &amp;\ge \sum_{k = LB }^{UB } \left( \left\lfloor \log_2 \binom{n}{k} \right\rfloor - 1 \right) \binom{n}{k} p^k (1 - p)^{n - k}  \\
    &amp;\ge \sum_{k = LB }^{UB } \left( \left\lfloor \log_2  \binom{n}{ LB  } \right\rfloor - 1 \right) \binom{n}{k} p^k (1 - p)^{n - k}  \\
    &amp;=  \left( \left\lfloor \log_2  \binom{n}{ LB  } \right\rfloor - 1 \right) \Pr[LB \le Z \le UB] \\
    &amp;\ge \left( n \mathbf{H}(p - \epsilon)  - \log_2 (n + 1) \right) (1- \delta / 2) \\
    &amp;=  \left( \frac{\mathbf{H}(p - \epsilon)}{\mathbf{H}(p) }  - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) (1- \delta / 2 ) \cdot n \cdot \mathbf{H}(p) \\
    &amp;=  \left( \frac{\mathbf{H} \left(p - \sqrt{\frac{\ln \frac{2}{\delta} }{2n} } \right) }{\mathbf{H}(p) }  - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) (1- \delta / 2 ) \cdot n \cdot \mathbf{H}(p) \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\exists N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n &gt; N\)</span>, <span class="math inline">\(\left( \frac{\mathbf{H} \left(p - \sqrt{\frac{\ln \frac{2}{\delta} }{2n} } \right) }{\mathbf{H}(p) } - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) \ge (1 - \delta / 2)\)</span>, which finishes the proof of lower bound.</p>
<p>As for the upper bound, observe that <span class="math display">\[
\binom{n}{k} p^k ( 1- p)^{n - k} \le 1
\]</span></p>
<p>Therefore, <span class="math inline">\(\binom{n}{k} \le p^{-k} ( 1- p)^{-(n - k) }\)</span>. Thus, we have <span class="math display">\[
\begin{aligned}
       \mathbf{E}[|s|] &amp;= \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
       &amp;\le \sum_{k = 1}^n \left( \log_2 \binom{n}{k} \right) \binom{n}{k} p^k (1 - p)^{n - k} \\
       &amp;\le \sum_{k = 1}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k (1 - p)^{n - k} } 
\end{aligned}
\]</span></p>
<p>The last term is exactly the entropy of the string. By independence of the bits, we know that it is equal to <span class="math inline">\(n \mathbf{H}(p)\)</span>.</p>
<p>Remark: we may also prove it algebraically. <span class="math display">\[
\begin{aligned}
       \mathbf{E}[|s|] 
       &amp;\le \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k (1 - p)^{n - k} } \\
       &amp;= \sum_{k = 1}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k } + \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ (1 - p)^{n - k} } \\
       &amp;= \left( p \log_2 \frac{1}{p} \right) \sum_{k = 1}^n  \binom{n}{k} k p^{k - 1} (1 - p)^{n - k}  \\
       &amp;\quad +  \left( ( 1- p) \log_2 \frac{1}{ 1 - p } \right) \sum_{k = 0}^{n - 1} \binom{n}{k} (n - k) p^k (1 - p)^{n - k - 1} \\
       &amp;= \left( p \log_2 \frac{1}{p} \right) [(x + (1 - p))^n]_{x = p}&#39;  +  \left( (1 - p) \log_2 \frac{1}{ 1 - p } \right) [(p + x)^n]_{x = 1-p}&#39; \\
       &amp;= n \mathbf{H}(p)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="reference.">Reference.</h4>
<p>[1] S. Har-Peled, “Chapter 26 Entropy, Randomness, and Information,” p. 6.<br />
[2] M. Mitzenmacher and E. Upfal, Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/43/">43</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">129</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
