<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/20/Peeling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/20/Peeling/" class="post-title-link" itemprop="url">Peeling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-20 21:41:54" itemprop="dateCreated datePublished" datetime="2020-10-20T21:41:54+11:00">2020-10-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-23 11:24:43" itemprop="dateModified" datetime="2020-10-23T11:24:43+11:00">2020-10-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(Z_1, Z_2, ..., Z_n\)</span> be i.i.d. copies of random variable <span class="math inline">\(Z\)</span> on <span class="math inline">\([0, 1]\)</span>. Let</p>
<ol type="1">
<li><p><span class="math inline">\(\mu_k \doteq \frac{1}{k} \sum_{i = 1}^k Z_i\)</span> for <span class="math inline">\(1 \le k \le n\)</span>,</p></li>
<li><p><span class="math inline">\(\mu = \mathbb{E}[Z]\)</span>,</p></li>
<li><p><span class="math inline">\(M_k\)</span> be the event such that <span class="math display">\[
     M_k \doteq \left\{ \mu_k + \sqrt{ \frac{ \log \frac{1}{\delta} }{2 k} } \le \mu \right\},
 \]</span> where <span class="math inline">\(0 &lt; \delta &lt; 1\)</span>.</p></li>
</ol>
<p>We are interested in bounding the probability of <span class="math display">\[
\Pr[ \cup_{k = 1}^n M_k ].
\]</span></p>
<h3 id="hoeffding-inequality">Hoeffding Inequality</h3>
<p>We first prove a bound of <span class="math display">\[
\Pr[ \cup_{k = 1}^n M_k ] \le n \delta
\]</span> by Hoeffding inequality.</p>
<p><strong><em>Hoeffding Inequality.</em></strong> <em>For a fixed integer <span class="math inline">\(s\)</span>, let <span class="math inline">\(Z_1, Z_2, ..., Z_s\)</span> be i.i.d. copies of a random variable <span class="math inline">\(Z\)</span> on <span class="math inline">\([0, 1]\)</span>. For any <span class="math inline">\(r &gt; 0\)</span></em> <span class="math display">\[
\Pr[ \mu - \mu_s  \ge r] \le \exp(- 2 s r^2 ). \\
\ 
\]</span></p>
<p>Applying Hoeffding inequality with <span class="math inline">\(s = k\)</span>, <span class="math inline">\(r = \sqrt{ \frac{ \log \frac{1}{\delta} }{2 k} }\)</span>, we get <span class="math display">\[
\Pr[ M_k ] \le \delta.
\]</span></p>
<p>By sub-additivity of probability, <span class="math display">\[
\Pr[ \cup_{k = 1}^n M_k ] \le \sum_{k = 1}^n \Pr[M_k] = n \delta.
\]</span></p>
<p>To make the bound meaningful, we require that <span class="math inline">\(\delta \le \frac{1}{n}\)</span>.</p>
<h3 id="beyond-the-naive-union-bound">Beyond The Naive Union Bound</h3>
<p>In this section, we avoid using union bound over <span class="math inline">\(n\)</span> events and improve the <span class="math inline">\(\Pr[ \cup_{k = 1}^n M_k ]\)</span> to <span class="math display">\[
\Pr[ \cup_{k = 1}^n M_k ] \le ( \log_\frac{1}{\alpha} n  + 1 ) \cdot \delta^\alpha,
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is any real number that satisfies <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>.</p>
<p>As a sanity check, suppose <span class="math inline">\(\delta = \frac{1}{n}\)</span> and <span class="math inline">\(\alpha = \frac{1}{2}\)</span>. The two bounds we have are</p>
<ol type="1">
<li><p><span class="math display">\[
 \Pr[ \cup_{k = 1}^n M_k ] \le n \delta = 1, 
 \]</span> which is trivial.</p></li>
<li><p><span class="math display">\[
 \begin{aligned}
     ( \log_\frac{1}{\alpha} n  + 1 ) \cdot \delta^\alpha 
     &amp;= (\log_2 n + 1) \cdot {1 \over \sqrt n} 
 \end{aligned}
 \]</span></p></li>
</ol>
<p>The proof relies on a strengthened version of <em>Hoeffding inequality.</em></p>
<p><strong><em>Fact [1].</em></strong> <em>For a fixed integer <span class="math inline">\(s\)</span>, let <span class="math inline">\(Z_1, Z_2, ..., Z_s\)</span> be i.i.d. copies of a random variable <span class="math inline">\(Z\)</span> on <span class="math inline">\([0, 1]\)</span>. For any <span class="math inline">\(r &gt; 0\)</span></em> <span class="math display">\[
\Pr[ \exists t \le s : \quad t \cdot \mu - t \cdot \mu_t  \ge r] \le \exp(- \frac{2 r^2}{s} ). \\  
\ 
\]</span></p>
<p><em>Proof.</em> We divide the <span class="math inline">\(M_k\)</span>'s into <span class="math inline">\(\lfloor \log_\frac{1}{\alpha} n \rfloor + 1\)</span> groups. For integer <span class="math inline">\(0 \le j \le \lfloor \log_\frac{1}{\alpha} n \rfloor\)</span>, define <span class="math display">\[
\begin{aligned}
    B_j &amp;\doteq \cup_{n \cdot \alpha^{j + 1} &lt; k \le  n \cdot \alpha^j } M_k \\
        &amp;= \left\{ \exists \quad n \cdot \alpha^{j + 1} &lt; k \le  n \cdot \alpha^j, \mu_k + \sqrt{ \frac{ \log \frac{1}{\delta} }{2 k} } \le \mu \right\} \\
        &amp;= \left\{ \exists \quad n \cdot \alpha^{j + 1} &lt; k \le  n \cdot \alpha^j, k \cdot \mu_k + \sqrt{ \frac{  k \log \frac{1}{\delta} }{2} } \le k \cdot \mu \right\} \\
        &amp;\subset \left\{ \exists \quad n \cdot \alpha^{j + 1} &lt; k \le  n \cdot \alpha^j, k \cdot \mu_k + \sqrt{ \frac{  n \cdot \alpha^{j + 1} \log \frac{1}{\delta} }{2} } \le k \cdot \mu \right\} \\
\end{aligned}
\]</span></p>
<p>Applying Fact [1] with <span class="math inline">\(s = \lfloor n \cdot \alpha^j \rfloor &lt; n \cdot \alpha^j\)</span>, <span class="math inline">\(r = \sqrt{ \frac{ n \cdot \alpha^{j + 1} \log \frac{1}{\delta} }{2 } }\)</span>, and by monotonicity of probability, we get <span class="math display">\[
\Pr[B_j] \le \exp( - \frac{  n \cdot \alpha^{j + 1} \log \frac{1}{\delta} }{2} \frac{ 2 }{ n \cdot \alpha^j } ) = \delta^{ \alpha }.
\]</span></p>
<p>Summing over all possible values of <span class="math inline">\(j\)</span>, we get <span class="math display">\[
\Pr[ \cup_{j = 0}^{ \lfloor \log_\frac{1}{\alpha} n \rfloor } B_j] \le  \sum_{j = 0}^{ \lfloor \log_\frac{1}{\alpha} n \rfloor } \Pr[B_j] \le( \log_\frac{1}{\alpha} n  + 1 ) \cdot \delta^\alpha.
\]</span></p>
<p><em>Remark:</em> The first inequality still follows from union bound. However the number of events is now bounded by <span class="math inline">\(\log_\frac{1}{\alpha} n + 1\)</span> instead of <span class="math inline">\(n\)</span>. This comes with a cost of increasing <span class="math inline">\(\delta\)</span> to <span class="math inline">\(\delta^\alpha\)</span>.</p>
<h3 id="reference">Reference</h3>
<p>[1] S. Lattimore, “Bandit Algorithms,”<br />
[2] S. Bubeck, “Bandits Games and Clustering Foundations,”</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/15/Stochastic-Multi-Armed-Bandits/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/15/Stochastic-Multi-Armed-Bandits/" class="post-title-link" itemprop="url">Stochastic Multi-Armed Bandits</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-15 23:26:43" itemprop="dateCreated datePublished" datetime="2020-10-15T23:26:43+11:00">2020-10-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-21 15:15:01" itemprop="dateModified" datetime="2020-10-21T15:15:01+11:00">2020-10-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The multi-arm bandit problem (MAB) models a casino with <span class="math inline">\(k\)</span> machines, where the reward of each machine is controlled by a fixed and unknown distribution. The player (agent) observes independent random rewards from generated from the distribution of a machine each time it pulls its arm. Because of limited budget, the player can only play the machines <span class="math inline">\(T\)</span> times. The goal of the player is to maximize its expected gain.</p>
<p>There is an inherent exploration-exploitation nature of the problem. The player has to explore what is a good machine while simultaneously exploits what seems to be a good machine.</p>
<div style="text-align:center">
<img src="https://jmichaux.github.io/assets/week6/octopus-bandit.jpeg" />
</div>
<h3 id="formulation">Formulation</h3>
<p>We study the restricted case where the reward distribution of each machine is bounded on <span class="math inline">\([0, 1]\)</span>. Define</p>
<ol type="1">
<li><span class="math inline">\(D_i:\)</span> the reward distribution on <span class="math inline">\([0, 1]\)</span> of the <span class="math inline">\(i\)</span>-th machine, where <span class="math inline">\(i \in [k]\)</span>,</li>
<li><span class="math inline">\(\mu_i:\)</span> the expected reward of the <span class="math inline">\(i\)</span>-th machine, where <span class="math inline">\(i \in [k]\)</span>,</li>
<li><span class="math inline">\(\mu^* = \max_{i \in [k]} \mu_i\)</span>,</li>
<li><span class="math inline">\(i^*:\)</span> the index of the machine with highest expected reward,</li>
<li><span class="math inline">\(\Delta_i = \mu^* - \mu_i\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(\vec Y_i\)</span> be the random reward vector whose elements are i.i.d. random variables from <span class="math inline">\(D_i\)</span>:</p>
<ol start="6" type="1">
<li><span class="math inline">\(\vec Y_i = (Y_{i, 1}, Y_{i, 2}, ..., Y_{i, T})\)</span></li>
</ol>
<p>Denote <span class="math inline">\(Y\)</span> be the matrix of the random variables</p>
<ol start="7" type="1">
<li><span class="math display">\[
 Y = \begin{bmatrix}
     \vec Y_1 \\
     \vec Y_2 \\
     ... \\
     \vec Y_k
 \end{bmatrix}
\]</span></li>
</ol>
<p>Each time the player pulls the arm, it observes one random variable from the matrix. Specially, let</p>
<ol start="8" type="1">
<li><span class="math inline">\(I_t:\)</span> the machine played at the <span class="math inline">\(t\)</span>-th trial,</li>
<li><span class="math inline">\(N_i(t):\)</span> the number of times the machine <span class="math inline">\(i\)</span> is played up to trial <span class="math inline">\(t\)</span>.</li>
</ol>
<p>Before trial <span class="math inline">\(t\)</span>, the number of random variables observed in <span class="math inline">\(\vec Y_{I_t}\)</span> is <span class="math inline">\(N_{I_t} (t - 1)\)</span>. At trial <span class="math inline">\(t\)</span>, a new random variable in <span class="math inline">\(\vec Y_{I_t}\)</span> is observed, and <span class="math inline">\(N_{I_t} (t) = N_{I_t} (t - 1) + 1\)</span>. Hence, the newly observed random variable is <span class="math inline">\(Y_{I_t, N_{I_t} (t)}\)</span>. For convenience, we define</p>
<ol start="11" type="1">
<li><span class="math inline">\(X_{t} \doteq Y_{I_t, N_{I_t} (t)}:\)</span> the rewards obtained at the <span class="math inline">\(t\)</span>-th trial.</li>
</ol>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/StochasticMultiArmBandit.png" width="500" height="250" /></p>
</div>
<p>When the player chooses a machine <span class="math inline">\(I_t\)</span> on trial <span class="math inline">\(t\)</span>, it is based on the history of his play, which consists of his past decisions, and the corresponding observed rewards:</p>
<ol type="1">
<li><span class="math inline">\(H_t: \{ (I_1, X_1), (I_2, X_2), ..., (I_{t -1}, X_{t - 1}) \}\)</span> the history up to trial <span class="math inline">\(t\)</span>.</li>
</ol>
<p>The strategy the player uses at trial <span class="math inline">\(t\)</span> is considered as a functions from <span class="math inline">\(H_t\)</span> to <span class="math inline">\([k]\)</span>:</p>
<ol start="11" type="1">
<li><span class="math inline">\(\phi_t: H_t \rightarrow [k]\)</span> a function that chooses a machine based on the history at trial <span class="math inline">\(t\)</span></li>
</ol>
<p>We define the policy as the strategies the player uses over all trials:</p>
<ol start="12" type="1">
<li><span class="math inline">\(\phi = (\phi_1, \phi_2, ..., \phi_T):\)</span> policy, the strategy for the pulling arms by the player</li>
</ol>
<p>The goal is to find a policy that maximizes the expected rewards, or equivalently, the one that minimizes the expected regret <span class="math display">\[ \small
\mathbb{R}(\phi) = T \cdot \mu^* - \mathbb{E}[\sum_{t = 1}^T X_t ]
\]</span></p>
<p>By definition of <span class="math inline">\(N_i(t)\)</span>'s, we have <span class="math inline">\(\sum_{i = 1}^k N_i(T) = T\)</span>. Then the regret can be written as <span class="math display">\[
\mathbb{R}(\phi) = \mathbb{E}[\sum_{i = 1}^k N_i(T)] \cdot \mu^* - \sum_{i = 1}^k \mathbb{E}[N_i(T) ] \cdot \mu_i = \sum_{i : \Delta_i &gt; 0} \mathbb{E}[N_i(T) ] \cdot \Delta_i
\]</span></p>
<h3 id="the-upper-confidence-bound-ucb-algorithm">The Upper Confidence Bound (UCB) Algorithm</h3>
<p>When making decision at trial <span class="math inline">\(t\)</span>, a good indicator of a machine's performance is its empirical reward: <span class="math display">\[ 
\hat \mu_{i, N_i(t - 1)} \doteq  \sum_{\tau = 1}^{N_i(t - 1) } \frac{1}{ N_i(t - 1) } Y_{i, \tau}
\]</span></p>
<p>But this might not reflect the true value of its expectation <span class="math inline">\(\mu_i\)</span> because of sampling variation. The UCB algorithm takes the variation into consideration.</p>
<p><strong><em>Fact.</em></strong> <em>Hoeffding Inequality.</em> <em>Let <span class="math inline">\(Z_1, Z_2, ..., Z_s\)</span> be i.i.d. copies of random variable <span class="math inline">\(Z\)</span> on <span class="math inline">\([a, b]\)</span>. Then</em> <span class="math display">\[
\Pr[ \frac{1}{s} \sum_{\tau = 1}^s Z_\tau - E[Z] \ge r] \le \exp(- \frac{2 s r^2}{(b - a)^2 } )
\]</span> <em>where r &gt; 0</em>.</p>
<p>Now, consider a <strong>fixed</strong> <span class="math inline">\(s\)</span> (<span class="math inline">\(1 \le s \le T\)</span>) and a failure probability <span class="math inline">\(\delta &gt; 0\)</span>. Applying Hoeffding inequality with <span class="math inline">\(Z_1 = Y_{i, 1}, Z_2 = Y_{i, 2}, ..., Z_s = Y_{i, s}\)</span>, <span class="math inline">\(a = 0, b = 1\)</span> and <span class="math inline">\(r = \sqrt { \log \frac{1}{\delta} \over 2 s }\)</span>, we get <span class="math display">\[ \small
\Pr \left[ \hat \mu_{i, s} - \mu_i \ge \sqrt { \log \frac{1}{\delta} \over 2  s } \right] \le \delta
\]</span></p>
<p>Similarly, by applying Hoeffding inequality with <span class="math inline">\(Z_1 = -Y_{i, 1}, Z_2 = -Y_{i, 2}, ..., Z_s = -Y_{i, s}\)</span>, <span class="math inline">\(a = -1, b = 0\)</span> and <span class="math inline">\(r = \sqrt { \log \frac{1}{\delta} \over 2 s }\)</span>, we have <span class="math display">\[ \small
\Pr \left[ \mu_i - \hat \mu_{i, s} \ge \sqrt { \log \frac{1}{\delta} \over 2  s } \right] \le \delta
\]</span></p>
<p>Inspired by this, the UCB defines the upper and lower bounds for the expected reward of the <span class="math inline">\(i\)</span>-th machine as follows: <span class="math display">\[ \small
UB_i ( t ) \doteq \hat \mu_{i, N_i(t - 1)} + \sqrt { \log t^\alpha \over 2  N_i(t - 1) } \\
LB_i ( t ) \doteq \hat \mu_{i, N_i(t - 1)} - \sqrt { \log t^\alpha \over 2  N_i(t - 1) } \\
\]</span> where <span class="math inline">\(\alpha &gt; 0\)</span> is a parameter to be set. By convention here that if <span class="math inline">\(N_i(t - 1) = 0\)</span>, <span class="math inline">\(\sqrt { \log t^\alpha \over 2 N_i(t - 1) } = \infty\)</span>.</p>
<p>The UCB algorithms chooses just the machine with highest upper bound. The lower bound is needed for its analysis.</p>
<blockquote>
<p><strong><em>UCB algorithm</em></strong><br />
1. For <span class="math inline">\(t \leftarrow 1\)</span> to <span class="math inline">\(T\)</span> do<br />
2. <span class="math inline">\(\qquad\)</span> Choose <span class="math inline">\(I_t = \arg \max_{i \in [k] } UB_i ( t )\)</span><br />
3. <span class="math inline">\(\qquad\)</span> Observe a reward <span class="math inline">\(X_t\)</span></p>
</blockquote>
<p>Intuitively, there are two cases when <span class="math inline">\(UB_i ( t )\)</span> is high:</p>
<ol type="1">
<li>The value <span class="math inline">\(\mu_i\)</span> is high;</li>
<li>The <span class="math inline">\(i\)</span>-th machine is under-explored.</li>
</ol>
<p><strong><em>Caveat:</em> <em>The reader might start to think that at trial <span class="math inline">\(t\)</span>, it holds that <span class="math inline">\(\Pr[\mu_i \ge UB_i(t) ]\)</span> (or <span class="math inline">\(\Pr[\mu_i \le LB_i(t) ]\)</span>) is bounded by <span class="math inline">\(\frac{1}{t^\alpha}\)</span>, according to Hoeffding inequality. It is not. Hoeffding inequality requires the number of samples to be a fixed number. On the other hand, <span class="math inline">\(N_i(t - 1)\)</span> is a random variable that depends on the history of the game. We will discuss this later.</em></strong></p>
<p>The following theorem states that the expected regret of UCB algorithm grows logarithmically with <span class="math inline">\(T\)</span>.</p>
<p><strong><em>Theorem.</em></strong> <em>If <span class="math inline">\(\alpha &gt; 2\)</span>, the regret of the UCB algorithm is bounded by</em> <span class="math display">\[ \small
\mathbb{R}(\phi) \le  \log T \cdot ( \sum_{i : \Delta_i &gt; 0 } \frac{2 \alpha }{\Delta_i } ) + \frac{\alpha}{ \alpha - 2 } \sum_{i : \Delta_i &gt; 0 } \Delta_i
\]</span></p>
<p><em>Proof:</em></p>
<p>We will show that for <span class="math inline">\(i : \Delta_i &gt; 0\)</span>, <span class="math display">\[ \small
\mathbb{E} [n_{i, T}] \le \frac{ \alpha \log T }{ \Delta_i^2} + \frac{\alpha}{\alpha - 2} 
\]</span></p>
<p>The key for the proof relies on a sufficient condition that the <span class="math inline">\(i\)</span>-th machine will not be chosen at trial <span class="math inline">\(t\)</span>:</p>
<blockquote>
<p>if the following events happen at trial <span class="math inline">\(t\)</span>, machine <span class="math inline">\(i\)</span> will not be chosen: 1. <span class="math inline">\(A_t: 2 \sqrt { \alpha \log t \over 2 N_i(t - 1) } \le \Delta_i\)</span><br />
2. <span class="math inline">\(B_t: UB_{ i^* } ( t ) &gt; \mu^*\)</span><br />
3. <span class="math inline">\(C_t: \mu_i &gt; LB_i ( t )\)</span></p>
</blockquote>
<p>This condition essentially prevents <span class="math inline">\(UB_i(t)\)</span> from exceeding <span class="math inline">\(\mu^*\)</span> while assures <span class="math inline">\(UB_{i^*}(t)\)</span> is higher than <span class="math inline">\(\mu^*\)</span>.</p>
<p><span class="math display">\[ \small
UB_i ( t ) = LB_i ( t ) + 2 \sqrt { \alpha \log t \over 2  N_i(t - 1) } &lt; \mu_i + \Delta_i = \mu^* &lt; UB_{ i^* } ( t )
\]</span></p>
<p>Let <span class="math inline">\(\beta = \lceil \frac{2 \alpha \log T}{\Delta_i^2} \rceil\)</span>. Note that <span class="math inline">\(A_t = \{ N_i(t - 1) \ge \frac{2 \alpha \log t}{\Delta_i^2} \} \subset \{ N_i(t - 1) \ge \beta \}\)</span>. Now, <span class="math display">\[
\{I_t = i \} \subset \overline{ A_t \cap B_t \cap C_t } = \bar A_t \cup \bar B_t \cup \bar C_t
\]</span></p>
<p>and <span class="math display">\[
\begin{array}{ll} 
    \mathbb{E}[n_{i, T}]    &amp;=  \mathbb{E}[ \sum_{t = 1}^T \mathfrak{1} \{I_t = i\} ]  \\
                            &amp;=  \mathbb{E} [ \sum_{t = 1}^T \mathfrak{1} \{I_t = i, \bar A_t \}+ \sum_{t = 1}^T \mathfrak{1} \{I_t = i, A_t \} ]  \\
                            &amp;\le  \mathbb{E} [ \sum_{t = 1}^T \mathfrak{1} \{I_t = i, \  N_i(t) &lt; \beta \} ] + \sum_{t = 1}^T \mathbb{E} [\mathfrak{1} \{I_t = i, A_t \} ]  \\
                            &amp;\le \beta + \sum_{t = \beta + 1}^T \Pr[ \{I_t = i\} \cap A_t ] \\
                            &amp;\le \beta + \sum_{t = \beta + 1}^T ( \Pr[ \bar B_t \cap A_t ] + \Pr[ \bar C_t \cap A_t ] ) \\
                            &amp;\le \beta + \sum_{t = \beta + 1}^T ( \Pr[ \bar B_t ] + \Pr[ \bar C_t ] ) \\
\end{array}
\]</span></p>
<p>It is non-trivial to bound <span class="math inline">\(\Pr[\bar B_t]\)</span>. For a fixed <span class="math inline">\(1 \le s \le t - 1\)</span>, define the event <span class="math display">\[
M_s : \frac{1}{s} \sum_{\tau = 1}^s Y_{i^*, \tau} + \sqrt{ \frac{\alpha \log t}{2 s} } \le \mu^*
\]</span></p>
<p>Since <span class="math inline">\(s\)</span> is fixed, by Hoeffding inequality, <span class="math inline">\(\Pr[M_s] \le \frac{1}{t^\alpha}\)</span>. Then <span class="math display">\[ \small
\begin{aligned}
    \bar B_t = \left\{ \mu_{ {i^*}, N_{i^*} (t - 1)} + \sqrt { \alpha \log t \over 2  N_{i^*} (t - 1) }  \le \mu^* \right\} \subset \cup_{s = 1}^{t - 1} M_s
\end{aligned}
\]</span></p>
<p>and <span class="math display">\[
\Pr[ \bar B_t ] \le \Pr[ \cup_{s = 1}^{t - 1} M_s ] \le \sum_{s = 1}^{t - 1} \Pr[M_s] &lt; t^{\alpha - 1}
\]</span></p>
<p>and <span class="math display">\[
\sum_{t = \beta + 1}^T \Pr[ \bar B_t ] &lt; \sum_{t = \beta + 1}^T t^{\alpha - 1} &lt; \int_{t = 1}^\infty \frac{1}{ x^{\alpha - 1} } dx  = \frac{1}{\alpha - 2} x^{-(\alpha - 2)} \mid_{t = 1}^\infty  = \frac{1}{\alpha - 2}
\]</span></p>
<p>Similarly, we have <span class="math display">\[ 
\sum_{t = \beta + 1}^T \Pr[ \bar C_t ] &lt; \frac{1}{\alpha - 2}
\]</span></p>
<p>Finally, <span class="math display">\[
\begin{aligned} \small
    \mathbb{E} [n_{i, T}] 
                &amp;\le \beta + \frac{2}{\alpha - 2} \\
                &amp;\le \frac{2 \alpha \log T}{\Delta_i^2} + 1 + \frac{2}{\alpha - 2} \\
                &amp;= \frac{2 \alpha \log T}{\Delta_i^2} + \frac{\alpha }{\alpha - 2} 
\end{aligned}
\]</span></p>
<p><strong><em>Remark:</em></strong> <em>Can we bound <span class="math inline">\(\Pr[ \bar B_t ]\)</span> as follows:</em> <span class="math display">\[
\begin{array}{ll} 
    \Pr[\bar B_t] &amp;= \sum_{\tau = 1}^{t - 1} \Pr[UB_{ i^* } ( t ) &lt; \mu^* \mid n_{i^*, t} = \tau] \cdot \Pr[n_{i^*, t} = \tau]\\
    &amp;= \sum_{\tau = 1}^{t - 1} \frac{1}{t^\alpha} \cdot \Pr[n_{i^*, t} = \tau] \\
    &amp;= \frac{1}{t^\alpha} 
\end{array}
\]</span></p>
<p>When we know <span class="math inline">\(n_{i^*, t} = \tau\)</span> for some <span class="math inline">\(\tau\)</span>, the <span class="math inline">\(n_{i^*, t}\)</span> rewards from machine <span class="math inline">\(i^*\)</span> can't be viewed as independent samples. E.g., if <span class="math inline">\(t\)</span> is large, the best machines has much larger expected reward than others, and <span class="math inline">\(n_{i^*, t}\)</span> is very small compared to <span class="math inline">\(t\)</span>. Then it is likely machine <span class="math inline">\(i^*\)</span> generates bad rewards in the history. The reward sequence of <span class="math inline">\(i^*\)</span> can't be viewed as a sequence of independent samples from distribution <span class="math inline">\(D_{i^*}\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h2 id="reference">Reference</h2>
<p>[1] D. Katselis, B. Miranda, and H. Hu, “Lecture 8: Multi-Armed Bandits”, ECE586 MDPs and Reinforcement Learning, Spring 2019, University of Illinois at Urbana-Champaign.<br />
[2] S. Bubeck and N. Cesa-Bianchi, “Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems,” arXiv:1204.5721 [cs, stat], Nov. 2012, Accessed: Oct. 19, 2020.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/21/Median-of-Mean/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/21/Median-of-Mean/" class="post-title-link" itemprop="url">Median-of-Mean</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-06-21 21:23:19 / Modified: 21:54:42" itemprop="dateCreated datePublished" datetime="2020-06-21T21:23:19+10:00">2020-06-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose that we have an algorithm <span class="math inline">\(A\)</span> that estimates the mean <span class="math inline">\(E[Y]\)</span> of a random variable <span class="math inline">\(Y\)</span> within some specified range <span class="math inline">\(r\)</span> with probability <span class="math inline">\(\rho &gt; \frac{1}{2}\)</span>. We can derive a new algorithm <span class="math inline">\(A^*\)</span> from <span class="math inline">\(A\)</span> that boosts the successful probability to <span class="math inline">\(1 - \delta\)</span>, as follows:</p>
<blockquote>
<ol type="1">
<li>Repeat <span class="math inline">\(A\)</span> for <span class="math inline">\(m = \frac{1}{2 (\rho - 0.5)^2 } \ln \frac{1}{\delta}\)</span> times.</li>
<li>Take the median of the <span class="math inline">\(m\)</span> outputs.</li>
</ol>
</blockquote>
<p>To prove it, define the indicator random variable <span class="math display">\[
X_i = \begin{cases}
    1, \text{ if the } i \text{-th output of } A \text{ is within the range } r \\
    0, \text{otherwise}
\end{cases}
\]</span></p>
<p>for <span class="math inline">\(1 \le i \le m\)</span>. Then <span class="math inline">\(E[X_i] = \rho &gt; 0.5\)</span>. Let <span class="math inline">\(\mu = E \left[\sum_{i = 1}^m X_i \right] = m\rho\)</span>.</p>
<p>By Hoeffding's inequality, for $&gt; 0 $, <span class="math display">\[
\Pr \left[ \left| \sum_{i = 1}^m X_i - \mu \right| \ge \lambda \right] \le \exp \left(- \frac{2\lambda^2}{m} \right)
\]</span></p>
<p>Replacing <span class="math inline">\(\lambda\)</span> with <span class="math inline">\(\mu - 0.5m\)</span>, and <span class="math inline">\(m\)</span> with <span class="math inline">\(m = \frac{1}{2 (\rho - 0.5)^2 }\ln \frac{1}{\delta}\)</span>, we get <span class="math display">\[
\begin{aligned}
    \Pr \left[ \sum_{i = 1}^m X_i \le 0.5m \right] &amp;\le \Pr \left[ \left| \sum_{i = 1}^m X_i - \mu \right| \ge m(\rho - 0.5) \right] \\
    &amp;\le \exp \left(- 2m(\rho - 0.5)^2 \right) \\
    &amp;= \delta   
\end{aligned}
\]</span></p>
<p>Note that if more than half of <span class="math inline">\(A\)</span> outputs are correct, then the output of <span class="math inline">\(A^*\)</span> is correct. This happens with probability at least <span class="math inline">\(1 - \delta\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/18/Entropy-and-Message-Transmission/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/Entropy-and-Message-Transmission/" class="post-title-link" itemprop="url">Entropy and Message Transmission</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-18 20:26:03" itemprop="dateCreated datePublished" datetime="2020-06-18T20:26:03+10:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-01 11:06:27" itemprop="dateModified" datetime="2020-07-01T11:06:27+10:00">2020-07-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss another application that sheds light on meaning of entropy, the message transmission problem. In this model, a sender transmits a message to the receiver through a channel. In real world, the channel could be an optical fiber, a wireless channel, a hard disk etc. In the final application, the computer that writes and reads information from the hard disk is both the sender and the receiver.</p>
<p>In real world the channel is not perfect and the message transmitted is distorted by noise. It is natural to ask whether the message can be transmitted accurately under the noise, i.e., whether the receiver can recover the noisy message.</p>
<p>To study the problem, we need a mathematical model for both the channel and the noise. We focus on the simplest binary channel with bits of 0 and 1. The noise causes the bits to flip and is modelled by the distribution on the bit-flips. We study the simplest one that flips each bit independently with some identical probability <span class="math inline">\(p &lt; 0.5\)</span>. For a message with length <span class="math inline">\(n\)</span>, the number of bit flips follows a Binomial distribution <span class="math inline">\(B(n, p)\)</span>. We call such channel a <em>binary symmetric channel</em>.</p>
<p>To protect the message against noise, a naïve way is to send the each bit multiple times. For example, if the sender wants to send a bit 1, it sends <span class="math inline">\(10\)</span> copies of <span class="math inline">\(1\)</span> as <span class="math inline">\(1111111111\)</span>. The receiver decides that the bit sent is 1, if the majority of the <span class="math inline">\(10\)</span> bits it receives is 1.</p>
<p>Could it be improved? Repeating each bit may not be the mot efficient way against the noise. In general, if the sender wants to send a message of <span class="math inline">\(k\)</span> bits, it can convert it into a new message of <span class="math inline">\(n\)</span> bits and sends the new one. The receiver considers the <span class="math inline">\(n\)</span> bits received as a whole, and try to recover the <span class="math inline">\(k\)</span>-bit message the sender wants to send.</p>
<p>We call the method used by the sender to convert the original message the <em>encoding function</em>, and the one used by the receiver to recover the message the <em>decoding function.</em></p>
<p><strong><em>Definition.</em></strong> A <span class="math inline">\((k, n)\)</span> encoding function <span class="math inline">\(\text{Enc}: \{0, 1\}^k \rightarrow \{0, 1\}^n\)</span> and a <span class="math inline">\((k, n)\)</span> decoding function <span class="math inline">\(\text{Dec}: \{0, 1\}^n \rightarrow \{0, 1\}^k\)</span>, where <span class="math inline">\(n \ge k\)</span>.</p>
<p>Since the noise flips the bits randomly, it is impossible to have an encoding function and a decoding function without error (unless <span class="math inline">\(p = 0\)</span>). Instead, we aim to control the probability of error within some specified threshold <span class="math inline">\(\delta\)</span>. To achieve this, we add redundancy to the message and encode a <span class="math inline">\(k\)</span>-bit one into an <span class="math inline">\(n\)</span>-bit one. Now, <span class="math inline">\(n - k\)</span> is the amount of redundancy introduced. We would like to make <span class="math inline">\(n - k\)</span> as small as possible. On the other hand, the value of <span class="math inline">\(n-k\)</span> should positively related to <span class="math inline">\(p\)</span>. The larger <span class="math inline">\(p\)</span> is, the noisier the channel is and the larger <span class="math inline">\(n - k\)</span> should be.</p>
<p>For fixed <span class="math inline">\(\delta\)</span> and <span class="math inline">\(p\)</span>, the <em>Shannon's Theorem</em> says that the smallest possible value of <span class="math inline">\(n - k\)</span> we can achieve is roughly <span class="math inline">\(n\mathbf{H}(p)\)</span>. Intuitively, <span class="math inline">\(\mathbf{H}(p)\)</span> is the amount of randomness the noise exerts on each bit. Therefore <span class="math inline">\(1 - \mathbf{H}(p)\)</span> is the maximum amount of information we can transmit by one bit.</p>
<p>Before we proceed, we prove the following lemma.</p>
<p><strong><em>Lemma 1</em></strong><br />
For <span class="math inline">\(q \in \mathbb{R}, q \le 1 / 2\)</span> and <span class="math inline">\(n \in \mathbb{N}\)</span>, it hold that <span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor q n \rfloor} \binom{n}{i} 
    &amp;\le 2^{ n \mathbf{H} ( q)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p><strong><em>Proof.</em></strong><br />
<span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor q n \rfloor} \binom{n}{i} 
    &amp;\le \frac{n + 1}{2} \binom{n }{\lfloor q n \rfloor }  \\
    &amp;\le \frac{n + 1}{2} 2^{ n \mathbf{H} \left( { \lfloor q n \rfloor } / { n } \right) } \\
    &amp;\le 2^{ n \mathbf{H} ( q)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p>The first inequality follows from that <span class="math inline">\(\binom{n}{i}\)</span> is increasing for <span class="math inline">\(i \le n / 2\)</span> and that the summation is over at most <span class="math inline">\((n + 1) /2\)</span> terms. The last one follows from that <span class="math inline">\(\mathbf{H}(x)\)</span> is increasing for <span class="math inline">\(x \le 1 / 2\)</span> and that <span class="math inline">\({ \lfloor q n \rfloor } / { n } \le { q n } / { n } = q &lt; 1/2\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Shannon's Theorem</em></strong><br />
Given a binary symmetric channel with flip probability <span class="math inline">\(p &lt; 1 / 2\)</span> and for any any <span class="math inline">\(\epsilon, \delta &gt; 0\)</span>, when <span class="math inline">\(n\)</span> is large enough,</p>
<ol type="1">
<li>if <span class="math inline">\(k \le n (1 - \mathbf{H}(p) - \epsilon)\)</span>, there exist <span class="math inline">\((k, n)\)</span> encoding and decoding functions such that the receiver fails to obtain the correct message with probability at most <span class="math inline">\(2\delta\)</span>.<br />
</li>
<li><span class="math inline">\(\nexists (k, n)\)</span> encoding and decoding functions with <span class="math inline">\(k \ge n (1 - \mathbf{H}(p) + \epsilon)\)</span> such that the decoding is correctly is at most <span class="math inline">\(\delta\)</span> for a <span class="math inline">\(k\)</span>-bit input message chosen uniformly at random.</li>
</ol>
<p><strong><em>Proof of 1.</em></strong></p>
<p><strong>Intuition.</strong> When an <span class="math inline">\(n\)</span>-bit message <span class="math inline">\(s\)</span> is transmitted through the channel, the number of flipped bits is roughly <span class="math inline">\(np\)</span>. As <span class="math inline">\(p &lt; {1} / {2}\)</span>, we can find a <span class="math inline">\(\lambda &gt; 0\)</span> such that <span class="math inline">\(p + \lambda &lt; {1} / {2}\)</span>. Let <span class="math inline">\(R\)</span> be the received <span class="math inline">\(n\)</span>-bit message and define <span class="math inline">\(d(s, R)\)</span> the number of different bits between <span class="math inline">\(s\)</span> and <span class="math inline">\(R\)</span>, i.e., the Hamming distance between <span class="math inline">\(s\)</span> and <span class="math inline">\(R\)</span>. Given <span class="math inline">\(s\)</span> and <span class="math inline">\(p\)</span>, let <span class="math inline">\(\mathfrak{D}(s, p)\)</span> be the distribution of <span class="math inline">\(R\)</span> on <span class="math inline">\(\{0, 1\}^n\)</span>.</p>
<p>By Hoeffding's inequality, it holds that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s, p) } [ |d(s, R) - pn | \ge \lambda n] \le \exp(- 2 \lambda^2 n).
\]</span></p>
<p>Define the <span class="math inline">\((p + \lambda)n\)</span> ball centered <span class="math inline">\(s\)</span> as <span class="math display">\[
B_{s, (p + \lambda)n} \doteq \{ r \in \{0, 1\}^n : d(s, r) &lt; (p + \lambda)n \}.
\]</span></p>
<p>The the Hoeffding's inequality implies that for large enough <span class="math inline">\(n\)</span>, we have <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s, p) } [ R \notin B_{s, (p + \lambda)n}] \le \delta.
\]</span></p>
<p>That is, with probability at most <span class="math inline">\(\delta\)</span>, the received message fall outside the ball <span class="math inline">\(B_{s, (p + \lambda)n}\)</span>. This motivates to decode each message in <span class="math inline">\(B_{s, (p + \lambda)n}\)</span> as the original message the sender wants to send.</p>
<p>Further, by Lemma 1, the size of <span class="math inline">\(B_{s, (p + \lambda) n}\)</span> is bounded by <span class="math display">\[
\begin{aligned}
    \sum_{i = 0}^{\lfloor (p + \lambda) n \rfloor} \binom{n}{i} 
    &amp;\le 2^{ n \mathbf{H} ( p + \lambda)  + \log \frac{n + 1}{2} }
\end{aligned}
\]</span></p>
<p>Therefore, the maximum number of non-overlapped balls we can pack into the message space <span class="math inline">\(\{0, 1\}^n\)</span> is<br />
<span class="math display">\[
\frac{2^n}{ 2^{ n \mathbf{H} ( p + \lambda)  + \log \frac{n + 1}{2} }  } = 2^{ n (1 - \mathbf{H} ( p + \lambda) - \frac{1}{n} \log \frac{n + 1}{2})   }
\]</span></p>
<p>When <span class="math inline">\(n\)</span> is large enough, this is at least <span class="math inline">\(2^{ n (1 - \mathbf{H} ( p ) - \epsilon) }\)</span>.</p>
<p><em>Remark: Hoeffding's inequality is an overkill. Chebyshev's inequality suffices for this proof.</em></p>
<p><strong>Designing Encoding and Decoding Functions.</strong></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Balls.png" /></p>
<p>This motivates one to design of encoding and decoding function as follows: find a set of messages <span class="math inline">\(s_1, s_2, ..., s_{2^k} \in \{0, 1\}^n\)</span>, such that <span class="math display">\[
B_{s_1, (p + \lambda)n}, B_{s_2, (p + \lambda)n}, ..., B_{s_{2^k} , (p + \lambda)n}
\]</span></p>
<p>are disjoint balls, and assign the messages in <span class="math inline">\(\{0, 1\}^k\)</span> to the ones in <span class="math inline">\(\{ s_i \}&#39;s\)</span> one by one. If the sending want to send <span class="math inline">\(i\)</span>, it sends <span class="math inline">\(s_i\)</span>. On receiving a message <span class="math inline">\(r\)</span>, the receiver determines which ball <span class="math inline">\(r\)</span> belongs to. The probability of decoding error is at most <span class="math inline">\(\delta\)</span>.</p>
<p><em>Question to ponder: can we find such a set efficiently, such that</em><br />
<span class="math display">\[
B_{s_1, (p + \lambda)n}, B_{s_2, (p + \lambda)n}, ..., B_{s_{2^k} , (p + \lambda)n}
\]</span> <em>are disjoint?</em></p>
<p><strong>Finding the Codewords.</strong></p>
<p>It is left to find a set of satisfactory <span class="math inline">\(s_1, s_2, ..., s_{2^k} \in \{0, 1\}^n\)</span>. The method we show here does not find a set of non-overlapped balls. Instead it aims to find a set such that</p>
<blockquote>
<p>if <span class="math inline">\(s_i\)</span> is transmitted, then the probability received message <span class="math inline">\(r\)</span> falls into the another ball, i.e., <span class="math inline">\(r \in B_{s_{j} , (p + \lambda)n}\)</span> (<span class="math inline">\(j \neq i\)</span>), is less than <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The property implies that the overlap between <span class="math inline">\(B_{s_{i} , (p + \lambda)n}\)</span> and <span class="math inline">\(\cup_{j \neq i} B_{s_{j} , (p + \lambda)n}\)</span> is less than <span class="math inline">\(\delta\)</span> (measured by probability). To formalize the statement, define the random variable <span class="math inline">\(S\)</span> to be the message sent and <span class="math inline">\(R\)</span> to be the one received. Given that <span class="math inline">\(S = s_i\)</span> is sent, the conditional probability of receiving <span class="math inline">\(R = r\)</span> is <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] = p^{d(s_i,r)} (1 - p)^{1 - d(s_i, r)}
\]</span></p>
<p>To goal is to prove that it holds for all <span class="math inline">\(i\)</span>, <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] = \sum_{r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n} } p_r \le \delta
\]</span></p>
<p>Then , by union bound, it holds that <span class="math display">\[
\begin{aligned} 
\Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \text{ is not recovered correctly} \mid S = s_i] 
    &amp;= \Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \notin B_{s_i, (p + \lambda)n}  \vee  R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  \mid S = s_i] \\
    &amp;\le \Pr_{R \sim \mathfrak{D}(s_i, p) }  [ R \notin B_{s_i, (p + \lambda)n}   \mid S = s_i]  + \Pr[ R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  \mid S = s_i] \\
    &amp;\le 2 \delta
\end{aligned}
\]</span> That is, <span class="math inline">\(s_i\)</span> is transmitted and decoded correctly with probability at least <span class="math inline">\(1 - 2 \delta\)</span>.</p>
<p>In what follows, we will prove the following claim:</p>
<p><strong>Claim 1</strong>. <em>If we generate <span class="math inline">\(2^{k + 1}\)</span> codewords uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>, then in expectation</em> <span class="math display">\[
\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} }  \left[ \sum_{i = 1}^{2^{k + 1} }  \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \le  2^k \delta
\]</span></p>
<p>Claim 1 implies that there exits a set of codewords <span class="math inline">\(s_1, s_2, ..., s_{2^{k + 1} }\)</span> with <span class="math display">\[
\sum_{i = 1}^{2^{k + 1} } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]    \le 2^k \delta
\]</span></p>
<p>If we send each <span class="math inline">\(s_i\)</span> with probability <span class="math inline">\(1 / 2^k\)</span>, then the expected error probability is already <span class="math inline">\(\delta\)</span>. But we can have a stronger result: we can find a set of <span class="math inline">\(2^k\)</span> codewords, such that for each <span class="math inline">\(i\)</span>, <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \le 2 \delta
\]</span></p>
<p>W.L.O.G., suppose that <span class="math inline">\(s_1, s_2, ..., s_{2^k}\)</span> are the ones with the smallest <span class="math inline">\(\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]\)</span> 's, it must be that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i]  \le \frac{2^k \delta }{2^k } = \delta
\]</span></p>
<p><strong><em>Proof of Claim 1</em></strong>. By Lemma 1, for a fixed <span class="math inline">\(r\)</span>, the number of strings <span class="math inline">\(s \in \{0, 1\}^n\)</span> such that <span class="math inline">\(d(s, r) &lt; (p + \lambda) n\)</span> is bounded by <span class="math display">\[
\begin{aligned}
    \sum_{k = 0}^{\lfloor (p + \lambda) n \rfloor} \binom{n}{k} 
        &amp;\le \frac{n + 1}{2} 2^{ n \mathbf{H} ( p + \lambda) }
\end{aligned}
\]</span></p>
<p>If a message <span class="math inline">\(s\)</span> is picked uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>, the probability such that <span class="math display">\[
\Pr_{s \sim \mathbf{U}(\{0, 1\}^n) } [d(s, r) &lt; (p + \lambda) n ] \le \frac{n + 1}{2^{n + 1} } 2^{ n \mathbf{H} \left(  p + \lambda \right) }
\]</span></p>
<p>By union bound,<br />
<span class="math display">\[
\begin{aligned}
    \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] 
    &amp;&lt; 2^k \frac{n + 1}{2^{n + 1} } 2^{ n \mathbf{H}( p + \lambda) }  \\
    &amp;= 2^{ k + \log (n + 1) +  n \mathbf{H} ( p + \lambda) - n - 1 } 
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(k \le n(1 - \mathbf{H}(p) - \epsilon)\)</span>, when <span class="math inline">\(n\)</span> is sufficient large, it follows<br />
<span class="math display">\[
\begin{aligned}
     \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] 
        &lt; 2^{ \log \frac{n + 1}{2} +  n ( \mathbf{H} ( p + \lambda) - \mathbf{H} ( p) - \delta) } 
        \le \frac{\delta}{2}
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    &amp;\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} } \left[ \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \\
    &amp;= \sum_{r \in \{0, 1\}^n }     \Pr_{s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } [ r \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n}  ] \cdot  \Pr_{R \sim \mathfrak{D}(s_i, p) } [ R = r \mid S = s_i] \\
    &amp;\le \frac{\delta}{2}
\end{aligned}
\]</span></p>
<p>and <span class="math display">\[
\underset{ {s_1, s_2, ...,s_{2^{k + 1} } \sim \mathbf{U}(\{0, 1\}^n) } } {\mathbb{E} }  \left[ \sum_{i = 1}^{2^{k + 1} }  \Pr_{R \sim \mathfrak{D}(s_i, p) } [R \in \cup_{j \neq i} B_{s_{j} , (p + \lambda)n } \mid S = s_i] \right] \le 2^{k + 1} \frac{\delta}{2} = 2^k \delta
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Proof of 2.</em></strong></p>
<p>The difficulty here is to prove that the claim holds for arbitrary <span class="math inline">\((k, n)\)</span> encoding and decoding functions. The proof relies on an important property:</p>
<blockquote>
<p>the decoding function <span class="math inline">\(\text{Dec}\)</span> is a function on <span class="math inline">\(\{0, 1\}^n\)</span></p>
</blockquote>
<p>For any <span class="math inline">\(r \in \{0, 1\}^n\)</span>, there is a unique output <span class="math inline">\(\text{Dec}(r) \in \{0, 1\}^k\)</span>. The decoding function needs to decide the unique message <span class="math inline">\(\{0, 1\}^n\)</span> the sender want to send.</p>
<p>Now, define <span class="math display">\[
S_i \doteq \{ r \in \{0, 1\}^n :  r \text{ is decoded correctly if } s_i \text{ is sent}   \}
\]</span></p>
<p>The <span class="math inline">\(S_i\)</span>'s are non-overlapped and we have, <span class="math display">\[
\cup_{i = 1}^{2^k} S_i \subset \{0, 1 \}^n
\]</span></p>
<p>If <span class="math inline">\(s_i\)</span> is sent, then by Hoeffding inequality, with probability at least <span class="math inline">\(1 - \exp(- 2\lambda^2 n)\)</span>, the received message <span class="math inline">\(R\)</span> is likely to fall into a ring centered at <span class="math inline">\(s_i\)</span>: <span class="math display">\[
\text{ring} (s_i) \doteq \{ r \in \{0, 1\}^n : |d(r, s_i) - pn |&lt; \lambda n \}
\]</span></p>
<p>Finally, if the messages <span class="math inline">\(s_1, s_2, ..., s_{2^k}\)</span> are sent uniformly at random, i.e., <span class="math inline">\(i \sim \mathbf{U}[1, 2^k]\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[ R \text{ is decoded correctly}]   &amp;=  \sum_{i = 1}^{2^k} \sum_{r \in S_i } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \Pr_{i \sim \mathbf{U}[1, 2^k]} [ S = s_i] \\ 
        &amp;=  \frac{1}{2^k} \sum_{i = 1}^{2^k} \sum_{r \in S_i } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] 
\end{aligned}
\]</span></p>
<p>Observed that <span class="math inline">\(S_i \subset \text{ring} (s_i) \cup (S_i \cap \text{ring} (s_i) )\)</span>, we get</p>
<p><span class="math display">\[
\begin{aligned}
 &amp;\Pr[ R \text{ is decoded correctly}] \\
        &amp;\le \frac{1}{2^k}  \sum_{i = 1}^{2^k} \left( \sum_{r \notin \text{ring} (s_i)  } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i]  +\sum_{r \in S_i \cap \text{ring} (s_i) } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \right) \\ 
        &amp;\le \frac{1}{2^k}  \sum_{i = 1}^{2^k} \left( \exp( -2\lambda^2n) +\sum_{r \in S_i \cap \text{ring} (s_i) } \Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \right) 
\end{aligned}
\]</span></p>
<p>Note that for <span class="math inline">\(r \in S_i \cap \text{ring} (s_i)\)</span>, as <span class="math inline">\(p &lt; 1 / 2\)</span>, it holds that <span class="math display">\[
\Pr_{R \sim \mathfrak{D}(s_i, p) } [R = r \mid S = s_i] \le p^{ \lceil(p - \lambda) n \rceil} (1 - p)^{ n - \lceil(p - \lambda) n \rceil }
\]</span></p>
<p>We obtain</p>
<p><span class="math display">\[
\begin{aligned}
        &amp;\Pr[ R \text{ is decoded correctly}] \\
        &amp;\le \exp( -2\lambda^2n) +  \frac{1}{2^k}  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } p^{ \lceil(p - \lambda) n \rceil} (1 - p)^{ n - \lceil(p - \lambda) n \rceil } \\
        &amp;\le \exp( -2\lambda^2n) +  \frac{1}{2^k}  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } p^{ (p - \lambda) n } (1 - p)^{ n - (p - \lambda) n  } \\
        &amp;= \exp( -2\lambda^2n) +  p^{ (p - \lambda) n } (1 - p)^{ n - (p - \lambda) n  }  \sum_{i = 1}^{2^k} \sum_{r \in S_i \cap \text{ring} (s_i) } \frac{1}{2^k}  \\
        &amp;= \exp( -2\lambda^2n) +   p^{ pn } (1 - p)^{ n - pn } \left(\frac{ 1 - p}{ p} \right)^{\lambda n} 2^{n - k}\\
        &amp;= \exp( -2\lambda^2n) +  2^{n - k - n \mathbf{H}(p)}  \left(\frac{ 1 - p}{ p} \right)^{\lambda n}
\end{aligned}
\]</span></p>
<p>Conditioning on that <span class="math inline">\(k \ge n (1 - \mathbf{H}(p) + \delta)\)</span>, <span class="math display">\[
\Pr[ R \text{ is decoded correctly}] \le \exp( -2\lambda^2n) +  2^{- n (\delta + \lambda \log \frac{ 1 - p}{ p} ) }  
\]</span></p>
<p>By choosing sufficient small <span class="math inline">\(\lambda\)</span> and large enough <span class="math inline">\(n\)</span>, <span class="math inline">\(\Pr[ R \text{ is decoded correctly}]\)</span> can be arbitrary small.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h2 id="reference">Reference</h2>
<p>[1] M. Mitzenmacher and E. Upfal, Probability and computing: an introduction to randomized algorithms and probabilistic analysis. New York: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/07/Entropy-and-Compression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/07/Entropy-and-Compression/" class="post-title-link" itemprop="url">Entropy and Compression</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-07 12:24:22" itemprop="dateCreated datePublished" datetime="2020-06-07T12:24:22+10:00">2020-06-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-13 23:57:00" itemprop="dateModified" datetime="2020-07-13T23:57:00+10:00">2020-07-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>One interpretation of entropy of a random variable is as a measure of how many unbiased, independent random bits in expectation we can extract from the random variable. Another one comes from compression of a sequence.</p>
<p>Assume that the sequence studied (denoted as <span class="math inline">\(s\)</span>) is a binary one. It consists of a concatenation of the outcomes of <span class="math inline">\(n\)</span> independent Bernoulli random variables. Without lose of generality, we assume that each bit comes up <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p \le { 1 / 2 }\)</span>. This is one of the simplest models of a sequence.</p>
<p>The fact that sequence could be a biased one makes it possible to represent it by a new sequence with shorter length in expectation. For example, suppose that <span class="math inline">\(p = \frac{1}{4}\)</span>, then for a pair of consecutive bits, it have</p>
<ol type="1">
<li>probability <span class="math inline">\(\frac{1}{16}\)</span> of being <span class="math inline">\(11\)</span>,<br />
</li>
<li>probability <span class="math inline">\(\frac{3}{16}\)</span> of being <span class="math inline">\(01\)</span>,</li>
<li>probability <span class="math inline">\(\frac{3}{16}\)</span> of being <span class="math inline">\(10\)</span>,</li>
<li>probability <span class="math inline">\(\frac{9}{16}\)</span> of being <span class="math inline">\(00\)</span>.</li>
</ol>
<p>If we use <span class="math inline">\(111\)</span> to represent <span class="math inline">\(11\)</span>, <span class="math inline">\(110\)</span> to represent <span class="math inline">\(01\)</span>, <span class="math inline">\(10\)</span> to represent <span class="math inline">\(10\)</span>, and <span class="math inline">\(0\)</span> to represent <span class="math inline">\(00\)</span>, then the expected number of representation bits per pair is <span class="math display">\[
\begin{array}{r}
    3 \cdot \frac{1}{16} + 3 \cdot \frac{3}{16} + 2 \cdot  \frac{3}{16} + 1 \cdot  \frac{9}{16} 
    = \frac{27}{16} 
    &lt; 2
\end{array}
\]</span></p>
<p>In general, compression is not limited to the way described above. We call the rule with which we compress a string <em>a compression function</em>.</p>
<h3 id="compression-function"><strong><em>Compression Function</em></strong></h3>
<p><em>Let <span class="math inline">\(S = \{0, 1\}^n\)</span> be the set of binary sequence of length <span class="math inline">\(n\)</span>, and <span class="math inline">\(T = \{ 0, 1\}^+\)</span> the set of binary sequence of any positive length. A compression function <span class="math inline">\(h: S \rightarrow T\)</span> is an injective (one-to-one) function from <span class="math inline">\(S\)</span> to <span class="math inline">\(T\)</span>.</em></p>
<p>In other words, each <span class="math inline">\(s \in S\)</span> is assigned a unique non-empty sequence (of arbitrary length) by <span class="math inline">\(h\)</span>.</p>
<p>Observe that the size of <span class="math inline">\(S\)</span> is <span class="math inline">\(2^n\)</span>. On the other hand, the number of sequences with length less than <span class="math inline">\(n\)</span> is <span class="math inline">\(\sum_{i = 1}^{n - 1} 2^{i} = 2^n - 1 &lt; 2^n\)</span>. For any <span class="math inline">\(h\)</span>, it is impossible for <span class="math inline">\(h\)</span> to map every <span class="math inline">\(s \in S\)</span> to a sequence with length less than <span class="math inline">\(n\)</span>. Under adversarial input, <span class="math inline">\(h\)</span> can not compress at all.</p>
<p>The hope is that when there is a distribution <span class="math inline">\(\mathfrak{D}\)</span> on <span class="math inline">\(S\)</span>, the expected length of the compressed sequences would be small: <span class="math display">\[
\mathbf{E} [ |h(s)| ] = \sum_{s \in S} p_s \cdot |h(s)|
\]</span></p>
<p>To illustrate the meaning of entropy, we study a simple case where <span class="math inline">\(\mathfrak{D}\)</span> is the joint distribution of <span class="math inline">\(n\)</span> <em>i.i.d.</em> Bernoulli random variables that come up <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p \le { 1 / 2 }\)</span> (by symmetry, the claim holds for the case of <span class="math inline">\(p &gt; { 1 / 2 }\)</span>).</p>
<p>The following theorem formalizes how good a compression function we can find.</p>
<h3 id="entropy-as-lower-bound-and-upper-bound"><strong><em>Entropy as Lower Bound and Upper Bound</em></strong></h3>
<p><em>Theorem.</em></p>
<ol type="1">
<li><span class="math inline">\(\forall \delta &gt; 0\)</span>, <span class="math inline">\(\exists\)</span> a compression <span class="math inline">\(h\)</span> and integer <span class="math inline">\(N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n \ge N\)</span>, it holds that <span class="math display">\[
\mathbf{E}[ |h(s)| ] \le (1 + \delta) n \mathbf{H}(p)
\]</span></li>
<li><span class="math inline">\(\forall \delta &gt; 0\)</span>, <span class="math inline">\(\exists\)</span> integer <span class="math inline">\(N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n \ge N\)</span>, it holds that for any compression <span class="math inline">\(h\)</span>, <span class="math display">\[
     \mathbf{E}[ |h(s)| ] \ge (1 - \delta) n \mathbf{H}(p)
 \]</span></li>
</ol>
<p><em>Intuitively, the entropy <span class="math inline">\(\mathbf{H}(p)\)</span> is the measure of the average number of bits in expectation we need after compression to represent a Bernoulli random variable that comes up <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p\)</span>.</em></p>
<p><em>The high level idea of the proof is that, with high probability, the number of ones in a <span class="math inline">\(s \in S\)</span> is roughly <span class="math inline">\(np\)</span>. There are roughly <span class="math inline">\(2^{n\mathbf{H}(p)}\)</span> such sequences. Therefore, we can use <span class="math inline">\(n \mathbf{H}(p)\)</span> bits to represent each sequence.</em></p>
<p><strong><em>Proof.</em></strong> The claim is trivially true if <span class="math inline">\(p = { 1 / 2 }\)</span>. Otherwise, <span class="math inline">\(p &lt; { 1 / 2 }\)</span>. We can pick some <span class="math inline">\(\epsilon &gt; 0\)</span>, such that <span class="math inline">\(p + \epsilon &lt; { 1 / 2 }\)</span>. Further, if <span class="math inline">\(n\)</span> is large enough, <span class="math inline">\(np + n\epsilon \le n / 2 - 1\)</span>. Hence, we may assume that <span class="math inline">\(\lceil np + n\epsilon \rceil \le n / 2\)</span> .</p>
<p>Denote <span class="math inline">\(Z\)</span> be the number of ones in the string <span class="math inline">\(s\)</span>. By Hoeffding Inequality, it holds that <span class="math display">\[
\Pr[ |Z - np| \ge n\epsilon ] \le \exp(-2n \epsilon^2)
\]</span></p>
<p>By the fact that <span class="math inline">\(Z\)</span> takes only integer values, we have <span class="math display">\[
\Pr[Z \ge \lceil np + n\epsilon \rceil] \le \exp(-2 n \epsilon^2 )
\]</span></p>
<p>We use the first bit output of <span class="math inline">\(h\)</span> as a flag. For a string <span class="math inline">\(s\)</span> with <span class="math inline">\(Z \ge \lceil np + n\epsilon \rceil\)</span>, we set the first bit to <span class="math inline">\(0\)</span> and then output the same sequence, i.e., <span class="math inline">\(h(s) = 0s\)</span>. In such case we do not compress the string at all and use <span class="math inline">\(n + 1\)</span> bits to represent it.</p>
<p>We set the first bit output to <span class="math inline">\(1\)</span> if <span class="math inline">\(Z &lt; \lceil np + n \epsilon \rceil \le n / 2\)</span>. The number of such sequences is bounded by <span class="math display">\[
\sum_{k = 0}^{\lceil np + n\epsilon \rceil - 1} \binom{n}{k} \le  \frac{n}{2} \binom{n}{\lceil np + n\epsilon \rceil - 1 } \le \frac{n}{2} 2^{ n \cdot \mathbf{H} \left( \frac{ \lceil np + n\epsilon \rceil - 1 }{n} \right) } \le \frac{n}{2} 2^{ n \mathbf{H} \left( p + \epsilon  \right) }
\]</span></p>
<p>The first inequality holds because <span class="math inline">\(\binom{n}{k}\)</span> increases for <span class="math inline">\(k &lt; n / 2\)</span> and that the summation is over <span class="math inline">\(\lceil np + n \epsilon \rceil \le n / 2\)</span> terms. The last inequality holds since <span class="math inline">\((\lceil np + n\epsilon \rceil - 1) / n \le ( np + n\epsilon ) / n = p + \epsilon &lt; 1 / 2\)</span> and <span class="math inline">\(\mathbf{H}( \cdot )\)</span> is a increasing function in the range <span class="math inline">\([0, 1/2]\)</span>.</p>
<p>We compress these sequences by representing each of them with a unique sequence of at most<br />
<span class="math display">\[
\begin{array}{rl}
    \log \left[ (n / 2) \cdot  2^{ n \cdot\mathbf{H} \left( p + \epsilon  \right) } \right]
    = n \cdot\mathbf{H} \left( p + \epsilon  \right)  + \log n -1
\end{array}
\]</span></p>
<p>bits. Considering the leading flag bit 1 (to indicate the sequence is a compressed one), we output at most <span class="math inline">\(n \mathbf{H} \left( p + \epsilon \right) + \log n\)</span> bits for sequences with <span class="math inline">\(Z &lt; \lceil np + n \epsilon \rceil\)</span>. This can be written as <span class="math inline">\(\left[ \mathbf{H} \left( p + \epsilon \right) + (1 / n) \cdot (\log n - 1) \right] n\)</span> bits and is smaller than <span class="math display">\[
(1 + \delta /{2} ) \cdot \mathbf{H} (p) \cdot n
\]</span></p>
<p>is <span class="math inline">\(\epsilon\)</span> is smaller enough and <span class="math inline">\(n\)</span> is large enough.</p>
<p>We are almost done with the proof. It is left to consider sequences with <span class="math inline">\(Z \ge \lceil np + n \epsilon \rceil\)</span>. For those sequence, we don't compress them and and set the leading flag bit to 0. The outputs for those sequences consist of at most <span class="math inline">\(1 + n\)</span> bits. However, by Hoeffding inequality, such sequences do not appear frequently and the expected output length can be made arbitrary small.</p>
<p>Specifically, <span class="math inline">\(\forall \delta &gt; 0\)</span>, we can find large enough <span class="math inline">\(N\)</span>, such that <span class="math inline">\(\forall n &gt; N\)</span>, it holds</p>
<p><span class="math display">\[
(n + 1 ) \cdot \exp(-2 n \epsilon^2 ) \le n \cdot (\delta / 2)  \cdot  \mathbf{H}(p)
\]</span></p>
<p>Then, in expectation, the bits outputted by <span class="math inline">\(h\)</span> is at most <span class="math display">\[
\begin{aligned}
    &amp;(n + 1) \cdot \exp(-2 n \epsilon^2 ) + [n \cdot \mathbf{H} \left( p + \epsilon  \right)   + \log n ] \cdot [ 1 - \exp(-2 n \epsilon^2 ) ] \\
    \le&amp; n \cdot (\delta / 2)  \cdot  \mathbf{H}(p) + [ n \cdot \mathbf{H} \left( p + \epsilon  \right)   + \log n ]\\
    \le&amp; n \cdot (\delta / 2)  \cdot  \mathbf{H}(p)  +  (1 + \delta /{2} ) \cdot \mathbf{H} (p) \cdot n\\
    \le&amp; \left(  1 + \delta \right) \cdot \mathbf{H}(p) \cdot n
\end{aligned}
\]</span></p>
<p>To prove the lower bound, first we need the following lemma.</p>
<p><strong><em>Lemma.</em></strong> <em>for <span class="math inline">\(s_1, s_2 \in S\)</span>, if <span class="math inline">\(s_1\)</span> has more ones than <span class="math inline">\(s_2\)</span>, i.e., <span class="math inline">\(\Pr(s_1) \ge \Pr(s_2)\)</span>, then the <span class="math inline">\(h\)</span> that minimizes the expected output length <span class="math inline">\(\mathbf{E}[ |h(s)| ]\)</span> should assign <span class="math inline">\(s_1\)</span> a sequence at most as long as <span class="math inline">\(s_2\)</span>, i.e., <span class="math inline">\(|h(s_1)| \le |h(s_2)|\)</span>.</em></p>
<p><em>Proof</em>. Otherwise, if we swap the output sequences of <span class="math inline">\(h(s_1)\)</span> and <span class="math inline">\(h(s_2)\)</span>, we lower value of <span class="math inline">\(\mathbf{E}[ |h(s)| ]\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Further, consider the number of <span class="math inline">\(s \in S\)</span> with <span class="math inline">\(\lfloor np - n \epsilon \rfloor\)</span> ones, <span class="math display">\[
\begin{aligned}
\binom{n}{\lfloor np - n\epsilon \rfloor } 
    &amp;\ge \frac{1}{n + 1} 2^{ n \cdot \mathbf{H} \left( { \lfloor np - n\epsilon \rfloor } / { n} \right) } \\
    &amp;\ge \frac{1}{n + 1} 2^{ n \cdot \mathbf{H}( { (np - n\epsilon - 1) } / {n} )} \\
    &amp;\ge 2^{ \lfloor n \cdot \mathbf{H}( p - \epsilon -1 / n) -  \log (n + 1)  \rfloor }
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(k = \lfloor n \cdot \mathbf{H}( p - \epsilon -1 / n) - \log (n + 1) \rfloor\)</span>. For large enough <span class="math inline">\(n\)</span>, it holds that <span class="math display">\[
k \ge (1 - \delta / 2) \cdot n \cdot \mathbf{H}(p)
\]</span></p>
<p>Further, since <span class="math display">\[
\sum_{i = 1}^{  k - 1  } 2^i \le \sum_{i = 0}^{  k - 1  } 2^i = 2^{  k } - 1 &lt; 2^k,
\]</span></p>
<p>there are strictly less than <span class="math inline">\(2^k\)</span> distinct binary sequences with length at most <span class="math inline">\(k - 1\)</span>.</p>
<p>Therefore, for the set of <span class="math inline">\(s \in S\)</span> with <span class="math inline">\(\lfloor np - n\epsilon \rfloor\)</span> ones, at least one of them has output length at least <span class="math inline">\(k\)</span>.</p>
<p>By the previous lemma, all sequences with more than <span class="math inline">\(\lfloor np - n\epsilon \rfloor\)</span> ones has length at least <span class="math inline">\(k\)</span>.</p>
<p>Denote <span class="math inline">\(Z\)</span> be the number of ones in the string <span class="math inline">\(s\)</span>. By Hoeffding Inequality, it holds that <span class="math display">\[
\Pr[ |Z - np| \ge n\epsilon ] \le \exp(-2n \epsilon^2)
\]</span></p>
<p>By the fact that <span class="math inline">\(Z\)</span> takes only integer values, we have <span class="math display">\[
\Pr[Z \le \lfloor np - n\epsilon \rfloor] \le \exp(-2 n \epsilon^2 )
\]</span></p>
<p>The expected output length of the sequences with more than <span class="math inline">\(\lfloor np - n\epsilon \rfloor\)</span> ones is at least <span class="math display">\[
\begin{aligned}
    &amp;[ 1 - \exp(-2n\epsilon^2) ] \cdot k \\
    =&amp;[ 1 - \exp(-2n\epsilon^2) ] \cdot  (1 - \delta / 2) \cdot n \cdot \mathbf{H}(p)
\end{aligned}
\]</span></p>
<p>This is at least <span class="math inline">\((1 - \delta) \cdot n \cdot \mathbf{H}(p)\)</span> when <span class="math inline">\(n\)</span> is large enough.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="huffman-code"><strong><em>Huffman Code</em></strong></h3>
<p>In this section, we show that the upper bound can be achieved by Huffman code.</p>
<blockquote>
<p><em><span class="math inline">\(\forall \delta &gt; 0\)</span>, <span class="math inline">\(\exists\)</span> a compression <span class="math inline">\(h\)</span> and integer <span class="math inline">\(N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n \ge N\)</span>, it holds that</em> <span class="math display">\[
\mathbf{E}[ |h(s)| ] \le (1 + \delta) n \mathbf{H}(p)
\]</span></p>
</blockquote>
<p>We begin with an important property of Huffman Code. Suppose that we have an alphabet <span class="math inline">\(\mathbb{U}\)</span> such that probability associated with each element in <span class="math inline">\(\mathbb{U}\)</span> is <span class="math inline">\(2^{-l}\)</span> for some integer <span class="math inline">\(l\)</span>. Then <span class="math inline">\(\exists h\)</span>, such that <span class="math display">\[
   \mathbf{E}_{X \in \mathbb{U} } [ |h(X)| ] =  n \mathbf{H}(X)
\]</span></p>
<p>For example, if <span class="math inline">\(\mathbb{U} = \{0, 1, 2, 3 \}\)</span> and the distribution <span class="math inline">\(p = \{ 0.5, 0.25, 0.125, 0.125 \}\)</span>, then we can have the following encoding</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Encoding.png" /></p>
<p>The expected coding length is <span class="math display">\[
0.5 \cdot 1 + 0.25 \cdot 2 + 2 \cdot 0.125 \cdot 3 = 1.75
\]</span></p>
<p>which is exactly the entropy of the random variable.</p>
<p>In general, if we have a random variable <span class="math inline">\(X\)</span>, we can round down its probability to the nearest integer negative power of <span class="math inline">\(2\)</span>. The expected code length is given by <span class="math display">\[
\sum_{i } p_i \left\lceil \log \frac{1}{p_i} \right\rceil \le \mathbf{H}(p) + 1
\]</span></p>
<p>Intuitively, the <span class="math inline">\(\left\lceil \log \frac{1}{p_i} \right\rceil\)</span> has enough slot to accommodate all elements with probabilities <span class="math inline">\(p_i\)</span>'s.</p>
<p>In particular, we view <span class="math inline">\(\{0, 1\}^n\)</span> as a large alphabet <span class="math inline">\(\mathbf{\Sigma }\)</span>. The alphabet has entropy <span class="math inline">\(n \cdot \mathbf{H}(p)\)</span>. We can have a encoding such that the expected output length is at most <span class="math inline">\(n \cdot \mathbf{H}(p) + 1\)</span>. For large enough <span class="math inline">\(n\)</span>, this is at most <span class="math inline">\((1 + \delta) n \cdot \mathbf{H}(p)\)</span>.</p>
<h3 id="relative-entropy-and-mutual-information"><strong><em>Relative Entropy and Mutual Information</em></strong></h3>
<h4 id="relative-entropy">Relative Entropy</h4>
<p>Given a distribution <span class="math inline">\(p\)</span>, we mistaken it as a distribution <span class="math inline">\(q\)</span>, the expected coding length is roughly <span class="math display">\[
\sum_i p_i \log \frac{1}{q_i}
\]</span></p>
<p>The discrepancy between the optimal coding is given by <span class="math display">\[
D(p || q) = \sum_i p_i \log \frac{1}{q_i} - \sum_i p_i \log \frac{1}{p_i} = \sum_i p_i \log \frac{p_i}{q_i}
\]</span></p>
<p>By definition, we know that this gap is always non-negative. We can prove it rigorously algebraically. We show two different approaches here.</p>
<ol type="1">
<li>By that <span class="math inline">\(f(x) = x \log x\)</span> is convex for <span class="math inline">\(x \ge 0\)</span>, we have <span class="math display">\[
\begin{aligned}
    D(p || q) &amp;= \sum_i p_i \log \frac{p_i}{q_i} \\
    &amp;= \sum_i q_i \cdot \frac{p_i}{q_i} \log \frac{p_i}{q_i} \\
    &amp;\ge \left( \sum_i q_i \cdot \frac{p_i}{q_i} \right) \log \left( \sum_i q_i \cdot \frac{p_i}{q_i} \right) \\
    &amp;= 0
\end{aligned}
\]</span></li>
<li>By that <span class="math inline">\(f(x) = -\log x\)</span> is convex (for <span class="math inline">\(x \ge 0\)</span> ), we have <span class="math display">\[
\begin{aligned}
    D(p || q) &amp;= \sum_i - p_i \log \frac{q_i}{p_i} \\
    &amp;\ge  -\log \left( \sum_i p_i \cdot \frac{q_i}{p_i} \right) \\
    &amp;= 0
\end{aligned}
\]</span></li>
</ol>
<h4 id="mutual-information">Mutual Information</h4>
<p>Given two random variable, the mutual information between them is defined as <span class="math display">\[
I(X; Y) = I(Y; X) = \mathbb{E}_{p(x, y) } \left[ \log \frac{ p(x, y)}{p(x) p(y) } \right]
\]</span></p>
<p>If we know <span class="math inline">\(p(x,y)\)</span> we use in expectation <span class="math inline">\(\mathbb{E}_{p(x, y) } \left[ \log \frac{1} { p(x, y)} \right]\)</span> to describe them. If we know only <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(p(y)\)</span>, the mutual information measured the bits we waste.</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;\sum p(x, y) \log \frac{ p(x, y)}{p(x) p(y) } \\
    =&amp;  \sum p(x, y) \log \frac{ 1 }{p(x)} \\
    &amp;+  \sum p(x, y) \log \frac{ 1 }{p(y)} \\
    &amp;-  \sum p(x, y) \log \frac{ 1 }{p(x, y)} \\
    =&amp; \mathbf{H} ( X ) + \mathbf{H} ( Y ) - \mathbf{H} ( X, Y ) \\
    =&amp;\sum p(x, y) \log \frac{ p(x | y)}{p(x)  } \\
    =&amp; \mathbf{H} ( X ) - \mathbf{H} ( X \mid Y) \\
    =&amp;\sum p(x, y) \log \frac{ p(x | y)}{ p(x) }  \\
    =&amp; \mathbf{H} ( Y ) - \mathbf{H} ( X \mid Y) \\
\end{aligned}
\]</span></p>
<p><strong><em>Lemma.</em></strong> <span class="math display">\[
\mathbf{I}(X; Y) \ge 0
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\mathbf{I}(X; Y) = D( p(x, y) \mid p(x) p(y) ) \ge 0
\]</span></p>
<p>Or <span class="math display">\[
\begin{aligned}
    \sum p(x, y) \log \frac{ p(x, y)}{p(x) p(y) } 
    &amp;= -\sum p(x, y) \log \frac{p(x) p(y) }{ p(x, y)} \\
    &amp;\ge - \log \sum p(x, y) \frac{p(x) p(y) }{ p(x, y)} \\
    &amp;=0   
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Corollary</em></strong> <span class="math display">\[
\mathbf{H} ( Y ) - \mathbf{H} ( X \mid Y) = \mathbf{H} ( X ) - \mathbf{H} ( X \mid Y) \ge 0
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Definition</em></strong> Conditional Relative Entropy <span class="math display">\[
\begin{aligned}
    \mathbf{D}( p(y \mid x) || q(y \mid x) ) &amp;\doteq  \mathbb{E}_{p(x, y) } \left[ \log \frac{ p(Y \mid X) }{ q(Y \mid X) }  \right]\\
    &amp;=\sum_{x}  \sum_{ y } p(y \mid x) \log \frac{ p(y \mid x) }{ q(y \mid x) }
\end{aligned}
\]</span></p>
<p><strong><em>Lemma.</em></strong> Relative entropy <span class="math inline">\(\mathbf{D}( p || q )\)</span> is convex in the pair <span class="math inline">\((p, q)\)</span>: if there are two distribution pairs <span class="math inline">\((p_1, q_1)\)</span> and <span class="math inline">\((p_2, q_2)\)</span>, then . <span class="math display">\[
\mathbf{D}( \lambda p_1 + (1 - \lambda) p_2 || \lambda q_1 + (1 - \lambda_1) q_2 ) \le \lambda \mathbf{D}( p_1 ||  q_1 ) + (1 - \lambda) \mathbf{D}( p_2 ||  q_2 )
\]</span></p>
<p><strong><em>Proof.</em></strong> For any fix value <span class="math inline">\(x\)</span>, it holds that <span class="math display">\[
\begin{aligned}
    &amp;[ \lambda p_1(x) + (1 - \lambda) p_2(x) ] \log \frac{ \lambda p_1(x) + (1 - \lambda) p_2(x) }{ \lambda q_1(x) + (1 - \lambda) q_2(x) } \\
    &amp;=-[ \lambda p_1(x) + (1 - \lambda) p_2(x) ] \log \frac { \lambda q_1(x) + (1 - \lambda) q_2(x) } { \lambda p_1(x) + (1 - \lambda) p_2(x) } \\
    &amp;= -[ \lambda p_1(x) + (1 - \lambda) p_2(x) ] \log \left( \frac { \lambda p_1(x)  } { \lambda p_1(x) + (1 - \lambda) p_2(x) } \frac { \lambda q_1(x) } { \lambda p_1(x)  } + \frac { (1 - \lambda) p_2(x)  } { \lambda p_1(x) + (1 - \lambda) p_2(x) } \frac { (1 - \lambda) q_2(x) } { (1 - \lambda) p_2(x)  } \right) \\
    &amp;\le -\lambda p_1(x) \log   \frac { \lambda q_1(x) } { \lambda p_1(x)  } - (1 - \lambda) p_2(x) \log \frac { (1 - \lambda) q_2(x) } { (1 - \lambda) p_2(x)  }
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="chain-rules"><strong><em>Chain Rules</em></strong></h3>
<p>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be random variables whose density function is <span class="math inline">\(p(x_1, x_2, ..., x_n)\)</span>, then</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbf{H}(X_1, X_2, ..., X_n) = \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1})\)</span><br />
<strong><em>Proof.</em></strong><br />
<span class="math display">\[
 \begin{aligned}
     \mathbf{H}(X_1, X_2, ..., X_n) 
         &amp;= \sum_{x_1, x_2, ..., x_n} p(x_1, x_2, ..., x_n)  \log \frac{1}{\prod_{i = 1}^n p(x_i \mid x_1, ..., x_{i - 1})  }  \\
         &amp;= \sum_{x_1, x_2, ..., x_n} p(x_1, x_2, ..., x_n) \sum_{i = 1}^n \log \frac{1}{ p(x_i \mid x_1, ..., x_{i - 1})  }  \\
         &amp;=\sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1}) \\
 \end{aligned}
 \]</span></p>
<p><strong><em>Corollary</em></strong> <span class="math display">\[
 \mathbf{H} ( X_1, X_2, ..., X_n) = \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1}) \le \sum_{i = 1}^n \mathbf{H}(X_i)
 \]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{I}(X_1, X_2, ..., X_n; Y) = \sum_{i = 1}^n \mathbf{I}(X_i ; Y\mid X_1, ..., X_{i - 1})\)</span><br />
<strong><em>Proof.</em></strong><br />
<span class="math display">\[
 \begin{aligned}
     \mathbf{I}(X_1, X_2, ..., X_n; Y)
     &amp;=  \mathbf{H}(X_1, X_2, ..., X_n) -  \mathbf{H}(X_1, X_2, ..., X_n \mid Y) \\
     &amp;= \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1})  - \sum_{i = 1}^n \mathbf{H}(X_i \mid X_1, ..., X_{i - 1}, Y) \\
     &amp;= \sum_{i = 1}^n \mathbf{I}(X_i ; Y\mid X_1, ..., X_{i - 1}) \\
 \end{aligned}
 \]</span><br />
</p></li>
<li><p><span class="math inline">\(\mathbf{D}( p(x, y) || q(x, y) ) = \mathbf{D}( p(x) || q(x) ) + \mathbf{D}( p(y \mid x) || q(y \mid x) )\)</span><br />
<strong><em>Proof.</em></strong> <span class="math display">\[
 \begin{aligned}
     \mathbf{D}( p(x, y) || q(x, y) ) 
     &amp;= \sum_{x, y} p(x, y) \log \frac{ p(x) p(y \mid x) }{q(x) q (y \mid x) }
     \\
     &amp;= \sum_{x, y} p(x, y) \log \frac{ p(x)  }{q(x) } + \sum_{x, y} p(x, y) \log \frac{  p(y \mid x) }{  q (y \mid x) }
     \\
     &amp;= \mathbf{D}( p(x) || q(x) ) + \mathbf{D}( p(y \mid x) || q(y \mid x) )
 \end{aligned}
 \]</span></p></li>
</ol>
<h3 id="reference.">Reference.</h3>
<p>[1]M. Mitzenmacher and E. Upfal, Probability and computing: an introduction to randomized algorithms and probabilistic analysis. New York: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/01/Entropy-and-Random-Bits/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/01/Entropy-and-Random-Bits/" class="post-title-link" itemprop="url">Entropy and Random Bits</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-01 00:16:09" itemprop="dateCreated datePublished" datetime="2020-06-01T00:16:09+10:00">2020-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-14 00:15:12" itemprop="dateModified" datetime="2020-07-14T00:15:12+10:00">2020-07-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="entropy">Entropy</h4>
<p>The entropy of a random variable <span class="math inline">\(X\)</span> is defined as <span class="math display">\[
\mathbf{H}[X] = \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right]
\]</span></p>
<p>Here we adopt the convention that <span class="math inline">\(0 \log 0 = 0\)</span>, as <span class="math inline">\(\lim_{x \rightarrow 0^+} x \log x = \lim_{x \rightarrow \infty} \frac{1 }{ x } \log \frac{1 }{ x } = 0\)</span>.</p>
<p>The entropy is oblivious to the specific values <span class="math inline">\(X\)</span> takes and is only sensitive to the probabilities with which <span class="math inline">\(X\)</span> takes these values.</p>
<blockquote>
<p>Example.<br />
1. <span class="math inline">\(X_1 = \begin{cases} e^{1000}, \text{with probability 0.5} \\ 0, \text{with probability 0.5} \end{cases}\)</span><br />
2. <span class="math inline">\(X_2 = \begin{cases} 1, \text{with probability 0.5} \\ -1, \text{with probability 0.5} \end{cases}\)</span></p>
</blockquote>
<p>By definition, both <span class="math display">\[
\mathbf{H}[X_1] = \mathbf{H}[X_2] = 0.5 \log_2 2 + 0.5 \log_2 2 = 1
\]</span></p>
<p>We also notice that the entropy of a random variable may not reflect the whether a random variable is concentrated at its mean or not.</p>
<p>The binary entropy function <span class="math inline">\(\mathbf{H}(p)\)</span> of a Bernoulli random variable <span class="math inline">\(X\)</span> that takes value <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p\)</span> is <span class="math display">\[
\begin{aligned}
    \mathbf{H}(p) = &amp;p \log_2 \frac{1}{p} + (1 - p) \log_2 \frac{1}{1 - p} \\
    = &amp;-p \log_2 p - (1 - p) \log_2 (1 - p)    
\end{aligned}
\]</span></p>
<p>By concavity of <span class="math inline">\(\log_2(\cdot )\)</span>, we know that <span class="math display">\[
\begin{aligned}
    \mathbf{H}(p)
    &amp;\le \log_2 \left( p \frac{1}{p} + (1 - p) \frac{1}{1 - p} \right) 
    &amp;= 1
\end{aligned}
\]</span></p>
<p>The equality holds when <span class="math inline">\(p = 0.5\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Entropy.png" /></p>
<p>Some interesting values:</p>
<ul>
<li><span class="math inline">\(p = 0.5\)</span>, <span class="math inline">\(\mathbf{H}(p) = 1\)</span></li>
<li><span class="math inline">\(p = 0.2/0.8\)</span>, <span class="math inline">\(\mathbf{H}(p) \approx 0.7\)</span></li>
<li><span class="math inline">\(p = 0.1/0.9\)</span>, <span class="math inline">\(\mathbf{H}(p) \approx 0.5\)</span></li>
</ul>
<p><strong><em>Lemma.</em></strong> Given a discrete range <span class="math inline">\(\mathcal{U}\)</span>, the entropy of a random variable <span class="math inline">\(X\)</span> defined on <span class="math inline">\(\mathcal{U}\)</span> is maximized when <span class="math inline">\(X\)</span> has uniform distribution, i.e., <span class="math display">\[
\mathbf{H} (X) \le \log |\mathcal{U}|
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\begin{aligned}
    \mathbf{H} (X) 
        &amp;= \sum_{x} p(x) \log \frac{1}{p(x) } \\ 
        &amp;\le  \log \sum_{x} p(x) \frac{1}{p(x) } \\
        &amp;= \log |\mathcal{U}|
\end{aligned}
\]</span></p>
<p>The inequality holds with equality only when <span class="math inline">\(p(x) = 1/ |\mathcal{U}|\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Lemma.</em></strong> Let <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> be two distributions and <span class="math inline">\(\lambda \in [0, 1]\)</span>, then <span class="math display">\[
\mathbf{H} (\lambda p(x) + (1 - \lambda) q(x) ) \ge \lambda \mathbf{H}(p(x) ) + (1 - \lambda) \mathbf{H} (q(x) )
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\begin{aligned}
    \lambda \mathbf{H}(p(x) ) + (1 - \lambda) \mathbf{H} (q(x) ) 
        &amp;= \sum_{x} \left( \lambda p(x) \log \frac{1}{p(x)} + (1 - \lambda) q(x) \log \frac{1}{q(x) }  \right) \\ 
        &amp;= (  \lambda p(x) + (1 - \lambda) q(x) )\sum_{x} \left( \frac{ \lambda p(x) }{  \lambda p(x) + (1 - \lambda) q(x) } \log \frac{1}{p(x)} + \frac{(1 - \lambda) q(x) }{ \lambda p(x) + (1 - \lambda) q(x) } \log \frac{1}{q(x) }  \right) \\
        &amp;\le \sum_{x} ( \lambda p(x) + (1 - \lambda) q(x) ) \log \frac{1}{ \lambda p(x) + (1 - \lambda) q(x) } \\ 
        &amp;= \mathbf{H} ( \lambda p(x) + (1 - \lambda) q(x) ) 
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Lemma.</em></strong> For two independent random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we have <span class="math display">\[
\mathbb{E} \left[ \log_2 \frac{1}{\Pr[X + Y] } \right] = \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right] + \mathbb{E} \left[ \log_2 \frac{1}{\Pr[Y] } \right]
\]</span></p>
<p>i.e., <span class="math display">\[
\mathbf{H}[X + Y] = \mathbf{H}[X] + \mathbf{H}[Y]
\]</span></p>
<p><strong><em>Proof.</em></strong> We prove it for the case of discrete random variables. <span class="math display">\[
\begin{array}{rrl}
    &amp;\mathbb{E} \left[ \log_2 \frac{1}{\Pr[X + Y] }  \right] 
    &amp;= \sum_{x, y} p(x, y) \log_2 \frac{1}{p(x, y)} \\
    &amp;&amp;= \sum_{x, y} p(x) p(y) \left( \log_2 \frac{1}{p(x)} + \log_2 \frac{1}{p(y) } \right) \\
    &amp;&amp;= \sum_{x} p(x) \sum_{y} p(y) \log_2 \frac{1}{p(x)} + \sum_{y} p(y) \sum_{x} p(x) \log_2 \frac{1}{p(y) } \\
    &amp;&amp;= \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right] + \mathbb{E} \left[ \log_2 \frac{1}{\Pr[Y] } \right]    
\end{array}
\]</span> <span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Definition.</em></strong> Given random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the joint entropy <span class="math inline">\(\mathbf{H}(X, Y)\)</span> with a distribution <span class="math inline">\(p(x, y)\)</span> is defined as <span class="math display">\[
    \mathbf{H}(X,Y) = \mathbb{E}\left[ \log \frac{1}{p(x, y)} \right]
   \]</span> When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables, it is equivalent to <span class="math display">\[
    \mathbf{H}(X,Y) = \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(x, y) }.
   \]</span></p>
<p><strong><em>Definition.</em></strong> The conditional entropy <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> with a distribution <span class="math inline">\(p(x, y)\)</span> is defined as <span class="math display">\[
   \begin{aligned}
       \mathbf{H}(Y \mid X) 
        &amp;= \sum_{x \in \mathcal{X} } p(x) \cdot \mathbf{H} (Y \mid X = x) \\
        &amp;= \sum_{x \in \mathcal{X} } p(x) \cdot \sum_{y \in \mathcal{Y} } p(y \mid x) \log \frac{1}{p(y \mid x) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) } \\
        &amp;= \mathbb{E} \left[  \log \frac{1}{p(y \mid x) } \right]
   \end{aligned}
   \]</span></p>
<ul>
<li><strong>Remark.</strong> <em>Conditioned on <span class="math inline">\(X = x\)</span>, <span class="math inline">\(Y\)</span> is a random variable and therefore we can define its Entropy as <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span>. The conditional entropy <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> is the average of the <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span> over the distribution of <span class="math inline">\(X\)</span>. Both <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> and <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span> represent a value. This is different from the conditional expectation: <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span> is a random variable and <span class="math inline">\(\mathbb{E}[Y \mid X = x]\)</span> is a value.</em></li>
</ul>
<p><strong><em>Chain Rule.</em></strong> For random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, it holds that <span class="math display">\[
   \begin{aligned}
       \mathbf{H}(X, Y) 
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(x, y) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) p(x) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) } + \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{ p(x) } \\
        &amp;= \mathbf{H}(Y \mid X) + \mathbf{H}(X)
   \end{aligned}
   \]</span></p>
<p><em>Corollary of</em> <strong><em>Chain Rule.</em></strong> <span class="math display">\[
   \mathbf{H}(X) + \mathbf{H}(Y \mid X) =  
   \mathbf{H}(Y) + \mathbf{H}(X \mid Y)
   \]</span></p>
<p>Or <span class="math display">\[
   \mathbf{H}(X) - \mathbf{H}(X \mid Y)=  
   \mathbf{H}(Y) - \mathbf{H}(Y \mid X)
   \]</span></p>
<h3 id="binomial-distribution">Binomial Distribution</h3>
<p>We study the relationship of binomial distribution <span class="math inline">\(B(n, p)\)</span> and entropy. As a rule of thumb, for reasonably large <span class="math inline">\(n\)</span> (say, <span class="math inline">\(n \ge 100\)</span>) and moderate size of <span class="math inline">\(p\)</span> (e.g., <span class="math inline">\(0.1 \le p \le 0.9\)</span>), the probability is concentrated on the interval <span class="math inline">\([np - \sqrt n, np + \sqrt n]\)</span>.</p>
<p><span class="math inline">\(N = 100, p = 0.5\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.5.png" /></p>
<p><span class="math inline">\(N = 100, p = 0.3\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.3.png" /></p>
<p><span class="math inline">\(N = 100, p = 0.1\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.1.png" /></p>
<p><span class="math inline">\(N = 1000, p = 0.1\)</span>. Note that <span class="math display">\[
\Pr[ 100 - \sqrt{1000} \le X \le 100 + \sqrt{1000} ] \approx 0.9993
\]</span> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=1000_P=0.1.png" /></p>
<p><em><strong>Lemma</strong>.</em> For <span class="math inline">\(n\in \mathbf{Z}^+\)</span>, <span class="math inline">\(k \in [n]\)</span>, it holds that <span class="math display">\[
\binom{n}{k} \ge \frac{1}{n + 1} 2^{n \mathbf{H}(\frac{k}{n} ) }
\]</span></p>
<p><em><strong>Proof</strong>.</em> Define <span class="math inline">\(q = \frac{k}{n}\)</span>. Then <span class="math display">\[
\begin{aligned}
    (q + (1 - q))^n &amp;= \sum_{i = 0}^n \binom{n}{i} q^i (1 - q)^{n - i}
\end{aligned}
\]</span></p>
<p>The summation consists of <span class="math inline">\((n + 1)\)</span> terms. We claim that <span class="math display">\[
\binom{n}{k} q^k (1 - q)^{n - k}
\]</span></p>
<p>is the largest one. For <span class="math inline">\(i \in [n]\)</span>, <span class="math display">\[
\frac{ \binom{n}{i + 1} q^{i + 1} (1 - q)^{n - i - 1} }{ \binom{n}{i } q^{i} (1 - q)^{n - i } } = \frac{n - i }{i + 1} \frac{ q}{1 - q}
\]</span></p>
<p>The ratio is at least <span class="math inline">\(1\)</span> when <span class="math inline">\(\frac{n - i}{i + 1} \ge \frac{1 - q}{ q }\)</span>. We see that when <span class="math inline">\(i = nq - 1 = k - 1\)</span>, it holds that <span class="math display">\[
\frac{n - i}{i + 1} = \frac{n - nq + 1}{ n q} \ge \frac{1 -  q}{ q }
\]</span> and when <span class="math inline">\(i = k\)</span>, <span class="math display">\[
\frac{n - i}{i + 1} = \frac{n - nq}{ n q + 1} &lt; \frac{1 -  q}{ q }
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    \binom{n}{k} q^k (1 - q)^{n - k} 
        &amp;= \binom{n}{k} 2^{k \log_2 q} 2^{ (n - k )\log_2 (1 - q)} \\
        &amp;= \binom{n}{k} 2^{ n [ q \log_2 q +  (1 - q )\log_2 (1 - q) ]} \\
        &amp;\ge \frac{1}{n + 1}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="extracting-random-bits">Extracting Random Bits</h4>
<p>Consider a uniform random variable <span class="math inline">\(X\)</span> at <span class="math inline">\([0, m - 1]\)</span>. By definition we know that <span class="math display">\[
\mathbf{H}[X] = \sum_{i = 0}^{m - 1} \frac{1}{m} \log_2 \frac{1}{\frac{1}{m} } = \log_2 m
\]</span></p>
<p>Given <span class="math inline">\(X\)</span> as an input, the following algorithm outputs a binary string <span class="math inline">\(s\)</span>, such that each bit of <span class="math inline">\(s\)</span> can be interpreted as the outcome of independently Bernoulli random variables with probability <span class="math inline">\(0.5\)</span>.</p>
<p>On the high level, we divide <span class="math inline">\(m\)</span> into intervals of powers of 2. When <span class="math inline">\(X\)</span> falls into an interval, we return a binary number that is within the range of that interval. Specifically, we rewrite <span class="math display">\[
m = 2^{d_k} + 2^{d_{k - 1} } + ... + 2^{d_1}
\]</span></p>
<p>where <span class="math inline">\(\lfloor \mathbf{H}[X] \rfloor = \lfloor \log_2 m \rfloor = d_k &gt; d_{k - 1} &gt; ... &gt; d_1 \ge 0\)</span> are the indexes of non-zero bits in the binary representation of <span class="math inline">\(m\)</span>. The extraction is as follows:</p>
<blockquote>
<p><strong><em>Algorithm 1.</em></strong></p>
<ol type="1">
<li>for <span class="math inline">\(i \leftarrow k\)</span> <em>down-to</em> <span class="math inline">\(1\)</span> <em>do</em>:<br />
</li>
<li><span class="math inline">\(\quad\)</span> <em>if</em> <span class="math inline">\(X &lt; 2^{d_i}\)</span> <em>then</em>:<br />
</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(\quad\)</span> <em>return a <span class="math inline">\(d_i\)</span>-bit binary representation of</em> <span class="math inline">\(X\)</span>;<br />
</li>
<li><span class="math inline">\(\quad\)</span> <em>else:</em><br />
</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(\quad\)</span> <span class="math inline">\(X \leftarrow X - 2^{d_i}\)</span>;</li>
</ol>
</blockquote>
<p><strong>Example (<span class="math inline">\(m = 13\)</span>).</strong> <em>It is interesting to note that when <span class="math inline">\(X = 12\)</span>, our proposed extraction method return no random bit, as <span class="math inline">\(0 &lt; 2^0\)</span>. In this case our proposed method has wasted some randomness.</em></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/ExtractingRandomBits.png" /></p>
<p><em>Question to ponder: Come up with some method that uses <span class="math inline">\(n\)</span> i.i.d. <span class="math inline">\(X\)</span>'s to extract binary string with expected length close to <span class="math inline">\(n \mathbf{H}[X]\)</span>.</em></p>
<p>Let <span class="math inline">\(s\)</span> be the binary string returned by Algorithm 1.</p>
<p><strong><em>Theorem 1</em></strong>. <span class="math display">\[
\lfloor \log_2 m \rfloor - 1 \le \mathbf{E}[|s|] \le \log_2 m
\]</span></p>
<p><em>Proof.</em> It is easy to see that <span class="math display">\[
  \mathbf{E}[|s| ] = \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } 
\]</span> which depends on <span class="math inline">\(m\)</span>. To show the upper bound, <span class="math display">\[
\begin{array}{lll}
    \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } - \log_2 m
    = \sum_{i = 1}^k \frac{2^{d_i} }{m } \log_2 \frac{2^{d_i} }{m } \le 0
\end{array}
\]</span> The inequality follows from <span class="math inline">\(\log_2 \frac{2^{d_i} }{m } \le 0\)</span> for <span class="math inline">\(1 \le i \le k\)</span>.</p>
<p>To show the lower bound, define the function <span class="math display">\[
f(m) = \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } 
\]</span></p>
<p>We prove by induction on <span class="math inline">\(m\)</span> that <span class="math inline">\(f(m) \ge \lfloor \log_2 m \rfloor - 1 = d_k - 1\)</span>. The cases holds trivially when <span class="math inline">\(m = 1\)</span>. Now suppose it holds for all integer less than <span class="math inline">\(m\)</span>. We prove that it holds for <span class="math inline">\(m\)</span>.</p>
<p>Case 1. <span class="math inline">\(m = 2^{d_k}\)</span>. Then <span class="math inline">\(f(m) = d_k = \log_2 m\)</span>.</p>
<p>Case 2. <span class="math inline">\(m &gt; 2^{d_k}\)</span>. Now<br />
<span class="math display">\[
\begin{aligned}
    f(m) &amp;= \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } \\
    &amp;= d_k \frac{2^{d_k} }{m} + \left( \sum_{i = 1}^{k - 1} d_i \frac{2^{d_i} }{m - 2^{d_k}  }  \right) \frac{m - 2^{d_k} }{m} \\
\end{aligned}
\]</span></p>
<p>But <span class="math inline">\(\sum_{i = 1}^{k - 1} 2^{d_i} = m - 2^{d_k}\)</span>. It follows by induction <span class="math display">\[
\sum_{i = 1}^{k - 1} d_i \frac{2^{d_i} }{m - 2^{d_k}  } = f(m - 2^{d_k}) \ge d_{k - 1} - 1
\]</span></p>
<p>Finally, <span class="math display">\[
\begin{aligned}
    f(m)
    &amp;\ge d_k \frac{2^{d_k} }{m} + \left( d_{k - 1} -1 \right) \frac{m - 2^{d_k} }{m} \\
    &amp;= d_k +  \left( d_{k - 1}- d_k -1 \right)  \frac{m - 2^{d_k} }{m} \\
    &amp;= d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( 1 - \frac{ 2^{d_k} }{m} \right)\\
    &amp;\ge  d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( 1 - \frac{ 2^{d_k} }{2^{d_k} + 2^{d_{k - 1} } } \right)\\
    &amp;=  d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( \frac{ 2^{d_{k - 1} } }{2^{d_k} + 2^{d_{k - 1} } } \right)\\
    &amp;=  d_k -  \left( \frac{ d_k  - d_{k - 1} + 1  }{2^{d_k - d_{k - 1} } + 1 } \right)\\     
    &amp;\ge  d_k -  \left( \frac{ d_k  - d_{k - 1} + 1  }{2^{d_k - d_{k - 1} } } \right)\\
    &amp;\ge d_k - 1
\end{aligned}
\]</span></p>
<p>The last inequality holds since <span class="math inline">\(d_{k - 1} \le d_k - 1\)</span> and the function <span class="math inline">\(f(x) = \frac{x + 1}{2^x}\)</span> decreases when <span class="math inline">\(x \ge 1\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h5 id="binary-strings">Binary Strings</h5>
<p>To illustrate the idea of entropy, we study another example of random bit extraction.</p>
<p><strong><em>Problem.</em></strong> <em>Let <span class="math inline">\(t\)</span> be an <span class="math inline">\(n\)</span>-bit binary string each bit of which is interpreted as the outcome of independent bias coin flip that comes up head probability <span class="math inline">\(p \le 0.5\)</span>. Given <span class="math inline">\(t\)</span> as input, we want to output a binary string <span class="math inline">\(s\)</span> (not necessary of length <span class="math inline">\(n\)</span>) of independent unbiased random bits.</em></p>
<p><strong><em>Theorem.</em></strong> There is an extraction algorithm, such that,<br />
1. <span class="math inline">\(\forall \delta \in (0, 1)\)</span>, <span class="math inline">\(\exists N &gt; 0\)</span>, <span class="math inline">\(s.t.\)</span>, <span class="math inline">\(\forall n \ge N\)</span>, <span class="math inline">\(\mathbf{E}[|s|] \ge (1 - \delta) n\mathbf{H}(p)\)</span>.<br />
2. <span class="math inline">\(\forall n &gt; 0\)</span>, <span class="math inline">\(\mathbf{E}[|s|] \le n\mathbf{H}(p)\)</span>.</p>
<p>The theorem states that the expected length of <span class="math inline">\(s\)</span> approximates <span class="math inline">\(n\mathbf{H}(p)\)</span>. Equivalently, <span class="math display">\[
\lim_{n \rightarrow \infty} \frac{\mathbf{E}[|s|]}{n} = \mathbf{H}(p)
\]</span></p>
<p><strong><em>Proof.</em></strong> Define the random variable <span class="math inline">\(Z\)</span> to be the number of ones in <span class="math inline">\(t\)</span>. For any integer <span class="math inline">\(k \ge 0\)</span>, there are <span class="math inline">\(\binom{n}{k}\)</span> strings with <span class="math inline">\(k\)</span> ones. We can map these strings to <span class="math inline">\(\{0, 1, ..., \binom{n}{k} - 1 \}\)</span>. Conditioned on <span class="math inline">\(Z = k\)</span>, the mapping of <span class="math inline">\(t\)</span> is uniform on <span class="math inline">\(\{0, 1, ..., \binom{n}{k} - 1 \}\)</span>. We can use Algorithm 1 to return a binary string <span class="math inline">\(s\)</span>. Now its expected length is <span class="math display">\[
\mathbf{E}[|s|] = \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k]
\]</span></p>
<p>Further, by Hoeffding's inequality, it holds that for <span class="math inline">\(0 &lt; \epsilon &lt; p\)</span>, <span class="math display">\[
\Pr[ |Z - np | \ge n\epsilon] \le \exp\left(- 2 n \epsilon^2 \right)
\]</span></p>
<p>By setting the failure probability <span class="math inline">\(\exp(-2 n \epsilon^2) = \delta / 2\)</span>, we get <span class="math inline">\(\epsilon = \sqrt{\frac{\ln \frac{2}{\delta} }{2n} }\)</span>.</p>
<p>Let <span class="math inline">\(LB = \lceil np - n\epsilon \rceil\)</span> and <span class="math inline">\(UB = \lfloor np + n\epsilon \rfloor\)</span>. Observing that <span class="math inline">\(Z\)</span> is an integer, we have <span class="math display">\[
\begin{array}{rl}  
    \Pr[LB \le Z \le UB] &amp;= \Pr[np - n\epsilon \le Z \le np + n\epsilon] \\
    &amp;\ge 1 - \delta/2
\end{array}
\]</span></p>
<p>As <span class="math inline">\(p \le 0.5\)</span>, it holds that <span class="math display">\[
n = \lceil n/2 - n\epsilon \rceil + \lfloor n/2 + n\epsilon \rfloor \ge \lceil np - n\epsilon \rceil + \lfloor np + n\epsilon \rfloor
\]</span></p>
<p>Therefore, <span class="math display">\[
LB \le UB \le n - LB \\
\]</span></p>
<p>Since <span class="math inline">\(\binom{n}{k}\)</span> increase for <span class="math inline">\(k &lt; n / 2\)</span> and decrease <span class="math inline">\(k &gt; n / 2\)</span>, it holds that <span class="math display">\[
\binom{n}{LB} \le \binom{n}{UB}
\]</span></p>
<p>For <span class="math inline">\(LB \le k \le UB\)</span>, we have <span class="math display">\[
\binom{n}{LB} \le \binom{n}{k} 
\]</span></p>
<p>Define <span class="math inline">\(q = \frac{ LB }{n}\)</span>, <span class="math display">\[
\binom{n}{LB}  \ge \frac{1}{n + 1} 2^{n \mathbf{H}( \frac{ LB }{n} ) } \ge \frac{1}{n + 1} 2^{n\mathbf{H}(p - \epsilon)} 
\]</span></p>
<p>By <strong><em>Theorem 1</em></strong>, we have <span class="math display">\[
\begin{aligned}
    \mathbf{E}[|s|] &amp;= \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
    &amp;\ge \sum_{k = LB}^{UB} \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
    &amp;\ge \sum_{k = LB }^{UB } \left( \left\lfloor \log_2 \binom{n}{k} \right\rfloor - 1 \right) \binom{n}{k} p^k (1 - p)^{n - k}  \\
    &amp;\ge \sum_{k = LB }^{UB } \left( \left\lfloor \log_2  \binom{n}{ LB  } \right\rfloor - 1 \right) \binom{n}{k} p^k (1 - p)^{n - k}  \\
    &amp;=  \left( \left\lfloor \log_2  \binom{n}{ LB  } \right\rfloor - 1 \right) \Pr[LB \le Z \le UB] \\
    &amp;\ge \left( n \mathbf{H}(p - \epsilon)  - \log_2 (n + 1) \right) (1- \delta / 2) \\
    &amp;=  \left( \frac{\mathbf{H}(p - \epsilon)}{\mathbf{H}(p) }  - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) (1- \delta / 2 ) \cdot n \cdot \mathbf{H}(p) \\
    &amp;=  \left( \frac{\mathbf{H} \left(p - \sqrt{\frac{\ln \frac{2}{\delta} }{2n} } \right) }{\mathbf{H}(p) }  - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) (1- \delta / 2 ) \cdot n \cdot \mathbf{H}(p) \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\exists N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n &gt; N\)</span>, <span class="math inline">\(\left( \frac{\mathbf{H} \left(p - \sqrt{\frac{\ln \frac{2}{\delta} }{2n} } \right) }{\mathbf{H}(p) } - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) \ge (1 - \delta / 2)\)</span>, which finishes the proof of lower bound.</p>
<p>As for the upper bound, observe that <span class="math display">\[
\binom{n}{k} p^k ( 1- p)^{n - k} \le 1
\]</span></p>
<p>Therefore, <span class="math inline">\(\binom{n}{k} \le p^{-k} ( 1- p)^{-(n - k) }\)</span>. Thus, we have <span class="math display">\[
\begin{aligned}
       \mathbf{E}[|s|] &amp;= \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
       &amp;\le \sum_{k = 1}^n \left( \log_2 \binom{n}{k} \right) \binom{n}{k} p^k (1 - p)^{n - k} \\
       &amp;\le \sum_{k = 1}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k (1 - p)^{n - k} } 
\end{aligned}
\]</span></p>
<p>The last term is exactly the entropy of the string. By independence of the bits, we know that it is equal to <span class="math inline">\(n \mathbf{H}(p)\)</span>.</p>
<p>Remark: we may also prove it algebraically. <span class="math display">\[
\begin{aligned}
       \mathbf{E}[|s|] 
       &amp;\le \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k (1 - p)^{n - k} } \\
       &amp;= \sum_{k = 1}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k } + \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ (1 - p)^{n - k} } \\
       &amp;= \left( p \log_2 \frac{1}{p} \right) \sum_{k = 1}^n  \binom{n}{k} k p^{k - 1} (1 - p)^{n - k}  \\
       &amp;\quad +  \left( ( 1- p) \log_2 \frac{1}{ 1 - p } \right) \sum_{k = 0}^{n - 1} \binom{n}{k} (n - k) p^k (1 - p)^{n - k - 1} \\
       &amp;= \left( p \log_2 \frac{1}{p} \right) [(x + (1 - p))^n]_{x = p}&#39;  +  \left( (1 - p) \log_2 \frac{1}{ 1 - p } \right) [(p + x)^n]_{x = 1-p}&#39; \\
       &amp;= n \mathbf{H}(p)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="reference.">Reference.</h4>
<p>[1] S. Har-Peled, “Chapter 26 Entropy, Randomness, and Information,” p. 6.<br />
[2] M. Mitzenmacher and E. Upfal, Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/30/Integer-Shortest-Path/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/30/Integer-Shortest-Path/" class="post-title-link" itemprop="url">Integer Shortest Path</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-30 11:15:49" itemprop="dateCreated datePublished" datetime="2020-05-30T11:15:49+10:00">2020-05-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-18 13:01:58" itemprop="dateModified" datetime="2020-06-18T13:01:58+10:00">2020-06-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Single source shortest path is one of most popular the topics taught in a introductory algorithm course. Given a graph <span class="math inline">\(G = \left&lt; V, E \right&gt;\)</span> with <span class="math inline">\(n\)</span> vertices and <span class="math inline">\(m\)</span> edges, the famous Dijkstra's algorithm has runtime <span class="math inline">\(O(m \log n)\)</span> by using a binary heap. When more advanced heap such such Fibonacci heap is deployed, the runtime can be improved to <span class="math inline">\(O(m + n \log n)\)</span>. Most introductory courses are satisfied with the above solutions without going further.</p>
<p>Here we explore the topic by considering a special case where all edge weights are non-negative integers within the set <span class="math inline">\(\{0, 1, 2, ..., C\}\)</span>. We develop new heaps incrementally and we will finally see that Dijkstra algorithm can do substantially better than <span class="math inline">\(O(m + n \log n)\)</span>.</p>
<h4 id="time-om-nc-space-onc.">Time: <span class="math inline">\(O(m + nC)\)</span>, Space: <span class="math inline">\(O(nC)\)</span>.</h4>
<p>As a warm up, we demonstrate an algorithm with runtime <span class="math inline">\(O(m + nC)\)</span> and space overhead <span class="math inline">\(O(nC)\)</span>. Here we exclude the space <span class="math inline">\(O(m + n)\)</span> required for storing the graph to simplify the discussion and comparison between the algorithms.</p>
<p>The idea is very simple. We know that the possible distance from any vertex to the source vertex (denoted as <span class="math inline">\(s\)</span>) is no less than <span class="math inline">\(nC\)</span>. It suffices to keep an array <span class="math inline">\(A_1\)</span> with length <span class="math inline">\(nC\)</span>, such that <span class="math inline">\(A_1[i]\)</span> maintains the set of vertices with temporary distance <span class="math inline">\(i\)</span>. In this context, we call <span class="math inline">\(A_1[i]\)</span> a bucket and the array <span class="math inline">\(A_1\)</span> itself the buckets.</p>
<p>Initially all <span class="math inline">\(A_1[i]\)</span>'s are empty except that <span class="math inline">\(A_1[0]\)</span> contains the source vertex. The algorithm iterates over <span class="math inline">\(A_1\)</span> to find an unmarked vertex with the minimum distance, marked it, update its neighbors' distances and move them to the corresponding buckets.</p>
<p>Each vertex is inserted into <span class="math inline">\(A_1\)</span> once. There are at most <span class="math inline">\(m\)</span> updates of vertices' distances. As the minimum distance of the unmarked vertices is monotonically increasing, we scan the array <span class="math inline">\(A_1\)</span> only once. Summing over the cost we get the <span class="math inline">\(O(m + nC)\)</span> time complexity bound.</p>
<h4 id="time-om-nc-space-oc.">Time: <span class="math inline">\(O(m + nC)\)</span>, Space: <span class="math inline">\(O(C)\)</span>.</h4>
<p>The space usage of the above algorithm can be improve to <span class="math inline">\(O(C)\)</span>. Relabel the vertices as <span class="math inline">\(s = v_0, v_1, v_2, ..., v_n\)</span> according to the order they are marked.</p>
<p>Suppose that Dijkstra's algorithm uses an array <span class="math inline">\(d\)</span> to record the vertices' distances to <span class="math inline">\(s\)</span>. Initially, <span class="math inline">\(d(s) = 0\)</span> and <span class="math inline">\(d(v) = \infty\)</span> for <span class="math inline">\(\forall v \neq s\)</span>. Now, consider the moment that <span class="math inline">\(k\)</span> (<span class="math inline">\(0 \le k &lt; n\)</span>) vertices have been marked. For <span class="math inline">\(i &gt; k\)</span>, the value of <span class="math inline">\(d(v_i)\)</span> is either</p>
<ol type="1">
<li><p><span class="math inline">\(d(v_i) = \infty\)</span>, if <span class="math inline">\(v_i\)</span> is not adjacent to any vertex <span class="math inline">\(v_j\)</span> for <span class="math inline">\(j \le k\)</span>. In this case <span class="math inline">\(v_i\)</span> is not in the array <span class="math inline">\(A_1\)</span>.</p></li>
<li><p><span class="math inline">\(d(v_i) = d(v_j) + w_{i,j}\)</span> for some <span class="math inline">\(j \le k\)</span>, where <span class="math inline">\(w_{i,j}\)</span> is the edge weight between <span class="math inline">\(v_i\)</span> and <span class="math inline">\(v_j\)</span>.</p></li>
</ol>
<p>By the property of Dijkstra's algorithm, at this moment it also holds that <span class="math display">\[
0 = d(s) = d_(v_0) \le d(v_1) \le d(v_2) ... \le d(v_k)
\]</span></p>
<p>Therefore, for <span class="math inline">\(i &gt; k\)</span>, either <span class="math inline">\(v_i\)</span> is not in <span class="math inline">\(A_1\)</span> or <span class="math display">\[
d(v_i) \le d(v_j) + w_{i,j} \le d(v_k) + C
\]</span></p>
<p>All un-marked vertices that are in <span class="math inline">\(A_1\)</span> must be in the range of <span class="math display">\[
A_1[d(v_k) ... d(v_k) + C]
\]</span></p>
<p>In general, let <span class="math inline">\(\mu\)</span> be the distance of the least marked vertex. It suffices to maintain a window of <span class="math inline">\(A_1[\mu...\mu+C]\)</span>. We can implement this by initializing <span class="math inline">\(A_1\)</span> with size <span class="math inline">\(C+1\)</span> and use it in a wrap-around manner.</p>
<h4 id="time-om-n-sqrt-c-space-oc.">Time: <span class="math inline">\(O(m + n \sqrt C)\)</span>, Space: <span class="math inline">\(O(C)\)</span>.</h4>
<p>In previous section, we successfully reduce the size of <span class="math inline">\(A_1\)</span> to <span class="math inline">\(1 + C\)</span>. But the runtime remains <span class="math inline">\(O(m + nC)\)</span>. To motivate the algorithm discussed in this section, we first provide an alternative view of this complexity:</p>
<ol type="1">
<li><p>We update the vertices' distance and move the vertices between buckets, which has cost <span class="math inline">\(O(m)\)</span>;</p></li>
<li><p>At each iteration of Dijkstra's algorithm, in the worst case, we need to scan the entire array of <span class="math inline">\(A_1\)</span> to find an unmarked vertex with minimum distance, which takes <span class="math inline">\(O(C)\)</span> times. This is repeated <span class="math inline">\(n\)</span> times.</p></li>
</ol>
<p>Where could we have wasted our time? Do we really have spend <span class="math inline">\(O(C)\)</span> time to find the unmarked vertex with minimum distance?</p>
<p>We can improve this to <span class="math inline">\(O(\sqrt C)\)</span> by using two-level buckets. We break the buckets in <span class="math inline">\(A_1\)</span> into <span class="math inline">\(\sqrt { C + 1}\)</span> blocks, each of size <span class="math inline">\(\sqrt { C + 1}\)</span> (assume here <span class="math inline">\(\sqrt { C + 1}\)</span> is an integer for simplicity). For each block, we use one flag bit to indicate whether this block is empty. This results in <span class="math inline">\(\sqrt { C + 1}\)</span> flags bits, which are stored in bit array <span class="math inline">\(A_2\)</span>. From now on,</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/1.png" /></p>
<p>Now, to find an unmarked vertex with minimum distance, we scan <span class="math inline">\(A_2\)</span> to find the first non-zero bit then the corresponding bucket in <span class="math inline">\(A_1\)</span>, which takes <span class="math inline">\(O(\sqrt C)\)</span> time. Totaling the runtime, we obtain a bound of <span class="math inline">\(O(m + n \sqrt C)\)</span>.</p>
<p><strong><em>Question to ponder: calculate the size of <span class="math inline">\(\sqrt C\)</span> for some values of <span class="math inline">\(C\)</span> which you consider reasonable in real applications.</em></strong></p>
<h4 id="time-o-m-log_-fracmn-c-space-oc.">Time: <span class="math inline">\(O( m \log_{ \frac{m}{n} } C)\)</span>, Space: <span class="math inline">\(O(C)\)</span>.</h4>
<p>It is natural to extend the idea to <span class="math inline">\(3\)</span>-level buckets. Suppose the block size if <span class="math inline">\(\Delta\)</span>. Then the third level contains a single bit block of size <span class="math inline">\(\Delta\)</span>, the second level contains <span class="math inline">\(\Delta\)</span> bit blocks (with total size <span class="math inline">\(\Delta^3\)</span>), and the first level contains the buckets <span class="math inline">\(A_1\)</span> of size <span class="math inline">\(\Delta^3 = C + 1\)</span>. It follows that <span class="math inline">\(\Delta = (C + 1)^\frac{1}{3}\)</span> and the time complexity is <span class="math inline">\(O(m + n C^\frac{1}{3} )\)</span>.</p>
<p>We may use four level buckets, in which <span class="math inline">\(\Delta = (C + 1)^\frac{1}{4}\)</span> and the time complexity is <span class="math inline">\(O(m + n C^\frac{1}{4})\)</span>.</p>
<p><strong><em>Question to ponder: calculate the size of <span class="math inline">\(C^\frac{1}{3}\)</span> and <span class="math inline">\(C^\frac{1}{4}\)</span> for some values of <span class="math inline">\(C\)</span> which you consider reasonable in real applications.</em></strong></p>
<p>Does the analysis carry for <span class="math inline">\(k\)</span>-level buckets for general value of <span class="math inline">\(k\)</span>? No. We can no longer consider the <span class="math inline">\(k\)</span> as a constant and have to take into consider the overhead of insertion, decrease-key and deletion explicitly.</p>
<p>For a <span class="math inline">\(k\)</span>-level bucket structure with bucket size <span class="math inline">\(\Delta = (1 + C)^\frac{1}{k} \ge 2\)</span>,</p>
<ol type="1">
<li><p>insertion takes <span class="math inline">\(O(k)\)</span> time,</p></li>
<li><p>decrease-key takes <span class="math inline">\(O(k)\)</span> time,</p></li>
</ol>
<p>as we need to maintain the indicators bits in <span class="math inline">\(2...k\)</span> levels. Finally,</p>
<ol start="3" type="1">
<li>delete-min takes <span class="math inline">\(O(k \Delta)\)</span> time,</li>
</ol>
<p>as we need to scan <span class="math inline">\(k\)</span> buckets, each of size <span class="math inline">\(\Delta\)</span>, to find the min element and delete it.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/2.png" /></p>
<p>The time complexity now becomes <span class="math display">\[
O(mk + n k \Delta) = O( mk + nkC^\frac{1}{k})
\]</span></p>
<p>Balancing the two terms, we get <span class="math inline">\(\frac{1}{k} = \frac{ \log \frac{m}{n} }{ \log C}\)</span>, i.e., <span class="math inline">\(k = \log_{\frac{m}{n} } C\)</span>. To summarize, the time is <span class="math display">\[
O(m \log_{\frac{m}{n} } C)
\]</span></p>
<p>and the space overhead is <span class="math display">\[
O\left( \sum_{i = 0}^{k - 1} C / \Delta^i \right) = O(C)
\]</span></p>
<p><strong><em>Question to ponder: calculate the size of <span class="math inline">\(\log_{\frac{m}{n} } C\)</span> for some values of <span class="math inline">\(C\)</span> which you consider reasonable in real applications.</em></strong></p>
<h4 id="time-om-n-fraclog-clog-log-c-space-o-fraclog-clog-log-c-2-.">Time: <span class="math inline">\(O(m + n \frac{\log C}{\log \log C})\)</span>, Space: <span class="math inline">\(O( (\frac{\log C}{\log \log C} )^2 )\)</span>.</h4>
<p>The core idea underlying the k-level bucket structure discussed in the previous section is that every integer <span class="math inline">\(w\)</span> in <span class="math inline">\([0, C]\)</span> can be written as a base-<span class="math inline">\(\Delta\)</span> number with length at most <span class="math inline">\(k\)</span>: <span class="math display">\[
w = \sum_{i = 0}^{k - 1} w_i \cdot \Delta^i
\]</span></p>
<p>where <span class="math inline">\(w_i \in [0, \Delta)\)</span>. For convenience, we write it as <span class="math display">\[
w = (w_{k - 1}, w_{k - 2}, ..., w_0)_\Delta
\]</span></p>
<p>Now the meaning of the <span class="math inline">\(k\)</span>-level buckets is clear. The <span class="math inline">\(k^{th}\)</span> level partitions the <span class="math inline">\(w\)</span>'s according to the value of <span class="math inline">\(w_{k - 1}\)</span>. The <span class="math inline">\((k - 1)^{th}\)</span> level further partitions the <span class="math inline">\(w\)</span>'s according to the value of <span class="math inline">\(w_{k - 2}\)</span>, and so on. The path from the top level to bottom level is uniquely determined by the sequence <span class="math inline">\((w_{k - 1}, w_{k - 2}, ..., w_0)_\Delta\)</span>. Indeed, this is a special case of <strong><em>trie</em></strong>, which has alphabet size <span class="math inline">\(\Delta\)</span> and contains only sequence of length <span class="math inline">\(k\)</span>.</p>
<p>But we may still waste some work. Remember that our goal for designing the data structure is to find the min element. Therefore, it suffices to distinguish the min element from the rest. We should not waste our effort in determining the relative order between the non-min elements.</p>
<p>In the following example, the min elements <span class="math inline">\(v_1, v_2\)</span> fall into the subtree of the first bucket on the <span class="math inline">\(k^{th}\)</span> level while <span class="math inline">\(v_{n - 1}, v_n\)</span> fall into the last one. As <span class="math inline">\(v_{n - 1}, v_n\)</span> are not min elements, it is unnecessary to maintain a subtree to separate them. Worse still, if later decrease-key is applied to either <span class="math inline">\(v_{n -1}\)</span> or <span class="math inline">\(v_n\)</span>, and if the new key has to be moved to a new bucket in the <span class="math inline">\(k^{th}\)</span> level, the effort we have spent in constructing the sub-tree is completely useless.</p>
<p>Instead, we can create an auxiliary set to the last bucket on level <span class="math inline">\(k\)</span>, to pus <span class="math inline">\(v_{n - 1}, v_n\)</span> into the set directly. Similar idea apply to <span class="math inline">\(v_3\)</span> in the example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/3.png" /></p>
<p>The new structure we get is as follows:</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/4.png" /></p>
<p>In general, we may not even has a <span class="math inline">\(k\)</span>-level trie. In the example below, we can already determine the min-element <span class="math inline">\(v_1\)</span> at some level <span class="math inline">\(i &gt; 1\)</span>. Why should we bother to expand the buckets further? Note that this idea is similar to that of trie compression used in <strong><em>Patricia trie</em></strong>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/5.png" /></p>
<p>We can be even more lazy. We expand a bucket only when it is necessary. See the example below as an example. In this example, <span class="math inline">\(C = 15\)</span> and <span class="math inline">\(\Delta = 4\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example1-2.png" /></p>
<p>Now, to delete-min, we need to determine the order of <span class="math inline">\(v_1, v_2\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example3.png" /></p>
<p>Inserting a new element follows the current structure, until it finds a bucket to fall in.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example4.png" /></p>
<p>Delete-min begin the search from the lowest level non-empty bucket block.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example5.png" /></p>
<p>After delete-min, we may need to delete the empty bucket blocks recursively. For each bucket block, we maintain counter for the number of non-empty buckets. This allows us to delete empty bucket blocks in <span class="math inline">\(O(k)\)</span> time.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example6.png" /></p>
<p>We decrease the key of <span class="math inline">\(v_4\)</span> to 9. Note that we do not need to compare <span class="math inline">\(v_4\)</span> and <span class="math inline">\(v_5\)</span> for the moment. Bucket block counter is maintained when <span class="math inline">\(v_4\)</span> is moved to another bucket.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example7.png" /></p>
<p>Delete-min is invoked again. We need to expand the bucket containing <span class="math inline">\(v_5\)</span> and <span class="math inline">\(v_4\)</span> to distinguish them.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example8.png" /></p>
<p>We continue to insert a new element <span class="math inline">\(v_6 = 12\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example9.png" /></p>
<p>We decrease the key of <span class="math inline">\(v_6\)</span> to <span class="math inline">\(10\)</span>. It is pushed down by one level.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example10.png" /></p>
<p>We are now prepared to analyze the amortized cost of these operations:</p>
<ol type="1">
<li><p>Insert: we pay an <span class="math inline">\(O(k)\)</span> amortized cost of insertion. Any subsequent pushed down operation on this element is charged to its insertion. Note that an element can be push-down by at most <span class="math inline">\(k\)</span> levels.</p></li>
<li><p>Decrease-key: we pay an <span class="math inline">\(O(1)\)</span> amortized cost of decrease-key operation. The push-down operation is charged to its insertion.</p></li>
<li><p>Delete-min: to find the min-element, we scan the bucket block in the lowest level to find the first non-empty bucket. This has time <span class="math inline">\(\Delta\)</span>. If the bucket found contains multiple elements, we perform an expansion and push the elements down, the cost of which is charged to insertion of these elements. Finally, we may need to delete empty bucket blocks recursively, which has cost <span class="math inline">\(O(k)\)</span>.</p></li>
</ol>
<p>Therefore, the Dijkstra's algorithm with this structure has runtime <span class="math inline">\(O(m + n(k + \Delta))\)</span>, which is minimized when <span class="math display">\[
k = \Delta = (C + 1)^\frac{1}{k}
\]</span></p>
<p>Solving the equation gives <span class="math inline">\(k = O(\frac{\log C}{\log \log C} )\)</span>.</p>
<h4 id="time-o-m-n-sqrtlog-c-space-osqrtlog-c-cdot-2sqrtlog-c-.">Time: <span class="math inline">\(O( m + n \sqrt{\log C} )\)</span>, Space: <span class="math inline">\(O(\sqrt{\log C} \cdot 2^{\sqrt{\log C} })\)</span>.</h4>
<p>Beginning from the naïve approach, we have improved a lot on both time and space overhead. Amazingly, this is not the end of the story. To motivate what is possibly the inefficient part of <span class="math inline">\(O(m + n \frac{\log C}{\log \log C})\)</span> approach, look at the following example.</p>
<p>Suppose that <span class="math inline">\(C\)</span> is a large number. Initially the structure is empty. We insert an element <span class="math inline">\(v_1\)</span> followed by <span class="math inline">\(v_2\)</span> and they fall into the same bucket in the <span class="math inline">\(k^{th}\)</span> level. Next, we perform a delete-min operation. To distinguish, we expand the buckets all the way down to <span class="math inline">\(1^{st}\)</span> level.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Drawback.png" /></p>
<p>But since there are two elements, why don't we compare them directly and return the minimum one? In such case we avoid the unnecessary expansions.</p>
<p>We can extend the idea further. If the number of elements in a bucket is too few, we refuse to expand it. Instead we make of copy of the elements and construct a comparison based priority queue (e.g., Fibonacci heap) on them. Further, note that we are only interested in the possible min-element. Therefore the heap is constructed for only one bucket that contains the min-element.</p>
<p>We use a parameter <span class="math inline">\(t\)</span> (to be determined) to control this number. In particular, if the number is at most <span class="math inline">\(t\)</span>, we construct a comparison based priority queue. We perform expansion only if the number exceeds <span class="math inline">\(t\)</span>.</p>
<ol type="1">
<li><p>Insert: insert an element as before. If the element falls into the bucket that is associated with a priority, then also insert this element into the heap.</p></li>
<li><p>Decrease-key: if the element is in the bucket that is associated with a priority queue, perform decrease-key in the queue. Otherwise, perform decrease-key as before. If the element falls into the bucket that is associated with a priority queue, then also insert this element into the heap.</p></li>
<li><p>Delete-min:</p></li>
</ol>
<blockquote>
<ol type="1">
<li>Find the first non-empty bucket in the lowest level<br />
</li>
<li><strong><em>IF</em></strong> the bucket has more than <span class="math inline">\(t\)</span> elements <strong><em>THEN</em></strong><br />
</li>
<li><span class="math inline">\(\qquad\)</span> Expand the bucket<br />
</li>
<li><span class="math inline">\(\qquad\)</span> <strong><em>GO TO STEP 1</em></strong></li>
<li><strong><em>ELSE</em></strong></li>
<li><span class="math inline">\(\qquad\)</span> <strong><em>IF</em></strong> <span class="math inline">\(\nexists\)</span> queue <strong><em>THEN</em></strong><br />
</li>
<li><span class="math inline">\(\qquad\qquad\)</span> Construct a queue</li>
<li><span class="math inline">\(\qquad\)</span> Perform delete-min in the queue</li>
</ol>
</blockquote>
<p>Let <span class="math inline">\(I(t), D(t), X(t)\)</span> be the time needed to perform insert, decrease-key and delete-min operation in a comparison based priority queue respectively. Then it is obvious that the the structure we propose, the amortized time we need for insert and decrease-key is</p>
<ol type="1">
<li>Insert: <span class="math inline">\(O(k + I(t))\)</span>.</li>
<li>Decrease-key: <span class="math inline">\(O(D(t) + I(t))\)</span>.</li>
</ol>
<p>To analyze the time needed for delete-min is more complicated. It relies on the implementation of the following:</p>
<blockquote>
<p>Find the first non-empty bucket in the lowest level.</p>
</blockquote>
<p>We record the previous deleted min element <span class="math inline">\(\mu\)</span> (initially set to 0). Denote the <span class="math inline">\(\Delta\)</span>-base representation of <span class="math inline">\(\mu\)</span> as <span class="math display">\[
\mu = (\mu_{k - 1}, \mu_{k - 2}, ..., \mu_0)_\Delta
\]</span> Observe that the non-empty bucket block in the lowest level must contain <span class="math inline">\(\mu\)</span>. Denote this level as <span class="math inline">\(i\)</span>. To find the first non-empty bucket, we start scan from <span class="math inline">\(\mu_{i - 1}^{th}\)</span> bucket in the <span class="math inline">\(i^{th}\)</span> level. Therefore, each bucket block is only scanned once. Further, the buck block in the <span class="math inline">\(i^{th}\)</span> level is created from the <span class="math inline">\((i + 1)^{th}\)</span> level because it contains more than <span class="math inline">\(t\)</span> elements. We can charge the scanned cost to these <span class="math inline">\(t\)</span> elements, each with <span class="math inline">\(\frac{\Delta}{t}\)</span>. As there are <span class="math inline">\(k\)</span> level, a element could be charge at most <span class="math inline">\(k\)</span> times. It follows that the amortized cost of delete-min is given by</p>
<ol start="3" type="1">
<li>Delete-min: <span class="math inline">\(O(X(t) + \frac{k \Delta}{t})\)</span></li>
</ol>
<p>For Fibonacci heap, <span class="math inline">\(I(t) = D(t) = 1\)</span> and <span class="math inline">\(X(t) = \log t\)</span>. Therefore, we have runtime <span class="math display">\[
O \left( m + n \left[k + \log t + \frac{k \Delta}{t} \right] \right)
\]</span></p>
<p>To minimize it, we need to set <span class="math inline">\(k = \log t = \frac{k \Delta}{t}\)</span>. It holds that <span class="math inline">\(t = 2^k\)</span> and <span class="math inline">\(t = \Delta = C^\frac{1}{k}\)</span>, which implies that <span class="math inline">\(2^{k} = 2^{ \frac{\log C}{k} }\)</span> and <span class="math inline">\(k = \sqrt {\log C}\)</span>. The time is <span class="math display">\[
O(m + n \sqrt{\log C})
\]</span></p>
<p>and the space overhead is <span class="math inline">\(O(k \Delta) = O(\sqrt{\log C} \cdot 2^{\sqrt{\log C} })\)</span>.</p>
<h3 id="reference">Reference</h3>
<p>[1]. David R. Karger, MIT Advanced Algorithm, 2013.<br />
[2]. B. V. Cherkassky, A. V. Goldberg, and C. Silverstein, “Buckets, Heaps, Lists, and Monotone Priority Queues,” SIAM J. Comput., vol. 28, no. 4, pp. 1326–1346, Jan. 1999</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/26/Splay-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/26/Splay-Tree/" class="post-title-link" itemprop="url">Splay Tree</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-26 16:19:41" itemprop="dateCreated datePublished" datetime="2020-05-26T16:19:41+10:00">2020-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-05 16:12:56" itemprop="dateModified" datetime="2020-06-05T16:12:56+10:00">2020-06-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Splay Tree [1] is a self-binary search tree that achieves <span class="math inline">\(O(\log n)\)</span> amortized time complexity for each operation. Unlike other balanced binary search trees, it does not require to maintain additional information to keep balance. Further, it has good property that more recently accessed nodes are more easily retrieved and achieves certain optimality even for an unknown sequence of operations.</p>
<h2 id="basic-operations">Basic Operations</h2>
<p>The philosophy of splay tree is very simple: just bring the last accessed node <span class="math inline">\(x\)</span> to the root of the tree, via a sequence of <em>rotations</em>. This brings the most recently visited nodes close to the root.</p>
<p>[Remark: For insertion, the last accessed node is the node inserted. For a successful search operation, the last accessed node is the node found. For an unsuccessful search/deletion, it is the predecessor or successor of the search node / deleted node. ]</p>
<p>The rotations that bring a node <span class="math inline">\(x\)</span> to the root is not unique. For example, the simplest one rotates <span class="math inline">\(x\)</span> with its parent until it becomes the root. However, we can easily find counter examples for this naïve approach. The rotations need to be carefully designed and are called <em>splays</em>. At each step the node being splayed is brought up at most 2 levels. It can be divided into three categories. Call <span class="math inline">\(x\)</span> the node being splayed, <span class="math inline">\(y\)</span> its parent and <span class="math inline">\(z\)</span> its grandparent (if they exist).</p>
<ol type="1">
<li>Root case: <span class="math inline">\(x\)</span> is the root. Do nothing.</li>
<li>The <em>Zig</em> case: <span class="math inline">\(y\)</span> is the root. We just perform <span class="math inline">\(rotate(x, y)\)</span>.
<ul>
<li>See the following figure. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/Zig.png" /></li>
</ul></li>
<li>Otherwise, <span class="math inline">\(x\)</span> has a grandparent <span class="math inline">\(z\)</span>. It is further divided into two categories:
<ol type="1">
<li><em>Zig-Zig</em> case: both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are left children or right children. We first perform <span class="math inline">\(rotate(y, z)\)</span> then <span class="math inline">\(rotate(x, y)\)</span>.<br />
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZig.png" /></li>
<li><em>Zig-Zag</em> case: <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are different children of their parents, i.e., <span class="math inline">\(x\)</span> is the left child and <span class="math inline">\(y\)</span> is the right child, or vice versa. We first perform <span class="math inline">\(rotate(x, y)\)</span> then <span class="math inline">\(rotate(y, z)\)</span>.<br />
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZag.png" /></li>
</ol></li>
</ol>
<p>The splay is performed repeatedly on <span class="math inline">\(x\)</span> until it reaches the root. It is also possible to perform equivalent operations in an top-down approach [1].</p>
<h2 id="time-analysis">Time Analysis</h2>
<p>There is no explicit control over the height of the tree. It is possible that the accessed node has depth <span class="math inline">\(\Omega(\log n)\)</span>. The intuition is that, however, the imbalanced situation is not easy to create and must accumulate from previous operations. The number such operations can not be too to create the imbalance. We can charge the cost of accessing the node with long depth to these operations. Intuitively, each previous operation is required to reserve some additional "energy" for the amount of imbalanced it creates, which is used later to fix the imbalance. When a long path is encountered, there should be enough "energy" to cover the search on the path and the splay of the access node.</p>
<p>It is left to give a quantitative scheme for keeping the "energy" and "using" the "energy". We need a few more definitions for a node <span class="math inline">\(x\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(T(x):\)</span> the subtree rooted at <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(w(x):\)</span> the weight associated with node <span class="math inline">\(x\)</span>. This weight is only used for analysis and is not kept explicitly by splay tree.</li>
<li><span class="math inline">\(s(x)=\sum_{y \in T(x)} w_y:\)</span> the sum of weights of nodes in <span class="math inline">\(T(x)\)</span>, which is referred to as the size of <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(r(x) \doteq \log r(x)\)</span>, the rank of <span class="math inline">\(x\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(T\)</span> be the splay tree. The potential function is defined as <span class="math display">\[
\Phi = \sum_{x \in T} r(x)
\]</span> which records the amount of "energy" in <span class="math inline">\(T\)</span>.</p>
<p>The amortized cost of an operation is defined as <span class="math display">\[
\text{amortized cost} = \text{actual cost} + \Delta \Phi
\]</span> where <span class="math inline">\(\Delta \Phi\)</span> is the change of the potential function.</p>
<p>Now, let <span class="math inline">\(r(x)\)</span> be the rank of <span class="math inline">\(x\)</span> before we performed a single splay operation on <span class="math inline">\(x\)</span> and <span class="math inline">\(r&#39;(x)\)</span> the one after. The <em>Access Lemma</em> states the <span class="math inline">\(\Phi\)</span> changes as follows:</p>
<blockquote>
<p><strong><em>Access Lemma</em></strong> For a single splay operation, the potential change is bounded by <span class="math display">\[ 3(r&#39;(x) - r(x)) - 2 \]</span> for <em>zig-zig</em> and <em>zig-zag</em> case and <span class="math display">\[ r&#39;(x) - r(x) \]</span> for the <em>zig</em> case.</p>
</blockquote>
<p>Before we prove this key lemma, we use it for the following theorem.</p>
<p><strong><em>Theorem.</em></strong> The amortized cost of the splaying a node <span class="math inline">\(x\)</span> to the root is <span class="math inline">\(O(1 + \log \frac{s(root) }{s(x) })\)</span>.</p>
<p><em>Proof.</em> To splay a node <span class="math inline">\(x\)</span> to the root, suppose that we have performed <span class="math inline">\(k\)</span> splays. Further, let <span class="math inline">\(r_i(x)\)</span> be the rank of <span class="math inline">\(x\)</span> after we performed the <span class="math inline">\(i^{th}\)</span> splay on <span class="math inline">\(x\)</span>. We have <span class="math inline">\(r_0(x) = \log s(x)\)</span> and <span class="math inline">\(r_k(x) = \log s(root)\)</span>. Observe that the actual cost of splay operation is <span class="math inline">\(2\)</span> for <em>zig-zig</em> and <em>zig-zag</em> case and 1 for the <em>zig</em> case. Hence, the amortized cost is <span class="math display">\[
\begin{aligned}
\text{amortized cost} 
&amp;\le \sum_{i = 1}^k 3(r_i(x) - r_{i - 1}(x)) +1 \\
&amp;= 3(r_k(x) - r_0(x)) + 1
\end{aligned}
\]</span> The added 1 results from the final possible <em>zig</em> operation.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>If we set <span class="math inline">\(w(x) = 1\)</span> for all nodes, then <span class="math inline">\(r_k(x) = \log n\)</span> and <span class="math inline">\(r_1(x) \ge \log 1 = 0\)</span>. Therefore, the amortized cost of splaying <span class="math inline">\(x\)</span> is bounded by <span class="math inline">\(O(\log n)\)</span>.</p>
<h3 id="proof-of-the-access-lemma.">Proof of The Access Lemma.</h3>
<h4 id="zig-zig"><strong><em>Zig-Zig</em></strong></h4>
<ul>
<li><p>Example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZig.png" /></p></li>
</ul>
<p>First, note that <span class="math display">\[
\begin{aligned}
        &amp;   \frac{1}{2} r&#39;(z) + \frac{1}{2} r(x)    \\
    =   &amp;   \frac{1}{2} \log s&#39;(z) + \frac{1}{2} \log s(x)   \\
    \le &amp;   \log \left[ \frac{s&#39;(z)}{2}  + \frac{ s(x) }{2} \right] \\
    &lt;   &amp;   \log \left[ \frac{s(z)}{2}  \right] \\
\end{aligned}
\]</span></p>
<p>The first inequality follows from that the function <span class="math inline">\(\log (\cdot )\)</span> is concave and the second one follows from <span class="math inline">\(s&#39;(z) + s(x) = s(z) - 1 &lt; s(z)\)</span>.</p>
<p>Therefore, <span class="math display">\[
r&#39;(z) + r(x) \le 2 \left[ r(z) - 1 \right]
\]</span></p>
<p>Now consider the potential change of the zig-zig operation: <span class="math display">\[
\begin{array}{rll}
        &amp;   r&#39;(x) + r&#39;(y) + r&#39;(z) - r(x) - r(y) - r(z) \\
    =   &amp;   r&#39;(y) + r&#39;(z) - r(x) - r(y) \\
    \le &amp;   r&#39;(x) + r&#39;(z) - r(x) - r(x) \\
    =   &amp;   [r&#39;(x) - r(x)]  +[r&#39;(z) + r(x)] - 2 r(x) \\
    \le &amp;   [r&#39;(x) - r(x)] +2[r(z) - 1] - 2 r(x) \\
    =   &amp;   3[r&#39;(x) - r(x)] - 2
\end{array}
\]</span></p>
<p>The first equality follows from <span class="math inline">\(r&#39;(x) = r(z)\)</span>, as <span class="math inline">\(s&#39;(x) = s(z)\)</span>. The first inequality holds since <span class="math inline">\(r(x) \le r(y)\)</span>.</p>
<h4 id="zig-zag"><strong><em>Zig-Zag</em></strong></h4>
<ul>
<li><p>Example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZag.png" /></p></li>
</ul>
<p>First, note that <span class="math display">\[
\begin{aligned}
        &amp;   \frac{1}{2} r&#39;(y) + \frac{1}{2} r&#39;(z)    \\
    =   &amp;   \frac{1}{2} \log s&#39;(y) + \frac{1}{2} \log s&#39;(z)   \\
    \le &amp;   \log \left[ \frac{s&#39;(y)}{2}  + \frac{ s&#39;(z) }{2} \right] \\
    &lt;   &amp;   \log \left[ \frac{s&#39;(x)}{2}  \right] \\
\end{aligned}
\]</span></p>
<p>The first inequality follows from that the function <span class="math inline">\(\log (\cdot )\)</span> is concave and the second one follows from <span class="math inline">\(s&#39;(y) + s&#39;(z) = s&#39;(x) - 1 &lt; s&#39;(x)\)</span>.</p>
<p>Therefore, <span class="math display">\[
r&#39;(y) + r&#39;(z) \le 2 \left[ r&#39;(x) - 1 \right]
\]</span></p>
<p>Now consider the potential change of the zig-zag operation: <span class="math display">\[
\begin{array}{rll}
        &amp;   r&#39;(x) + r&#39;(y) + r&#39;(z) - r(x) - r(y) - r(z) \\
    =   &amp;   r&#39;(y) + r&#39;(z) - r(x) - r(y) \\
    \le &amp;   r&#39;(y) + r&#39;(z) - r(x) - r(x) \\
    \le &amp;   2 \left[ r&#39;(x) - 1 \right] - 2 r(x) \\
    =   &amp;   2[r&#39;(x) - r(x)] - 2
\end{array}
\]</span></p>
<p>The first equality follows from <span class="math inline">\(r&#39;(x) = r(z)\)</span>, as <span class="math inline">\(s&#39;(x) = s(z)\)</span>. The first inequality holds since <span class="math inline">\(r(x) \le r(y)\)</span>.</p>
<h4 id="zig"><strong><em>Zig</em></strong></h4>
<ul>
<li><p>Example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/Zig.png" /></p></li>
</ul>
<p>Now consider the potential change of the zig-zag operation: <span class="math display">\[
\begin{array}{rll}
        &amp;   r&#39;(x) + r&#39;(y)  - r(x) - r(y)  \\
    =   &amp;   r&#39;(y)  - r(x)  \\
    \le &amp;   r&#39;(x)  - r(x) 
\end{array}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>We now show the implementation of basic BST operations and their amortized cost. Throughout the analysis we assume that <span class="math inline">\(w(x) = 1\)</span>.</p>
<h3 id="cost-of-insertion">Cost of Insertion</h3>
<p>Inserting a node <span class="math inline">\(x\)</span> is the same as that in a BST. Then we splay node <span class="math inline">\(x\)</span> to the root. We charge the cost of going down the tree by the cost of splaying <span class="math inline">\(x\)</span>, which has amortized cost <span class="math inline">\(O(\log n)\)</span>. It is left to analyze the change of <span class="math inline">\(\Phi\)</span>. Let <span class="math inline">\(root = x_1, x_2, ..., x_k, x_{k + 1} = x\)</span> be the root to <span class="math inline">\(x\)</span> path right after inserting <span class="math inline">\(x\)</span>. The insertion of <span class="math inline">\(x\)</span> increases <span class="math inline">\(s(x_1), s(x_2), ..., s(x_{k })\)</span> by 1, therefore, <span class="math display">\[
\begin{array}{llr}
\Delta \Phi &amp;= \sum_{i = 1}^k \log \frac{s(x_i) + 1 }{s(x_i)} \\
            &amp;= \log \prod_{i = 1}^k \frac{s(x_i) + 1 }{s(x_i)} \\
            &amp;\le \log \frac{s(x_1) + 1}{ s(x_k ) }
\end{array}
\]</span> The last inequality follows from that <span class="math inline">\(s(x_i) \ge s(x_{i + 1}) + 1\)</span> for <span class="math inline">\(i \in [k]\)</span>. Therefore, <span class="math inline">\(\Delta \Phi = O(\log n)\)</span> and the overall amortized cost of insertion is <span class="math inline">\(O(\log n)\)</span>.</p>
<p>Remark: <span class="math inline">\(key(x_{k})\)</span> is either the predecessor or successor of <span class="math inline">\(key(x)\)</span>.</p>
<h3 id="cost-of-search">Cost of Search</h3>
<p>Whether the search is successful or not, we splay the last accessed node to the root and charge the cost of going down the tree to the splay-operations. Therefore, the amortized cost is <span class="math inline">\(O(\log n)\)</span>.</p>
<p>Remark: in case of unsuccessful search, we find either the predecessor or successor of the search key.</p>
<h3 id="cost-of-deletion">Cost of Deletion</h3>
<p>As in a BST, deletion is a little bit tricky. Let <span class="math inline">\(x\)</span> be the node to delete and <span class="math inline">\(y\)</span> be its parent (if exists). There are 3 cases</p>
<ul>
<li><p>If <span class="math inline">\(x\)</span> is a leaf node, then we just delete it and splay <span class="math inline">\(y\)</span> to the root.</p></li>
<li><p>If <span class="math inline">\(x\)</span> has only one child, we replace <span class="math inline">\(x\)</span> with its child as the child of <span class="math inline">\(y\)</span>. Then we delete <span class="math inline">\(x\)</span> and splay <span class="math inline">\(y\)</span> to the root.</p></li>
</ul>
<p>The final case uses the first two cases as a sub-routine. We first discuss the amortized cost of the first two cases. The cost of going down to <span class="math inline">\(x\)</span> can be charged to the cost of splaying <span class="math inline">\(y\)</span> to the root, which has amortized cost <span class="math inline">\(O(\log n)\)</span>. Deleting the node decreases the potential function, which will only lower the overall amortized cost. Therefore, the amortized cost of deletion is bounded by <span class="math inline">\(O(\log n)\)</span>.</p>
<ul>
<li>If <span class="math inline">\(x\)</span> has two children, we find the predecessor node <span class="math inline">\(pred(x)\)</span> of <span class="math inline">\(x\)</span>, which is the node with largest key in <span class="math inline">\(x\)</span>'s left sub-tree. Note that <span class="math inline">\(pred(x)\)</span> has at most one child. Then we delete the node <span class="math inline">\(pred(x)\)</span>. This reduces to case 1 or case 2. Finally we replace the <span class="math inline">\(key(x)\)</span> with <span class="math inline">\(key(pred(x))\)</span>.</li>
</ul>
<p>In the following sections we analyze the properties of the splay tree on a sequence <span class="math inline">\(S\)</span> of <span class="math inline">\(m\)</span> operations.</p>
<h2 id="static-optimality">Static Optimality</h2>
<p>Suppose that <span class="math inline">\(S\)</span> consists of only successful search operations. Let $p_i &gt; 0 $ be the frequency that the <span class="math inline">\(i^{th}\)</span> element is accessed. Define a static binary search tree to be the one whose structure is fixed. In particular, it can not perform rotations.</p>
<p><strong><em>Theorem.</em></strong> (Static Optimality) The total cost of <span class="math inline">\(S\)</span> performed on the splay tree is at most a constant times the minimum possible cost of <span class="math inline">\(S\)</span> performed a static binary search tree, plus <span class="math inline">\(O(m )\)</span>.</p>
<p><em>Proof.</em> Let <span class="math inline">\(T^*\)</span> be the static binary search tree that minimizes the cost of <span class="math inline">\(S\)</span>. For the <span class="math inline">\(i^{th}\)</span> element, let <span class="math inline">\(l_i\)</span> be the depth of <span class="math inline">\(i\)</span> in <span class="math inline">\(T^*\)</span>, i.e. the number of nodes on the path from <span class="math inline">\(i\)</span> to the root, so <span class="math inline">\(l(root) = 1\)</span>. The total cost in <span class="math inline">\(T^*\)</span> is given by <span class="math inline">\(\sum_{i = 1}^n p_i \cdot m \cdot l_i\)</span>. We are going to prove that the cost for splay tree is <span class="math display">\[
O(m + \sum_{i = 1}^n p_i \cdot m \cdot l_i)
\]</span> Suppose that <span class="math inline">\(T^*\)</span> has maximum depth <span class="math inline">\(d_{max}(T^*)\)</span>. We claim that for any <span class="math inline">\(k &lt; d_{max}(T^*)\)</span>, <span class="math inline">\(T^*\)</span> must contain exactly <span class="math inline">\(2^k\)</span> nodes with depth <span class="math inline">\(k\)</span>. Otherwise, we can cut some leaf node from its parent and pad it into the <span class="math inline">\(i^{th}\)</span> level. This will only decrease the access cost, contradicting <span class="math inline">\(T^*\)</span> being the optimal static binary tree. Then, <span class="math inline">\(d_{max}(T^*) \le \lfloor \log_2 n \rfloor + 1\)</span> and <span class="math display">\[
\sum_{i = 1}^n 3^{-l_i} \le \frac{1}{2} \sum_{k = 1}^{ \lfloor \log_2 n \rfloor + 1 } (\frac{2}{3})^k \le 1
\]</span> To investigate the time for splay tree, we choose a different weight than earlier and just set <span class="math inline">\(w_i = 3^{-l_i}\)</span>. Now the amortized cost of accessing <span class="math inline">\(i^{th}\)</span> element is given by <span class="math display">\[
O(1 + \log_2 \frac{s(root)}{s(i)}) = O(1 + \log_2 \frac{1}{3^{-l_i}}) = O(1 + l_i)
\]</span> Let <span class="math inline">\(c_k\)</span> be the actual cost of the <span class="math inline">\(k^{th}\)</span> operation in <span class="math inline">\(S\)</span> and <span class="math inline">\(a_k\)</span> be the amortized cost. Further, let <span class="math inline">\(\Phi_k\)</span> be the potential of the splay tree after the <span class="math inline">\(k^{th}\)</span> operation. Then <span class="math display">\[
\sum_{k = 1}^m a_k = \sum_{k = 1}^m (c_k + \Phi_k - \Phi_0) = \sum_{k = 1}^m c_k + \Phi_m - \Phi_0
\]</span> Hence, the actual cost on this sequence is bounded by <span class="math display">\[
\begin{aligned}
\sum_{k = 1}^m c_k &amp;= \Phi_0 - \Phi_m + \sum_{k = 1}^m a_k \\
&amp;=  \Phi_0 - \Phi_m + \sum_{i = 1}^m p_i \cdot m \cdot \log 3^{l_i} \\
&amp;=  \Phi_0 - \Phi_m + \sum_{i = 1}^m p_i \cdot m \cdot (l_i \log 3 + 1)  \\
&amp;= m + \Phi_0 - \Phi_m + \sum_{i = 1}^m p_i \cdot m \cdot l_i \cdot \log 3 
\end{aligned}
\]</span> It is left to bound <span class="math inline">\(\Phi_0 - \Phi_m\)</span>. Let <span class="math inline">\(r^0(i)\)</span> be the rank of <span class="math inline">\(i^{th}\)</span> before and <span class="math inline">\(r^m(i)\)</span> be the rank after the operations. It holds that <span class="math inline">\(r^0(i) \le \log_2 1 = 0\)</span> and <span class="math inline">\(r^m(i) \ge \log_2 w(i)\)</span>. Now, <span class="math display">\[
\begin{aligned}
\Phi_0 - \Phi_m &amp;= \sum_{i = 1}^n \left( r^0(i) - r^m(i) \right) \\
&amp;\le \sum_{i  = 1}^n \log 3^{l_i} \\
&amp;=  \sum_{i =1}^n l_i \log 3 \\
\end{aligned}
\]</span> Under the assumption that each item is accessed at least once, we know <span class="math inline">\(p_i \cdot m \ge 1\)</span> and <span class="math inline">\(\Phi_0 - \Phi_m \in O(\sum_{i = 1}^m p_i \cdot m \cdot l_i \cdot \log 3 )\)</span>.</p>
<p><strong><em>Remark</em></strong>. Of course, we could bound <span class="math display">\[
\sum_{i =1}^n l_i \log 3 \le (1 + 2 \cdot 2 + 3 \cdot 2^2 + 4 \cdot 2^3 + ... + (\lfloor \log_2 n \rfloor + 1) \cdot 2^{ \lfloor \log_2 n \rfloor }) \log 3
\]</span> Let <span class="math inline">\(\Sigma=1 + 2 \cdot 2 + 3 \cdot 2^2 + 4 \cdot 2^3 + ... + (\lfloor \log_2 n \rfloor + 1) \cdot 2^{ \lfloor \log_2 n \rfloor }\)</span>, then <span class="math display">\[
2 \cdot \Sigma - \Sigma = (\lfloor \log_2 n \rfloor + 1) \cdot 2^{ \lfloor \log_2 n \rfloor + 1} - \sum_{i = 0}^{\lfloor \log_2 n \rfloor - 1} 2^i \in O(n \log n)
\]</span></p>
<p>Note that <span class="math inline">\(n \log n\)</span> is a lower bound of the access cost of the optimal static binary search tree, under the assumption that each item is accessed at least once.</p>
<p><strong><em>Remark</em></strong>:</p>
<p>In general, <span class="math display">\[
\sum_{i = 0}^k x^i = \frac{x^{k + 1} - 1}{x - 1}
\]</span> Applying <span class="math inline">\(x \cdot \frac{\partial}{\partial x}\)</span> to both sides, <span class="math display">\[
\begin{aligned}
\sum_{i = 1}^k i x^{i} 
&amp;= x \cdot \frac{(k + 1) x^k (x - 1) - (x^{k + 1} - 1)}{(x - 1)^2} \\
&amp;= x \cdot \frac{(k + 1) (x^{k +1} - x^k) - (x^{k + 1} - 1)}{(x - 1)^2} \\
&amp;= x \cdot \frac{ k x^{k +1} - (k + 1) x^k  + 1}{(x - 1)^2} \\
&amp;= x \cdot \left( \frac{k x^k }{x - 1} - \frac{ x^k  - 1}{(x - 1)^2} \right)\\
\end{aligned}
\]</span></p>
<h2 id="working-set-property">Working Set Property</h2>
<p>Suppose that <span class="math inline">\(S = \{x_1, x_2, ..., x_m\}\)</span> and let <span class="math inline">\(t(k)\)</span> be the number of different nodes accessed before <span class="math inline">\(x_k\)</span> since the last access of node <span class="math inline">\(x_k\)</span>, or since the beginning of the sequence if <span class="math inline">\(x_k\)</span> is the first access.</p>
<p><strong><em>Theorem</em>.</strong> The overall cost of <span class="math inline">\(S\)</span> on the splay tree is bounded by <span class="math display">\[
O(m + n \log n + \sum_{k = 1}^n \log t(k) )
\]</span> <strong><em>Proof.</em></strong> We maintain <span class="math inline">\(w(x) = \frac{1}{(j + 1)^2}\)</span>, if the sequence accesses <span class="math inline">\(j\)</span> different nodes since the last accessed of node <span class="math inline">\(x\)</span>, or <span class="math inline">\(w(x) = \frac{1}{n^2}\)</span> if it is never accessed before. By definition of <span class="math inline">\(t(k)\)</span>, we have <span class="math inline">\(w(x_k) = \frac{1}{(t(x_k) + 1)^2}\)</span>. Now <span class="math display">\[
W \doteq \sum_{x \in T}^n w(x) \in O(1)
\]</span> For a node <span class="math inline">\(x_k\)</span> , if it is accessed for the first time, the amortized cost is <span class="math display">\[
O(1 + \log \frac{W}{w(x_k)} ) = O(\log n) = O(\log t(k))
\]</span> Summing over all nodes <span class="math inline">\(x\)</span> in the splay tree <span class="math inline">\(T\)</span>, we get <span class="math inline">\(O(n \log n)\)</span>.</p>
<p>Otherwise, if an item <span class="math inline">\(x_k\)</span> is accessed before, the amortized cost of accessing <span class="math inline">\(x_k\)</span> is <span class="math display">\[
O\left( 1 + \log \frac{W}{w(x_k) } \right) = O \left( 1 + \log t(k) \right)
\]</span> After each access of a node <span class="math inline">\(x_k\)</span> and splaying it to the root, we modify the weights of the nodes as follows: for all node <span class="math inline">\(x\)</span> with <span class="math inline">\(t(x) &lt; t(x_k)\)</span> (the nodes accessed after the last access of <span class="math inline">\(x_k\)</span>), we set <span class="math display">\[
w(x) = \frac{1}{(t(x) + 1)^2}
\]</span> This guarantees that <span class="math inline">\(w(x_k) = {1} / {(1 + t(k))^2}\)</span> for <span class="math inline">\(k \in [m]\)</span>.</p>
<p>The weight of the root does not change but the weight of other nodes may decrease after the reassignment. Therefore, the overall potential decreases. The amortized cost of the this access only decreases and is still bounded by <span class="math inline">\(O(1 + \log t(k))\)</span>.</p>
<p>Hence <span class="math display">\[
\sum_{k = 1}^m a_k \in O(n \log n + m + \sum_{k = 1}^m \log t(k) )
\]</span> Finally, it is easy to see that <span class="math inline">\(\Phi_0 - \Phi_m \in O(n \log n)\)</span>. Therefore, <span class="math display">\[
\sum_{k = 1}^m c_k \in O(n \log n + m + \sum_{k = 1}^m \log t(k) )
\]</span> <span class="math inline">\(\blacksquare\)</span></p>
<h2 id="static-finger-property">Static Finger Property</h2>
<p>The static finger property states that splay tree achieves some kind of locality. Suppose we order the nodes in the tree in increasing order according to their keys and labels them as <span class="math inline">\(1, 2, ..., n\)</span>. For a node <span class="math inline">\(x\)</span>, we use <span class="math inline">\(\pi(x)\)</span> to denote its order.</p>
<p><strong><em>Theorem.</em></strong> For any fixed <span class="math inline">\(i \in [n]\)</span>, the total cost of <span class="math inline">\(S\)</span> on the splay tree is bounded by <span class="math display">\[
O(n \log n + m + \sum_{k = 1}^m \log (|\pi(x_k) - i| + 1) )
\]</span> <strong><em>Proof.</em></strong> For <span class="math inline">\(x \in T\)</span>, we set <span class="math inline">\(w(x) = \frac{1}{(|pi(x) - i| + 1)^2}\)</span>. Then <span class="math display">\[
W \doteq \sum_{x \in T} \in O(1)
\]</span> and the amortized cost of accessing <span class="math inline">\(x_k\)</span> is <span class="math display">\[
O(1 + \log \frac{W}{w(x_k)}) = O(1 + \log (|\pi(x_k) - i| + 1))
\]</span> The theorem then follows from <span class="math inline">\(\Phi_0 - \Phi_m = O(n \log n)\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span>.</p>
<h2 id="reference">Reference</h2>
<p>[1]. Sleator, D.D. and Tarjan, R.E., 1983. A data structure for dynamic trees. Journal of computer and system sciences, 26(3), pp.362-391.</p>
<p>[2]. Jelani Nelson, Lecture 7, CS 224: Advanced Algorithms.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/23/Total-Law-of-Variance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/23/Total-Law-of-Variance/" class="post-title-link" itemprop="url">Total Law of Variance</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-05-23 21:16:25 / Modified: 21:33:04" itemprop="dateCreated datePublished" datetime="2020-05-23T21:16:25+10:00">2020-05-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables. Then the conditional expectation <span class="math display">\[
E[Y \mid X]
\]</span> is a random variable that depends on <span class="math inline">\(X\)</span>. Correspondingly we define its variance as <span class="math display">\[
Var[E[Y \mid X]] = E[\left( E[Y \mid X] \right)^2] - (E\left[ E[Y \mid X] \right] )^2
\]</span> The <strong><em>Law of Total Variance</em></strong> states that <span class="math display">\[
\begin{aligned}
Var[Y] 
&amp;= E[Y^2] - (E[Y])^2    \\
&amp;= E[E[Y^2 \mid X]] - E[\left( E[Y \mid X] \right)^2] + E[\left( E[Y \mid X] \right)^2] - (E\left[ E[Y \mid X] \right] )^2 \\
&amp;= E[E[Y^2 \mid X] - \left( E[Y \mid X] \right)^2] + Var[ E[Y \mid X] ] \\
&amp;= E[Var[Y \mid X] ] + + Var[ E[Y \mid X] ]
\end{aligned}
\]</span> <span class="math inline">\(\blacksquare\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/05/Cycle-Core-Connection-Game/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/05/Cycle-Core-Connection-Game/" class="post-title-link" itemprop="url">Cycle-Core Connection Game</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-05 10:56:51" itemprop="dateCreated datePublished" datetime="2020-05-05T10:56:51+10:00">2020-05-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-30 14:33:36" itemprop="dateModified" datetime="2021-01-30T14:33:36+11:00">2021-01-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Eisenbrand et al [1] proposed an interesting technique called cycle connection game. In the setting, there is a set of <span class="math inline">\(n\)</span> core points connected by an undirected cycle. Each core point is associated with a client. There is a pair of opposite directed edges between each core point and its associated client. Each edge has a weight. We denote <span class="math inline">\(G\)</span> be the resulting graph.</p>
<p>An example is shown as follows:</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/CycleCoreConnectionGame.png" /></p>
<p>Given an instance, we would like to sample a set of clients randomly (we call the sampled clients the "marked" ones). Then each client sends a unit capacity flow to its nearest marked client (could be itself). The cost of each flow is the sum of edges weights it travels. We would like to bound the expected cost of sending the flows.</p>
<p>The selection process requires a probability <span class="math inline">\(p\)</span> as input and consists of two steps:</p>
<ol type="1">
<li>Pick a client uniformly at random. This step ensures that at least one client is selected.</li>
<li>For the rest clients, pick each of them independently with some probability <span class="math inline">\(p\)</span>.</li>
</ol>
<p>To analyze the expected cost, we divide the edges into three categories:</p>
<ol type="1">
<li>Directed edges from the clients to the core points, denoted as <span class="math inline">\(\mathcal{E}_{in}\)</span>;</li>
<li>Directed edges from the core points to the clients, denoted as <span class="math inline">\(\mathcal{E}_{out}\)</span>;</li>
<li>Edges on the cycle, denoted as <span class="math inline">\(\mathcal{C}\)</span>.</li>
</ol>
<p>The following theorem holds:</p>
<blockquote>
<p><em>Theorem.</em> The expected cost is bounded by <span class="math display">\[
\sum_{e \in \mathcal{E}_{in} \cup \mathcal{E}_{out} } w_e + \frac{1}{2p} \sum_{e \in \mathcal{C} } w_e
\]</span></p>
</blockquote>
<p><strong><em>Proof.</em></strong><br />
For each edge <span class="math inline">\(e\)</span>, define a random variable <span class="math inline">\(X_e\)</span> to be the flow value of <span class="math inline">\(e\)</span>. For <span class="math inline">\(e \in \mathcal{E}_{in}\)</span>, it holds that <span class="math inline">\(X_e \in \{0, 1\}\)</span>. However, for <span class="math inline">\(e \in \mathcal{E}_{out} \cup \mathcal{C}\)</span>, it is possible that <span class="math inline">\(X_e &gt; 1\)</span>. By linearity of expectation, <span class="math display">\[
    \mathbb{E} \left[ \sum_{e \in \mathcal{E}_{in} \cup \mathcal{E}_{out} } w_e \cdot X_e + \sum_{e \in \mathcal{C} } w_e \cdot X_e \right] = \sum_{e \in \mathcal{E}_{in} \cup \mathcal{E}_{out} } w_e \cdot \mathbb{E} \left[  X_e \right]  + \sum_{e \in \mathcal{C} } w_e \cdot \mathbb{E} \left[  \cdot X_e \right].
\]</span></p>
<ol type="1">
<li><p>For <span class="math inline">\(e \in \mathcal{E}_{in}\)</span>, there is a unit flow on <span class="math inline">\(e\)</span> if only the associated client is not sampled. This happens with probability <span class="math inline">\((1 - \frac{1}{n})(1 - p)\)</span>, where <span class="math inline">\(n\)</span> is the number of clients. Therefore, <span class="math inline">\(\mathbb{E} [X_e \cdot w_e] = (1 - \frac{1}{n})(1 - p) \cdot w_e\)</span> and <span class="math display">\[
     \mathbb{E} \left[ \sum_{e \in \mathcal{E}_{in} } X_e \cdot w_e \right] = (1 - \frac{1}{n})(1 - p) \cdot \sum_{ e \in \mathcal{E}_{in} } w_e.
\]</span></p></li>
<li><p>For <span class="math inline">\(e \in \mathcal{E}_{out}\)</span>, the analysis is a little tricky. For each particular sample instance, it holds that <span class="math inline">\(\sum_{e \in \mathcal{E}_{out}} X_e = \sum_{e \in \mathcal{E}_{in}} X_e\)</span>, by flow conservation. Therefore, <span class="math display">\[
     \mathbb{E} \left[  \sum_{e \in \mathcal{E}_{out}} X_e \right] = \mathbb{E} \left[ \sum_{e \in \mathcal{E}_{in}} X_e \right] = (1 - \frac{1}{n})(1 - p) n.
\]</span> By symmetry, <span class="math inline">\(\{ X_e \}_{ e \in \mathcal{E}_{out} }\)</span> are identical distributed random variables. Therefore <span class="math inline">\(\mathbb{E} [X_e] = (1 - \frac{1}{n})(1 - p)\)</span>. It concludes that <span class="math display">\[
     \mathbb{E} \left[ \sum_{e \in \mathcal{E}_{out} } X_e \cdot w_e \right] = (1 - \frac{1}{n})(1 - p) \cdot \sum_{ e \in \mathcal{E}_{out} } w_e.
\]</span></p></li>
<li><p>For <span class="math inline">\(e \in \mathcal{C}\)</span>, the analysis of <span class="math inline">\(\mathbb{E} [X_e]\)</span> is the hardest part. We look at the suboptimal flow routing scheme such that each un-marked client sends a unit flow to a nearest marked client, with respect to unit cycle edge weight. Let <span class="math inline">\(Y_e\)</span> be the number of flows on edge <span class="math inline">\(e\)</span>. Clearly, <span class="math display">\[
     \mathbb{E} \left[ \sum_{e \in \mathcal{C} } X_e \cdot w_e \right] \le \mathbb{E} \left[ \sum_{e \in \mathcal{C} } Y_e \cdot w_e \right]. 
 \]</span></p>
<p>Further, for each client <span class="math inline">\(j\)</span>, let the random variable <span class="math inline">\(Y_j\)</span> be the number of cycle edges travelled by its flow, then <span class="math display">\[
     \sum_{e \in \mathcal{C} } Y_e = \sum_j Y_j.
 \]</span></p>
<p>For a fixed <span class="math inline">\(j\)</span> and some integer <span class="math inline">\(0 \le k \le \lfloor (n - 2) / 2 \rfloor\)</span>, the event <span class="math inline">\(Y_j &gt; k\)</span> happens if and only if <span class="math inline">\(j\)</span> is not marked and none of the first <span class="math inline">\(k\)</span> client neighbors to the left and right of <span class="math inline">\(j\)</span> is marked, the probability of which is given by <span class="math display">\[
     \Pr[Y_j &gt; k] = \left( 1 - \frac{2k + 1}{n} \right) q^{2k + 1}. 
 \]</span></p>
<p>where <span class="math inline">\(q = (1 - p)\)</span>. Now the expectation of <span class="math inline">\(Y_j\)</span> is given by <span class="math display">\[
 \begin{aligned}
     \mathbb{E} [Y_j] 
         &amp;= \sum_{k = 0}^{\lfloor (n - 2) / 2 \rfloor } \left( 1 - \frac{2k + 1}{n} \right) q^{2k + 1} \\
         &amp;\le e^{ - \frac{ 1 }{n} } q \sum_{k = 0}^{\lfloor (n - 2) / 2 \rfloor } e^{ -  k \frac{ 2 }{n} } q^{k2} \\ 
         &amp;\le e^{ - \frac{ 1 }{n} } q \sum_{k = 0}^{\lfloor (n - 2) / 2 \rfloor } (e^{ - \frac{ 2 }{n} } q^{2} )^k \\ 
         &amp;= e^{ - \frac{ 1 }{n} } q \frac{ (1 - (e^{ - \frac{ 2 }{n} } q^{2} )^{\lfloor (n - 2) / 2 \rfloor } ) }{1 -  (e^{ - \frac{ 2 }{n} } q^{2} ) } \\
         &amp;\le \frac{ e^{ - \frac{ 1 }{n} } q }{ 1 -  (e^{ - \frac{ 2 }{n} } q^{2} ) } \\
         &amp;\le \frac{1}{ 2 (1 - e^{ - \frac{ 1 }{n} } q) }.
 \end{aligned}
 \]</span> The final inequality holds as <span class="math inline">\(2x - 2x^2 \le 1 - x^2\)</span>. Replacing <span class="math inline">\(q\)</span> with <span class="math inline">\(1 - p\)</span>, we have <span class="math display">\[
     \mathbb{E} [Y_j] \le  \frac{1}{ 2 (1 - e^{ - \frac{ 1 }{n} } (1 - p)) } \le  \frac{1}{ 2 (1 - e^{ - \frac{ 1 }{n} }  + e^{ - \frac{ 1 }{n} } p) }  \le \frac{1}{2p}.
 \]</span></p>
<p>Remark: a more coarse calculation which leads to the same result would be <span class="math display">\[
     \mathbb{E} [Y_j] \le \sum_{k = 0}^\infty (1 - p)^{2k + 1} \le \frac{1 - p}{1 - (1 - p)^2 } \le \frac{1}{2p}.
 \]</span></p>
<p>By symmetry, the <span class="math inline">\(Y_j\)</span>'s are identical distributed random variables. So are the variable <span class="math inline">\(Y_e\)</span>'s for <span class="math inline">\(e \in \mathcal{C}\)</span>. It follows <span class="math display">\[
 \begin{aligned}
     \mathbb{E} [Y_e] 
         = \mathbb{E} [Y_j] 
         &amp;\le \frac{1}{2p}. 
 \end{aligned}
 \]</span> and <span class="math display">\[
 \begin{aligned}
     \mathbb{E} \left[ \sum_{e \in \mathcal{C}} Y_e \cdot w_e \right]  
         &amp;\le \frac{1}{2p} \sum_{e \in \mathcal{C}} w_e.
 \end{aligned}
 \]</span></p></li>
</ol>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong>Reference:</strong></p>
<ol type="1">
<li><p>Eisenbrand, Friedrich, Fabrizio Grandoni, Thomas Rothvoß, and Guido Schäfer. "Approximating connected facility location problems via random facility sampling and core detouring." In Proceeding of Nineteenth annual ACM-SIAM Symposium (SODA'08), no. CONF, pp. 1174-1183. 2008.</p></li>
<li><p>Eisenbrand, F., Grandoni, F., Rothvoß, T., Schäfer, G., 2010. Connected facility location via random facility sampling and core detouring. Journal of Computer and System Sciences 76, 709–726.</p></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/04/21/Linear-Transformation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/Linear-Transformation/" class="post-title-link" itemprop="url">Linear Transformation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-21 22:13:49" itemprop="dateCreated datePublished" datetime="2020-04-21T22:13:49+10:00">2020-04-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-03 22:14:46" itemprop="dateModified" datetime="2020-05-03T22:14:46+10:00">2020-05-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In linear algebra, we study vectors in some space <span class="math inline">\(\mathcal{R}^n\)</span>. Linear transformation is defined on <span class="math inline">\(\mathcal{R}^n\)</span>. For <span class="math inline">\(v, u \in \mathcal{R}^n\)</span>, and <span class="math inline">\(c \in \mathcal{R}\)</span>, a linear transformation <span class="math inline">\(T : \mathcal{R}^n \rightarrow \mathcal{R}^n\)</span> satisfies</p>
<ol type="1">
<li><span class="math inline">\(T(v + u) = T(v) + T(u)\)</span></li>
<li><span class="math inline">\(T(cv) = c T(v)\)</span></li>
</ol>
<p>However, the "linear thing" is not limited to vectors. As an illustration, note that a vector <span class="math display">\[
v = \begin{bmatrix}
    v_1 \\ v_2 \\ . \\ . \\ . \\ v_n
\end{bmatrix}
\]</span></p>
<p>can be viewed as the image of some function <span class="math inline">\(f : \{1, 2, ..., n \} \rightarrow \mathcal{R}\)</span>, such that <span class="math inline">\(f(i) = v_i\)</span>. It is natural to study the functions <span class="math inline">\(\{\mathbb{I} \rightarrow \mathcal{R} \}\)</span>, where <span class="math inline">\(\mathbb{I} \subset \mathcal{R}\)</span> is some open interval. For any <span class="math inline">\(x \in \mathbb{I}\)</span>, <span class="math inline">\(f(x)\)</span> gives the value of <span class="math inline">\(x\)</span>. The only difference with vectors is that, it not possible to enumerate all value of <span class="math inline">\(x \in \mathbb{I}\)</span>. Instead, we specify a rule of obtaining the value at <span class="math inline">\(x\)</span>.</p>
<p>For any two functions <span class="math inline">\(f_1, f_2 : \mathbb{I} \rightarrow \mathcal{R}\)</span>, we can add them together and obtain a new function <span class="math inline">\(f_1 + f_2 : \mathbb{I} \rightarrow \mathcal{R}\)</span>. Addition is closed in the space <span class="math inline">\(\{\mathbb{I} \rightarrow \mathcal{R} \}\)</span>. Then for <span class="math inline">\(x \in \mathbb{I}\)</span>, <span class="math display">\[
(f_1 + f_2) (x) = f_1(x) + f_2(x)
\]</span></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/LinearTransformation/Addition.jpg" /></p>
<p>which is similar to that adding up coordinates of two vector. We can also define multiplication: for <span class="math inline">\(c \in \mathcal{R}^n\)</span>, <span class="math inline">\(f : \mathbb{I} \rightarrow \mathcal{R}\)</span>, <span class="math inline">\(cf\)</span> is a function in <span class="math inline">\(\{\mathbb{I} \rightarrow \mathcal{R} \}\)</span> such that <span class="math display">\[
(cf)(x) = c f(x)
\]</span></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/LinearTransformation/Scale.jpg" /></p>
<p>Multiplication is closed in the space <span class="math inline">\(\{\mathbb{I} \rightarrow \mathcal{R} \}\)</span>.</p>
<p><em>Example.</em> The derivative operation <span class="math inline">\(T: \{f: \mathbb{I} \rightarrow \mathcal{R} \wedge \text{differentiable} \} \rightarrow \{\mathbb{I} \rightarrow \mathcal{R} \}\)</span> is linear transformation. It is easy to verify for differentiable functions <span class="math inline">\(f_1, f_2 \in \mathbb{I} \rightarrow \mathcal{R}\)</span>, and <span class="math inline">\(c \in \mathcal{R}\)</span>, we have</p>
<ol type="1">
<li><span class="math inline">\((f_1 + f_2)&#39; = f_1&#39; + f_2&#39;\)</span></li>
<li><span class="math inline">\((c f_1)&#39; = c f_1&#39;\)</span></li>
</ol>
<p>What is more, we can extend some other concepts in linear algebra. In linear algebra, "eigenvector" is a vector whose direction does not change under a linear transformation. For derivative operation, the function <span class="math inline">\(e^x\)</span> is un-changed after the transformation: <span class="math display">\[
(e^x)&#39; = e^x
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/04/14/Catalan-Number/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/14/Catalan-Number/" class="post-title-link" itemprop="url">Catalan Number</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-14 17:22:31 / Modified: 20:33:46" itemprop="dateCreated datePublished" datetime="2020-04-14T17:22:31+10:00">2020-04-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(S\)</span> be the set of binary strings with length <span class="math inline">\(2n\)</span> and equal number of <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>'s. That is, if <span class="math inline">\(s \in S\)</span>, then <span class="math inline">\(s\)</span> contains <span class="math inline">\(n\)</span> zeros and <span class="math inline">\(n\)</span> ones. Define <span class="math display">\[
\sigma(s) = \text{ number of 0 in } s \\
\lambda(s) = \text{ number of 1 in } s
\]</span></p>
<p>Then <span class="math display">\[
S = \{ s : s \in (0, 1)^{2n} \wedge \sigma(s) = \lambda(s) \}
\]</span></p>
<p>Denote <span class="math inline">\(s[1:i]\)</span> the initial segment of <span class="math inline">\(s\)</span> with length. Of interest is the subset <span class="math inline">\(P \subset S\)</span> that contains strings whose initial segments has more (or equal) number of <span class="math inline">\(0\)</span> than <span class="math inline">\(1\)</span>: <span class="math display">\[
P \doteq \{ s : \sigma(s[1:i]) \ge \lambda(s[1:i]), \forall i \in [2n] \} \subset S
\]</span></p>
<p>The cardinality of <span class="math inline">\(P\)</span> is given by <span class="math display">\[
|P| = \frac{1}{n + 1} \binom{2n}{n}
\]</span></p>
<p>which is known as Catalan number.</p>
<p>Let <span class="math inline">\(S \setminus P\)</span> be the set of strings that do not satisfy the requirement and <span class="math inline">\(T = \{ s : s \in (0, 1)^{2n} \wedge \sigma(s) = n - 1 \wedge \lambda(s) = n + 1 \}\)</span></p>
<p>The proof relies on the following theorem.</p>
<p><strong><em>Theorem</em></strong>. There is a bijection <span class="math inline">\(f\)</span> between <span class="math inline">\(S \setminus P\)</span> and <span class="math inline">\(T\)</span>.</p>
<p>Before we prove it, we see how it gives Catalan number: <span class="math display">\[
\begin{aligned}
    |P| 
        &amp;= |S| - |S\setminus P| \\
        &amp;= \binom{2n}{n} - |T|  \\
        &amp;= \binom{2n}{n} - \binom{2n}{n - 1}  \\
        &amp;= \binom{2n}{n} - \binom{2n}{n} \frac{n}{n + 1} \\
        &amp;= \frac{1}{n + 1} \binom{2n}{n}
\end{aligned}
\]</span></p>
<p><em>Proof of Theorem.</em></p>
<p><span class="math inline">\(\longrightarrow\)</span>:</p>
<p>If <span class="math inline">\(s \in S \setminus P\)</span>, then there exists some <span class="math inline">\(i \in [1, n - 1]\)</span>, such that <span class="math inline">\(\sigma(s[1:i]) &lt; \lambda(s[1:i])\)</span>. Then we can select the smallest such <span class="math inline">\(i\)</span>. Note the <span class="math inline">\(i\)</span> must be an odd number. Hence we can write <span class="math inline">\(i = 2j + 1\)</span>, <span class="math inline">\(\sigma(s[1:i]) = j\)</span> and <span class="math inline">\(\lambda(s[1:i]) = j + 1\)</span>.</p>
<p>Further, as <span class="math inline">\(s \in S\)</span>, we have <span class="math inline">\(\sigma(s) = \lambda(s) = n\)</span>. Therefore, <span class="math display">\[
\sigma(s[i + 1:n]) = n - j \\
\lambda(s[i + 1:n]) = n - j - 1
\]</span></p>
<p>To construct an <span class="math inline">\(s&#39; \in T\)</span> from <span class="math inline">\(s\)</span>, we can flipped every bit in <span class="math inline">\(s[i + 1:n]\)</span>, i.e., <span class="math display">\[
s&#39; = (s[1:i], \ \ \overline{s[i + 1:n] } )
\]</span></p>
<p>For sanity check, the number of zeros in <span class="math inline">\(s&#39;\)</span> is <span class="math inline">\(j + n -j - 1 = n - 1\)</span> and the number of ones is <span class="math inline">\(j + 1 + n - j = n + 1\)</span>.</p>
<p>It is easy to see that this is a injection.</p>
<p><span class="math inline">\(\longleftarrow\)</span>:</p>
<p>If <span class="math inline">\(s&#39; \in T\)</span>, then there must exist some <span class="math inline">\(i \in [1, n - 1]\)</span>, such that <span class="math inline">\(\sigma(s&#39;[1:i]) &lt; \lambda(s&#39;[1:i])\)</span>. Then we can select the smallest such <span class="math inline">\(i\)</span>. Note the <span class="math inline">\(i\)</span> must be an odd number. Hence we can write <span class="math inline">\(i = 2j + 1\)</span>, <span class="math inline">\(\sigma(s&#39;[1:i]) = j\)</span> and <span class="math inline">\(\lambda(s&#39;[1:i]) = j + 1\)</span>.</p>
<p>Further, as <span class="math inline">\(s&#39; \in T\)</span>, we have <span class="math inline">\(\sigma(s&#39;) = n - 1\)</span> and <span class="math inline">\(\lambda(s&#39;) = n + 1\)</span>. Therefore, <span class="math display">\[
\sigma(s&#39;[i + 1:n]) = n - 1 - j\\
\lambda(s&#39;[i + 1:n]) = n + 1  - j - 1 = n - j 
\]</span></p>
<p>To construct an <span class="math inline">\(s \in S \setminus P\)</span> from <span class="math inline">\(s&#39;\)</span>, we can flipped every bit in <span class="math inline">\(s&#39;[i + 1:n]\)</span>, i.e., <span class="math display">\[
s = (s&#39;[1:i], \ \ \overline{s&#39;[i + 1:n] } )
\]</span></p>
<p>For sanity check, the number of zeros in <span class="math inline">\(s\)</span> is <span class="math inline">\(j + n - j = n\)</span> and the number of ones is <span class="math inline">\(j + 1 + n - 1 - j = n\)</span>.</p>
<p>It is easy to see that this is a injection.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/04/11/k-Core-Decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/11/k-Core-Decomposition/" class="post-title-link" itemprop="url">k-Core Decomposition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-11 16:07:17 / Modified: 16:46:51" itemprop="dateCreated datePublished" datetime="2020-04-11T16:07:17+10:00">2020-04-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given a graph <span class="math inline">\(G = \left&lt; V, E \right&gt;\)</span>, and a subset <span class="math inline">\(S \subset V\)</span>, the sub-graph <span class="math inline">\(G(S)\)</span> induced by <span class="math inline">\(S\)</span> is defined as <span class="math display">\[
G(S) = \left&lt; S, E_S \right&gt;
\]</span></p>
<p>where <span class="math inline">\(E_S = \{ e : e = (u, v) \in E \wedge (u, v) \in S \times S\}\)</span>.</p>
<p><span class="math inline">\(G(S)\)</span> is called a <span class="math inline">\(k\)</span>-core if it is the maximal sub-graph wuch that every vertex in <span class="math inline">\(S\)</span> has degree at least <span class="math inline">\(k\)</span> inside this sub-graph. Given the definition of <span class="math inline">\(k\)</span>-core, the core number of a vertex <span class="math inline">\(v \in V\)</span> is the maximum <span class="math inline">\(k\)</span> such tha t <span class="math inline">\(\exists S \subset V\)</span>, <span class="math inline">\(G(S)\)</span> is a <span class="math inline">\(k\)</span>-core and <span class="math inline">\(v \in S\)</span>.</p>
<p>The key observation is that, given a <span class="math inline">\(k\)</span>-core <span class="math inline">\(G(S)\)</span>, if we remove every vertex <span class="math inline">\(v \in V\)</span> (as long as edges incident to <span class="math inline">\(v\)</span>) such that <span class="math inline">\(d(v) &lt; k\)</span>, then <span class="math inline">\(G(S)\)</span> remains a <span class="math inline">\(k\)</span>-core. Since</p>
<ol type="1">
<li>If <span class="math inline">\(d(v) &lt; k\)</span>, then <span class="math inline">\(v \notin S\)</span>.</li>
<li>Removing edges incident to <span class="math inline">\(v\)</span> will not affect edges inside <span class="math inline">\(G(S)\)</span>. Then, the vertices' degree inside <span class="math inline">\(G(S)\)</span> remains unchanged.</li>
</ol>
<p>This gives an algorithm for computing the core number of each vertex.</p>
<blockquote>
<ol type="1">
<li>for <span class="math inline">\(v \in V\)</span>:</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(check[v] \leftarrow\)</span> false</li>
<li>while not <span class="math inline">\(check[v] =\)</span> true <span class="math inline">\(\forall v \in V\)</span>:</li>
<li><span class="math inline">\(\quad\)</span> Get an <span class="math inline">\(v\)</span> with smallest <span class="math inline">\(d(v)\)</span> and <span class="math inline">\(check[v] =\)</span> false</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(check[v] \leftarrow\)</span> true</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(core[v] \leftarrow d(v)\)</span></li>
<li><span class="math inline">\(\quad\)</span> for <span class="math inline">\(u \in N(v)\)</span> such that <span class="math inline">\(d(u) &gt; k\)</span>:</li>
<li><span class="math inline">\(\qquad\)</span> Delete <span class="math inline">\((v, u)\)</span></li>
<li><span class="math inline">\(\qquad\)</span> <span class="math inline">\(d(u) \leftarrow d(u) - 1\)</span></li>
</ol>
</blockquote>
<p>We can use an array <span class="math inline">\(D\)</span> such that <span class="math inline">\(D[i]\)</span> is a linked list and contains all vertices with degree <span class="math inline">\(i\)</span>. Therefore, line 4 can be executed in <span class="math inline">\(O(1)\)</span> time.</p>
<h3 id="reference">Reference</h3>
<p>[1] Batagelj, V. and Zaversnik, M., 2003. An O (m) algorithm for cores decomposition of networks. arXiv preprint cs/0310049.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/04/02/QuakeHeap/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/02/QuakeHeap/" class="post-title-link" itemprop="url">Quake Heap</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-02 19:23:49" itemprop="dateCreated datePublished" datetime="2020-04-02T19:23:49+11:00">2020-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-05 10:57:54" itemprop="dateModified" datetime="2020-05-05T10:57:54+10:00">2020-05-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Quake Heap [1] is a min heap (or max heap) that supports the following operations:</p>
<table>
<thead>
<tr class="header">
<th>Operation</th>
<th>Amortized Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>insert(x)</td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="even">
<td>decrease-key(x,k)</td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="odd">
<td>delete-min()</td>
<td><span class="math inline">\(O(\log n)\)</span></td>
</tr>
</tbody>
</table>
<p>with <span class="math inline">\(O(n)\)</span> space. For comparison based min heap, either <span class="math inline">\(insert(x)\)</span> or delete-min() takes <span class="math inline">\(O(\log n)\)</span> amortized time. Otherwise, we would have an <span class="math inline">\(o(n \log n)\)</span> comparison based sorting algorithm.</p>
<p>In this blog we walk through its construction step by step and illustrate the philosophy behind. Some of the ideas also appear in other designs of min heaps.</p>
<h3 id="insertion">Insertion</h3>
<p>For insertion we are going to be lazy. Consider inserting the elements 1, 2, 3, 4, 5 into the heap. For each insertion, we just create a singleton node while maintaining a pointer (the "min-pointer") to the minimum node. The picture below shows the heap we obtain after five insertions. Each insertion has time complexity <span class="math inline">\(O(1)\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/Insertion.jpg" /></p>
<h3 id="deletion-and-binomial-heap">Deletion and Binomial Heap</h3>
<p>It would be easier to consider the case delete-min() without decrease-key(x) first. Now we try to delete the minimum element from heap. The first step, of course, is to remove the element <span class="math inline">\(1\)</span>, after which we get the following heap.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-1.jpg" /></p>
<p>Here comes the problem. The "min-pointer" is removed along with the deletion. To maintain it, it seems that we need to loop over the entire list to update the min, which takes <span class="math inline">\(O(n)\)</span> time. If we do nothing else than merely scanning all elements, we need another <span class="math inline">\(O(n)\)</span>-scanning for the next deletion. We hope that the comparison performed for the current delete-min operation can reduce the ones we need to perform in the future.</p>
<p>The idea is to merge the elements into trees. <!-- Initially, we view each element as a tree with a single node. Recursively, if the root of tree $A$ is compared with and is less than the root of tree $B$, we make $A$ as a subtree of $B$.  --> Intuitively, the trees "remember" the comparison performed before and save us from comparing the same pairs again.</p>
<p>Various tree structure can be used to record the comparison and this may result in the design of different data structure. The one picked by Quake heap is the tournament tree. It only compares and merges (linking) two trees of the same height. A new node is created as the root of the two trees. The new node's value equals to the smaller one of the roots of the two trees.</p>
<p>For example, in the following example, both the node 4 and 5 are viewed as trees with height one. We can compare and link the two trees. A new node with value 4 (the smaller value between 4 and 5) is created as the new root of the merged tree.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-2.jpg" /></p>
<p>We can continue to link node 3 and 2.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-3.jpg" /></p>
<p>The linking is performed recursively. Noting the the current two trees have the same height, we link them. We need to also update the "min-pointer" during the linking.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-4.jpg" /></p>
<p>The construction takes <span class="math inline">\(O(n)\)</span> time. But we can charge the cost of these link operations into the creation of the trees' roots before. We have performed 3 link operations for the four nodes each being the root of a singleton tree. On average, we pay <span class="math inline">\(O(1)\)</span> per operation. To make the amortized argument more vivid, we can think that conceptually we an additional coin for each singleton tree creation. Therefore, before linking, we have four coins as follows:</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-1-Coin.jpg" /></p>
<p>Merging node 5 and 4 has actual cost <span class="math inline">\(O(1)\)</span> and creates a new tree root. We use the coin of node 5 to pay the actual cost. Element 4 is the new root and it still has a coin. By using the coin idea, we charge (or averaging) the cost of this link operation to the cost of the creation of node 5.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-2-Coin.jpg" /></p>
<p>Merging node 3 and 2 uses one coin. We use the coin reserved by node 3. After this, the elements 4 and 2 (the elements that are roots of some trees) still have a coin.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-3-Coin.jpg" /></p>
<p>Merging the two trees left uses one coin. We use the coin reserved by node 4. After this, the element 2 is still a root and still has a coin.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-4-Coin.jpg" /></p>
<p>In general, we may need to pay more than <span class="math inline">\(O(1)\)</span> for delete-min(). Consider we would like to perform the operation for the following.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-5-Coin.jpg" /></p>
<p>We need to remove all nodes on a root-to-leaf path. In this example it is the path that contains element 2.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-6-Coin.jpg" /></p>
<p>After the removal, some children of the nodes in this root-to-leaf path are now exposed as tree roots. In this example, node 4 and 3 are tree roots. We need to reserve two coins node 4 and 3. The coins are reserved for potential linking performed in the future.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin1-7-Coin.jpg" /></p>
<p>Finally we need to compare and link any two trees of the same height, in a recursive manner. This part is paid by the coins reserved for the tree roots.</p>
<p>To analyze the amortized running time of delete-min(), we define the height of the tree as <span class="math inline">\(H\)</span>. We need to remove <span class="math inline">\(H\)</span> nodes from the root-to-leaf path, expose <span class="math inline">\(\max \{H - 1, 0 \}\)</span> node as tree roots and reserve coins for them, followed by recursively linking two trees with the same height. The final step is "free" and covered by the coins reserved. Therefore the amortized cost is <span class="math inline">\(H + \max \{H - 1, 0\} = \max \{ 2H - 1, H \} \le 2H\)</span>.</p>
<p><em>Key Observation:</em></p>
<blockquote>
<p><strong><em>To bound the time of delete-min() to <span class="math inline">\(O(\log n)\)</span>, it is critical to control the size of <span class="math inline">\(H\)</span>.</em></strong></p>
</blockquote>
<p>To analyze how large <span class="math inline">\(H\)</span> could be, we study how it grow. Initially, a tree with <span class="math inline">\(H = 1\)</span> contains only 1 key. When we link two trees with <span class="math inline">\(H = 1\)</span>, the new tree has height <span class="math inline">\(H = 2\)</span> and <span class="math inline">\(2\)</span> keys (although it has <span class="math inline">\(3\)</span> nodes). In general, a tree with height <span class="math inline">\(H = h\)</span> contains <span class="math inline">\(2^{h - 1}\)</span> keys and this can be easily proved by induction.</p>
<blockquote>
<p>If there is no decrease-key operation, the tree with height <span class="math inline">\(h\)</span> has <span class="math inline">\(2^{h - 1}\)</span> keys and <span class="math inline">\(2^h - 1\)</span> nodes. As there are at most <span class="math inline">\(n\)</span> keys, the maximum possible height, denoted as <span class="math inline">\(H\)</span>, is bounded by <span class="math inline">\(\log n\)</span>.</p>
</blockquote>
<p>The heap we have constructed so far has a special name called "Binomial Heap".</p>
<h3 id="decrease-key">Decrease-Key</h3>
<p>If there is only insert() and decrease-key() operation, min heap is easy to implemented. We can use a linked list / dynamic array, while maintaining the min pointer / min index.</p>
<p>If we need to incorporate delete-min() operation, the decrease-key(x) itself is still easy, while the former requires significant modification. For decrease-key(x), we adopt a lazy strategy. If after decreasing, <span class="math inline">\(x\)</span>'s value is still smaller than its parent (or <span class="math inline">\(x\)</span> is the root of the tree and has no parent), then everything is fine. For example, if we decrease <span class="math inline">\(4\)</span> to <span class="math inline">\(3.5\)</span> (the node 4 has parent 2), then we do not need to modify the tree after the operation.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DecreaseKey-1.jpg" alt="drawing" width="330" /></p>
<p>On the other hand, if we decrease 4 to 1, then its value is smaller than its parent 2.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DecreaseKey-2.jpg" /></p>
<p>We need to detach the highest node containing value 4 from its parent node (in the meanwhile we maintain the "min-pointer").</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DecreaseKey-3.jpg" /></p>
<p>The only concern with this operation is the height of a heap. In the previous, the tree on the right hand side has height <span class="math inline">\(3\)</span> but contains only two keys. For a tree with <span class="math inline">\(n\)</span> keys, it could happens that it height is <span class="math inline">\(\omega(\log n)\)</span>. Indeed, it could be as large as <span class="math inline">\(O(n)\)</span>. This will invalidate the strategy we propose in the last section, which requires the height to be <span class="math inline">\(O(\log n)\)</span>, to achieve the <span class="math inline">\(O(\log n)\)</span> amortized complexity for delete-min().</p>
<p>There are various ways to fix this, the idea of which will give rise to Pairing heap [<em>Pairing heap achieve <span class="math inline">\(O(\log n)\)</span> amortized bound for deletion but requires <span class="math inline">\(O(\log n)\)</span> amortized time for both insertion and decrease-key(x). Its design philosophy is closely related to Splay tree.</em>], Fibonacci heap, etc. Here we discuss about the solution of Quake heap.</p>
<p>First, Quake heap records the number of nodes at each height explicitly, denoted as <span class="math inline">\(n_1, n_2, ..., n_H\)</span>, where <span class="math inline">\(H\)</span> is the maximum possible height. Further, it specifies a parameter <span class="math inline">\(\alpha \in (0, 5, 1)\)</span> and maintains the invariant that <span class="math inline">\(n_{i + 1} \le \alpha \cdot n_i\)</span> for each <span class="math inline">\(i \ge 1\)</span>. This guarantees that <span class="math inline">\(H = O(\log n)\)</span>.</p>
<p>In the following discussion, we take <span class="math inline">\(\alpha = 0.75\)</span>.</p>
<p>Let's discuss how the maintain the invariant <span class="math inline">\(n_{i + 1} \le \alpha n_i\)</span> for all <span class="math inline">\(i \ge 1\)</span> for insert(x), decrease-key(x) and delete-min() operation.</p>
<ol type="1">
<li><p>For the operation insert(x), it create a singleton node at the bottom layer and increases <span class="math inline">\(n_1\)</span> by 1. If the invariant holds before insertion, it holds afterward.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/RecordingHeight.jpg" /></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/RecordingHeightAndInsertion.jpg" /></p></li>
<li><p>For decrease, it will not change the number of nodes at each height. The following example shows the case where the children whose key is decreased is detached from its parent. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DecreaseKey-2.jpg" /></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DecreaseKey-3.jpg" /></p></li>
<li><p>Delete-min is the most difficult part. To maintain the invariant, now delete-min() consists of four part:</p>
<ul>
<li>Delete the min key (and all nodes that contain the min key). As before, this step may expose some children of the deleted nodes as new tree roots.</li>
<li>Recursively link all trees of the same height. After linking, scan all trees to maintain the min pointer.</li>
<li>Quake: if there is a level <span class="math inline">\(i\)</span> such that <span class="math inline">\(n_{i + 1} &gt; \alpha n_i\)</span>, remove all nodes with level greater than <span class="math inline">\(i\)</span>.</li>
</ul></li>
</ol>
<p>It is left to analyze the running time step by step. We use a potential function for analysis. By previous section, the potential should contain the number of trees. Further, by the last step of delete-min operation, it is suggested to includes the number of nodes in the potential function, to cover the cost of node removal. Inspired by this, our first design is given by <span class="math display">\[
\Phi = \# \text{nodes} + 2 \cdot \# \text{trees}
\]</span></p>
<p>We analyze the time for each step of delete-min()</p>
<ol type="1">
<li><p>Before delete-min(), the invariant holds and therefore the maximum height <span class="math inline">\(H = O(\log n)\)</span>. Deleting the min-key and its associated nodes take actual time <span class="math inline">\(O(\log n)\)</span>. As for <span class="math inline">\(\Phi\)</span>, the number of nodes decreases (indeed, this decrease will cover the removal of the nodes containing the min key) and the number of trees increases by at most <span class="math inline">\(O(\log n)\)</span>. Therefore, the amortized cost for the first step is <span class="math inline">\(O(\log n)\)</span>.</p></li>
<li><p>Linking two trees of the same height has actual cost <span class="math inline">\(1\)</span>. It creates a new node and decreases the number of trees by 1. Therefore, <span class="math inline">\(\Delta \Phi = -1\)</span>. Hence the amortized cost is 0. This also explains why we need an coefficient 2 associated with #trees in the potential function. It is left to analyze the cost of scanning all tree. After linking, all trees are of different height. It suffices to show that the maximum height is <span class="math inline">\(O(\log n)\)</span> to bound the scanning cost to <span class="math inline">\(O(\log n)\)</span>. Before linking <span class="math inline">\(H = O(\log n)\)</span>. As there are only <span class="math inline">\(n\)</span> keys, there are at most <span class="math inline">\(n\)</span> trees with height <span class="math inline">\(H\)</span> before and during the linking procedure. Linking <span class="math inline">\(n\)</span> trees of the same height results in a new tree with height <span class="math inline">\(\log n\)</span>. Therefore, after linking, <span class="math inline">\(H\)</span> increases by at most <span class="math inline">\(\log n\)</span> and <span class="math inline">\(H = O(\log n)\)</span> still holds. Therefore, the amortized cost for the second step is <span class="math inline">\(O(\log n)\)</span>.</p></li>
<li><p>Suppose in this step we delete <span class="math inline">\(k\)</span> nodes. The actual cost is <span class="math inline">\(k\)</span>. However, the number of nodes decreases by <span class="math inline">\(k\)</span>. It seems that the amortized cost is 0 for this step. But it isn't. We have increased the number of tree. Denote <span class="math inline">\(d_{i + 1}\)</span> the number of nodes at level <span class="math inline">\(i + 1\)</span> with two children before removal and <span class="math inline">\(s_{i + 1}\)</span> the number of nodes with only one children. We have <span class="math inline">\(n_{i + 1} = d_{i + 1} + s_{i + 1}\)</span>. On the other hand, we know <span class="math inline">\(n_i \ge 2 d_{i + 1} + s_{i + 1}\)</span>. By the fact that <span class="math inline">\(n_{i + 1} &gt; \alpha n_i\)</span>, we know that <span class="math display">\[
 \begin{aligned}
 s_{i + 1} 
     &amp;= 2(d_{i + 1} + s_{i + 1}) - (2 d_{i + 1} + s_{i + 1})\\
     &amp;\ge 2 n_{i + 1} - n_i \\
     &amp;\ge (2 \alpha - 1) n_i 
 \end{aligned}
 \]</span></p>
<p>as we create at most <span class="math inline">\(n_i\)</span> new trees, we can charge the creation of new trees into <span class="math inline">\(s_{i + 1}\)</span> and modify the potential as <span class="math display">\[
 \Phi = \# \text{nodes} + 2 \cdot \# \text{trees} + \frac{2}{2\alpha - 1} \# \text{nodes with one child}
 \]</span></p>
<p>Now step 3 has amortized cost 0. For other steps, this modification will increase the cost of decrease-key(x) by at most <span class="math inline">\(O(1)\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin2-1.jpg" /> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin2-2.jpg" /> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin2-3.jpg" /> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin2-4.jpg" /> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin2-5.jpg" /> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin2-6.jpg" /> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/QuakeHeap/DeleteMin2-7.jpg" /></p></li>
</ol>
<p>Caveat: we can't swap the order of step 2 and step 3, since linking tree may create violation to the invariant and we need step 3 to fix this.</p>
<h3 id="reference">Reference</h3>
<p>[1] Chan, Timothy M. "Quake heaps: a simple alternative to Fibonacci heaps." In Space-Efficient Data Structures, Streams, and Algorithms, pp. 27-32. Springer, Berlin, Heidelberg, 2013.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/04/01/Ball%20and%20Bins/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/01/Ball%20and%20Bins/" class="post-title-link" itemprop="url">Ball and Bins</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-01 10:24:00" itemprop="dateCreated datePublished" datetime="2020-04-01T10:24:00+11:00">2020-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-01 12:40:24" itemprop="dateModified" datetime="2021-01-01T12:40:24+11:00">2021-01-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose we would like to throw <span class="math inline">\(n\)</span> balls into <span class="math inline">\(n\)</span> bins uniformly at random. We claim that the bin with maximum load has <span class="math inline">\(\Theta( \frac{\ln n}{\ln \ln n})\)</span> ball with high probability.</p>
<h3 id="upper-bound">Upper Bound</h3>
<p>The probability that a particular bin gets <span class="math inline">\(k\)</span> balls is given by</p>
<p><span class="math display">\[
\binom{n}{k}\frac{1}{n^k} \left( 1 - \frac{1}{n} \right)^{n - k} \le \binom{n}{k}\frac{1}{n^k} \le \frac{1}{k!} \le \frac{  e^k }{ k^k}
\]</span></p>
<p>where the last inequality comes from <span class="math inline">\(\frac{k^k}{k!} \le e^{ k }\)</span> (by Tailor expansion of <span class="math inline">\(e^x\)</span> at the origin with <span class="math inline">\(x = k - 1\)</span> or <span class="math inline">\(x = k\)</span>).</p>
<p>If we would like to use union bound over <span class="math inline">\(n\)</span> bins to show that no bin has more than <span class="math inline">\(k\)</span> ball with probability at least <span class="math inline">\(1 - \frac{1}{n}\)</span>, we need to find some <span class="math inline">\(k\)</span> that satisfies <span class="math display">\[
\left( \frac{e}{ k} \right)^k = \frac{1}{n^2}.
\]</span></p>
<p>Taking <span class="math inline">\(\log\)</span> of both side, <span class="math display">\[
k \ln k - k = 2 \ln n \implies k \ln k = \Theta( \ln n) \implies \ln k = \Theta( \ln \ln n). 
\]</span></p>
<p>Substituting <span class="math inline">\(k \ln k = \Theta( \ln n)\)</span> with <span class="math inline">\(\ln k = \Theta( \ln \ln n)\)</span>, we get <span class="math display">\[
    k  = \Theta\left( \frac{\ln n}{\ln \ln n} \right).
\]</span></p>
<!-- We can trivially take $k = e \frac{ 2 }{ e - 1} \frac{ \ln n }{ \ln \ln n  } \le 3.2 \frac{ \ln n }{ \ln \ln n  }$. To verify this 

1. For $x \ge 0$, we have $x - e \ln x \ge 0$. Hence $\frac{1}{ e } \ge \frac{\ln x }{x}$. Replacing $x$ with $\ln \ln n$ (Assuming that $n \ge e$ ), we get $\frac{1}{ e} \ge \frac{\ln \ln \ln n }{ \ln \ln n}$

2. Therefore, 
   $$
        \begin{aligned}
            e\left( \frac{k}{ e} \right)^k
                &\ge \left( \frac{\ln n  }{ \ln \ln n} \right)^k \\ 
                &= \exp \left( k (\ln \ln n - \ln \ln \ln n ) \right) \\
                &= \exp \left( \frac{ 2 }{ e - 1} e  \ln n  - \frac{ 2 }{ e - 1} e \frac{\ln n \cdot \ln \ln \ln n }{ \ln \ln n}  \right) \\
                &\ge \exp \left( \frac{ 2 }{ e - 1}[e \ln n  -  \ln n ] \right) \\
                &= n^2
        \end{aligned}
    $$ -->
<!-- As when $k = O( { \ln n \over \ln \ln n})$, we have $k^k = O(n)$. It suffices to take $k = O( \alpha e { \ln n \over \ln \ln n})$. To check this more careful, note that 


$$
1 + k \ln k - k \ln \alpha e \ge 2 \ln n
$$ 

If we set $k = l \frac{\ln n}{ \ln \ln n}$ for some $l$, then  

$$
k \ln k = k [\ln l + \ln \ln n - \ln \ln \ln n] = k \ln l + l \ln n - k \ln \ln \ln n 
$$

When $l = \frac{2}{e - 1} \alpha e$, we need to show that 

$$
 (\frac{2}{e - 1}  \alpha e - 2) \ln n -  \frac{2}{e - 1}  \alpha e \frac{\ln n \cdot \ln \ln \ln n }{ \ln \ln n} \ge 0
$$

For let $y = \ln \ln n$, $\frac{\ln y}{ y} \le \frac{1}{e}$ always holds for $y > 1$ (assuming that $n > e^e$), as 
$$
(y - e \ln y)' = 1 - e / y = 0 \rightarrow y = e
$$

and $\frac{\ln y}{y}$ increases as $y < e$ and decreases as $y > e$. 

Now, 
$$
\begin{aligned}
    &(\frac{2}{e - 1}  \alpha e - 2) \ln n -   \frac{2}{e - 1} \alpha e \frac{\ln n \cdot \ln \ln \ln n }{ \ln \ln n} \\
    \ge&  ( \frac{2}{e - 1}  \alpha e - 2) \ln n -  \frac{2}{e - 1}  \alpha  \ln n \\
    =& ( \frac{2}{e - 1}  \alpha (e - 1) - 2) \ln n  \\
    =& 2(  \alpha - 1) \ln n  \\
    \ge& 0
\end{aligned}
$$

with the assumption that $\alpha \ge 1$.  -->
<h3 id="lower-bound">Lower Bound</h3>
<p>The lower bound is a little bit harder to prove. First note for <span class="math inline">\(x &gt; 1\)</span>, the function <span class="math display">\[
y = (x - 1) \ln (1 - \frac{1}{x})
\]</span></p>
<p>is decreasing. As <span class="math display">\[
\begin{array}{rrl}
    &amp;y&#39;  &amp;= \ln (1 - \frac{1}{x} ) + (x - 1) \frac{ 1 }{\frac{x - 1}{x} } \frac{1}{ x^2 } \\
        &amp;&amp;= \ln (1 - \frac{1}{x} ) + \frac{1}{ x } \\
\end{array}
\]</span> and for <span class="math inline">\(x &gt; 1\)</span>, <span class="math display">\[
\begin{array}{rrl}
    &amp;y&#39;&#39; &amp;= \frac{ 1 }{\frac{x - 1}{x} } \frac{1}{ x^2 } - \frac{1}{x^2} \\
        &amp;&amp;= (\frac{x}{x - 1} - 1) \frac{1}{x^2} \\
        &amp;&amp;\ge 0
\end{array}
\]</span></p>
<p>Further, note that <span class="math inline">\(x \rightarrow \infty\)</span>, <span class="math inline">\(y&#39; \rightarrow 0\)</span>. It concludes that <span class="math inline">\(y&#39; \le 0\)</span>.</p>
<p>Therefore, the probability that a bin gets <span class="math inline">\(k\)</span> balls is given by (for <span class="math inline">\(k \ge 1\)</span>)</p>
<p><span class="math display">\[
\begin{aligned}    
    \binom{n}{k}\frac{1}{n^k} \left( 1 - \frac{1}{n} \right)^{n - k} 
        &amp;\ge (\frac{n}{k} )^k \frac{1}{n^k} \left( 1 - \frac{1}{n} \right)^{n - 1} \\
        &amp;\ge \frac{1}{k^k} \frac{1}{e}
\end{aligned}
\]</span></p>
<p>If we set <span class="math inline">\(k = \frac{1}{3} \frac{ \ln n }{ \ln \ln n }\)</span>, then <span class="math display">\[
k^k \le ( \ln n)^k = n^{1/3} 
\]</span></p>
<p>Hence, <span class="math display">\[
\frac{1}{k^k} \frac{1}{e} \ge e^{ - 1} n^{-1 / 3}
\]</span></p>
<p>This probability is rather smaller. At this point it seems that a single bin is unlikely to success. But by linearity of expectation, the expected number of bins with <span class="math inline">\(k\)</span> balls is given by <span class="math inline">\(e^{ - 1} n^{ 2/ 3}\)</span>. In particular, define the indicator variables <span class="math inline">\(X_i\)</span> such that <span class="math inline">\(X_i = 1\)</span> if the <span class="math inline">\(i\)</span>-th bin receives <span class="math inline">\(k\)</span> balls and let <span class="math inline">\(X = \sum X_i\)</span> be the number of bins with <span class="math inline">\(k\)</span> balls. Then</p>
<ol type="1">
<li><span class="math inline">\(E[X_i] = \Pr[X_i = 1] \ge e^{-1} n^{ - 1 / 3}\)</span>.</li>
<li><span class="math inline">\(E[X] = \sum E[X_i] \ge e^{ - 1} n^{ 2/ 3}\)</span>.</li>
</ol>
<p>As in expectation there are <span class="math inline">\(e^{-1} n^{2/ 3}\)</span> such bins, it seems that we are likely to find one. To formalize the intuition, we need to make sure that <span class="math inline">\(X\)</span> does not deviate from its expectation much. To see why this might cause a problem, consider the random variable <span class="math inline">\(Y\)</span> such that <span class="math display">\[
\begin{aligned}
    \Pr[Y = e^{-1} n^{ 5 / 3} ]  &amp;= \frac{1}{n} \\ 
    \Pr[Y = 0 ]  &amp;= 1 - \frac{1}{n} \\ 
\end{aligned}
\]</span></p>
<p>It has expectation <span class="math inline">\(E[Y] = e^{ - 1} n^{ 2 / 3}\)</span>. But it is very likely that <span class="math inline">\(Y = 0\)</span>.</p>
<p>We investigate the variance of <span class="math inline">\(X\)</span>: <span class="math display">\[
\begin{aligned}
Var[X] 
    &amp;= \sum_i Var[X_i] + \sum_{i \neq j} Cor(X_i, X_j) \\
    &amp;\le    \sum_i Var[X_i] \\
    &amp;\le    \sum_i E[X_i^2] \\
    &amp;=      \sum_i E[X_i] \\
    &amp;\le    n
\end{aligned}
\]</span></p>
<p>The first inequality follows from</p>
<blockquote>
<p>Fact 1: For <span class="math inline">\(i \neq j\)</span>, <span class="math inline">\(Cor(X_i, X_j) &lt; 0\)</span></p>
</blockquote>
<p>Intuitively, that bin <span class="math inline">\(i\)</span> obtains ball makes it more difficult for bin <span class="math inline">\(j\)</span> to obtain ball.</p>
<p><strong><em>Question to ponder: prove it rigorously.</em></strong></p>
<p>Finally, by Chebyshev Inequality, we have <span class="math display">\[
\Pr[ |X - E[X]| \ge E[X] ] \le \frac{ Var[X] }{ E[X]^2} \le \frac{n}{ e^{-2} n^{4  / 3}} = \frac{e^2}{n^ {1 / 3} }
\]</span></p>
<!-- > Proof of Fact 1. 

Observe that $Cor(X_i, X_j) = E[X_iX_j] - E[X_i] E[X_j]$. Denote $C_i$ and $C_j$ the number of balls bin $i$ and bin $j$ get respectively. As $X_i$ and $X_j$ are indicator variables, $X_i X_j$ is an indicator variable that equals to $1$ only when $X_i = 1$ and $X_j = 1$. 

$$
\begin{aligned}    
    E[X_i X_j] &= \sum_{C_i = c_i \ge k, C_j = c_j \ge k,  c_i + c_j \le n } \Pr[C_i = c_i \wedge C_j = c_j] \\
    &= \sum_{c_i \ge k, c_j \ge k, c_i + c_j \le n} \binom{n}{c_i + c_j} \binom{c_i + c_j}{ c_i}  (\frac{1}{n} )^{ c_i } (\frac{1}{n})^{ c_j } (1 - \frac{2}{n} )^{ n - c_i - c_j} 
\end{aligned}
$$

On the other hand, for fix $c_i, c_j$, we have 
$$
\Pr[C_i = c_i ] = \binom{n}{c_i} (\frac{1}{n})^{ c_i }  (1 - \frac{1}{n} )^{ n - c_i} \\
\Pr[C_j = c_j ] = \binom{n}{c_j} (\frac{1}{n})^{ c_j }  (1 - \frac{1}{n} )^{ n - c_j}
$$

We claim that $\Pr[C_i = c_i \wedge C_j = c_j] \le \Pr[C_i = c_i ] \Pr[C_j = c_j ]$, as 

1. $\binom{n}{c_i + c_j} \binom{c_i + c_j}{ c_i}   \le \binom{n}{c_i} \binom{n}{c_j}$
2. Denote $z = n - c_i - c_j$, then 
    $$
    \begin{aligned}
        &(1 - \frac{2}{n} )^z \le (1 - \frac{1}{n} )^{ z + n} \\
        \Longleftrightarrow 
        &(\frac{n - 2}{n - 1})^z \le (\frac{ n - 1}{ n })^n \\
        \Longleftrightarrow 
        &(1 - \frac{1}{n - 1})^z \le (1 - \frac{  1}{ n })^n
    \end{aligned}
    $$ -->
<h4 id="appendix">Appendix</h4>
<p>In the appendix we discuss a rough approximation of <span class="math inline">\(k^k\)</span> for some <span class="math inline">\(k \in \mathcal{Z}_+\)</span>. We claim that <span class="math display">\[
e \cdot \frac{k^k}{e^k} \le k! \le e \sqrt k \cdot \frac{k^k}{e^k}
\]</span></p>
<h5 id="remark-1">Remark 1:</h5>
<p>As (<span class="math inline">\(\ln\)</span> here is based on <span class="math inline">\(e\)</span>) <span class="math display">\[
\int_{x = 1}^k \ln x \ dx = x \ln x |_{x = 1}^k - \int_{x = 1}^k x \ d \ln x = k \ln k - (k - 1)
\]</span></p>
<p>We get</p>
<p><span class="math display">\[
\ln k! = \sum_{i = 1}^k \ln i \ge \int_{x = 1}^k \ln x \ dx  = k \ln k - (k - 1)
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
\begin{aligned}
\ln k! 
&amp;= \sum_{i = 1}^k \ln i \\
&amp;= \frac{1}{2} \sum_{i = 1}^{k - 1} [\ln i + \ln (i + 1) ] + \frac{1}{2} \ln k \\
&amp; \le  \int_{x = 1}^k \ln x \ dx + \frac{1}{2} \ln k \\
&amp;= (k + \frac{1}{2}) \ln k - (k - 1)
\end{aligned}
\]</span></p>
<!-- ##### Remark 2: 
An easier way to prove the lower bound is 

$$
\frac{k^k}{k!} = \frac{k^{k - 1} }{ (k - 1)!} \le \sum_{i = 1}^\infty \frac{(k - 1)^i}{i !} = e^{k - 1} 
$$

We have $e \frac{k^k}{ e^k } \le k!$.  -->
<h5 id="reference">Reference:</h5>
<ol type="1">
<li>Sanjeev Arora, Cos 521: Advanced Algorithm Design, Lecture 1: Course Intro and Hashing</li>
<li>Sariel Har-Peled, Chapter 4, The Occupancy and Coupon Collector problems.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/31/Sampling-With-Or-Without-Replacement/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/31/Sampling-With-Or-Without-Replacement/" class="post-title-link" itemprop="url">Sampling With Or Without Replacement</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-31 22:58:36 / Modified: 23:59:13" itemprop="dateCreated datePublished" datetime="2020-03-31T22:58:36+11:00">2020-03-31</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose that there are <span class="math inline">\(a\)</span> blue balls and <span class="math inline">\(b\)</span> red balls in a box and we would like to sample <span class="math inline">\(k\)</span> balls from them. We can sample the balls with replacement such that after picking each ball we put it back into the box, or we can do it without replacement.</p>
<p>For convenience of discussion, denote <span class="math inline">\(n = a + b\)</span>, <span class="math inline">\(p = \frac{a}{n}\)</span> and <span class="math inline">\(q = \frac{b}{n}\)</span>. Define <span class="math inline">\(X_i\)</span> (<span class="math inline">\(i \in [k]\)</span>) the indicator random variable of whether the <span class="math inline">\(i\)</span>-th selected ball is blue. Let <span class="math display">\[
X = \sum X_i
\]</span></p>
<p>Then we claim that <span class="math display">\[
E[X] = \sum_{i \in [k]} E[X_i] = kp
\]</span></p>
<p>The first equality holds by linearity of expectation. The second equality states that in expectation, <span class="math inline">\(p\)</span> fraction of the selected balls are blue.</p>
<p>The claim is trivial when we sample with replacement, as <span class="math inline">\(\Pr[X_i = 1] = p\)</span>. To prove that the probability applies for the case of sampling without replacement requires a little more effort, as shown by the following</p>
<ol type="1">
<li>There are in total <span class="math inline">\(\begin{pmatrix} n \\ k\end{pmatrix}\)</span> possible combinations of choosing <span class="math inline">\(k\)</span> balls out of <span class="math inline">\(n\)</span>.</li>
<li>Among them there are <span class="math inline">\(\begin{pmatrix} a \\ 1 \end{pmatrix} \cdot \begin{pmatrix} n - 1 \\ k - 1 \end{pmatrix}\)</span> combinations such that the <span class="math inline">\(i\)</span>-th ball is blue.</li>
<li>Therefore, <span class="math inline">\(\Pr[X_i = 1] = \frac{ \begin{pmatrix} a \\ 1 \end{pmatrix} \cdot \begin{pmatrix} n - 1 \\ k - 1 \end{pmatrix} }{ \begin{pmatrix} n \\ k\end{pmatrix} } = \frac{a}{n} = p\)</span>.</li>
</ol>
<p>However, the two sampling scheme have different variance.</p>
<h5 id="variance-of-sampling-with-replacement">Variance of Sampling With Replacement</h5>
<p>By independence of <span class="math inline">\(X_i\)</span> <span class="math display">\[
Var[X] \doteq \sigma^2 = \sum_{i \in [k]} Var[X_i] = k p q
\]</span></p>
<h5 id="variance-of-sampling-without-replacement">Variance of Sampling Without Replacement</h5>
<p>In this case, the <span class="math inline">\(X_i\)</span>'s are not independent. <span class="math display">\[
\begin{aligned}
       Var[X]  
        &amp;= E \left[ X^2 \right] - \left( E[X] \right)^2 \\
        &amp;= \sum_{i \in [k]} E[X_i^2] + \sum_{i, j \in [k]} E[X_i X_j] - (kp)^2 \\
        &amp;=  kp + k(k - 1) \frac{ \begin{pmatrix} a \\ 2 \end{pmatrix} \cdot \begin{pmatrix} n  - 2 \\ k - 2 \end{pmatrix} }{ \begin{pmatrix} n \\ k\end{pmatrix} } - (kp)^2 \\
        &amp;=  kp + k(k - 1) \frac{ a(a-1) }{ n(n - 1) } - (kp)^2 \\
        &amp;=  kp + kp \frac{ (k - 1) (a-1) }{ (n - 1) } - (kp)^2 \\
        &amp;=  kp ( 1 + \frac{ (k - 1) (a-1) }{ (n - 1) } - k \frac{a}{ n } ) \\
        &amp;=  kp \frac{n^2 - n + ka n - kn - an + n - kan + ka }{ n (n - 1) } \\
        &amp;=  kp \frac{n^2  - kn - an  + ka }{ n(n - 1) } \\
        &amp;=  kp \frac{(n - a) (n - k)}{ n (n - 1)} \\
        &amp;=  kpq \frac{n - k}{n - 1} \\
        &amp;= \sigma^2 \frac{n - k}{n - 1}
\end{aligned}
\]</span></p>
<p>We have shown that sampling without replacement has smaller variance than sampling with replacement. Moreover, if <span class="math inline">\(n \succ k\)</span>, then <span class="math inline">\(\frac{n - k}{n - 1} \approx 1\)</span> and the two sampling scheme have almost the same variance. We can bound the concentration behavior of <span class="math inline">\(X\)</span> by Chebyshev's inequality: for any <span class="math inline">\(c &gt; 0\)</span>, <span class="math display">\[
\Pr[ |X - E[X] | \ge c \cdot \sigma] \le \frac{Var[X] }{c^2 \sigma^2} = \frac{1}{c^2}
\]</span></p>
<p>The <span class="math inline">\(3\)</span>-sigma rule says only <span class="math inline">\(X\)</span> deviates more than <span class="math inline">\(3\sigma\)</span> from its expectation with probability at most <span class="math inline">\(11.11\%\)</span> <span class="math display">\[
\Pr[ |X - E[X] | \ge 3 \cdot \sigma] \le \frac{Var[X] }{c^2 \sigma^2} = \frac{1}{9} \approx 0.1111
\]</span></p>
<p>It is also interesting to observe that the variance most related to the sample size <span class="math inline">\(k\)</span> and probabilities <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> , but not <span class="math inline">\(n\)</span>!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/30/LSH/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/30/LSH/" class="post-title-link" itemprop="url">LSH</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-30 16:14:46" itemprop="dateCreated datePublished" datetime="2020-03-30T16:14:46+11:00">2020-03-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-03 22:17:23" itemprop="dateModified" datetime="2020-05-03T22:17:23+10:00">2020-05-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong><em>Given a set <span class="math inline">\(S \subset U\)</span> of <span class="math inline">\(n\)</span> points, a metric <span class="math inline">\(d\)</span> defined on <span class="math inline">\(U\)</span>, and a query point <span class="math inline">\(v\)</span>, the nearest neighbor search (NNS) problem finds <span class="math inline">\(v\)</span>'s nearest point in <span class="math inline">\(S\)</span>:</em></strong> <span class="math display">\[
u = \arg\min_{s \in S} d(v, s)
\]</span> Potential applications include web search and clustering.</p>
<p>If <span class="math inline">\(U\)</span> is a Euclidean space <span class="math inline">\(\mathcal{R}^d\)</span> for some <span class="math inline">\(d\)</span>, the hardness of NNS depends largely on the value of <span class="math inline">\(d\)</span>. Suppose <span class="math inline">\(d = 1\)</span>, then we can pre-process the points by building a balanced binary search tree on <span class="math inline">\(S\)</span>. Finding the nearest point to <span class="math inline">\(v \notin S\)</span> reduces to searching for its predecessor and successor, which takes <span class="math inline">\(O(\log n)\)</span> time. For <span class="math inline">\(d = 2\)</span>, we can construct a Voronoi diagram.</p>
<p>For higher dimensions <span class="math inline">\(d\)</span> it is not easy to extend the index structures. A naïve search loops over all points and takes <span class="math inline">\(O(nd)\)</span> time. It is natural to ask whether we can do better. If we are willing to sacrifice accuracy for efficiency, this is possible. Instead of finding the exact nearest neighbor, we relax our goal to finding an approximate nearest neighbor whose distance is within <span class="math inline">\(1 + \epsilon\)</span> of the minimum one.</p>
<p>One possible solution is Johnson–Lindenstrauss decomposition. We can generate a matrix <span class="math inline">\(A \in R^{ O( \frac{ \log n }{ \epsilon^2 } )\times d}\)</span> and multiply it with every element in <span class="math inline">\(S\)</span>. The mapping <span class="math inline">\(u \rightarrow Au\)</span> preserves pair-wise distance up to a relative error of <span class="math inline">\(\epsilon\)</span>. The pre-processing takes <span class="math inline">\(O( \frac{n d \log n}{\epsilon^2} )\)</span> time. To answer an query, we map <span class="math inline">\(v\)</span> to <span class="math inline">\(Av\)</span>, which takes <span class="math inline">\(O(\frac{ d\log n }{ \epsilon^2 } )\)</span> time. Then we perform a loop over all elements of <span class="math inline">\(\{ Au : u \in S\}\)</span>, which takes <span class="math inline">\(O(\frac{ n \log n }{ \epsilon^2 } )\)</span> time. Therefore, the total time complexity is <span class="math inline">\(O(\frac{ (n + d) \log n }{ \epsilon^2 } )\)</span>.</p>
<h3 id="hashing">Hashing</h3>
<p>[1] solves the problem by hashing. We call the pair of points that are close to each other the <em>similar pair</em> and the pair that are far from each other the <em>dissimilar pair</em>. Intuitively, we want similar pairs to be hashed to the same bucket and that dissimilar pairs to be hashed to different buckets.</p>
<p>Instead of finding the <span class="math inline">\((1 + \epsilon)\)</span> approximate nearest neighbor directly, [1] further reduces it to the <span class="math inline">\((c, r)\)</span>-approximate neighbor search (ANNS<span class="math inline">\((c, r)\)</span>) problem, which is defined as follows:</p>
<p><strong><em>Given a query point <span class="math inline">\(v\)</span>, if <span class="math inline">\(\exists s \in S\)</span>, then return a point <span class="math inline">\(u\)</span>, such that</em></strong> <span class="math display">\[
d(v, u) \le c \cdot r
\]</span></p>
<p>The reduction from <span class="math inline">\(1 + \epsilon\)</span> to ANNS(<span class="math inline">\(c, r\)</span>) is not easy. Here we show one that returns the <span class="math inline">\((1 + \epsilon)^2\)</span> approximate nearest neighbor. Let <span class="math inline">\(d_{\max} = \max_{s, s&#39;} d(s, s&#39;)\)</span> and <span class="math inline">\(d_{\min} = \min_{s, s&#39;} d(s, s&#39;)\)</span>. We build a set of structures that answers ANNS(<span class="math inline">\(1 + \epsilon\)</span>, r) queries for the values of <span class="math inline">\(r\)</span>: <span class="math display">\[
d_{\min}, \ d_{\min} (1 + \epsilon ), \ d_{min} (1 + \epsilon)^2,\ ...,\ d_{\max}
\]</span></p>
<p>Then given the query point <span class="math inline">\(v\)</span>, we do a binary search to find the smallest <span class="math inline">\(r\)</span> that returns a point. This imposes an additional cost of <span class="math inline">\(\ln {\ln \frac{d_{\max} }{d_{\min} } \over \ln (1 + \epsilon) }\)</span>.</p>
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\text{NNS} \longrightarrow (1+\epsilon) \text{NNS} \longrightarrow \text{ANNS}(c, r) \longrightarrow (r, cr, p_1, p_2)\text{-LSH} \longrightarrow \text{Boosting Probability}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">The Problem Reduction Chain</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h3 id="r-cr-p_1-p_2-locality-sensitive-hashing-family"><span class="math inline">\((r, cr, p_1, p_2)\)</span>-Locality Sensitive Hashing Family</h3>
<p>Now we show how to solve the ANNS<span class="math inline">\((c, r)\)</span> problem with hashing. Designing a perfecting hashing that solves the problem directly is not easy. It is desirable to have some hashing which successes with some probability and then boost its probability systematically. To achieve this, we introduce the idea of <span class="math inline">\((r, cr, p_1, p_2)\)</span>-locality sensitive hashing family.</p>
<p><strong><em>Given a set of point <span class="math inline">\(V\)</span>, a metric <span class="math inline">\(d(\cdot , \cdot)\)</span> defined on <span class="math inline">\(V\)</span>, a collection of hash function <span class="math inline">\(\mathcal{H} = \{ h: V \rightarrow Z\}\)</span> is called <span class="math inline">\((r, cr, p_1, p_2)\)</span> locality sensitive if for any <span class="math inline">\(v, u \in V\)</span> and an <span class="math inline">\(h\)</span> sampled uniformly from <span class="math inline">\(\mathcal{H}\)</span>, it holds that</em></strong></p>
<ol type="1">
<li>If <span class="math inline">\(d(v, u) &lt; r\)</span>, then <span class="math inline">\(\Pr_{h \in \mathcal{H} } [h(v) = h(u)] \ge p_1\)</span>.</li>
<li>If <span class="math inline">\(d(v, u) \ge cr\)</span>, then <span class="math inline">\(\Pr_{h \in \mathcal{H} } [h(v) = h(u)] \le p_2\)</span></li>
</ol>
<p>where <span class="math inline">\(p_2 &lt; p_1 \le 1\)</span>. Intuitively, pairs near to each other should be more likely to collide. Hence <span class="math inline">\(p_1 = p_2^{\rho}\)</span> for some <span class="math inline">\(\rho = \log p_1 / \log p_2 &lt; 1\)</span>.</p>
<p>Ideally, we have <span class="math inline">\(p_1 \approx 1\)</span> and <span class="math inline">\(p_2 \approx 0\)</span>. For other cases we can magnify the gap between <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> and boost <span class="math inline">\(p_1\)</span> to approximate <span class="math inline">\(1\)</span> and <span class="math inline">\(p_2\)</span> to approximate <span class="math inline">\(0\)</span>. The hardness of the boosting highly depends on the value of <span class="math inline">\(c\)</span>.</p>
<h4 id="example">Example</h4>
<p>Suppose that <span class="math inline">\(U = \{0, 1\}^d\)</span>, <span class="math inline">\(d(u, v) = u \oplus v = |u - v|_1\)</span> and <span class="math inline">\(\mathcal{H} = \{ h_i (u) = u_i \}_{i \in [d]}\)</span>, where <span class="math inline">\(h_i\)</span> selects the <span class="math inline">\(i\)</span>th bit of <span class="math inline">\(u\)</span>. Picking an <span class="math inline">\(h\)</span> uniformly from <span class="math inline">\(\mathcal{H}\)</span> and applying <span class="math inline">\(h\)</span> to <span class="math inline">\(u\)</span> is equivalent to selecting a bit uniformly from <span class="math inline">\(u\)</span>. Therefore,</p>
<ol type="1">
<li>If <span class="math inline">\(d(v, u) &lt; r\)</span> (<span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> has less then <span class="math inline">\(r\)</span> common bits), then <span class="math inline">\(\Pr_{h \in \mathcal{H} } [h(v) = h(u)] \ge \frac{d - r}{d} = 1 - \frac{r}{d} = p_1\)</span>.</li>
<li>If <span class="math inline">\(d(v, u) \ge cr\)</span>, then <span class="math inline">\(\Pr_{h \in \mathcal{H} } [h(v) = h(u)] \le \frac{d - cr}{d} = 1 - \frac{cr}{d} = p_2\)</span></li>
</ol>
<p>Now <span class="math inline">\(p_1 \approx \exp(- r / d)\)</span> and <span class="math inline">\(p_2 \approx \exp(- cr / d)\)</span>. Hence <span class="math inline">\(\rho \approx \frac{1}{c}\)</span>. Indeed it can be proven rigorously that <span class="math display">\[
\rho = \frac{\ln (1 - \frac{r}{d}) }{ \ln (1 - \frac{cr}{d}) } \le \frac{1}{c}
\]</span></p>
<p>as <span class="math inline">\(( 1 - \frac{r}{d})^c \ge 1 - \frac{cr}{d}\)</span>. There are many ways to prove this. Since we are dealing with probability, we give a probabilistic interpretation of this inequality. Suppose that are <span class="math inline">\(c\)</span> independent events each happening with probability <span class="math inline">\(\frac{r}{d}\)</span>. Then the probability that at least one of them happens is given by <span class="math display">\[
1 - \Pr[\textbf{none of them happens } ] = 1 - (1 - \frac{r}{d})^c
\]</span></p>
<p>On the other hand, by union bound, this is upper bounded by <span class="math inline">\(\frac{cr}{d}\)</span>.</p>
<h3 id="boosting-the-probabilities">Boosting the Probabilities</h3>
<p>We would like the boost the probability such that <span class="math inline">\(p_1 \approx 1\)</span> and <span class="math inline">\(p_2 \approx 0\)</span>. We follow a two-step approach. First we sample <span class="math inline">\(k\)</span> (to be determined) hash functions independently from <span class="math inline">\(\mathcal{H}\)</span> can concatenate them as a single hash function <span class="math inline">\(g\)</span>: <span class="math display">\[
g(v) = [h_1(v),  h_2(v), ... ,h_k(v)]
\]</span></p>
<p>Here <span class="math inline">\(g\)</span> hash a point <span class="math inline">\(v\)</span> to a <span class="math inline">\(k\)</span> dimensional vector. After this, <span class="math inline">\(g(v) = g(u)\)</span> happens only if <span class="math inline">\(h_i(v) = h_i(u)\)</span> for all <span class="math inline">\(i \in [k]\)</span>. By independence of <span class="math inline">\(h_i\)</span>'s,</p>
<ol type="1">
<li><p>If <span class="math inline">\(d(v, u) \ge cr\)</span>, the collision probability of <span class="math inline">\(u\)</span> with <span class="math inline">\(v\)</span> drops to <span class="math inline">\(p_2^k\)</span>: <span class="math display">\[
 \Pr [ g(v) = g(u) ] \le p_2^k
 \]</span></p></li>
<li><p>On the other hand, if <span class="math inline">\(d(v, u) &lt; r\)</span>, the collision probability also drops to <span class="math inline">\(p_1^k = (p_2^k)^\rho\)</span> <span class="math display">\[
 \Pr [ g(v) = g(u) ] \ge  p_1^k = p_2^{\rho k} = (p_2^{k})^\rho
 \]</span> which is relatively small and not desirable.</p></li>
</ol>
<p>We need a second level of boosting to increase the collision probability of similar pairs. Instead of using only one function <span class="math inline">\(g\)</span>, we use a collection of independent copies: <span class="math display">\[
g_1, g_2, ..., g_l
\]</span></p>
<p>where <span class="math inline">\(l\)</span> is the number to be determined. Now,</p>
<ul>
<li>If <span class="math inline">\(d(v, u) &lt; r\)</span>, the probability that at least one of the <span class="math inline">\(g_i(u)\)</span> equals to <span class="math inline">\(g_i(v)\)</span> is given by <span class="math display">\[
  \begin{aligned}
      \Pr \left[ \exists i \in [l], \ s.t., \ g_i(u) = g_i(v) \right] 
          &amp;= 1 - (1 - p_1^k)^l  \\
          &amp;= 1 - (1 - (p_2^k)^\rho)^l \\
          &amp;\ge 1 - \exp( - (p_2^k)^\rho l )
  \end{aligned}
  \]</span></li>
</ul>
<p>We can take</p>
<ol type="1">
<li><p><span class="math inline">\(p_2^k = \frac{1}{n}\)</span>, i.e., <span class="math inline">\(k = \frac{\log n }{ \log 1 / p_2 }\)</span>.</p></li>
<li><p><span class="math inline">\(\exp( - (p_2^k)^\rho l ) = \frac{1}{n}\)</span>, i.e., <span class="math inline">\(l = \frac{1}{(p_2^k)^\rho } \ln n = n^\rho \ln n\)</span>.</p></li>
</ol>
<p>For a fixed query point <span class="math inline">\(q\)</span> and for any fixed <span class="math inline">\(i \in [l]\)</span>, by linearity of expectation, the expected number of dissimilar collision is given by</p>
<p><span class="math display">\[
\begin{aligned}
    E \left[ |\{ u \in S: d(q, u) \ge cr \wedge g_i(q) = g_i(u)  \}| \right] 
        &amp;= \sum_{ u \in S : \  d(q, u) \ge cr } \Pr[g_i(v)= g_i(u)] \\
        &amp;\le n \cdot \frac{1}{n} = 1
\end{aligned}
\]</span></p>
<p>Therefore, there are <span class="math inline">\(l\)</span> of dissimilar collisions over all <span class="math inline">\(i \in [l]\)</span>. This implies an expected overhead of <span class="math inline">\(l\)</span> examinations of dissimilar points. However, by Markov inequality, the number of such points is less than <span class="math inline">\(2l\)</span> with probability at least <span class="math inline">\(1 / 2\)</span>. The time for examination of dissimilar point is <span class="math inline">\(O(ld) = O(d n^\rho \ln n)\)</span>.</p>
<h3 id="time-and-space">Time and Space</h3>
<p>Now, given a query point <span class="math inline">\(q\)</span>, computing <span class="math inline">\(g_i(q)\)</span> for a fixed <span class="math inline">\(i\)</span> takes <span class="math inline">\(O(k)\)</span> time. Computing <span class="math inline">\(l\)</span> of them takes <span class="math inline">\(O(kl)\)</span> time. In expectation, there are <span class="math inline">\(l\)</span> dissimilar collision points with <span class="math inline">\(q\)</span>. Checking one collision point takes <span class="math inline">\(O(d)\)</span> time. Therefore, the expected query time is given by (note that we stop upon finding the first ANN of <span class="math inline">\(q\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
    O(kl + dl) 
    &amp;= O(\frac{\ln n}{\ln \frac{1}{p_2}} n^\rho \ln n + d n^\rho \ln n) \\
    &amp;= O(n^{ \rho} \ln^2 n / \ln \frac{1}{p_2} + d n^\rho \ln n) \\
    &amp;= \tilde{O}( n^\rho )
\end{aligned}
\]</span></p>
<p>As for the space usage, for each point <span class="math inline">\(u\)</span> in <span class="math inline">\(S\)</span>, we need to compute <span class="math inline">\(g_i(u)\)</span> and store this value (we need to store this, because <span class="math inline">\(g_i(u)\)</span> is self could be a large number and when putting <span class="math inline">\(\{ g_i(u) : u \in S\}\)</span> into a hash table, we need to treat <span class="math inline">\(g_i(u)\)</span> as the key and store it for later collision comparison. But is it possible that when there is a collision, we re-compute this value again to avoid storing it ?). We need to repeat this for <span class="math inline">\(l\)</span> hash functions. Therefore, the space usage is given by <span class="math display">\[
O(nkl) = O(n^{1 + \rho} \ln^2 n / \ln \frac{1}{p_2})
\]</span></p>
<h3 id="another-boosting-scheme">Another Boosting Scheme</h3>
<p>In last section, we have a scheme that checks <span class="math inline">\(O(n^\rho \ln n)\)</span> dissimilar points in expectation. In this section we achieve it with high probability.</p>
<ol type="1">
<li>First, to reduce the collision probability of dis-similar pairs, we concatenate <span class="math inline">\(k\)</span> functions from <span class="math inline">\(\mathcal{H}\)</span>. This gives a <span class="math display">\[
     (r, cr, p_1^k, p_2^k)
\]</span> locative sensitive hash function. We determine the value of <span class="math inline">\(k\)</span> later. <!-- We pick $k = \frac{\ln n}{\ln \frac{1}{p_2} }$, such that $p_2^k = 1 / n$. In expectation, at most $1$ dis-similar collision occurs.  --></li>
<li>Second, to boost the collision probability of similar pairs, we use <span class="math inline">\(l\)</span> concatenated hash functions independently. The probability at least one collision occurs is given by <span class="math display">\[
     1 - (1 - p_1^k)^l \ge 1 - e^{-p_1^k \cdot l} 
\]</span> We require this to happen with constant probability <span class="math inline">\(1 - \frac{1}{e}\)</span>, hence we need to set <span class="math inline">\(l = \frac{1}{p_1^k} = (\frac{1}{p_2^k})^\rho\)</span>. <!-- where $l = n^\rho$. The final equality holds since $p_1^k \cdot l = p_2^{k \cdot \rho} \cdot l = n^{-\rho} \cdot l = 1$ --></li>
<li>Now consider a query point <span class="math inline">\(q\)</span>. We compute <span class="math inline">\(g_1(q), g_2(q), ..., g_l(q)\)</span> and check the points <span class="math inline">\(u\)</span>'s such that <span class="math inline">\(g_i(u) = g_i(q)\)</span> for some <span class="math inline">\(i \in [l]\)</span>. Note that we check at most the first <span class="math inline">\(4 n p_2^k l + 1\)</span> points we found. Why <span class="math inline">\(4 n p_2^k l + 1\)</span>? In expectation, there are <span class="math inline">\(n p_2^k l\)</span> dissimilar collisions. By Markov inequality, the probability of dissimilar collisions is no more than <span class="math inline">\(4l\)</span> is at most <span class="math inline">\(1 / 4\)</span>. By union bound, <span class="math display">\[
 \begin{aligned}
     &amp;\Pr[\exists \text{ more than } 4l \text{ bad collision } \vee \nexists \text{ good collision }] \\
     &amp;\le \frac{1}{e} + \frac{1}{4} \\
     &amp;\le \frac{1}{2.5} + \frac{1}{4} \\
     &amp;= \frac{5}{16}
 \end{aligned}
 \]</span></li>
</ol>
<p>Therefore, the boosting strategy successes with constant probability. Now consider the running time of such boosting strategy. When a query point <span class="math inline">\(q\)</span> comes, we need to compute <span class="math inline">\(g_1(q), g_2(q), ..., g_l(q)\)</span>, which takes time <span class="math inline">\(kl\)</span>. Further, the strategy checks at most <span class="math inline">\(4 n p_2^k l + 1\)</span> points and checking each point takes <span class="math inline">\(O(d)\)</span> time. The overall time used is <span class="math display">\[
O(kl + (4 n p_2^k l + 1)d)
\]</span></p>
<p>To minimize the running time, we need to choose proper value of <span class="math inline">\(k\)</span>, such that <span class="math display">\[
k = 4np_2^k d \rightarrow \ln k + k \ln \frac{1}{p_2} = \ln (4nd) 
\]</span></p>
<p>It suffice to take <span class="math inline">\(k = \frac{\ln 4nd}{ \ln \frac{1}{p_2} }\)</span>. It follows that <span class="math display">\[
p_2^k = \frac{1}{4nd} \\
l = \left( \frac{1}{p_2^k} \right)^\rho = (4nd)^\rho 
\]</span></p>
<p>and the running time is given by <span class="math display">\[
O\left( (4nd)^\rho \frac{\ln (4nd)}{\ln \frac{1}{p_2} } \right)
\]</span></p>
<p>As <span class="math inline">\(n \succ d\)</span>, this is equivalent to <span class="math display">\[
O\left( (4nd)^\rho \frac{\ln n}{\ln \frac{1}{p_2} } \right)
\]</span></p>
<p>We can repeat the above steps by <span class="math inline">\(\ln n / \ln \frac{16}{5}\)</span> times, to boost the success probability to <span class="math inline">\(1 - \frac{1}{n}\)</span>. The overall running time is given by <span class="math display">\[
O\left( (4nd)^\rho \frac{\ln^2 n}{\ln \frac{1}{p_2} } \right)
\]</span></p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>Indyk, P. and Motwani, R., 1998, May. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (pp. 604-613).</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/23/Working-Set-BST/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/23/Working-Set-BST/" class="post-title-link" itemprop="url">Working Set BST</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-23 23:59:24" itemprop="dateCreated datePublished" datetime="2020-03-23T23:59:24+11:00">2020-03-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-29 22:14:08" itemprop="dateModified" datetime="2020-03-29T22:14:08+11:00">2020-03-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Various implementations of Balanced Binary Search Tree (BST) guaranteed membership queries in <span class="math inline">\(O(\log n)\)</span> time, where <span class="math inline">\(n\)</span> is the number of elements in some set <span class="math inline">\(S\)</span>. The working set property is a stronger requirement than <span class="math inline">\(O(\log n)\)</span> time complexity. Suppose that an item <span class="math inline">\(x \in S\)</span> is accessed <span class="math inline">\(t\)</span> queries before, then we want to perform <span class="math inline">\(\text{find} (x)\)</span> in <span class="math inline">\(O(\log t)\)</span> time.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/WorkingSetBSForest.jpg" /></p>
<p>The idea is to maintain a sequence of Balanced BST's, denoted as <span class="math inline">\(T_0, T_1, T_2, ...\)</span>. Denote the size of <span class="math inline">\(T_i\)</span> for some <span class="math inline">\(i \ge 0\)</span> as <span class="math inline">\(n_i\)</span>. Then the <span class="math inline">\(i\)</span>-th tree maintains the latest <span class="math inline">\(n_i\)</span> accessed items. The sizes of <span class="math inline">\(n_i\)</span>'s satisfy</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;   n_0 = 1 \\
    &amp;   n_1 = 2 \\
    &amp;   n_2 = 4 \\
    &amp;   n_3 = 16 \\
    &amp;   ...     \\
    &amp;   n_i = 2^{2^{i - 1} }  \\
    &amp;   n_{i + 1} = n_i^2 = 2^{2^i } \\
    &amp;   ... \\
    &amp;   n_k = 2^{2^{k - 1} }  
\end{aligned}
\]</span></p>
<p>The largest tree has an index that equals to the smallest <span class="math inline">\(k\)</span> with <span class="math inline">\(2^{k - 1} \ge \log n\)</span> (assuming that <span class="math inline">\(n \ge 1\)</span>). It implies that <span class="math inline">\(k = O (\log \log n)\)</span>. The complexities for searching an element on these trees are <span class="math display">\[
\begin{aligned}
    &amp;   c_0 = 1 + \log 1    \\
    &amp;   c_1 = 1 + \log 2    \\
    &amp;   c_2 = 1 + \log 4    \\
    &amp;   c_3 = 1 + \log 16   \\
    &amp;   ...     \\
    &amp;   c_k = 1 + \log 2^{2^{k - 1}}\\
\end{aligned}
\]</span></p>
<p>When a query <span class="math inline">\(\text{find} (x)\)</span> is invoked, we first search it on <span class="math inline">\(T_0\)</span>. If we find it successfully, we stop. Otherwise we search it on <span class="math inline">\(T_1\)</span> and so on. If <span class="math inline">\(x\)</span> is accessed <span class="math inline">\(t\)</span> queries before (and is not accessed in the latest <span class="math inline">\(t - 1\)</span> queries if <span class="math inline">\(t \ge 1\)</span>), then it is contained in the tree <span class="math inline">\(T_{i_x}\)</span>, where <span class="math display">\[
i_x \doteq \text{ the smallest non-negative integer, } s.t. \ 2^{i_x - 1} \ge t
\]</span></p>
<p>As a sanity check, note that when <span class="math inline">\(t = 0\)</span> (<span class="math inline">\(x\)</span> is the latest accessed item), we have <span class="math inline">\(i_x = 0\)</span> as <span class="math inline">\(2^{-1} \ge 0\)</span>. By definition of definition of <span class="math inline">\(i_x\)</span>, <span class="math inline">\(x\)</span> is not contained in any <span class="math inline">\(T_i\)</span> with <span class="math inline">\(i &lt; i_x\)</span>. The time to search <span class="math inline">\(x\)</span> is <span class="math display">\[
\begin{aligned}
    &amp;(1 + \log 1) + (1 + \log 2) + (1 + \log 4) + (1 + \log 16) + ... + (1 + \log 2^{2^{i_x - 1} }) \\
    &amp;=i_x + 1  + \sum_{i = 0}^{i_x - 1} 2^{i} \\
    &amp;=i_x + 1  + 2^{i_x} - 1 \\
    &amp;=O(\log \log t) + O(\log t) \\
    &amp;=O(\log t)    
\end{aligned}
\]</span></p>
<p>Finally, we need to update the trees <span class="math inline">\(T_0\)</span>, <span class="math inline">\(T_1\)</span>, ... <span class="math inline">\(T_{i_x}\)</span> to reflect the last access of <span class="math inline">\(x\)</span>. In particular, we 1) insert <span class="math inline">\(x\)</span> into <span class="math inline">\(T_0\)</span>, <span class="math inline">\(T_1\)</span>, ... <span class="math inline">\(T_{i_x - 1}\)</span>; 2) for each <span class="math inline">\(i &lt; i_x\)</span>, we delete the least recently accessed item; 3) update the access time of <span class="math inline">\(x\)</span> in <span class="math inline">\(T_{i_x}\)</span>.</p>
<p>Step 1) takes <span class="math inline">\(O(\log t)\)</span> time. To implement steps 2) and 3) in <span class="math inline">\(O(\log t)\)</span> time, we create a min-heap <span class="math inline">\(H_i\)</span> for each tree <span class="math inline">\(T_i\)</span> that keeps the the access time of items in <span class="math inline">\(T_i\)</span>. Both fetching least recently accessed item and updating access time can be performed in <span class="math inline">\(O(\log n_i)\)</span> time. Therefore, steps 2) and 3) take <span class="math inline">\(O(\log t)\)</span> time in total.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/09/Hessian/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/09/Hessian/" class="post-title-link" itemprop="url">Hessian</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-09 13:18:20 / Modified: 14:09:55" itemprop="dateCreated datePublished" datetime="2020-03-09T13:18:20+11:00">2020-03-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>Theorem.</em> A twice continuous differentiable function $f: ^n  ightarrow  $ is convex if and only if <span class="math inline">\(\nabla^2 f(x) \succeq 0\)</span> for <span class="math inline">\(x \in \mathbb{ R } ^n\)</span>.</p>
<p><em>Proof.</em> We use that fact that <span class="math inline">\(f\)</span> is convex iff <span class="math inline">\(f(y) \ge f(x) + \nabla f(x)^T (y - x)\)</span> for all <span class="math inline">\(y, x \in \mathbb{ R } ^n\)</span>.</p>
<ol type="1">
<li><p><em>If:</em> For <span class="math inline">\(x, y \in \mathbb{ R } ^n\)</span>, exists <span class="math inline">\(z = \lambda x + (1 - \lambda) y\)</span>, where <span class="math inline">\(\lambda \in [0, 1]\)</span>, such that (by Taylor series expansion and Lagrange reminder) <span class="math display">\[
\begin{aligned}
 f(y) 
     &amp;= f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f(z) (y - x) \\
     &amp;\ge f(x) + \nabla f(x)^T (y - x) 
\end{aligned}
\]</span></p></li>
<li><p><em>Only If:</em> Rearranging the Taylor series we get, <span class="math display">\[
 \begin{aligned}
     f(y) - f(x) - \nabla f(x)^T (y - x) 
         &amp;= \frac{1}{2} (y - x)^T \nabla^2 f(z) (y - x) 
 \end{aligned}
 \]</span></p>
<p>Which implies that, <span class="math display">\[
 \begin{aligned}
     (y - x)^T \nabla^2 f(z) (y - x) \ge 0
 \end{aligned}
 \]</span></p>
<p>Since <span class="math inline">\(\nabla^2 f(x)\)</span> is continuous, <span class="math inline">\(\forall \epsilon &gt; 0, \exists \delta &gt; 0\)</span>, such that <span class="math inline">\(\forall z \in \mathbb{B}(x, \delta)\)</span>, it holds that <span class="math display">\[
 \| \nabla^2 f(z) - \nabla^2 f(x) \|_F \le \epsilon^2 / n^4
 \]</span></p>
<p>where <span class="math inline">\(\| A \|_F \doteq Tr(A^TA)\)</span> for <span class="math inline">\(A \in \mathbb{ R } ^{n \times n}\)</span>. Note that <span class="math inline">\(\| A \|_F \le \epsilon^2 / n^4\)</span> implies that <span class="math inline">\(A_{i,j} \ge -\epsilon / n^2\)</span>.</p>
<p>Without lose of generality, suppose that <span class="math inline">\(s \doteq y - x\)</span> is a unit vector. Then <span class="math inline">\(x + \delta s \in \mathbb{B}(x, \delta)\)</span>, and <span class="math inline">\(z \in [x, x + \delta s]\subset \mathbb{B}(x, \delta)\)</span> (where <span class="math inline">\([x, x+ \delta s]\)</span> is the line segment between <span class="math inline">\(x\)</span> and <span class="math inline">\(x + \delta s\)</span>). It follows: <span class="math display">\[
 \begin{aligned}
     \delta^2 s^T \left(\nabla^2 f(x) - \nabla^2 f(z) \right) s
         &amp;\ge \sum_{i, j} \frac{\epsilon}{n^2} \delta |s_i| \cdot \delta | _j| \\
         &amp;= \delta \frac{\epsilon}{n^2} (|s|)^2
 \end{aligned}
 \]</span></p>
<p>where <span class="math inline">\(|s| = \sum_{i \in [n]} |s_i| \le \sqrt n \| s \|\)</span>. Therefore,</p>
<p><span class="math display">\[
 \begin{aligned}
     s^T \left(\nabla^2 f(x) - \nabla^2 f(z)  \right) s
         &amp;\ge -\epsilon \| s \|^2
         = -\epsilon
 \end{aligned}
 \]</span></p>
<p>Finally, <span class="math display">\[
 \begin{aligned}
     s^T \nabla^2 f(x) s
         &amp;= s^T \nabla^2 f(z) s + s^T \left(\nabla^2 f(x) - \nabla^2 f(z) \right) s \\
         &amp;\ge -\epsilon
 \end{aligned}
 \]</span></p>
<p>As <span class="math inline">\(\epsilon\)</span> can be arbitrary small, it hold that <span class="math inline">\(s^T \nabla^T f(x) s \ge 0\)</span>. This holds for any <span class="math inline">\(\| s \| = 1\)</span>, it concludes that <span class="math inline">\(\nabla^2 f(x) \succeq 0\)</span>.</p></li>
</ol>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/03/08/Norm-with-PSD-Matrix/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/08/Norm-with-PSD-Matrix/" class="post-title-link" itemprop="url">Norm with PSD Matrix</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-08 11:42:32" itemprop="dateCreated datePublished" datetime="2020-03-08T11:42:32+11:00">2020-03-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-09 15:28:01" itemprop="dateModified" datetime="2020-03-09T15:28:01+11:00">2020-03-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(A \in \mathbb{ R } ^{n \times n}\)</span> be positive semi-definite matrix. Then <span class="math inline">\(f (x) \doteq \sqrt {x^T A x}: \mathbb{ R } ^{n \times n} \rightarrow \mathbb{ R }\)</span> is a norm.</p>
<p><em>Proof.</em> We only check that it satisfies triangle inequality, i.e., <span class="math inline">\(\forall x, y \in \mathbb{R}^n\)</span> <span class="math display">\[
\begin{aligned}
    f(x + y) 
        &amp;\le f(x) + f(y)
        \\
    &amp;\longleftrightarrow
        \\
    \sqrt{ (x + y)^T A (x + y)^T } 
        &amp;\le \sqrt{ x^T A x} + \sqrt{y^T A y} 
        \\
    &amp;\longleftrightarrow
        \\
    y^T A x
        &amp;\le \sqrt{ x^T A x}  \sqrt{y^T A y} 
\end{aligned}
\]</span> Since <span class="math inline">\(A\)</span> is PSD, then <span class="math inline">\(\exists B \in \mathbb{ R } ^{n \times n}\)</span>, s.t., <span class="math inline">\(A = B^T B\)</span>. Define <span class="math inline">\(\bar y = B y\)</span> and <span class="math inline">\(\bar x = B x\)</span>, we need to prove <span class="math display">\[
\bar y^T \bar x \le \| \bar x\| \cdot \| \bar y \|
\]</span> But this is just Cauchy-Schwarz inequality.</p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description">你生之前悠悠千載已逝，未來還會有千年沉寂的期待</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
