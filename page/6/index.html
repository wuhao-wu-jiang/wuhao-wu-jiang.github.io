<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/08/24/Projection-of-A-Random-Unit-Vector-on-Its-First-Dimension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/24/Projection-of-A-Random-Unit-Vector-on-Its-First-Dimension/" class="post-title-link" itemprop="url">Projection of A Random Unit Vector on Its First Dimension</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-24 16:27:36" itemprop="dateCreated datePublished" datetime="2018-08-24T16:27:36+10:00">2018-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-30 14:20:30" itemprop="dateModified" datetime="2020-05-30T14:20:30+10:00">2020-05-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(x = &lt;x_1, x_2, ..., x_d&gt;\)</span> be an <span class="math inline">\(d\)</span> dimension random unit vector, i.e., a vector generated uniformly at random from the sphere of the unit ball. What is the probability that <span class="math inline">\(|x_1| \ge \frac{1}{n}\)</span> for some given <span class="math inline">\(n \ge 2\)</span>?</p>
<p>When <span class="math inline">\(d = 1\)</span>, then <span class="math inline">\(x_1 = 1\)</span> or <span class="math inline">\(x_1 = -1\)</span>. Therefore <span class="math inline">\(P[|x_1| \ge \frac{1}{n}] = 1\)</span>.</p>
<p>When <span class="math inline">\(d = 2\)</span>,</p>
<p><span class="math display">\[
P[|x_1| \ge \frac{1}{n}] = \frac{4}{2 \pi} \int_{1/n}^1 \sqrt{1 - t^2} d_t = \frac{2}{\pi} \arccos \frac{1}{n}
\]</span></p>
<p>By <span class="math display">\[
\begin{aligned}
\arcsin(z) &amp;= z + \left({\frac {1}{2} } \right) {\frac {z^{3} } {3} } +\left({\frac {1\cdot 3}{2\cdot 4} } \right){\frac {z^{5} } {5} } +\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6} } \right){\frac {z^{7} } {7} } +\cdots \\\\
&amp;=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!} } \cdot {\frac {z^{2n+1} } {2n+1} } \\\\
&amp;=\sum _{n=0}^{\infty }{\frac { {\binom {2n}{n} } z^{2n+1} } {4^{n}(2n+1)} } \qquad |z|\leq 1
\end{aligned}
\]</span></p>
<p>We get <span class="math display">\[
\begin{aligned}
\arccos \frac{1}{n} &amp;= \frac{\pi}{2} - \arcsin \frac{1}{n} \\\\
&amp;= \frac{\pi}{2} - (\frac{1}{n} + \left({\frac {1}{2} } \right) {\frac {\frac{1}{n}^{3} } {3} } +\left({\frac {1\cdot 3}{2\cdot 4} } \right){\frac {\frac{1}{n}^{5} } {5} } +\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6} } \right){\frac {\frac{1}{n}^{7} } {7} } +\cdots  ) \\\\
&amp;\ge \frac{\pi}{2} - \frac{1}{n} - \frac{1}{2 n^3}\left( \sum_{i = 0}^\infty 1 / 2^i \right)  \qquad since \ n \ge 2 \\\\
&amp;\ge  \frac{\pi}{2} - \frac{5}{4n}
\end{aligned}
\]</span></p>
<p>It follows that <span class="math display">\[
P[|x_1| \ge \frac{1}{n}] = \frac{2}{\pi} \arccos \frac{1}{n} \ge 1 - \frac{1}{n}
\]</span></p>
<p>For general values of <span class="math inline">\(d\)</span>, we would like to give an easier estimation by upper bounding the following integration</p>
<p><span class="math display">\[
\begin{aligned}
\begin{matrix} 
\int_{x_1 = 0}^{1 / n} &amp; \underbrace{\int_{x_2 = 0}^{1} \int_{x_3 = 0}^{1} ... \int_{x_d = 0}^{1} }  &amp; 1 \  d_{x_d} d_{x_{d - 1} }  ... d_{x_1} 
\le \frac{1}{n} &amp;\underbrace{\int_{x_2 = 0}^{1} \int_{x_3 = 0}^{1} ... \int_{x_d = 0}^{1} }  &amp; 1 \  d_{x_d} d_{x_{d - 1} }  ... d_{x_2} \\\\
&amp; x_2^2 + x_3^2 + ... + x_d^2 \le 1 - x_1^2 &amp;
&amp; x_2^2 + x_3^2 + ... + x_d^2 = 1 &amp;
\end{matrix} 
\end{aligned}
\]</span></p>
<p>Using the result that the surface area of a <span class="math inline">\(d - 1\)</span> dimension sphere is given by</p>
<p><span class="math display">\[
{\frac {2\pi ^{\frac {d}{2} } }{\Gamma \left({\frac {d}{2} } \right)} } 
\]</span></p>
<p>We get <span class="math display">\[
\begin{aligned}
    P[|x_1| \ge \frac{1}{n}] 
    &amp;= 1 - \frac{1}{n} \frac{ {\frac {2\pi ^{\frac {d}{2} } }{\Gamma \left({\frac {d}{2} } \right)} } }{ {\frac {2\pi ^{\frac {d + 1}{2} } }{\Gamma \left({\frac {d + 1}{2} } \right)} } } \\
    &amp;= 1 - \frac{1}{n} \frac{ \Gamma \left({\frac {d + 1}{2} } \right)}{ \Gamma \left({\frac {d}{2} } \right) \pi^{\frac {1}{2} } } \\
    &amp;\ge 1 - \sqrt{ \frac{d}{2\pi} } \frac{e}{n}
\end{aligned}
\]</span></p>
<p>where by Stirling’s formula <span class="math display">\[
\begin{aligned}
    \frac{ \Gamma \left({\frac {d + 1}{2} } \right)}{ \Gamma \left({\frac {d}{2} } \right)} &amp;\le (\frac{d + 1}{2})^{\frac {d + 1}{2} - 1 / 2} e^{1 / (12 (\frac {d + 1}{2}))}  / (\frac{d}{2})^{\frac{d}{2} - 1 / 2} \\
    &amp;= (1 + \frac{1}{d})^{d/2} e^{1 / (6(d + 1))} (\frac{d}{2})^{1 / 2} \\
    &amp;\le e (\frac{d}{2})^{1 / 2}
\end{aligned}
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/08/23/Unbiased%20Estimator%20of%20Sampling%20Variance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/23/Unbiased%20Estimator%20of%20Sampling%20Variance/" class="post-title-link" itemprop="url">Unbiased Estimator of Sampling Variance</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-23 17:30:22" itemprop="dateCreated datePublished" datetime="2018-08-23T17:30:22+10:00">2018-08-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-22 11:44:17" itemprop="dateModified" datetime="2019-09-22T11:44:17+10:00">2019-09-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a set of <span class="math inline">\(i.i.d.\)</span> random variables with mean <span class="math inline">\(\mu\)</span> and varaince <span class="math inline">\(\sigma^2\)</span>. Define <span class="math inline">\(\overline X = \frac{1}{n} \sum_i X_i\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
\sum_i (X_i - \overline X)^2 
&amp;= \sum_i (X_i - \mu  - (\overline X - \mu))^2 \\
&amp;= \sum_i (X_i - \mu)^2 - 2 \sum_i (X_i - \mu)(\overline X - \mu) + n (\overline X - \mu)^2 \\
&amp;= \sum_i (X_i - \mu)^2 - n (\overline X - \mu)^2
\end{aligned}
\]</span></p>
<p>Define <span class="math inline">\(S = \sum_i X_i\)</span>. Note that <span class="math inline">\(S\)</span> is a random variable with mean <span class="math inline">\(n\mu\)</span> and variance <span class="math inline">\(n\sigma^2\)</span>. Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
E[ \sum_i (X_i - \overline X)^2 ] 
&amp;= \sum_i E[(X_i - \mu)^2] - n E[(\overline X - \mu)^2] \\
&amp;= n \sigma^2 - \frac{1}{n} E \left[ (S - n\mu)^2 \right] \\
&amp;= (n - 1) \sigma^2
\end{aligned}
\]</span></p>
<p>Another way of looking this is that, <span class="math display">\[
\begin{aligned}
\frac{1}{n}\sum_i (X_i - \overline X)^2  
&amp;= \frac{1}{n} (\sum_{i} X_i^2 - 2 \overline X (\sum_{i} X_i) + n(\overline X)^2) \\
&amp;= \frac{1}{n}( \sum_{i} X_i^2 - n(\overline X)^2 ) \\
&amp;= \frac{1}{n} \sum_{i} X_i^2 - \overline X^2 \\
&amp;= \frac{1}{n} \sum_{i} X_i^2 - \mu^2 - (\overline X^2 - \mu^2)
\end{aligned}
\]</span> Therefore, <span class="math display">\[
\begin{aligned}
E[\frac{1}{n}\sum_i (X_i - \overline X)^2]
&amp;= \sigma^2 - \frac{1}{n} \sigma^2
\end{aligned}
\]</span></p>
<p>We can also prove this via linear algebra. Let <span class="math inline">\(X = (X_1, X_2, ..., X_n)\)</span> be a <span class="math inline">\(n\)</span>-dimension vector. Also, denote <span class="math inline">\(e = (1, 1, ..., 1)\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
\overline X &amp;= \frac{1}{n} e^T X  \\
\sum_{i} (X_i - \overline X)^2 &amp;= (X - \frac{1}{n} e e^T X)^T (X -  \frac{1}{n} e e^T X) \\
&amp;= X^T (I -  \frac{1}{n} e e^T)^T (I -  \frac{1}{n} e e^T)X \\
&amp;= X^T (I -  \frac{1}{n} e e^T)(I -  \frac{1}{n} e e^T)X \\
&amp;= X^T (I -  2 \frac{1}{n} e e^T +  \frac{1}{n^2 } e e^T e e^T)X \\
&amp;= X^T (I -  2 \frac{1}{n} e e^T +  \frac{1}{n} e e^T)X \\
&amp;= X^T (I -  \frac{1}{n} e e^T)X \\
&amp;= Tr[X^T (I -  \frac{1}{n} e e^T)X] \\
&amp;= Tr[(I -  \frac{1}{n} e e^T) X X^T]
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(E[X X^T] = \mu^2 e e^T + \sigma^2 I\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
E[\sum_{i} (X_i - \overline X)^2] &amp;= Tr[(I -  \frac{1}{n} e e^T) E[X X^T]] \\
&amp;=Tr[(I -  \frac{1}{n} e e^T)(\mu^2 e e^T + \sigma^2 I)] \\
&amp;=Tr[(ee^T  - ee^T)\mu^2 + (I -  \frac{1}{n} e e^T)\sigma^2] \\
&amp;=Tr[(I -  \frac{1}{n} e e^T)\sigma^2] \\
&amp;=Tr[I -  \frac{1}{n} e e^T] \sigma^2 
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(n - 1\)</span> is the trace as well as the rank of <span class="math inline">\(I - \frac{1}{n}e e^T\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/08/02/Empty-Bins/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/02/Empty-Bins/" class="post-title-link" itemprop="url">Empty Bins</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-08-02 14:06:16 / Modified: 17:29:56" itemprop="dateCreated datePublished" datetime="2018-08-02T14:06:16+10:00">2018-08-02</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose we throw <span class="math inline">\(m\)</span> balls into <span class="math inline">\(n\)</span> bins uniformly at random, what is the probability that at least <span class="math inline">\(\epsilon n\)</span> (<span class="math inline">\(0 &lt; \epsilon &lt; 1\)</span>) bins remain empty?</p>
<p>When we throw a ball, if there are at least <span class="math inline">\(\epsilon n\)</span> empty bins, we increase the number of non-empty bins with probability at least <span class="math inline">\(\epsilon\)</span>. Let <span class="math inline">\(X_1, X_2, ..., X_m\)</span> be <span class="math inline">\(m\)</span> i.i.d Bernoulli random variable that takes <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\epsilon\)</span>. The event that <span class="math inline">\(\epsilon n\)</span> empty bins remains is upper bounded by <span class="math display">\[
\Pr [\sum_{i=1}^m X_i \le (1 - \epsilon) n] = \Pr [\sum_{i=1}^m X_i \le \frac{\epsilon m - \epsilon m + n - \epsilon n}{\epsilon m} \epsilon m] = \Pr [\sum_{i=1}^m X_i \le (1 - (1 - \frac{(1 - \epsilon ) n}{\epsilon m}) \frac{}{} \epsilon m]
\]</span></p>
<p>When <span class="math inline">\(\epsilon\)</span> is fixed, by setting <span class="math inline">\(\frac{1}{\lambda} \doteq \frac{n}{m} =\frac{\epsilon}{1 - \epsilon} (1 - \sqrt{\frac{2n }{\epsilon m} })\)</span> (this is solvable since the left hand side is increasing with <span class="math inline">\(1 / \lambda\)</span> while the right hand side is decreasing with <span class="math inline">\(1 / \lambda\)</span>), we have <span class="math inline">\((1 - \frac{(1 - \epsilon) n}{\epsilon m })^2\frac{\epsilon m}{2} = n\)</span>. Then by Chernoff bound we get <span class="math display">\[
\Pr [\sum_{i=1}^m X_i \le (1 - \epsilon) n] \le  = \exp (-n)
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/22/Connectivity-Parameters/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/22/Connectivity-Parameters/" class="post-title-link" itemprop="url">Connectivity Parameters</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-07-22 11:10:56 / Modified: 13:53:52" itemprop="dateCreated datePublished" datetime="2018-07-22T11:10:56+10:00">2018-07-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given an un-directed graph <span class="math inline">\(G = \langle V, E \rangle\)</span> without self loops and duplicates,</p>
<p><strong>Connectivity</strong>. The connectivity of an edge <span class="math inline">\(e = (u, v)\)</span>, denoted as <span class="math inline">\(\lambda_e\)</span>, is the minimum number of edges to remove to disconnect <span class="math inline">\(u\)</span> from <span class="math inline">\(v\)</span>.</p>
<p><strong>Strength</strong>. The strength of an edge <span class="math inline">\(e = (u, v)\)</span>, denoted as <span class="math inline">\(s_e\)</span>, is the maximum positive integer <span class="math inline">\(k\)</span>, s.t., it belongs to a <span class="math inline">\(k\)</span>-connected vertex induced sub-graph. A sub-graph is called <span class="math inline">\(k\)</span>-connected if its min-cut is at least <span class="math inline">\(k\)</span>.</p>
<h4 id="theorem">Theorem</h4>
<ol type="1">
<li><span class="math inline">\(\sum_{e \in E} 1 / s_e \le n - 1\)</span>, where <span class="math inline">\(n = |V|\)</span>.</li>
<li><span class="math inline">\(\sum_{e \in E} 1 / \lambda_e \le n - 1\)</span></li>
</ol>
<p><strong>Proof</strong>. We prove claim 1 by induction on the number of vertices <span class="math inline">\(n\)</span>. When <span class="math inline">\(n = 2\)</span>, <span class="math inline">\(|E| = 1\)</span>, the strength of the only edge is <span class="math inline">\(1\)</span>. Therefore <span class="math inline">\(\sum_{e \in E} 1 / s_e = 1 / 1 = 1\)</span>. Suppose claim 1 holds for all graphs with <span class="math inline">\(n - 1\)</span> vertices. For a graph with <span class="math inline">\(n\)</span> vertices, consider a particular min-cut <span class="math inline">\((S, \bar{S}) \in V \times V\)</span>. If an edge <span class="math inline">\(e = (u, v)\)</span> is in the min-cut, then its strength is at most <span class="math inline">\(\lambda = G(S, \bar{S})\)</span> (the min-cut value), since we need to remove at most <span class="math inline">\(\lambda\)</span> edges to disconnect <span class="math inline">\(u\)</span> from <span class="math inline">\(v\)</span> in arbitrary vertex induced sub-graph that contains <span class="math inline">\(e\)</span>. But the original graph <span class="math inline">\(G\)</span> contains <span class="math inline">\(e\)</span> and has connectivity <span class="math inline">\(\lambda\)</span>. It follows that the strength of <span class="math inline">\(e\)</span> is <span class="math inline">\(\lambda\)</span> and</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{e \in E} \frac{1}{s_e} 
&amp;= \sum_{e \in G_S} \frac{1}{s_e} + \sum_{e \in G_{\bar{S} } } \frac{1}{s_e} + \sum_{e \in (S, \bar{S}) } \frac{1}{s_e} \\
&amp;\le |S| - 1 + |\bar{S}| - 1 + \lambda \frac{1}{ \lambda } \\
&amp;\le n - 1
\end{aligned}
\]</span></p>
<p>Finally, similar argument applies to claim 2.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/21/Graph%20Sparsification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/21/Graph%20Sparsification/" class="post-title-link" itemprop="url">Graph Sparsification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-21 19:52:33" itemprop="dateCreated datePublished" datetime="2018-07-21T19:52:33+10:00">2018-07-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-22 13:11:24" itemprop="dateModified" datetime="2018-07-22T13:11:24+10:00">2018-07-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given a dense graph <span class="math inline">\(G = \langle V,E \rangle\)</span> and relative error threshold <span class="math inline">\(\epsilon\)</span>, we want to get a spare graph <span class="math inline">\(G&#39; = \langle V, E&#39; \rangle\)</span>, such that <span class="math inline">\(G&#39;(S, \bar{S}) \in (1 \pm \epsilon) G(S, \bar{S})\)</span> for any cut <span class="math inline">\((S, \bar{S}) \in V \times V\)</span>, where <span class="math inline">\(G&#39;(S, \bar{S})\)</span> stands for the number of edges between <span class="math inline">\(S\)</span> and <span class="math inline">\(S&#39;\)</span> in <span class="math inline">\(G&#39;\)</span>.</p>
<p><span class="math inline">\(G&#39;\)</span> can be constructed as follows:</p>
<blockquote>
<ol type="1">
<li><p>Keep all vertices <span class="math inline">\(V\)</span> in <span class="math inline">\(G&#39;\)</span>,</p></li>
<li><p>Keep each edge <span class="math inline">\(e \in E\)</span> in <span class="math inline">\(G&#39;\)</span> with probability <span class="math inline">\(p\)</span>. If an edge is kept, set its weight to <span class="math inline">\(1/p\)</span>.</p></li>
</ol>
</blockquote>
<p>We will determine the value of <span class="math inline">\(p\)</span> later.</p>
<p>There are many possible <span class="math inline">\(G&#39;\)</span>'s -- precisely, <span class="math inline">\(2^{|E|}\)</span> of <span class="math inline">\(G&#39;\)</span>'s. At the first glimpse, it not clear what we get out of them. However, we will show that for a large fraction of them satisfy our requirement. In other words, we get a <em>good</em> <span class="math inline">\(G&#39;\)</span> with high probability.</p>
<p>To show this, first observe that each edge has expected weight <span class="math inline">\(p \cdot 1 / p + (1 - p) \cdot 0 = 1\)</span>. Therefore, by linearity of expectation, the expected value of each cut in <span class="math inline">\(G&#39;\)</span> equals to its value in <span class="math inline">\(G\)</span>, i.e., <span class="math inline">\(E[G&#39;(S, \bar{S})] = G(S, \bar{S})\)</span>, for <span class="math inline">\(\forall (S, \bar{S}) \in V \times V\)</span>.</p>
<p>By proper choice of <span class="math inline">\(p\)</span>, all <span class="math inline">\(G&#39;(S, \bar{S})\)</span> will concentrate around its expectation. Moreover, the more edges a cut <span class="math inline">\((S, \bar{S}) \in G\)</span> has, the more likely its expectation has a smaller deviation. This motivates that <span class="math inline">\(p\)</span> is set according to the min-cut of <span class="math inline">\(G\)</span>.</p>
<p>Throughout discussion below, we denote <span class="math inline">\(n\)</span> as the number of vertices, <span class="math inline">\(m\)</span> the number of edges, and <span class="math inline">\(c^*\)</span> the min-cut value in <span class="math inline">\(G\)</span>.</p>
<h4 id="two-important-results">Two Important Results</h4>
<p>The first one is the famous combinatorial ramification of Karger Algorithm.</p>
<p><strong>Theorem 1</strong>. The number of cuts whose weight are within <span class="math inline">\(\alpha c^*\)</span> is upper bounded by <span class="math inline">\(n^{2 \alpha} \frac{2^{2\alpha}}{ (2\alpha)! } \frac{e}{2 \pi \sqrt 2}\)</span>, where <span class="math inline">\(\alpha \ge 1, 2 \alpha \in N^+\)</span>.</p>
<p>Define <span class="math inline">\(f(x)\)</span> (<span class="math inline">\(x \in R, x \ge 1\)</span>) the number of cuts in <span class="math inline">\(G&#39;\)</span> with value <span class="math inline">\(x c^*\)</span>, where <span class="math inline">\(x c^*\)</span> is an integer. <span class="math inline">\(F(\alpha) = \sum_{x \le \alpha} f(x)\)</span> is bounded by <span class="math inline">\(n^{2\alpha}\)</span>.</p>
<p>The second is Chernoff Bound.</p>
<p><strong>Theorem 2</strong>. Let <span class="math inline">\(\{ X_i \}\)</span> be a set of random variables and <span class="math inline">\(X \doteq \Sigma X_i\)</span>, <span class="math inline">\(E[X] = \mu\)</span>, then <span class="math inline">\(Pr[|X - \mu| \ge \epsilon \mu] \le 2 \exp(- \epsilon^2 \mu /3)\)</span>.</p>
<p>By Chernoff bound, the probability that a min-cut derivate more than <span class="math inline">\(\epsilon c^*\)</span> is bounded by <span class="math inline">\(2 \exp (-\epsilon^2 p c^* / 3)\)</span>. Similarly, the bound for a cut with value <span class="math inline">\(\alpha c^*\)</span> is <span class="math inline">\(2 \exp (-\epsilon^2 \alpha p c^* / 3)\)</span>.</p>
<p>Denote <span class="math inline">\(c&#39; = cp\)</span>. It follows by union bound that the probability that some cut deviate more than <span class="math inline">\(\epsilon\)</span> times it expected value is at most <span class="math inline">\(\sum_x f(x) \cdot 2 \exp(-\epsilon^2xc&#39;/3)\)</span>.</p>
<p>It can be shown that to maximum this value, we need <span class="math inline">\(F(\alpha)\)</span> takes the value <span class="math inline">\(n^{2 \alpha}\)</span> any for <span class="math inline">\(\alpha \ge 1, 2\alpha \in N^+\)</span>. Hence the sum is less than <span class="math display">\[
2f(1)e^{-\epsilon^2 c&#39; /3} + 2 \int_{x = 1}^\infty {\partial n^{2x} \over \partial x} e^{-\epsilon^2 x c&#39;/ 3} dx
\]</span></p>
<p>That is <span class="math display">\[
2 n^2 e^{-\epsilon^2 c&#39; /3}(1 + {\ln n^2 \over \epsilon^2 c&#39; /3 - \ln n^2}) 
\]</span></p>
<p>If we let <span class="math inline">\(p = 3(d + 2) \ln n / (\epsilon^2 c)\)</span> for some parameter d, then the bound becomes <span class="math inline">\(O(n^{-d})\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/21/Karger%20Algorithm%20I/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/21/Karger%20Algorithm%20I/" class="post-title-link" itemprop="url">Karger Algorithm I</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-21 12:53:14" itemprop="dateCreated datePublished" datetime="2018-07-21T12:53:14+10:00">2018-07-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-09-04 01:30:08" itemprop="dateModified" datetime="2018-09-04T01:30:08+10:00">2018-09-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Karger Algorithm</p>
<p>The Karger algorithm was proposed by David R. Karger [<em>Global Min-cuts in RNC, and Other Ramifications of a Simple Min-Cut Algorithm</em>]. It is a randomized algorithm to find a min-cut of an undirected connected graph.</p>
<p>The idea of the algorithm is simple -- randomly pick an edge from the graph, and merge the endpoints of this edge into one. The edges incident on two merged endpoints is now incident on the new endpoint. Repeat this process until two nodes are left on the graph. We show that the probability that the number of edges remained between the two nodes equals the value of min-cut is at least <span class="math inline">\(\frac{2}{n(n - 1)}\)</span>.</p>
<p>Formally, suppose we are given an undirected graph <span class="math inline">\(G = (V, E)\)</span> with <span class="math inline">\(n = |V|\)</span> vertices, <span class="math inline">\(m = |E|\)</span> edges, and <span class="math inline">\(c^*=\)</span> <em>size of the min-cut</em>.</p>
<h4 id="karger-algorithm">Karger Algorithm</h4>
<ol type="1">
<li>Select an edge <span class="math inline">\(e = (u,v) \in E\)</span> uniformly with probability <span class="math inline">\(1/|E|\)</span>.<br />
</li>
<li>Contract <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> into a new vertex <span class="math inline">\(w\)</span>, and all edges -- except <span class="math inline">\(e\)</span>-- incident on <span class="math inline">\(u\)</span> or <span class="math inline">\(v\)</span> are now incident on <span class="math inline">\(w\)</span>. In other words,</li>
</ol>
<p><span class="math display">\[
 \begin{aligned}
  V  \leftarrow &amp; V /  \{u,v\} \cup \{w\}, \\
  E  \leftarrow &amp; E /  \{(x, y) | (x,y) \in E \ and \ x \in \{u, v\} \}  \\
   &amp; \cup \{(w,y) | \exists  (x,y) \in E \ and \ x \in \{u, v\} \ and \ y \notin \{u,v\} \}
 \end{aligned}
\]</span></p>
<ol start="3" type="1">
<li>Repeat the above process until |V| = 2.</li>
</ol>
<p>In the end, there is only one cut between the remaining two vertices, whose size we denote as <span class="math inline">\(c\)</span>. If follows that <span class="math inline">\(P(c = c^*) = \frac{2}{n ( n - 1)}\)</span>.</p>
<p>By repeating the process <span class="math inline">\(\frac{n (n - 1)}{2} \log n\)</span> times, the probability we don't find the size of min-cut is at most</p>
<p><span class="math display">\[
\big( 1 - \frac{2}{n (n - 1)} \big)^{\frac{n (n - 1)}{2} \log n} \le e^{log n} = \frac{1}{n}
\]</span></p>
<p>Now we prove that <span class="math inline">\(P(c = c^*) = \frac{2}{n ( n - 1)}\)</span>.</p>
<p>The key observation is that the degree each vertex is at least <span class="math inline">\(c^*\)</span>. Therefore there are at least <span class="math inline">\(\frac{c^* n}{2}\)</span> edges for a graph with <span class="math inline">\(n\)</span> vertices. Given a particular min-cut, which has <span class="math inline">\(c^*\)</span> edges, the probability that any of its edges is selected during the contraction operation is at most</p>
<p><span class="math display">\[
\frac{c^*}{|E|} \le \frac{c^*}{ \frac{c^* n}{2} } = \frac{2} { n }
\]</span></p>
<p>The second observation is that, if the edges of a min-cut are not selected during contraction, then the min-cut value of the contracted graph remains the same. After <span class="math inline">\(i\)</span> contractions, there are <span class="math inline">\(n - i\)</span> vertices and the probability a min-cut survives the <span class="math inline">\(i + 1\)</span> contraction is most <span class="math inline">\(\frac{2}{n - i}\)</span>. After <span class="math inline">\(n - 1\)</span> round, the min-cut remains with probability at least <span class="math display">\[
(1 - \frac{2}{n})(1 - \frac{2}{n - 1})...(1 - \frac{2}{3}) = \binom{n}{2}^{-1}
\]</span> Some interesting corollaries could be derived from this.</p>
<h4 id="corollary-one">Corollary one</h4>
<blockquote>
<p>There are <span class="math inline">\(\frac{n (n - 1)}{ 2 }\)</span> min-cuts in the graph, since the events of outputting each min-cut are disjoint.</p>
</blockquote>
<p>By slightly modifying the algorithm, we can give another fact.</p>
<p><strong>Definition</strong>. A <span class="math inline">\(\alpha\)</span>-min-cut is a cut with no more than <span class="math inline">\(\alpha c^*\)</span> edges.</p>
<h4 id="corollary-two">Corollary two</h4>
<p>For any <span class="math inline">\(\alpha\)</span> s.t. <span class="math inline">\(2 \alpha\)</span> is an integer, the number of <span class="math inline">\(\alpha\)</span>-min-cuts is at most <span class="math inline">\(n^{2 \alpha} \frac{2^{2\alpha}}{ (2\alpha)! } \frac{e}{2 \pi \sqrt 2}\)</span>, which is less than <span class="math inline">\(n^{2 \alpha}\)</span>.</p>
<p><strong>Proof</strong>: We perform the contraction as before but stop when there are <span class="math inline">\(2\alpha\)</span> vertexes left. Then we output one the <span class="math inline">\(2^{2\alpha - 1}\)</span> cuts uniformly at random.</p>
<p>The probability a <span class="math inline">\(\alpha\)</span>-min-cut is outputted with probability at least <span class="math display">\[
\begin{aligned}
&amp; (1 - \frac{2 \alpha }{n})(1 - \frac{ 2 \alpha }{ n-1 })...(1 - \frac{2 \alpha }{ 2\alpha + 1 }) \cdot \frac{1}{2^{2 \alpha - 1}}  \\
&amp;=2 \frac{ (n - 2 \alpha)! (2 \alpha)! }{ 2^{2\alpha} n!  } \\
&amp;\ge \frac{ 2 } { 2^{2\alpha} } 
\sqrt{2 \pi (n - 2 \alpha)} \frac{(n - 2\alpha)^{(n - 2\alpha)} }{e ^{(n - 2\alpha)} }
\sqrt{2 \pi (2 \alpha)} \frac{(2\alpha)^{(2\alpha)} }{e ^{(2\alpha)} } 
\frac{1}{e \sqrt n} \frac{e ^{n} }{ n^{n} } \\
&amp;= \frac{ 2 \cdot 2\pi } { e \cdot 2^{2\alpha} } 
\sqrt \frac{(n - 2\alpha) 2\alpha}{n} 
\frac{ (2\alpha)^{(2\alpha)} (n - 2\alpha)^{(n - 2\alpha)} }{ n^n }
\end{aligned}
\]</span> As <span class="math inline">\(\frac{(n - 2\alpha) 2\alpha}{n}\)</span> is minimized to <span class="math inline">\(\frac{1}{2}\)</span> for <span class="math inline">\(n \ge 2, \ 1 \le 2 \alpha \le n - 1\)</span>, the above inequality becomes <span class="math display">\[
\ge \frac{ \sqrt{2} \cdot 2\pi \cdot (2\alpha)^{(2\alpha)}} { e \cdot 2^{2\alpha} } 
\frac{ 1 }{ n^{(2\alpha)} }
\]</span> The number of <span class="math inline">\(\alpha\)</span>-min-cuts is bounded by <span class="math display">\[
n^{2 \alpha} \frac{2^{2\alpha}}{ (2\alpha)! } \frac{e}{2 \pi \sqrt 2} 
\]</span> <strong>Remark</strong>: For <span class="math inline">\(k \in N\)</span>, we have <span class="math display">\[
\sqrt{2 \pi k} \cdot \frac{k^k}{e^k} \le k! \le e \sqrt k \cdot \frac{k^k}{e^k}
\]</span></p>
<h4 id="corollary-three">Corollary three</h4>
<p>The number of <span class="math inline">\(\alpha\)</span>-min-cuts is at most <span class="math inline">\(e^{7/6} n^{2\alpha} \frac{2^{ 2 \alpha} }{( 2 \alpha + 1 )^ {2 \alpha} }\)</span> for any <span class="math inline">\(\alpha \ge 1\)</span>.</p>
<p><strong>Proof</strong>: We perform the contraction as before but stop when there are <span class="math inline">\(\lceil 2\alpha \rceil\)</span> vertexes left. Then we output one the <span class="math inline">\(2^{\lceil 2\alpha \rceil - 1}\)</span> cuts uniformly at random.</p>
<p>The probability a <span class="math inline">\(\alpha\)</span>-min-cut is outputted with probability at least <span class="math display">\[
\begin{aligned}
&amp; (1 - \frac{2 \alpha}{n }) (1 - \frac{2 \alpha}{n - 1 }) ... (1 - \frac{2 \alpha}{\lceil 2 \alpha \rceil + 1 }) 2^{ - (\lceil 2 \alpha \rceil -1)} \\
&amp;= \frac{(n - 2 \alpha) (n - 1 - 2 \alpha) ... (\lceil 2 \alpha \rceil + 1 - 2 \alpha)}{ n (n - 1) ...(\lceil 2 \alpha \rceil + 1)} 2^{ - (\lceil 2 \alpha \rceil -1)} \\
&amp;= \frac{\Gamma(n - 2 \alpha + 1)}{ \Gamma(\lceil 2 \alpha \rceil + 1 - 2 \alpha) } \frac{\Gamma(\lceil 2 \alpha \rceil + 1)}{\Gamma(n + 1)} 2^{ - (\lceil 2 \alpha \rceil -1)} \\
&amp;\ge \sqrt{2\pi} \frac{1}{\sqrt {n - 2 \alpha + 1} }(\frac{n - 2 \alpha + 1}{e})^ {n - 2 \alpha + 1} \\ 
&amp;\ \ \cdot \frac{1}{\sqrt {2 \pi} e^{1 / 12 (\lceil 2 \alpha \rceil + 1 - 2 \alpha ) } } \sqrt{\lceil 2 \alpha \rceil + 1 - 2 \alpha)} (\frac{e}{\lceil 2 \alpha \rceil + 1 - 2 \alpha})^ {\lceil 2 \alpha \rceil + 1 - 2 \alpha} \\
&amp;\ \ \cdot \sqrt{2\pi} \frac{1}{ \sqrt{\lceil 2 \alpha \rceil + 1} }(\frac{ \lceil 2 \alpha \rceil + 1 }{e})^ {\lceil 2 \alpha \rceil + 1} \\
&amp;\ \ \cdot \frac{1}{\sqrt {2 \pi} e^{1 / 12 (n + 1) } } \sqrt{(n + 1)} (\frac{e}{n + 1})^ {n + 1} 2^{ - (\lceil 2 \alpha \rceil -1)} \\
&amp;= \frac{1}{e^{2 / 12}} \frac{\sqrt{\lceil 2 \alpha \rceil + 1 - 2 \alpha} }{ \sqrt {n - 2 \alpha + 1} } \frac{\sqrt{(n + 1)} }{\sqrt{\lceil 2 \alpha \rceil + 1}} 
e^{(\lceil 2 \alpha \rceil + 1 - 2 \alpha ) - (n - 2 \alpha + 1) + (n + 1) - (\lceil 2 \alpha \rceil + 1)} \\
&amp;\ \ \cdot (\frac{n - 2 \alpha + 1}{n + 1})^ {n - 2 \alpha + 1} (\frac{1}{n + 1})^{2\alpha} \cdot 
\frac{( \lceil 2 \alpha \rceil + 1 )^ {\lceil 2 \alpha \rceil + 1} }{ (\lceil 2 \alpha \rceil + 1 - 2 \alpha)^ {\lceil 2 \alpha \rceil + 1 - 2 \alpha}} 2^{ - (\lceil 2 \alpha \rceil -1)} \\
&amp;\ge  \frac{1}{e^{1 / 6}} (\frac{1}{n + 1})^{2\alpha} \cdot 
\frac{( \lceil 2 \alpha \rceil + 1 )^ {\lceil 2 \alpha \rceil + 1 / 2} }{ (\lceil 2 \alpha \rceil + 1 - 2 \alpha)^ {\lceil 2 \alpha \rceil + 1 / 2 - 2 \alpha}} 2^{ - (\lceil 2 \alpha \rceil -1)} \\
&amp;\ge \frac{1}{e^{1 / 6}} (\frac{1}{n + 1})^{2\alpha} \cdot 
\frac{( \lceil 2 \alpha \rceil + 1 )^ {2 \alpha} }{2^{ 2 \alpha} } \\
&amp;= \frac{1}{e^{1 / 6}} \frac{1}{n^{2\alpha}} \frac{1}{(1 + 1/n)^{2\alpha}}
\frac{( \lceil 2 \alpha \rceil + 1 )^ {2 \alpha} }{2^{ 2 \alpha} } \\
&amp;\ge \frac{1}{e^{7/6} } \frac{1}{n^{2\alpha}} \frac{( \lceil 2 \alpha \rceil + 1 )^ {2 \alpha} }{2^{ 2 \alpha} } \\
&amp;\ge \frac{1}{e^{7/6} } \frac{1}{n^{2\alpha}} \frac{( 2 \alpha + 1 )^ {2 \alpha} }{2^{ 2 \alpha} }
\end{aligned}
\]</span></p>
<p>To bound the inequality, we need the stirling formula for Gamma functions.</p>
<p><strong>Lemma</strong>. <span class="math inline">\(\forall x \ge 0\)</span>, <span class="math display">\[
\Gamma(x + 1) = x \Gamma(x) \\
\sqrt{2\pi} \frac{1}{\sqrt x} (\frac{x}{e})^x 
\le 
\Gamma(x)
\le
\sqrt{2\pi} \frac{1}{\sqrt x} (\frac{x}{e})^x e^{\frac{1}{12 x}}
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/20/Random-Points-on-Shpere/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/20/Random-Points-on-Shpere/" class="post-title-link" itemprop="url">Random Points on Shpere</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-20 10:45:18" itemprop="dateCreated datePublished" datetime="2018-07-20T10:45:18+10:00">2018-07-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-09-16 18:08:01" itemprop="dateModified" datetime="2018-09-16T18:08:01+10:00">2018-09-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>To generate a random unit vector in <span class="math inline">\(R^n\)</span>, the most trivial approach is to use spherical coordinates and generate the angles uniformly at random. Here we introduce another way : sample <span class="math inline">\(n\)</span> independent identical random variables from Gaussian distribution <span class="math inline">\(N(0, 1)\)</span>, denoted as <span class="math inline">\(x_1, x_2, ..., x_n\)</span>. Then a unit vector <span class="math inline">\(v\)</span> is obtained by</p>
<p><span class="math display">\[
v = \frac{1}{\sum_{i = 1}^n x_i^2 }(x_1, x_2, ..., x_n) 
\]</span></p>
<p>The key observation is that all points at a fixed distance from the origin is equally generated. To see this, the joints distribution of <span class="math inline">\(x_1, x_2, ..., x_n\)</span> is given by</p>
<p><span class="math display">\[
p = \frac{1}{(2 \pi)^{n / 2} } \exp \big( - \frac{\sum_{i=1}^n x_i^2 }{ 2 } \big)
\]</span></p>
<p><span class="math inline">\(p\)</span> is fixed when <span class="math inline">\(\sum_{i = 1}^n x_i^2\)</span> is fixed.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/18/Hashing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/18/Hashing/" class="post-title-link" itemprop="url">Hashing</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-18 15:07:42" itemprop="dateCreated datePublished" datetime="2018-07-18T15:07:42+10:00">2018-07-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-03 00:17:49" itemprop="dateModified" datetime="2020-05-03T00:17:49+10:00">2020-05-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(S = \{ x_1, x_2, ..., x_m \}\)</span> be a set of distinct element from domain <span class="math inline">\(U\)</span>. A function <span class="math inline">\(h: U \rightarrow [n]\)</span> that takes element from <span class="math inline">\(U\)</span> to a smaller domain <span class="math inline">\([n]\)</span> is called a hash function, where <span class="math inline">\([n] = \{ 0, 1, ..., n - 1\}\)</span> and <span class="math inline">\(n \le |U|\)</span>. In the case where <span class="math inline">\(n &lt; |U|\)</span>, there must be some <span class="math inline">\(i \in [n]\)</span>, s.t., <span class="math inline">\(|h^{-1}(i)| &gt; 1\)</span>. We are interested in the number of total duplicate value in <span class="math inline">\(h(S) = \{ h(x_1), h(x_2), ..., h(x_m) \}\)</span>.</p>
<p>If both <span class="math inline">\(S\)</span> and <span class="math inline">\(h\)</span> are fixed, there is nothing uncertain. There are two ways to add some randomness to our analysis</p>
<ol type="1">
<li><p><span class="math inline">\(h\)</span> is fixed, and <span class="math inline">\(S\)</span> follows some distribution on <span class="math inline">\(U\)</span>. As an example, let <span class="math inline">\(U = [0, 2^{20} - 1]\)</span> and <span class="math inline">\(h(x) = x \ \&amp; \ 1023\)</span> that returns the last <span class="math inline">\(10\)</span> bits of <span class="math inline">\(x\)</span> (in binary form). If <span class="math inline">\(S\)</span> is sampled uniformly at random from <span class="math inline">\(U\)</span>, then on average for each <span class="math inline">\(x_i \in S\)</span>, there are <span class="math inline">\(\frac{m - 1}{1024}\)</span> elements that have the same value as <span class="math inline">\(x_i\)</span> under <span class="math inline">\(h\)</span>.</p></li>
<li><p>The assumption that <span class="math inline">\(S\)</span> follows some distribution is too strong. Instead we assume that given <span class="math inline">\(S\)</span>, we select a function <span class="math inline">\(h\)</span> uniformly, from a set of functions <span class="math inline">\(\mathcal{H}\)</span>. Now we are going to analyze the probability <span class="math inline">\(P[ h(x_i) = h(x_j) ]\)</span> for <span class="math inline">\(i \neq j\)</span>.</p></li>
</ol>
<p>Let <span class="math inline">\(p &gt; |U|\)</span> be a prime number and <span class="math display">\[
\mathcal{H} \doteq \{ h(x) = ax + b \mod p \mod n | 1 \le a &lt; p, 0 \le b &lt; p \}
\]</span></p>
<p>It follows that if <span class="math inline">\(h\)</span> is sampled uniformly at random, <span class="math inline">\(P[h(x_i) = h(x_j)] \le 1 / n\)</span> for <span class="math inline">\(i \neq j\)</span>.</p>
<h5 id="lemma-1.">Lemma 1.</h5>
<p>For <span class="math inline">\(x_i \neq x_j\)</span>, and <span class="math inline">\(s, t \in [p], s \neq t\)</span>, there is a unique pair <span class="math inline">\((a,b) \in [p]^+ \times [p]\)</span> <span class="math display">\[
    a x_i + b \mod p = s \\
    a x_j + b \mod p = t
\]</span> where <span class="math inline">\([p]^+ \doteq \{1, 2, ..., p-1 \}\)</span>.</p>
<p><strong>Proof</strong>: Since <span class="math inline">\([p]\)</span> is a finite filed and <span class="math inline">\(x_i \neq x_j\)</span>, the matrix</p>
<p><span class="math display">\[
\begin{bmatrix}
x_i \mod p, 1 \\
x_j \mod p, 1
\end{bmatrix} \in [p]^{2 \times 2}
\]</span></p>
<p>is invertible. Therefore,</p>
<p><span class="math display">\[
\begin{bmatrix}
a \\ 
b
\end{bmatrix} 
= 
\begin{bmatrix}
x_i \mod p, 1 \\
x_j \mod p, 1
\end{bmatrix}^{-1}
\begin{bmatrix}
s \\ 
t
\end{bmatrix}
\]</span> is unique. Moreover, <span class="math inline">\(a \neq 0\)</span>. Otherwise <span class="math inline">\(b = s\)</span> and <span class="math inline">\(b = t\)</span>, a contradiction to <span class="math inline">\(s \neq t\)</span>.</p>
<h5 id="lemma-2.">Lemma 2.</h5>
<p>For <span class="math inline">\(x_i \neq x_j\)</span>, <span class="math display">\[
\begin{aligned}
P[h(x_i = x_j)] 
&amp;= \sum_{s, t \in [p], s \neq t, s \mod n = t \mod n} P[ h(x_i) = s \wedge h(x_j) = t] \\
&amp;= \sum_{s, t \in [p], s \neq t, s = t \mod n} \frac{1}{ (p-1)p } \\
&amp;= \sum_{s \in [p]} \sum_{t \in [p], t \neq s, t = s \mod n} \frac{1}{ (p-1)p } \\
&amp;\le \sum_{s \in [p]} \frac{p - 1}{ n } \frac{1}{ (p-1)p } \\
&amp;= p \frac{p - 1}{ n } \frac{1}{ (p-1)p } \\
&amp;= \frac{1}{ n }
\end{aligned}
\]</span></p>
<h5 id="lemma-3.">Lemma 3.</h5>
<p>The expected number of pair <span class="math inline">\(x_i, x_j\)</span>, s.t., <span class="math inline">\(x_i \neq x_j \wedge h(x_i) = h(x_j)\)</span> is given by</p>
<p><span class="math display">\[
E[\sum_{x_i \neq x_j} 1_{h(x_i) = h(x_j)}] = \sum_{x_i \neq x_j} P[h(x_i = x_j)] = \binom{m}{2} \frac{1}{n} = \frac{m (m - 1)}{ 2n }
\]</span></p>
<p>When <span class="math inline">\(m^2 \le n\)</span>, then <span class="math inline">\(E[\sum_{x_i \neq x_j} 1_{h(x_i) = h(x_j)}] \le \frac{1}{2}\)</span>. By Markov inequality,</p>
<p><span class="math display">\[
P[\exists \quad \text{collision}] \le \frac{1}{ 2 }
\]</span></p>
<p>Lemma 4.</p>
<p>With probability at most <span class="math inline">\(1/2\)</span>, some bin contains more than <span class="math inline">\(\frac{m}{n} + \sqrt{2m}\)</span> balls.</p>
<p>For a fixed bin, let <span class="math inline">\(Y\)</span> denote the number of ball in this bin. Further, define <span class="math display">\[
Y_i = \begin{cases}
1, \qquad \text{if the } i \text{-th ball falls into the bin} \\
0, \qquad \text{otherwise}
\end{cases}
\]</span> Then <span class="math inline">\(Y = \sum_i Y_i\)</span>, and <span class="math display">\[
E[Y] = \frac{m}{n} \\
\]</span> By pair-wise independence, we have <span class="math display">\[
Var[Y] = \sum_i Var[Y_i] = m \frac{1}{n} \left(1 - \frac{1}{n} \right)
\]</span> Therefore, by Chebyshev inequality, we have <span class="math display">\[
\Pr \left[ |Y - E[Y]| \ge \sqrt{2n} \sqrt{ m \frac{1}{n} \left(1 - \frac{1}{n} \right)} \right] = \Pr[|Y - E[Y]| \ge \sqrt{2n} \cdot \sigma] \le \frac{Var[Y]}{2n\sigma^2} = \frac{1}{2n}
\]</span> Finally, applying union bound over <span class="math inline">\(n\)</span> bins, we get <span class="math display">\[
\Pr[\text{some bin gets more than } \sqrt{2m} \text{ balls} ] \le \frac{1}{2}
\]</span></p>
<h5 id="reference">Reference:</h5>
<ol type="1">
<li>Sanjeev Arora, Cos 521: Advanced Algorithm Design, Lecture 1: Course Intro and Hashing</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/17/Sampling-Probability/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/17/Sampling-Probability/" class="post-title-link" itemprop="url">Sampling Probability</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-17 21:34:45" itemprop="dateCreated datePublished" datetime="2018-07-17T21:34:45+10:00">2018-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-18 00:19:26" itemprop="dateModified" datetime="2018-07-18T00:19:26+10:00">2018-07-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose we have a set of number <span class="math inline">\(\{x_1, x_2, ..., x_n \}\)</span> and we would like to estimate its sum <span class="math inline">\(s = \sum_i x_i\)</span>. However, we can only access the set <span class="math inline">\(\{x_1, x_2, ..., x_n \}\)</span> by sampling an element <span class="math inline">\(x_i, 1 \le i \le n\)</span> randomly. Let <span class="math inline">\(p_i\)</span> be the probability that we get <span class="math inline">\(x_i\)</span>. If <span class="math inline">\(p_i\)</span> is known (either before sampling or the moment we get a sampled element, depending on application), then we can still construct an unbiased estimator of <span class="math inline">\(s\)</span>. To see this, define</p>
<p><span class="math display">\[
y_i = \frac{x_i} {p_i}
\]</span></p>
<p>and <span class="math inline">\(Y\)</span> a random variable that takes value <span class="math inline">\(y_i\)</span> with probability <span class="math inline">\(p_i\)</span>. The mean of <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[
E[Y] = \sum_i p_i y_i = \sum_i p_i \frac{x_i} {p_i} = \sum_i x_i = s. 
\]</span></p>
<p>Moreover, the variance of <span class="math inline">\(Y\)</span> is minimized when <span class="math inline">\(p_i = \frac{x_i} { s }\)</span> for all <span class="math inline">\(i\)</span>. It follows that <span class="math inline">\(y_i = \frac{ x_i } { x_i / s } = s\)</span> is a constant and <span class="math inline">\(Y\)</span> has variance <span class="math inline">\(0\)</span> (which is the smallest variance possible as its must be non-negative).</p>
<blockquote>
<p>Remark: we can also verify this by formulating this as an optimization problem then using Lagrangian method.</p>
</blockquote>
<p>In some applications, some elements in <span class="math inline">\(\{x_1, x_2, ..., x_n \}\)</span> may have same value. Suppose there are <span class="math inline">\(m\)</span> different value in <span class="math inline">\(\{x_1, x_2, ..., x_n \}\)</span> and define <span class="math inline">\(X_j ( 1 \le j \le m)\)</span> the set of elements with the <span class="math inline">\(j\)</span>-th largest value. Denote the sum of probability of elements in <span class="math inline">\(X_j\)</span> as <span class="math inline">\(P_j = \sum_{x \in X_j} p_x\)</span>. Also, let <span class="math inline">\(f(x_i)\)</span> denote the group that element <span class="math inline">\(x_i\)</span> belongs to, i.e., <span class="math inline">\(x_i \in X_{ f( x_i ) }\)</span>. Consider</p>
<p><span class="math display">\[
y&#39;_i = \frac{ x_i |X_{ f(x_i) }| } { P_{ f(x_i) } }
\]</span></p>
<p>and <span class="math inline">\(Y&#39;\)</span> a random variable that takes value <span class="math inline">\(y&#39;_i\)</span> with probability <span class="math inline">\(p_i\)</span>. The mean of <span class="math inline">\(Y&#39;\)</span> is also an unbiased estimator of <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E[Y&#39;] 
&amp;= \sum_{i = 1}^n p_i y&#39;_i \\
&amp;= \sum_{i = 1}^n p_i \frac{ x_i |X_{ f(x_i) }| } { P_{ f(x_i) } } \\
&amp;= \sum_{j = 1}^m \sum_{x \in X_j} p_x \frac{ x |X_{ f(x) }| } { P_j } \\
&amp;= s
\end{aligned}
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/04/Fabonacci-Number/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/04/Fabonacci-Number/" class="post-title-link" itemprop="url">Fabonacci Number</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-04 11:05:16" itemprop="dateCreated datePublished" datetime="2018-07-04T11:05:16+10:00">2018-07-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-01 23:29:59" itemprop="dateModified" datetime="2020-11-01T23:29:59+11:00">2020-11-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Consider the Fibonacci sequence <span class="math inline">\(\{ F_i \}_{i \ge 1}\)</span>: <span class="math display">\[
1, 1, 2, 3, 5, 8, 13, 21, ....
\]</span></p>
<p>which is defined recursively as follows: <span class="math display">\[
\begin{cases}
    F_1 = 1 \\
    F_2 = 1 \\
    F_{n + 2} = F_{n + 1} + F_{n} \quad  n \ge 1
\end{cases}
\]</span></p>
<p>We discuss some techniques for computing the (approximate) closed form formula for <span class="math inline">\(F_n\)</span> for <span class="math inline">\(n \ge 1\)</span>.</p>
<h4 id="method-1"><strong><em>Method 1</em></strong></h4>
<p>We guess that <span class="math inline">\(F_n\)</span> is of closed form as <span class="math inline">\(F_n = z^n\)</span> for some <span class="math inline">\(z \in \R\)</span>. As <span class="math inline">\(F_{n + 2} = F_{n + 1} + F_n\)</span>, <span class="math inline">\(z\)</span> needs to satisfies <span class="math display">\[
z^2 = z + 1
\]</span></p>
<p>The quadratic equation has two roots, <span class="math inline">\(z_1 = \frac{1 + \sqrt 5}{2}\)</span> and <span class="math inline">\(z_1 = \frac{1 - \sqrt 5}{2}\)</span> respectively.</p>
<p>Let <span class="math inline">\(a, b \in \R\)</span>. <span class="math inline">\(F_n = a z_1^n + b z_2^n\)</span> is also a possible solution that satisfies <span class="math inline">\(F_{n + 2} = F_{n + 1} + F_n\)</span>. We can compute the exact value of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> by the constraints <span class="math inline">\(F_1 = F_2 = 1\)</span>: <span class="math display">\[
a z_1 + b z_2 = 1 \\
a z_1^2 + b z_2^2 = 1
\]</span></p>
<p>i.e., <span class="math display">\[
a (1 + \sqrt 5) + b (1 - \sqrt 5) = 2 \\
a (6 + 2\sqrt 5) + b (6 - 2 \sqrt 5) = 4
\]</span></p>
<p>Therefore <span class="math inline">\(a = 1 / \sqrt 5\)</span>, <span class="math inline">\(b = -1 / \sqrt 5\)</span>, and<br />
<span class="math display">\[
 F_n = \frac{ (\frac{1 + \sqrt 5} {2})^n  - (\frac{1 - \sqrt 5} {2})^n }{\sqrt 5}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="method-2"><strong><em>Method 2</em></strong></h4>
<p>Consider the intervals derived from the sequences: <span class="math display">\[
    1, 1, 2, 3, 5, 8, 13, 21, .... 
\]</span></p>
<p>$$ , , , , ...</p>
<p>$$</p>
<p>Define <span class="math inline">\(x_n = \frac{F_{n + 1} }{F_n}\)</span> and denote <span class="math inline">\(I_n\)</span> the <span class="math inline">\(n\)</span>-th interval. Then <span class="math display">\[
I_n = [x_{2n - 1}, x_{2n } ]
\]</span></p>
<p>By observation of the first few <span class="math inline">\(I_n\)</span>'s, they seems to be nested closed intervals, with shrinking lengths. If the length of <span class="math inline">\(I_n\)</span> converges to <span class="math inline">\(0\)</span>, then the limit of <span class="math inline">\(x_n\)</span> exists. Suppose that <span class="math display">\[
\lim_{n} x_n = \phi
\]</span></p>
<p>Then <span class="math display">\[
\lim_n F_n = O(\phi^n )
\]</span></p>
<p>We will prove this in what follows.</p>
<p>The key transformation we performed is <span class="math display">\[
x_{n} = \frac{ F_{n + 1} }{F_{n} } = \frac{ F_{n } + F_{n - 1 } }{ F_{n} } = 1 + \frac{1}{ x_{ n - 1} }.
\]</span></p>
<h5 id="lemma-1">Lemma 1</h5>
<p><span class="math display">\[
        x_1 &lt; x_3 &lt; x_5 &lt; x_7 &lt;  ... \\
        x_2 &gt; x_4 &gt; x_6 &gt; x_8 &gt;  ... \\
\]</span></p>
<p><em>Proof:</em> For <span class="math inline">\(n \ge 1\)</span>, <span class="math display">\[
x_{2n + 2} - x_{2n} = \frac{1}{ x_{2n + 1} } - \frac{1}{ x_{2n - 1} } = \frac{ x_{2n - 1} - x_{2n + 1} }{ x_{2n + 1} x_{2n - 1} }.
\]</span></p>
<p>Similarly, <span class="math display">\[
x_{2n + 1} - x_{2n - 1} = \frac{1}{ x_{2n} } - \frac{1}{ x_{2n - 2} } = \frac{ x_{2n - 2} - x_{2n} }{ x_{2n} x_{2n - 2} }.
\]</span></p>
<p>As <span class="math inline">\(x_3 - x_1 = 3 /2 - 1 &gt; 0\)</span>, and <span class="math inline">\(x_4 - x_2 = 5 / 3 - 2 &lt; 0\)</span>, the lemma follows from induction.</p>
<p><span class="math inline">\(\square\)</span></p>
<h5 id="lemma-2.-for-n-ge-1">Lemma 2. For <span class="math inline">\(n \ge 1\)</span>,</h5>
<p><span class="math display">\[
x_{2n} - x_{2n - 1} \ge 0
\]</span></p>
<p><em>Proof:</em> Clearly this holds for <span class="math inline">\(n = 1\)</span>. Consider <span class="math inline">\(n \ge 2\)</span>, <span class="math display">\[
x_{2n} - x_{2n - 1} = \frac{1}{x_{2n - 1} } - \frac{1}{x_{2n - 2} }  = - \frac{ x_{2n - 1} - x_{2n - 2}   }{x_{2n - 1} \cdot x_{2n - 2}} =  \frac{ x_{2n - 2} - x_{2n - 3}   }{x_{2n - 1} \cdot x_{2n - 2} \cdot x_{2n - 2} \cdot x_{2n - 3} }
\]</span></p>
<p>The claim follows from induction.</p>
<h5 id="corollary-2.-for-n-ge-2">Corollary 2. For <span class="math inline">\(n \ge 2\)</span>,</h5>
<p><span class="math display">\[
\begin{aligned}
    I_n &amp;= x_{2n} - x_{2n - 1} \\
        &amp;= \frac{ x_{2n - 2} - x_{2n - 3}   }{x_{2n - 1} \cdot x_{2n - 2} \cdot x_{2n - 2} \cdot x_{2n - 3} } \\
        &amp;\le \frac{ x_{2n - 2} - x_{2n - 3}   }{ x_{2n - 1} } \\
        &amp;\le \frac{ x_{2n - 2} - x_{2n - 3}   }{ x_3 } \\
        &amp;= \frac{2}{3} (x_{2n - 2} - x_{2n - 3}) \\
        &amp;= \frac{2}{3} I_{n - 1}
\end{aligned}
\]</span></p>
<p>Hence <span class="math inline">\(\lim_n |I_n| = 0\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Combining lemma 1 and 2, we conclude that <span class="math inline">\(\lim x_n = \phi\)</span> exists. As <span class="math display">\[
\lim x_n = \phi = \lim (1 + \frac{1}{x_{n - 1} } ) = 1 + \frac{1}{\phi }
\]</span></p>
<p>We conclude that <span class="math inline">\(\phi = \frac{ 1 + \sqrt{5} }{2 }\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/06/20/Bennett-s-Inequality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/20/Bennett-s-Inequality/" class="post-title-link" itemprop="url">Bennett's Inequality</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-20 21:56:51" itemprop="dateCreated datePublished" datetime="2018-06-20T21:56:51+10:00">2018-06-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-17 15:31:00" itemprop="dateModified" datetime="2020-07-17T15:31:00+10:00">2020-07-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let <span class="math inline">\(X\)</span> be a random variable such that <span class="math inline">\(X \le M\)</span> and <span class="math inline">\(E[X]= \mu\)</span>. Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a set of <span class="math inline">\(i.i.d\)</span> copies of <span class="math inline">\(X\)</span> and <span class="math inline">\(\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i\)</span>. Then <span class="math display">\[
\begin{aligned}
P \{ \overline{X} \ge \mu + t\} 
&amp;= P\{ e^{\lambda \overline{X} } \ge e^{\lambda (\mu + t)} \} \\
&amp;\le \frac{E[e^{\overline{\lambda X} }]}{e^{\lambda(\mu + t)} } \\
&amp;= \frac{E[e^{\lambda\sum_{i=1}^n X_i} ]}{e^{\lambda n (\mu + t)} } \\
&amp;= \frac{ \prod_{i=1}^n E[e^{\lambda X} ]}{e^{\lambda n (\mu + t)} } \\
&amp;= \frac{ \prod_{i=1}^n  \sum_{j=1}^\infty \frac{\lambda^j E[X^j]}{j!}  }{e^{\lambda n (\mu + t)} } \\
&amp;= \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \sum_{j=2}^\infty \frac{\lambda^j E[X^{j-2}X^2]}{j!})  }{e^{ \lambda n(\mu + t)} } \\ 
&amp;\le \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \sum_{j=2}^\infty \frac{\lambda^j M^{j-2} E[X^2]}{j!})  }{e^{\lambda  n (\mu + t)} } \\
&amp;\le \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \frac{E[X^2]}{M^2} \sum_{j=2}^\infty \frac{\lambda^j M^{j} }{j!})  }{e^{\lambda   n (\mu + t)} } \\
&amp;\le \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \frac{E[X^2]}{M^2} (e^{\lambda M} - 1 - \lambda M))  }{e^{\lambda  n (\mu + t)} } \\
&amp;= \big( \frac{  e^{\lambda \mu + \frac{E[X^2]}{M^2} (e^{\lambda M} - 1 - \lambda M )} }{e^{\lambda   (\mu + t)} } \big)^n \\
&amp;= \big( e^{ -\lambda t + \frac{E[X^2]}{M^2} (e^{\lambda M} - 1 - \lambda M )}  \big)^n \\
\end{aligned}
\]</span></p>
<p>By setting the derivative of <span class="math inline">\(\lambda\)</span> to 0, we get <span class="math display">\[
-t + {E[X^2] \over M^2} ( M e^{\lambda M} - M) = 0 \rightarrow  \lambda = {1 \over M} \ln { {( {t M \over E[X^2] }  + 1 })}
\]</span> Therefore, <span class="math display">\[
\begin{aligned}
P \{ \overline{X} \ge \mu + t\} 
&amp;\le (e^{ - {t \over M} \ln { {( {t M \over E[X^2] }  + 1 } }) + \frac{E[X^2]}{M^2}{ {( {t M \over E[X^2] }  + 1  - 1 - \ln { {( {t M \over E[X^2] }  + 1 } })} })})^n \\
&amp;= e^{\frac{nE[X^2]}{M^2}{ {( {t M \over E[X^2] }  - ({t M \over E[X^2] }  + 1 ) \ln { {( {t M \over E[X^2] }  + 1 } })} })} \\
&amp;= e^{\frac{nE[X^2]}{M^2} h(\frac{tM}{E[X^2]})}
\end{aligned}
\]</span> where <span class="math inline">\(h(x) = x - (1 + x) \ln (1 + x)\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/05/21/SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/21/SVD/" class="post-title-link" itemprop="url">SVD</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-05-21 10:49:11" itemprop="dateCreated datePublished" datetime="2018-05-21T10:49:11+10:00">2018-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-04 22:50:56" itemprop="dateModified" datetime="2020-01-04T22:50:56+11:00">2020-01-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Singular vector decomposition is one of the highlights of linear algebra.</p>
<p>Before diving into the topic, we have a brief review of linear transformation. Given a matrix <span class="math inline">\(A \in R^{m \times n}\)</span> with rank <span class="math inline">\(r\)</span>, we can view it as a linear transformation from <span class="math inline">\(R^n \rightarrow R^m: f(x) = Ax, \forall x \in R^n\)</span>. It takes a vector in the row space, i.e., the subspace spanned by row vectors of <span class="math inline">\(A\)</span>, to a vector in the column space, the subspace spanned by column vectors of <span class="math inline">\(A\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Linear%20Mapping.jpg" /></p>
<p>Denote the row space as <span class="math inline">\(S(A^T) = \{ \sum_{i = 1}^n \lambda_i A[i,:], \lambda_i \in R, \forall i \in [n] \}\)</span>, and the subspace perpendicular to <span class="math inline">\(S(A^T)\)</span> as <span class="math inline">\(N(A^T) = \{ x \in R^m, s.t., Ax = 0\}\)</span>. Similarly, we can define <span class="math inline">\(S(A)\)</span> the columns space of <span class="math inline">\(A\)</span> and <span class="math inline">\(N(A)\)</span> the space perpendicular to <span class="math inline">\(S(A)\)</span>. Note that <span class="math inline">\(S(A^T)\)</span> and <span class="math inline">\(S(A)\)</span> has the same dimension <span class="math inline">\(r\)</span>. For any <span class="math inline">\(x \in R^m\)</span>, we can write <span class="math inline">\(x = y + z\)</span>, such that <span class="math inline">\(y \in S(A^T)\)</span> and <span class="math inline">\(z \in N(A^T)\)</span>. The operation <span class="math inline">\(Ax = Ay + Az\)</span>, takes the <span class="math inline">\(y\)</span> component to a vector in <span class="math inline">\(S(A)\)</span> and the <span class="math inline">\(z\)</span> component to point <span class="math inline">\(0\)</span>, as illustrated by the figure.</p>
<p><em>The goal of singular vector decomposition is to find an orthogonal base <span class="math inline">\(v_1, v_2, ..., v_r\)</span> in <span class="math inline">\(S(A^T)\)</span>, and an orthogonal base <span class="math inline">\(u_1, u_2, ..., u_r\)</span>, such that <span class="math inline">\(f(v_i) = A v_i = \sigma_i u_i\)</span> for all <span class="math inline">\(i \in [r]\)</span> and <span class="math inline">\(\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_r &gt; 0\)</span>.</em></p>
<p>In some sense, there is a one to one correspondence between the orthogonal bases <span class="math inline">\(\{ v_1, v_2, ..., v_r \}\)</span> and <span class="math inline">\(\{ u_1, u_2, ..., u_r \}\)</span>.</p>
<p>If we write <span class="math inline">\(V = [v_1, v_2, ..., v_r]\)</span>, <span class="math inline">\(\Sigma = \left[ \begin{aligned} \begin{matrix} &amp;\sigma_1 \\ &amp;&amp;\sigma_2 \\ &amp;&amp;&amp; ... \\ &amp;&amp;&amp;&amp;\sigma_r \end{matrix} \end{aligned} \right]\)</span> and <span class="math inline">\(U = [u_1, u_2, ..., u_r]\)</span>, then <span class="math display">\[
A V = U \Sigma
\]</span></p>
<p>As the columns of <span class="math inline">\(V\)</span> are orthogonal base, it is invertible and <span class="math inline">\(V^{-1} = V^T\)</span>. Hence <span class="math display">\[
A = U \Sigma V^{-1} = U \Sigma V^T
\]</span></p>
<p>Expanding the expression gives: <span class="math display">\[
A = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + ... + \sigma_r u_r v_r^T
\]</span></p>
<p>Further, <span class="math display">\[
A^T A = V \Sigma^2 V^T, A A^T = U \Sigma^2 U^T
\]</span> which implies that <span class="math display">\[
A^T A V = V \Sigma^2, A A^T U = U \Sigma^2
\]</span> The <span class="math inline">\(v_i\)</span>'s and <span class="math inline">\(u_i\)</span>'s are eigenvectors of <span class="math inline">\(A^T A\)</span> and <span class="math inline">\(A A^T\)</span> respectively.</p>
<h2 id="existence">EXISTENCE</h2>
<p><em>Proof Of The Existence of SVD</em>:</p>
<p>The key is to look at <span class="math inline">\(A^T A\)</span>. Note that <span class="math inline">\(A^TA\)</span> is semi-positive and rank <span class="math inline">\(r\)</span>. As <span class="math inline">\(A^T A x = 0 \Leftrightarrow x A^T A x = 0 \Leftrightarrow Ax = 0\)</span>. It follows that <span class="math inline">\(A^T A\)</span> and <span class="math inline">\(A\)</span> has the same null space and therefore the same row space. Further, as <span class="math inline">\(A^T A\)</span> is semi-positive, it has <span class="math inline">\(r\)</span> positive eigenvalue. Denote the eigenvalues of <span class="math inline">\(A^TA\)</span> in decreasing order as <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_r &gt; 0\)</span> and the corresponding (unit length) eigenvectors as <span class="math inline">\(v_1, v_2, ..., v_r\)</span>. The <span class="math inline">\(v_i\)</span>'s are orthogonal, as</p>
<ul>
<li>For <span class="math inline">\(i \neq j\)</span>, <span class="math inline">\(v_i^T A^T A v_j = \lambda_i v_i^T v_j = \lambda_j v_i^T v_j\)</span>. As <span class="math inline">\(\lambda_i \neq \lambda_j\)</span>, <span class="math inline">\(v_i^T v_j = 0\)</span>.</li>
</ul>
<p>They constitute an orthogonal base of the row space of <span class="math inline">\(A^T A\)</span> and the row space <span class="math inline">\(A\)</span>.</p>
<p>Now we use the <span class="math inline">\(v_i\)</span>'s to search for <span class="math inline">\(u_i\)</span>'s as follows: let <span class="math inline">\(\sigma = \sqrt{\lambda_i}\)</span>, we claim the <span class="math display">\[
u_i = \frac{1}{\sigma_i} A v_i, \forall i \in [r]
\]</span> are what we want.</p>
<ol type="1">
<li>The <span class="math inline">\(u_i\)</span>'s are orthogonal: <span class="math inline">\(u_i^T u_j = \frac{1}{\sigma_i \sigma_j} v_i^T A^T A v_j = \frac{\sigma_j^2 }{\sigma_i \sigma_j} v_i^T v_j = 0\)</span></li>
<li>The <span class="math inline">\(u_i\)</span>'s are unite vectors: <span class="math inline">\(u_i^T u_i = \frac{1}{\sigma_i^2} v_i^T A^T A v_i = 1\)</span></li>
</ol>
<p>Hence <span class="math inline">\(u_i\)</span>'s are an orthogonal base. As the columns of <span class="math inline">\(A\)</span> has dimension <span class="math inline">\(r\)</span>, the <span class="math inline">\(u_i\)</span>'s covers the entire columns space. This completes the proof.<br />
<span class="math inline">\(\blacksquare\)</span></p>
<h2 id="geometric-interpretation">GEOMETRIC INTERPRETATION</h2>
<p>To understand the geometry of SVD, we first extend the matrix <span class="math inline">\(V\)</span> to an orthogonal base in <span class="math inline">\(R^n\)</span> and <span class="math inline">\(U\)</span> to an orthogonal base in <span class="math inline">\(R^m\)</span>. By adding proper zero rows and columns to <span class="math inline">\(\Sigma\)</span>, we still have <span class="math inline">\(A = U \Sigma V^T\)</span>.</p>
<p>Now, the transformation <span class="math inline">\(f(x)\)</span> is decomposed into three steps: <span class="math display">\[
x \rightarrow V^Tx \rightarrow \Sigma (V^T x) \rightarrow U(\Sigma V^T x)
\]</span></p>
<p>It suffices to under the effect of multiplying an orthogonal matrix <span class="math inline">\(V\)</span>. It is indeed a rotation. To see this, multiply <span class="math inline">\(V\)</span> by <span class="math inline">\(e_i\)</span> gives <span class="math inline">\(v_i\)</span>, i.e., <span class="math inline">\(V e_i = v_i\)</span>, for all for <span class="math inline">\(i \in [n]\)</span>. Therefore, <span class="math inline">\(V\)</span> takes the orthogonal base <span class="math inline">\(I = [e_1, e_2, ..., e_n]\)</span> to the orthogonal base <span class="math inline">\(V = [v_1, v_2, ..., v_n]\)</span>.</p>
<p>Note that the inverse <span class="math inline">\(V^{-1}\)</span> of <span class="math inline">\(V\)</span> is also an orthogonal matrix and hence a rotation. Indeed it rotates the orthogonal base <span class="math inline">\(V = [v_1, v_2, ..., v_n]\)</span> back to <span class="math inline">\(I = [e_1, e_2, ..., e_n]\)</span>. To understand this, observe that <span class="math inline">\(V^{-1} = V^T\)</span>. For arbitrary <span class="math inline">\(v_i\)</span>, we have <span class="math inline">\(V^T v_i = e_i\)</span>.</p>
<p>Therefore, <span class="math inline">\(V^T x\)</span> corresponds to a rotation, in the reverse direction to the one defined by <span class="math inline">\(V\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Rotation.jpg" /></p>
<p>Next, the matrix <span class="math inline">\(\Sigma\)</span> scales the <span class="math inline">\(i\)</span>-th dimension of <span class="math inline">\(V^T x\)</span> by <span class="math inline">\(\sigma_i\)</span>, for all <span class="math inline">\(i \in [n]\)</span>. Combined with the first step, if <span class="math inline">\(x = v_i\)</span>, then it is first rotation to the direction <span class="math inline">\(e_i\)</span>, then scaled by a factor <span class="math inline">\(\sigma_i\)</span>.</p>
<p>Finally, <span class="math inline">\(U\)</span> is an orthogonal matrix and corresponds to another rotation, which take <span class="math inline">\(e_i \in R^m\)</span> to a vector <span class="math inline">\(u_i\)</span>.</p>
<p><em>Corollary</em>: Define the Frobenius norm of a matrix <span class="math inline">\(A\)</span> as <span class="math display">\[
||A|_F = \sqrt{\sum_{i, j} a_{i, j}^2 }
\]</span> Then <span class="math inline">\(||A||_F^2 = ||\Sigma||^2 = \sum_{i = 1}^r \sigma_i^2\)</span>.</p>
<p><em>Proof:</em> Note that rotating the vectors do not change their distance from the origin. Recall that <span class="math inline">\(U^T\)</span> and <span class="math inline">\(V\)</span> are both rotations. Hence <span class="math inline">\(||A||_F^2 = ||AV||_F^2 = ||U^T A V||_F^2 = ||U^T U \Sigma V^T V||_F^2= ||\Sigma||_F^2\)</span>.</p>
<p><span class="math inline">\(\square\)</span>.</p>
<h2 id="applications">APPLICATIONS</h2>
<p>An important property of the <span class="math inline">\(v_i\)</span> is that</p>
<ol type="1">
<li><span class="math inline">\(v_1 = \arg\max_{v \in R^n, ||v|| = 1} ||Av||\)</span>.</li>
<li><span class="math inline">\(v_2 = \arg\max_{v \in R^n, ||v|| = 1, v \perp v_1} ||Av||\)</span>.</li>
<li>...</li>
<li><span class="math inline">\(v_r = \arg\max_{v \in R^n, ||v|| = 1, v \perp v_1, v \perp v_2, ..., v \perp v_{r - 1}} ||Av||\)</span></li>
</ol>
<p><em>Proof</em>: For any <span class="math inline">\(v \in R^n, ||v|| = 1\)</span>, we can write <span class="math inline">\(v = \sum_{i = 1}^n a_i v_i\)</span>, where <span class="math inline">\(\sum_{i = 1}^n a_i^2 = 1\)</span>, since <span class="math inline">\(v_i\)</span>'s are an orthogonal base. Then <span class="math display">\[
\begin{aligned}
||Av||^2 
    &amp;= x^T A^T A x \\
    &amp;=  x^T V \Sigma U^T U \Sigma V^T x \\
    &amp;= ||\Sigma V^T x||^2 \\
    &amp;= \sum_{i = 1}^r a_i^2 \sigma_i^2 
\end{aligned}
\]</span> which is maximized to <span class="math inline">\(\sigma_1^2\)</span> when <span class="math inline">\(a_1 = 1\)</span> and <span class="math inline">\(a_i = 0, i \ge 2\)</span>. Hence (1) is proved.</p>
<p>Similarly, when <span class="math inline">\(||v|| = 1, v \perp v_1, v_2, ..., v_k\)</span> for some <span class="math inline">\(k &lt; r\)</span>, then <span class="math inline">\(a_1 = a_2 = ... a_k = 0\)</span>, and <span class="math inline">\(v = \sum_{i = k + 1}^r a_i v_i\)</span>. <span class="math display">\[
\begin{aligned}
||Av||^2 
    &amp;= \sum_{i = k + 1}^r a_i^2 \sigma_i^2 
\end{aligned}
\]</span> which achieves maximum when <span class="math inline">\(a_{k + 1} = 1\)</span> and <span class="math inline">\(a_i = 0, i &gt; k + 1\)</span>. In this case <span class="math inline">\(v = v_{k + 1}\)</span>.</p>
<h3 id="best-fit-k-subspace">Best Fit <span class="math inline">\(k\)</span> subspace</h3>
<p>An implication is that, the subspace space <span class="math inline">\(V_k\)</span> spanned by <span class="math inline">\((v_1, v_2, ..., v_k)\)</span>, is the best <span class="math inline">\(k\)</span>-dimension subspace of <span class="math inline">\(R^n\)</span> that fits the row vectors of <span class="math inline">\(A\)</span>, namely, <span class="math inline">\(A[1, :], A[2, :], ..., A[m, :]\)</span>. Here fits mean the sum of the square of the perpendicular distance from the row vector to their projections on the subspace is minimized. Denote <span class="math inline">\(S\)</span> any subspace of <span class="math inline">\(R^n\)</span>, then for <span class="math inline">\(v \in R^n\)</span>, we can decompose <span class="math inline">\(v\)</span> into two part: the part <span class="math inline">\(v_S\)</span> that is within <span class="math inline">\(S\)</span> and the part <span class="math inline">\(v_{S_\perp}\)</span> perpendicular to <span class="math inline">\(S\)</span>. Then <span class="math display">\[
||v||^2 = ||v_{S}||^2 + ||v_{S_\perp}||^2
\]</span> Minimizing <span class="math inline">\(||v_{S_\perp}||^2\)</span> is equivalent to maximizing <span class="math inline">\(||v_{S}||^2\)</span>. The subspace space by <span class="math inline">\((v_1, v_2, ..., v_k)\)</span> is the subspace that maximize the sum of the squares of the projections of row vectors to the subspace.</p>
<p>We prove this by induction. When <span class="math inline">\(k = 1\)</span>, the proof holds trivially by the definition of <span class="math inline">\(v_1\)</span>. In particular, <span class="math inline">\(A v_1\)</span> is the lengths of the projections of the row to the line that go through <span class="math inline">\(v_1\)</span>, since <span class="math inline">\(v_1\)</span> is a unit vector. Further, the corresponding projected vectors in <span class="math inline">\(R^n\)</span> are <span class="math display">\[
Av_1 v_1^T  = \sigma_1 u_1 v_1^T
\]</span></p>
<p>For general <span class="math inline">\(k\)</span>, by induction hypothesis <span class="math inline">\(V_{k - 1}\)</span> is the best <span class="math inline">\(k-1\)</span> dimension space that fit rows of <span class="math inline">\(A\)</span>. Denote <span class="math inline">\(N(V_{k -1})\)</span> the subspace that is perpendicular to <span class="math inline">\(V_{k - 1}\)</span>, which has dimension <span class="math inline">\(n - k + 1\)</span>. For any <span class="math inline">\(k\)</span> dimensional subspace <span class="math inline">\(S \cap N(V_{k - 1}) \neq \empty\)</span>, since <span class="math inline">\(dim(S) + dim(N(V_{k - 1})) = k + n - k + 1 \ge n\)</span>. Let <span class="math inline">\(s_k \in S \cap N(V_{k - 1})\)</span> be a unit vector. By the definition of <span class="math inline">\(v_k\)</span>, we have <span class="math display">\[
||Av_k||^2 \ge ||As_k||^2
\]</span> Now we can extend <span class="math inline">\(s_k\)</span> to a base of <span class="math inline">\(S\)</span>, denoted as <span class="math inline">\(s_1, s_2, ..., s_k\)</span>. By induction hypothesis, it holds <span class="math display">\[
\sum_{i = 1}^{k - 1} ||A v_i||^2 \ge \sum_{i = 1}^{k - 1} ||A s_i||^2 
\]</span> Hence <span class="math display">\[
\sum_{i = 1}^{k} ||A v_i||^2 \ge \sum_{i = 1}^{k} ||A s_i||^2 
\]</span> But the former is exactly sum of the squares of the lengths of the projections to <span class="math inline">\(V_k\)</span> and the later is the one to <span class="math inline">\(S\)</span>. Finally, we mention that the projection of <span class="math inline">\(A\)</span> to <span class="math inline">\(V_k\)</span> is given by <span class="math display">\[
A_k = \sum_{i = 1}^k \sigma_i u_i v_i ^T 
\]</span></p>
<p>Corollary 1. For any rank <span class="math inline">\(k\)</span> matrix <span class="math inline">\(B\)</span>, we have <span class="math display">\[
||A - A_k||_F \le ||A - B||_F
\]</span></p>
<p>Intuitively, if we view square root of the sum of square distance of row vectors to the origin (note that the origin can be viewed as a zero dimension subspace.)</p>
<p>Corollary 2. Define the 2-norm of a matrix <span class="math inline">\(A\)</span> as <span class="math display">\[
||A||_2 = \sigma_1 = \max_{v \in R^n, ||v|| = 1} ||Av||
\]</span> The <span class="math inline">\(2\)</span>-norm is the square root of the maximum sum of squares of the projections of the row vectors to a one dimension subspace. Then an implication of the SVD is that, for any rank <span class="math inline">\(k\)</span> matrix <span class="math inline">\(B\)</span>, we have <span class="math display">\[
||A - A_k||_2 \le ||A - B||_2
\]</span></p>
<p><em>Proof:</em> First note that for <span class="math inline">\(v = \sum_{i = 1}^r a_i v_i\)</span>, such that <span class="math inline">\(\sum_{i = 1}^k a_i^2 = 1\)</span>, <span class="math display">\[
||(A - A_k) v|| = \sum_{i = k + 1}^r a_i \sigma_i
\]</span> it is maximized when <span class="math inline">\(a_{k + 1} = 1\)</span> and <span class="math inline">\(||A - A_k||_2 = \sigma_{k + 1}\)</span>. Now, as <span class="math inline">\(dim(A_{k + 1}) + dim(N(B^T)) = k + 1 + n - k \ge n\)</span>, <span class="math inline">\(\exists v \in R^n\)</span>, such that <span class="math inline">\(Bv = 0\)</span> and <span class="math inline">\(v = \sum_{i = 1}^{k + 1} a_i v_i\)</span> and <span class="math inline">\(||v|| = 1\)</span>. Now <span class="math display">\[
||A - B||_2 \ge ||(A - B)v|| = ||Av|| = \sqrt{\sum_{i = 1}^{k + 1} a_i^2 \sigma_i^2 } \ge \sigma_{k + 1}
\]</span> which finishes our proof.</p>
<!-- *Proof 2:* Rewrite $A = \sum_{i = 1}^n \sigma_i u_i v_i^T$. Note that $v_i$'s are an orthogonal base of $R^n$. Therefore, we can rewrite the rows of any matrix $B$ as a linear combination of $v_i$'s.   -->
<h3 id="computing-svd">Computing SVD</h3>
<p>If suffices to compute the eigenvectors and eigenvalues of <span class="math inline">\(A^T A\)</span>, i.e., <span class="math inline">\(\sigma_i^2\)</span>'s and <span class="math inline">\(v_i\)</span>'s. In the simplest case, <span class="math inline">\(\sigma_1 &gt; \sigma_2 \ge \sigma_3 \ge ... \ge \sigma_r\)</span>, then we can take powers of <span class="math inline">\(A^T A\)</span>, <span class="math display">\[
(A^T A)^k = V \Sigma^{2k} V^T = \sum_{i = 1}^r \sigma_i^{2k} v_i v_i^T
\]</span> Dividing the matrix by <span class="math inline">\(\sigma_1^{2k}\)</span>, <span class="math display">\[
\frac{1}{\sigma_1^{2k} } (A^T A)^k = V \Sigma^{2k} V^T = v_1 v_1^T + \sum_{i = 2}^r \frac{\sigma_i^{2k}}{\sigma_1^{2k} }  v_i v_i^T
\]</span> The second part converges to 0 as <span class="math inline">\(k \rightarrow \infty\)</span>. Then for any vector <span class="math inline">\(v\)</span> that is not perpendicular to <span class="math inline">\(v_1\)</span>, we have <span class="math display">\[
\lim_k \frac{1}{\sigma_1^{2k} } (A^T A)^k v =  v_1 (v_1^T v) = (v_1^T v) v_1
\]</span> which is a multiple of <span class="math inline">\(v_1\)</span>. Normalizing the vector recovers <span class="math inline">\(v_1\)</span>.</p>
<p>When there is a tie of the largest eigenvalues, that is <span class="math inline">\(\sigma_1 = \sigma_2 = ... = \sigma_t\)</span> for some <span class="math inline">\(t &lt; r\)</span>, and suppose that <span class="math inline">\(v\)</span> is not perpendicular to <span class="math inline">\(v_1, v_2, ..., v_t\)</span> simultaneously, then <span class="math display">\[
\lim_k \frac{1}{\sigma_1^{2k} } (A^T A)^k v =  \text{the projection of } v \text { to  }  V_t
\]</span> Suppose we are satisfied with this result, we still need to address two problems.</p>
<ol type="1">
<li>We use <span class="math inline">\(\sigma_1^{2k}\)</span> to normalize the resulting vector, which is unknown.</li>
<li>How can we find a vector <span class="math inline">\(v\)</span> that is not perpendicular to all <span class="math inline">\(v_1, .., v_t\)</span> simultaneously, i.e., its projection to <span class="math inline">\(V_t\)</span> is not zero.</li>
</ol>
<!-- The first one is much easier: we use $\sum_{i = 1}^k \sigma_i^{2k}$ as the denominator instead. This can be computed by the Frobenius norm of $A^{2k}$.  Indeed we have the following lemma


Hence 
$$
\lim_k \frac{1}{||(A^T A)^{k} ||_F } (A^T A)^k  =  \lim_k \sum_{i = 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  v_i v_i^T
$$
For the largest $t$ singular values, the terms $\sum_{i = 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } \rightarrow \frac{1}{t}$, and the rest converge to $0$.  -->
<p>We defer the discussion of the first issue. As for the second issue, we tackle the problem for picking a vector <span class="math inline">\(v\)</span> uniformly at random from the <span class="math inline">\(n\)</span>-dimension unit ball, and then normalize it by <span class="math inline">\(v / ||v||\)</span>, in the hope that it is a "good" vector.</p>
<p><em>Lemma:</em> With probability at least <span class="math inline">\(1 / 2\)</span>, that <span class="math inline">\(v^T v_1 \ge 1 /\sqrt{n}\)</span>.</p>
<p><img src="https://pic2.zhimg.com/80/v2-a11c6a482d1f70b3ee0052ee132cbef9_hd.jpg" /></p>
<p><em>Proof:</em> By symmetry of the <span class="math inline">\(n\)</span>-dimension unit ball, it suffices to show that for an arbitrary given fixed direction, the projection of <span class="math inline">\(v\)</span> to that direction is at least <span class="math inline">\(1 / 4 \sqrt n\)</span> with probability <span class="math inline">\(1 / 2\)</span>. For convenience, we choose this direction to be <span class="math inline">\(e_1\)</span>.</p>
<p>Denote the volume of an <span class="math inline">\(n\)</span>-dimensional unit ball as <span class="math inline">\(V(n)\)</span>.</p>
<p>Now, we would like to use two cylinders to estimate the probability of <span class="math inline">\(|e_1 ^T v| / ||v|| \le \frac{1}{4 \sqrt n}\)</span>.</p>
<p>Note that the points <span class="math inline">\(\{v \}\)</span> inside the ball with <span class="math inline">\(e_1^T v / ||v|| \le \frac{1}{4 \sqrt n}\)</span> is completely contained in a cylinder centered in the equator, with radius one and height <span class="math inline">\(\frac{1}{4 \sqrt n}\)</span> that is parallel to <span class="math inline">\(e_1\)</span>. Its volume is given by <span class="math inline">\(\frac{1}{4 \sqrt n} V(n - 1)\)</span>. Hence: <span class="math display">\[
\Pr \left[ |v^T e_1| \le \frac{1}{4 \sqrt n} \right] \le \frac{2 \frac{1}{4 \sqrt n} V(n - 1)}{V(n)}
\]</span> On the other hand, we can use a cylinder centered at the equator to lower bound the volume of <span class="math inline">\(n\)</span>-dimension unit ball. The cylinder is inside the ball, with height <span class="math inline">\(2\frac{1}{\sqrt n}\)</span> and radius <span class="math inline">\((1 - (\frac{1}{ \sqrt n})^2)^{1/ 2}\)</span> . Therefore, <span class="math display">\[
\begin{aligned}
    V(n) 
    &amp; \ge 2 \frac{1}{\sqrt n} \left(1 - (\frac{1}{\sqrt n})^2 \right)^{(n - 1) / 2} V(n - 1) \\
    &amp;\ge 2 \frac{1}{\sqrt n} \left( 1 - \frac{1}{n} \right)^{n / 2} V(n - 1)
\end{aligned}
\]</span> Note that <span class="math inline">\((1 - \frac{1}{n})^n\)</span> increases with <span class="math inline">\(n\)</span>, and for <span class="math inline">\(n \ge 2\)</span>, <span class="math inline">\((1 - \frac{1}{n})^n \ge \frac{1}{4}\)</span> and <span class="math inline">\(\left( 1 - \frac{1}{n} \right)^{n / 2} \ge \frac{1}{2}\)</span> <span class="math display">\[
\Pr \left[ v^T e_1 \le \frac{1}{4 \sqrt n} \right] \le \frac{\frac{1}{4 \sqrt n} V(n - 1)}{V(n)} \le \frac{1}{2}
\]</span> <span class="math inline">\(\square\)</span></p>
<p><em>Theorem</em> After <span class="math inline">\(\frac{1}{\epsilon} \ln \frac{n}{\epsilon}\)</span> iterations, we can obtain a vector whose component that is perpendicular to <span class="math inline">\(V_t\)</span> is at most <span class="math inline">\(\epsilon\)</span> fraction of its square length, with probability at least <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<p><em>Proof:</em> If <span class="math inline">\(\sigma_i &lt; (1 - \epsilon) \sigma_1\)</span>, then <span class="math display">\[
\frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } \le \frac{\sigma_i}{\sigma_1}^{2k} \le (1-\epsilon)^{2k}
\]</span> Denote <span class="math inline">\(\sigma_1 = \sigma_2 = ... = \sigma_t\)</span> for some <span class="math inline">\(t &lt; r\)</span> the largest singular values. By the previous lemma, we pick a <span class="math inline">\(v\)</span> from the unit ball at random, and take <span class="math inline">\(v \leftarrow v / ||v||\)</span>. Denote <span class="math inline">\(v = \sum_{i = 1}^n {a_i} v_i\)</span>, then with probability at least <span class="math inline">\(1/ 2\)</span> we have <span class="math inline">\(\sqrt{ \sum_{i = 1}^t a_i^2} \ge \frac{1}{4 \sqrt n}\)</span>. On the other hand, <span class="math inline">\(v\)</span> is a unit vector, hence <span class="math inline">\(\sum_{i = 1}^n a_i^2 = 1\)</span>. Therefore, <span class="math inline">\(\sum_{i = t + 1}^n a_i^2 \le 1 - \frac{1}{16 n}\)</span></p>
<p>Now <span class="math display">\[
\begin{aligned}    
\frac{1}{||(A^T A)^{k} ||_F } (A^T A)^k v 
    &amp;= \sum_{i = 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  v_i v_i^T v \\
    &amp;= \sum_{i = 1}^r \frac{a_i \sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  v_i \\
    &amp;= \sum_{i = 1}^t \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } a_i v_i  + \sum_{i = t + 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  a_i v_i
\end{aligned}
\]</span></p>
<p>For the second term <span class="math display">\[
\begin{aligned}
||\sum_{i = t + 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  a_i v_i ||^2
    &amp;= \sum_{i = t + 1}^r (\frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  a_i)^2 \\
    &amp;\le \sum_{i = t + 1}^r (\frac{\sigma_i^{2k}}{ \sum_{j = 1}^t \sigma_1^{2k} }  a_i)^2 \\
    &amp;\le \frac{ (1 - \epsilon)^{4k} }{t^2} \sum_{i = t + 1}^r a_i^2 \\
    &amp;\le \frac{ (1 - \epsilon)^{4k} }{t^2}
\end{aligned}
\]</span></p>
<p>For the first term <span class="math display">\[
\begin{aligned}
||\sum_{i = 1}^t \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } a_i v_i ||^2 
    &amp;= \sum_{i = 1}^t (\frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} })^2 a_i^2 \\
    &amp;\ge \sum_{i = 1}^t (\frac{ 1 }{ t + (n - t)(1 - \epsilon)^{2k} })^2 a_i^2 \\
    &amp;\ge (\frac{ 1 }{ t + n (1 - \epsilon)^{2k} } )^2 \sum_{i = 1}^t a_i^2 \\
    &amp;\ge (\frac{ 1 }{ t + n (1 - \epsilon)^{2k} } )^2 \frac{1}{16n}
\end{aligned}
\]</span></p>
<p>If we take <span class="math inline">\(k = O( \frac{1}{\epsilon} \ln \frac{n}{\epsilon})\)</span>, then the second term is at most <span class="math inline">\(\epsilon\)</span> fraction of the first term.</p>
<p>Remark: picking a point uniformly at random can be done as follows: first pick a point <span class="math inline">\(v\)</span> uniformly at random from the high dimension cube: <span class="math inline">\([-1, 1]^n\)</span>, then checks whether the point is inside the ball (<span class="math inline">\(||v|| &lt; 1\)</span>). This however, could suffers from efficiency problem as the volume of the ball approaches to <span class="math inline">\(0\)</span> as the number of dimension increases. More efficient sampling method is needed. But it is not the current focus of this article.</p>
<h2 id="reference">Reference</h2>
<ol type="1">
<li>GILBERT STRANG, Introduction to Linear Algebra.</li>
<li>Venkatesan Guruswami and Ravi Kannan, Note 2, Singular Value Decomposition, 15-496/15-859X: Computer Science Theory for the Information Age, Spring 2012, CMU</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/04/18/FORA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/18/FORA/" class="post-title-link" itemprop="url">FORA [1]</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-18 22:51:54" itemprop="dateCreated datePublished" datetime="2018-04-18T22:51:54+10:00">2018-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-02 18:57:47" itemprop="dateModified" datetime="2018-07-02T18:57:47+10:00">2018-07-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose that we know the values of <span class="math inline">\(\pi^o(s, t)\)</span> for all <span class="math inline">\(t \in V\)</span> and <span class="math inline">\(r(s, v_i)\)</span> for all <span class="math inline">\(v_i \in V\)</span>. We would like to evaluate the value of <span class="math display">\[
\pi(s, t) = \pi^o(s, t) + \sum_{i = 1}^n r(s, v_i) \pi(v_i, t)
\]</span></p>
<p>where <span class="math inline">\(\pi(v_i, t)\)</span> denotes the probability of a random starts from <span class="math inline">\(v_i\)</span> and terminate at <span class="math inline">\(t\)</span>. Let <span class="math inline">\(\tilde \pi(v_i, t)\)</span> be our estimation of <span class="math inline">\(\pi(v_i,t)\)</span>, then we have an estimation of <span class="math inline">\(\pi(s, t)\)</span>: <span class="math display">\[
\tilde\pi(s, t) = \pi^o(s, t) + \sum_{i = 1}^n r(s, v_i) \tilde\pi(v_i, t)
\]</span> To get <span class="math inline">\(\tilde\pi(v_i, t)\)</span>, we perform <span class="math inline">\(w_i\)</span> (a value to be determined latter) random walks that start from <span class="math inline">\(v_i\)</span> and count the fraction of walks that terminate at <span class="math inline">\(t\)</span>. Formally, let <span class="math inline">\(X_{i, j}\)</span> be a Bernoulli random variable that takes value 1 if the <span class="math inline">\(j\)</span>-th walk from <span class="math inline">\(v_i\)</span> terminates at <span class="math inline">\(t\)</span>. Then <span class="math display">\[
E[X_{i,j}] = \pi(v_i, t)
\]</span> Let <span class="math inline">\(\tilde \pi(v_i, t) = \frac{1}{w_i} \sum_{j = 1}^{w_i} X_{i,j}\)</span> , so <span class="math display">\[
E[r(s, v_i) \tilde \pi(v_i, t)] = \frac{r(s, v_i)}{w_i} \sum_{j = 1}^{w_i} E[X_{i,j}] = r(s, v_i) \pi(v_i, t)
\]</span> A naïve use of Chenoff bound to us <span class="math inline">\(w_i = \Omega(\frac{\log (1 / p_{fail} ) }{\epsilon^2 \pi(v_i, t)} )\)</span> (which is even worse than evaluating <span class="math inline">\(\pi(s, t)\)</span> by Monte Carlo directly), such that <span class="math inline">\(r(s, v_i) \tilde \pi(v_i, t)\)</span> takes value within <span class="math inline">\(\big[(1 - \epsilon) r(s, v_i) \pi(v_i, t), (1 + \epsilon) r(s, v_i) \pi(v_i, t)\big]\)</span>with probability <span class="math inline">\(1 - p_{fail}\)</span>. The problem here is that when <span class="math inline">\(\pi(v_i, t)\)</span> is small, we need large number of random walks to get a good approximation of it. On the other hand, when <span class="math inline">\(\pi(v_i, t)\)</span> is small, the contribution of <span class="math inline">\(r(s, v_i)\pi(v_i, t)\)</span> to <span class="math inline">\(\sum_{i = 1}^n r(s, v_i) \pi(v_i, t)\)</span> is small, which means that more deviation is allowed on the estimation of <span class="math inline">\(\pi(v_i, t)\)</span>.</p>
<blockquote>
<p>What if the amount of deviation on the estimation of <span class="math inline">\(r(s, v_i) \pi(v_i, t)\)</span> we allow is <span class="math inline">\(\pi(s, t)\)</span>? How about <span class="math inline">\(\pi(s, t) / n\)</span>? More clever way of allocating deviation? Better Chernoff bound?</p>
</blockquote>
<p>[1] solves this by bounding the cumulative deviation of <span class="math inline">\(\sum_{i = 1}^n r(s, v_i) \tilde \pi(v_i, t)\)</span> as a whole. The statistical tool it turns to is a special Chernoff bound:</p>
<h4 id="theorem">Theorem</h4>
<blockquote>
<p>Let <span class="math inline">\(Y_1, Y_2, ..., Y_w\)</span> be independent random variables with <span class="math display">\[
Pr[Y_i = 1] = p_i \qquad Pr[Y_i = 0 ]  = 1 - p_i.
\]</span> Let <span class="math inline">\(Y = \frac{1}{w} \sum_{i = 1}^w b_i Y_i\)</span> with <span class="math inline">\(b_i &gt; 0\)</span>, and <span class="math inline">\(v = \frac{1}{w} \sum_{i = 1}^w b_i^2 p_i\)</span>. Then <span class="math display">\[
Pr[|Y - E[Y]| \ge \lambda ] \le 2 \exp \big(-\frac{\lambda^2 w} {2 v + 2 b \lambda / 3} \big)
\]</span> where <span class="math inline">\(b = \max\{ b_1, b_2, ..., b_w\}\)</span></p>
</blockquote>
<p>To apply this bound, we perform in total <span class="math inline">\(w = \sum_{i = 1}^n w_i\)</span> random walks. The first <span class="math inline">\(w_1\)</span> walks starts from <span class="math inline">\(v_1\)</span> , the next <span class="math inline">\(w_2\)</span> walks starts from <span class="math inline">\(v_2\)</span>, and so on. Let <span class="math display">\[
\{ Y_{1}, ..., Y_{w_1}, Y_{w_1 + 1}, ..., Y_{w_1 + w_2}, ..., Y_{\sum_{i = 1}^{n -1}w_i + 1}, ..., Y_{w} \}
\]</span> be the corresponding independent indicator random variables that take value 1 if the walks terminate at <span class="math inline">\(t\)</span> and</p>
<p><span class="math display">\[
b_j = \frac{w}{w_i} r(s, v_i)
\]</span> if the <span class="math inline">\(j\)</span>-th walk starts from vertex <span class="math inline">\(v_i\)</span>. So <span class="math display">\[
b = \max \{b_1, b_2, ..., b_w\} = \max_{1 \le i \le n} \frac{w}{w_i} r(s, v_i)
\]</span> It can be verified that <span class="math display">\[
E[Y] = \frac{1}{w} \sum_{i = 1}^w E[Y_i] = \frac{1}{w} \sum_{i = 1}^n \sum_{j = 1}^{w_i} \frac{w}{w_i} r(s, v_i) \pi(v_i, t) = \sum_{i = 1}^n r(s, v_i) \pi(v_i, t)
\]</span> To bound the deviation of <span class="math inline">\(\tilde \pi(s, t)\)</span> from <span class="math inline">\(\pi(s, t)\)</span>, we notice that <span class="math display">\[
|\tilde \pi(s, t) - \pi(s, t)| = | \sum_{i = 1}^n r(s, v_i) \tilde \pi(v_i, t)  -  \sum_{i = 1}^n r(s, v_i) \pi(v_i, t)| = |Y - E[Y]|
\]</span> So <span class="math display">\[
Pr[|Y - E[Y]| \ge \epsilon \pi(s, t)] \le 2 \exp ( - \frac{ \epsilon^2 \pi^2(s, t) \ w}{2 v + 2 b \epsilon \pi(s, t) / 3})
\]</span> But notice that <span class="math display">\[
v = \frac{1}{w} \sum_{i = 1}^w b_i^2 p_i \le  \big( \frac{b}{w}  \sum_{i = 1}^w b_i p_i \big) = b \ E[Y] \le b \ \pi(s, t)
\]</span> Thus <span class="math display">\[
\begin{align}
Pr[|Y - E[Y]| \ge \epsilon \pi(s, t)] 
&amp;\le 2 \exp ( - \frac{ \epsilon^2 \pi^2(s, t) \ w}{2 v + 2 b \epsilon \pi(s, t) / 3}) \\\\
&amp;\le 2 \exp ( - \frac{ \epsilon^2 \pi^2(s, t) \ w}{2 b \pi(s, t) + 2 b \epsilon \pi(s, t) / 3}) \\\\
&amp;\le 2 \exp ( - \frac{ \epsilon^2 \pi (s, t) \ w}{b (2   + 2\epsilon  / 3)}) \\\\
&amp;\le p_{fail}
\end{align}
\]</span> So <span class="math display">\[
w \ge b  \frac{2 + 2\epsilon / 3}{\epsilon^2 \delta} \log \frac{2}{p_{fail} }
\]</span> We would like to minimize <span class="math inline">\(b\)</span> in order to minimize <span class="math inline">\(w\)</span>, which can be formulated as an convex optimization problem:</p>
<p><span class="math display">\[
\begin{align} 
\min \quad &amp;b \\\\
s.t.   \quad &amp;b \ge r(s, v_i) / k_i \\\\
&amp;\sum_i k_i = 1 \\\\
&amp;k_i \ge 0
\end{align}
\]</span></p>
<p>Here we rewrite <span class="math inline">\(\frac{w_i}{w} = k_i\)</span>. One simple way to solve this is to guess that <span class="math inline">\(b\)</span> is minimized when all <span class="math inline">\({r(s, v_i)}/{k_i}\)</span> equals. This gives <span class="math inline">\(k_i = r(s, v_i) / r_{sum}\)</span>. We can prove this is indeed optimal: if this is not the case, there must be some <span class="math inline">\(k_i &lt; r(s, v_i) / r_{sum}\)</span>. Then <span class="math inline">\(b \ge r(s, v_i) / k_i &gt; r_{sum}\)</span> .</p>
<p>We can also solve this problem by a heavy mechanism: KKT conditions. First, define <span class="math display">\[
L(b, k, \lambda, \beta, , \mu) = b + \sum_{i = 1}^n \lambda_i (r(s, v_i) / k_i - b) + \sum_{i = 1}^n \beta_i (0 - k_i) + \mu (\sum_{i = 1}^n k_i - 1)
\]</span> Thus, the KKT conditions are <span class="math display">\[
\begin{align}
    &amp; r(s, v_i) / k_i - b \le 0 &amp;\forall i \in [1, n] \\\\
    &amp; -k_i \le 0 &amp;\forall i \in [1, n] \\\\
    &amp; \sum_i k_i  - 1 = 0 \\\\
    &amp; \lambda_i \ge 0, \beta_i \ge 0 &amp;\forall i \in [1, n] \\\\
    &amp; \lambda_i (r(s, v_i) / k_i - b) = 0 &amp;\forall i \in [1, n] \\\\
    &amp;\beta_i (0 - k_i) = 0 &amp;\forall i \in [1, n] \\\\
    &amp; \frac{\partial L}{\partial b}  = 1 - \sum_{i = 1}^n \lambda_i = 0 \\\\
    &amp; \frac{\partial L}{\partial k_i} = -\lambda_i r(s, v_i) / k_i^2 - \beta_i + \mu = 0 &amp;\forall i \in [1, n]\\\\
\end{align}
\]</span> So one solution is given by <span class="math display">\[
\begin{align}
&amp;b = r_{sum} \\\\
&amp;k_i = r(s, v_i) / r_{sum} &amp; \forall i \in [1, n] \\\\
&amp;\lambda_i = r(s, v_i) / r_{sum}^2 &amp; \forall i \in [1, n] \\\\
&amp;\beta_i = 0 &amp; \forall i \in [1, n] \\\\
&amp;\mu = 1
\end{align}
\]</span></p>
<p>which implies <span class="math display">\[
w \ge b  \frac{2 + 2\epsilon / 3}{\epsilon^2 \delta} \log \frac{2}{p_{fail} } \\\\
w_i = w k_i = \frac{w r(s, v_i)}{r_{sum} }
\]</span></p>
<h4 id="citations">Citations</h4>
<p>[1] Wang, Sibo, et al. "FORA: Simple and Effective Approximate Single-Source Personalized PageRank." <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. ACM, 2017.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/11/05/Lebesgue%20Measure/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/11/05/Lebesgue%20Measure/" class="post-title-link" itemprop="url">Lebesgue Measure</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-11-05 21:48:44" itemprop="dateCreated datePublished" datetime="2017-11-05T21:48:44+11:00">2017-11-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-17 16:53:33" itemprop="dateModified" datetime="2020-10-17T16:53:33+11:00">2020-10-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Lebesgue Measure</p>
<p>It is quiet intuitive that intervals <span class="math inline">\((a,b),(a,b],[a,b),[a,b]\)</span> have length <span class="math inline">\(b - a\)</span>. Now we want measure sets of real numbers more general than intervals, such as the set of all rational numbers.</p>
<p>In order to do so, we need a new model, a new concept of length. Before we show what Lebesgue Measure is, it is natural to have following question in mind:</p>
<blockquote>
<p><strong>1. What do we mean when we talk about the length of a set? </strong><br />
<strong>2. Does every set has a length?</strong><br />
<strong>3. What properties do the sets that have length satisfy?</strong></p>
</blockquote>
<p>One way to think of length of sets is regarding it as a function from sets to real numbers. Translating the above questions into the language of functions, we get</p>
<blockquote>
<p><strong>1. What is the function?</strong><br />
<strong>2. Especially, what is the domain of the function?</strong><br />
<strong>3. What are the typical properties of the function?</strong></p>
</blockquote>
<p>Before we delve into the details of constructing Lebesgue Measure, we shows some important goals of that construction (We forgo <strong>why we need such goals</strong> for the moment. Yet again, keep this question in mind):</p>
<blockquote>
<ol type="1">
<li>Intervals like <span class="math inline">\((a,b],[a,b],[a,b),(a,b)\)</span> are measurable and have length <span class="math inline">\(b - a\)</span>.<br />
. If set <span class="math inline">\(E\)</span> is measurable, then its complement <span class="math inline">\(\overline{E}\)</span> is measurable.<br />
. Countable unions of measurable sets are measurable.</li>
</ol>
</blockquote>
<p>One immediate corollary of goal (2) and (3) is that</p>
<blockquote>
<p>4 . Countable intersections of measurable sets are measurable.</p>
</blockquote>
<p>Actually, if we replace (3) with (4), we can derive (3) by combining (2) and (4). This is due to the symmetry inherent in set operation <span class="math display">\[
\overline{E_1 \cup E_2} = \overline{E_1} \cap \overline{E_2}
\]</span></p>
<p>Another corollary of (1), (2) and (3) is</p>
<blockquote>
<p>5 . Countable intersections and unions of intervals are measurable.</p>
</blockquote>
<h4 id="two-components-for-lebesgue-measure">Two Components for Lebesgue Measure</h4>
<ol type="1">
<li><span class="math inline">\(m^*(E) \doteq \inf \{ \sum_n l(I_n) : I_n \ \text{is open interval and } E \subset \cup_n I_n\}\)</span>, where <span class="math inline">\(l(I_n)\)</span> denotes the length of <span class="math inline">\(I_n\)</span>.<br />
</li>
<li>For any set A, a measurable set E must satisfy <span class="math inline">\(m^*(A) \ge m^*(A \cap E) + m^*(A \cap \overline{E})\)</span>.</li>
</ol>
<p>Discussion: (1) is a quiet intuitive rule. It states the length of a set is the least upper bound(supermum) of the sum of length of open intervals whose union covers this set. <strong>But why don't we use the sum of lengths of intervals that constitute the set as the length of this set?</strong><br />
One possible explanation is that there are sets which are not countable unions or intersection intervals. So we have to use open intervals to approximate them. <strong>What do these set look like?</strong> Keep this question in mind.</p>
<ol start="2" type="1">
<li>The second condition is required so that we find a set of sets that are close under complement, countable union and intersection operation with respect to the length definition of (1). <strong>Yet why (1) alone is not enough?</strong>. We will tough more about this issue when we come to non-Lebusgue measurable sets.</li>
</ol>
<p>More formally, let <span class="math display">\[
\begin{aligned}
\mathfrak{L} = \{ &amp;E: \forall A \subset R, \ m^*(A) \ge m^*(A \cap E) + m^*(A \cap \overline{E}), \\\\
&amp; where \ m^*(E) \doteq \inf \{ \sum_n l(I_n) : I_n \ \text{is open interval and } E \subset \cup_n I_n\} \}
\end{aligned}
\]</span> we will show that</p>
<ol type="1">
<li><span class="math inline">\([a,b], [a,b), (a,b], (a,b) \in \mathfrak{L}\)</span>, and <span class="math inline">\(m^*([a,b])=m^*([a,b))=m^*((a,b])=m^*((a,b))= b - a\)</span>.<br />
.<span class="math inline">\(\forall E \in \mathfrak{L}, \overline{E} \in \mathfrak{L}.\)</span><br />
.<span class="math inline">\(\{ E_n\}_{n \ge 1} \subset \mathfrak{L}, \cup_n E_n \in \mathfrak{L}\)</span><br />
.<span class="math inline">\(\{ E_n\}_{n \ge 1} \subset \mathfrak{L}, \cap_n E_n \in \mathfrak{L}\)</span></li>
</ol>
<p>Before we move on, we notice the construction of <span class="math inline">\(\mathfrak{L}\)</span> so far involves (a) definition of length (b) constraint on the sets involved. <strong>Is it the only way of constructing <span class="math inline">\(\mathfrak{L}?\)</span></strong> In other words, if we replace (a) or (b) with something else, we get a new set <span class="math inline">\(\mathfrak{L&#39;}\)</span>. Will <span class="math inline">\(\mathfrak{L&#39;}\)</span> satisfy the above four properties? <strong>Measure Theory</strong> studies these general topics.</p>
<h5 id="theorem">Theorem</h5>
<p><span class="math inline">\(\forall E \in \mathfrak{L}, \overline{E} \in \mathfrak{L}.\)</span></p>
<p>Proof: Straightforward from definition of <span class="math inline">\(\mathfrak{L}\)</span>.</p>
<h5 id="lema">Lema</h5>
<p>If <span class="math inline">\(E \subset \cup_n E_n, \text{ then } m^*(E) \le \sum_n m^*(E_n)\)</span>.</p>
<p>Proof: If <span class="math inline">\(\sum_n m^*(E_n) = \infty\)</span> then, this is obvious. Otherwise, for each n, <span class="math inline">\(m^*(E_n) \le \infty\)</span>, so there exist an set of open intervals <span class="math inline">\(\{I_k^n \}\)</span>, s.t.,</p>
<p><span class="math inline">\(E_n \subset \cup_k I_k^n \\ m^*(E_n) + {\epsilon \over 2^n}\ge \sum_k l(I_k^n)\)</span></p>
<p>But <span class="math inline">\(\{I_k^n : k \ge 1, n \ge 1\}\)</span> is a cover of E, so</p>
<p><span class="math display">\[
m^*(E) \le \sum_n \sum_k l(I_k^n) \le \sum_n (m^*(E_n) + {\epsilon \over 2^n}) = \sum_n m^*(E_n) + \epsilon
\]</span> As <span class="math inline">\(\epsilon\)</span> takes arbitrary values, we have proved the inequality.</p>
<h5 id="corollary">Corollary</h5>
<p><span class="math inline">\(\text{If } E \subset E_1, \text{ then } m^*(E) \le m^*(E_1)\)</span>.</p>
<h5 id="theorem-1">Theorem</h5>
<p><span class="math inline">\(\{ E_n\}_{n \ge 1} \subset \mathfrak{L}, \cup_n E_n \in \mathfrak{L}.\)</span></p>
<p>Proof:</p>
<p>Let <span class="math inline">\(F_n \doteq E_n - \cup_{i = 1}^{n - 1} E_i\)</span>, so <span class="math inline">\(\{ F_n \}_{n \ge 1}\)</span> are disjoint and <span class="math inline">\(\cup F_n = \cup E_n\)</span>. In the following we denote <span class="math inline">\(F \doteq \cup F_n\)</span></p>
<p>By induction on n, we show that <span class="math display">\[
\forall A \subset R, m^*(A) \ge \sum_{i = 1}^n m^*(A \cap F_i) + m^*(A \cap \overline{\cup_{i = 1}^n F_i}) \ \ \ \ \ \ \ \ \ \ (1)
\]</span> when <span class="math inline">\(n = 1\)</span>, this clearly holds. Suppose the inequality above holds up to n, now we consider <span class="math inline">\(n + 1\)</span>. First of all, we notice that, <span class="math display">\[
m^*(A) \ge  m^*(A \cap F{n + 1}) + m^*(A \cap \overline{F{n + 1}})
\]</span> by substituting A in (1) with <span class="math inline">\(A \cap \overline{F_{n + 1}}\)</span>, <span class="math display">\[
\begin{aligned}
m^*(A \cap \overline{F{n + 1}}) \ge &amp;\sum_{i = 1}^n m^*(A \cap \overline{F{n + 1}} \cap F_i) + m^*(A \cap \overline{F{n + 1}} \cap \overline{\cup_{i = 1}^n F_i}) \\
=&amp; \sum_{i = 1}^n m^*(A \cap F_i) + m^*(A  \cap \overline{\cup_{i = 1}^{n + 1} F_i})
\end{aligned}
\]</span> so, <span class="math inline">\(\forall n \in N\)</span>, <span class="math display">\[
\begin{aligned}
m^*(A) &amp;\ge \sum_{i = 1}^n m^*(A \cap F_i) + m^*(A \cap \overline{\cup_{i = 1}^n F_i}) \\
&amp; \ge m^*(A \cap (\cup_{i = 1}^n F_i)) + m^*(A \cap \overline{\cup_{i = 1}^n F_i})
\end{aligned}
\]</span> let <span class="math inline">\(n \rightarrow \infty\)</span> <span class="math display">\[
\begin{aligned}
m^*(A)  \ge m^*(A \cap F) + m^*(A \cap \overline{F})
\end{aligned}
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/15/VC%20Dimension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/15/VC%20Dimension/" class="post-title-link" itemprop="url">VC Dimension</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-15 16:20:33" itemprop="dateCreated datePublished" datetime="2017-02-15T16:20:33+11:00">2017-02-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-18 18:31:54" itemprop="dateModified" datetime="2018-07-18T18:31:54+10:00">2018-07-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Definition</p>
<p>A set system is a tuple <span class="math inline">\((S, R)\)</span>, where <span class="math inline">\(S\)</span> is a set and <span class="math inline">\(R\)</span> is a collection of subsets of <span class="math inline">\(S\)</span>.</p>
<h4 id="definition">Definition</h4>
<p>Given <span class="math inline">\((S, R)\)</span>, <span class="math inline">\(X \subset S\)</span> is shattered by <span class="math inline">\(R\)</span> if <span class="math inline">\(\forall X&#39; \subset X\)</span>, <span class="math inline">\(\exists R&#39; \in R\)</span>, such that <span class="math inline">\(X&#39; = S \cap R&#39;\)</span>.</p>
<h4 id="definition-1">Definition</h4>
<p>The <strong>VC</strong> dimension of <span class="math inline">\((S, R)\)</span> is the maximum cardinality of some <span class="math inline">\(X \subset S\)</span> that is shatterred by <span class="math inline">\(R\)</span>.</p>
<p>Note: If we view <span class="math inline">\((S, R)\)</span> as a classifaction problem, i.e., <span class="math inline">\(S\)</span> is a set of points and <span class="math inline">\(R\)</span> a set of functions, the <strong>VC</strong> dimension d is the maximum size of a subset <span class="math inline">\(X\)</span> in which R could separate arbitrary set points from the rest. In some sense, it measures how complex a separation plane <span class="math inline">\(R\)</span> could form.</p>
<h4 id="definition-2">Definition</h4>
<p>Given a nonegative integer <span class="math inline">\(n\)</span> and <span class="math inline">\(d\)</span>, we define <span class="math inline">\(\binom{n}{\le d} = \sum_{i = 0}^{d} \binom{n}{i}\)</span></p>
<h4 id="lemma">Lemma</h4>
<p>If <span class="math inline">\((S, R)\)</span> has <span class="math inline">\(VC\)</span> dimension <span class="math inline">\(d\)</span>, then <span class="math inline">\(|R| \le \binom{n}{\le d}\)</span>, where <span class="math inline">\(|S| = n\)</span>.</p>
<p>Proof: Let <span class="math inline">\(X \subset S\)</span> be the set that has size <span class="math inline">\(d\)</span> and is shatterred by <span class="math inline">\(R\)</span>. Define <span class="math inline">\(R|X\)</span> as <span class="math inline">\(\{R&#39; \cap X | R&#39; \in R \}\)</span>. It is obvious that <span class="math inline">\((X, R|X)\)</span> has <strong>VC</strong> dimension <span class="math inline">\(d\)</span>. Also, the number of subset of <span class="math inline">\(X\)</span> is given by <span class="math display">\[
2^d = \binom{d}{\le d} = |\ R|X\ |
\]</span></p>
<p>Now we will show this lemma by induction. Label the elements in <span class="math inline">\(S - X\)</span> in arbitrary order as <span class="math inline">\(\{ x_{d + 1}, x_{d + 2}, ..., x_{n} \}\)</span>. Define <span class="math inline">\(X_i\)</span> = <span class="math inline">\(X \cup \{x_{d+1}, x_{d + 2}, ..., x_{d+i} \}\)</span> for <span class="math inline">\(0 \le i \le n - d\)</span>, where <span class="math inline">\(X_0 = X\)</span>. So <span class="math inline">\(|X_i| = d + i\)</span>. We will show that <span class="math inline">\(|R | X_i| \le \binom{d + i} {\le d}\)</span> holds.</p>
<p>Suppose the assumption holds for <span class="math inline">\(R|X_i\)</span>, now consider <span class="math inline">\(R|X_{i + 1}\)</span>. The elements <span class="math inline">\(r\)</span> in <span class="math inline">\(R|X_{i}\)</span> can be divided into</p>
<ol type="1">
<li><span class="math inline">\(D_1 = \{ r | r \in R | X_{i + 1}, r \cup x_{d + 1} \in R|X_{i + 1} \}\)</span>.</li>
<li><span class="math inline">\(D_2 = \{ r | r \notin R | X_{i + 1}, r \cup x_{d + 1} \in R|X_{i + 1} \}\)</span>.</li>
<li><span class="math inline">\(D_2 = \{ r | r \in R | X_{i + 1}, r \cup x_{d + 1} \notin R|X_{i + 1} \}\)</span>.</li>
</ol>
<p>We can verify that <span class="math inline">\(|R|X_i | = |D_1| + |D_2| + |D_3|\)</span>, <span class="math inline">\(|R|X_{i + 1}| = 2|D_1| + |D_2| + |D_3|\)</span>.</p>
<p>We show that <span class="math inline">\(|D_1| \le \binom{d + i}{\le d - 1}\)</span>. This is because the <strong>VC</strong> dimension of <span class="math inline">\((X_i, D_1)\)</span> is at most <span class="math inline">\(d - 1\)</span>. Otherwize, suppose <span class="math inline">\(Y \subset X_i\)</span> is shatterred by <span class="math inline">\(D_1\)</span> and <span class="math inline">\(|Y| = d\)</span>. It is apparent <span class="math inline">\(Y \cup x_{d + 1}\)</span> is shattered by <span class="math inline">\(R|X_{i + 1}\)</span> in <span class="math inline">\(X_{i + 1}\)</span>. A contradition.</p>
<p>So <span class="math display">\[
\begin{aligned}
|R | X_{i + 1}| 
&amp; = |D_1| + |R | X_{i}| \\\\
&amp; \le \binom{d + i}{\le d - 1} + \binom{d + i}{\le d} \\\\
&amp; = \sum_{j = 0}^{d - 1} \binom{d + i}{j} + \sum_{j = 0}^{d}\binom{d +i}{j} \\\\ 
&amp; = \sum_{j = 1}^{d} \binom{d + i}{j - 1} + \sum_{j = 1}^{d}\binom{d +i}{j} + \binom{d + i}{0} \\\\
&amp; = \sum_{j = 1}^d \binom{d + i + 1}{j} + \binom{d + i +1}{0} \\\\
&amp; = \sum_{j = 0}^d \binom{d + i + 1}{j} \\\\
&amp; = \binom{d + i + 1}{\le d} 
\end{aligned}
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/02/TF-IDF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/02/TF-IDF/" class="post-title-link" itemprop="url">TF-IDF</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-02 20:38:54" itemprop="dateCreated datePublished" datetime="2017-02-02T20:38:54+11:00">2017-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-02 18:29:42" itemprop="dateModified" datetime="2018-07-02T18:29:42+10:00">2018-07-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>TF-IDF stands for term frequency and inverse document frequency. It evaluates the importance of a word to a document in a corpus, which increases with the number of times it appears in the document but is offset by its frequency in the corpus. More precisely, for TF, we define <span class="math display">\[
tf(term| document) = \frac{\mbox{the number of times 
&quot;term&quot; appears in &quot;document&quot;} }{\mbox{total number of words in &quot;document&quot;} }
\]</span></p>
<p>For IDF, we define <span class="math display">\[
idf(term) = 1 + \log(\frac{\mbox{the total number of documents} }{\mbox{the number of documents this term appears in} } )
\]</span></p>
<p>Now suppose we have a query. Let <span class="math inline">\(Q = \{t_1, t_2, ..., t_n \}\)</span> the set of different term in the query. We convert <span class="math inline">\(Q\)</span> into a vector, <span class="math display">\[
V(Q) = [tf(t_1 | Q)* idf(t_1), tf(t_2|Q) * idf(t_2),..., tf(t_n|Q) * idf(t_n)]
\]</span></p>
<p>Similarily, for a document <span class="math inline">\(D\)</span>, we can get a vector <span class="math inline">\(V(D)\)</span>. Now the similarity between <span class="math inline">\(Q\)</span> and <span class="math inline">\(D\)</span> is given by <span class="math display">\[
\frac{V(Q) \times V(D)}{||V(Q)|| \times ||V(D)||}
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/02/Johnson-Lindenstrauss%20Lemma/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/02/Johnson-Lindenstrauss%20Lemma/" class="post-title-link" itemprop="url">Johnson-Lindenstrauss Lemma</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-02 20:38:54" itemprop="dateCreated datePublished" datetime="2017-02-02T20:38:54+11:00">2017-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-20 12:24:37" itemprop="dateModified" datetime="2021-01-20T12:24:37+11:00">2021-01-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Last modified date 19-Jan-2021.</p>
</blockquote>
<h1 id="problem">Problem</h1>
<p>Let <span class="math inline">\(\mathcal{U} \subset \R^d\)</span> be a set of <span class="math inline">\(n\)</span> points. We want to find a linear transformation <span class="math inline">\(A \in \R^{k \times d}\)</span> that embeds the points in <span class="math inline">\(\mathcal{U}\)</span> to <span class="math inline">\(\R^k\)</span> while preserving the pair-wise Euclidean distances.</p>
<p>If <span class="math inline">\(k \ge d\)</span>, the problem is trivial. There is an isomorphic mapping between <span class="math inline">\(\R^d\)</span> and a <span class="math inline">\(d\)</span>-dimensional subspace in <span class="math inline">\(\R^k\)</span>. Let <span class="math inline">\(u_1, ..., u_d \in \R^k\)</span> and <span class="math inline">\(e_1, ..., e_d \in \R^d\)</span> be unit vectors, such that for <span class="math inline">\(i \in [d]\)</span>, <span class="math inline">\(u_i\)</span> (<span class="math inline">\(e_i\)</span>) has the <span class="math inline">\(i\)</span>-th entry equal to 1 and other entries equal to 0. Then one possible isomorphic mapping is</p>
<p><span class="math display">\[
A = u_1 e_1^T + ... + u_d e_d^T.
\]</span></p>
<p>If <span class="math inline">\(k &lt; d\)</span>, then such an isomorphic embedding is impossible. However, it is feasible if we aim to preserve the pair-wise distance up to a multiplicative error <span class="math inline">\(\epsilon \in (0, 1)\)</span> with high probability: <span class="math inline">\(\forall x, y \in \mathcal{U}\)</span>, <span class="math display">\[
||A x - A y ||^2 \in (1 \pm \epsilon) || x - y ||^2.
\]</span></p>
<p>The smaller <span class="math inline">\(k\)</span> is, the less space we need to store the embedded vectors <span class="math inline">\(A \mathcal{U} = \{ Ax : x \in \mathcal{U} \}\)</span>. It remains to study how small <span class="math inline">\(k\)</span> can be?</p>
<blockquote>
<p>Theorem. For any set <span class="math inline">\(\mathcal{U} \subset \R^d\)</span> of <span class="math inline">\(n\)</span>-points, there is a matrix <span class="math inline">\(A \in \R^{k \times d}\)</span>, such that with probability at least <span class="math inline">\(1 - \frac{1}{n}\)</span>, <span class="math display">\[
\forall x, y \in \mathcal{U}, ||A x - A y ||^2 \in (1 \pm \epsilon) || x - y ||^2, 
\]</span> where <span class="math inline">\(k = O\left( \frac{ \log n }{ \epsilon^2} \right)\)</span>.</p>
</blockquote>
<p>The value of <span class="math inline">\(k\)</span> depends only on <span class="math inline">\(n\)</span> but not <span class="math inline">\(d\)</span>.</p>
<h1 id="properties">Properties</h1>
<h2 id="column-perspective">Column Perspective</h2>
<p>On the high level, we try to squeeze <span class="math inline">\(d\)</span> (in expectation) orthonormal vectors into a smaller space <span class="math inline">\(\R^k\)</span>. We do this by generating a set of <span class="math inline">\(d\)</span> random vectors <span class="math inline">\(u_1, u_2, ..., u_d \in \R^k\)</span>, such that:</p>
<ol type="1">
<li>Each coordinate of <span class="math inline">\(u_i\)</span> is generated independently for <span class="math inline">\(i \in [d]\)</span>;</li>
</ol>
<p>and in expectation, they are orthonormal:</p>
<ol start="2" type="1">
<li><span class="math inline">\(\mathbb{E} \big[ ||u_i ||^2 \big] = 1\)</span> for <span class="math inline">\(i \in [d]\)</span>;<br />
</li>
<li><span class="math inline">\(\mathbb{E} \big[ \left&lt; u_i, u_j \right&gt; \big] = 0\)</span> for <span class="math inline">\(i, j \in [d], i \neq j\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(e_1, ..., e_d \in \R^d\)</span> be unit vectors in <span class="math inline">\(\R^d\)</span> and <span class="math inline">\(e_i\)</span> (<span class="math inline">\(i \in [d]\)</span>) has the <span class="math inline">\(i\)</span>-th entry equal to 1. Define <span class="math display">\[
A = \begin{bmatrix}
    u_1, u_2, ..., u_d
\end{bmatrix} = u_1 e_1^T + ... + u_d e_d^T.
\]</span></p>
<p>For a fixed pair <span class="math inline">\(x, y \in \mathcal{U}\)</span>, define <span class="math inline">\(z = x - y\)</span>. Then <span class="math display">\[
    \begin{aligned}
        || A x - A y ||^2 
            &amp;= || A z ||^2 \\
            &amp;= || \sum_{ i \in [d] } z[i] \cdot u_i||^2 \\
            &amp;= \sum_{i \in [d] } (z[i])^2 ||u_i||^2 + \sum_{ i, j \in [d], i \neq j} (z[i] \cdot z[j]) \cdot \left&lt; u_i, u_j \right&gt;.
    \end{aligned}
\]</span></p>
<p>Taking expectation of both side, and by assumption of <span class="math inline">\(v_t, t \in [d]\)</span>, it holds that <span class="math display">\[
\mathbb{ E } \big[ ||z||^2 \big] = ||z||^2. 
\]</span></p>
<h2 id="row-perspective">Row Perspective</h2>
<p>We can also characterize <span class="math inline">\(A\)</span> by the properties of its rows. Let <span class="math inline">\(\tilde A\)</span> be a scaled version of <span class="math inline">\(A\)</span>, such that <span class="math display">\[
A = \frac{1}{ \sqrt k} \tilde{A}.
\]</span></p>
<p>Denote <span class="math inline">\(v_1, v_2, ..., v_k \in \R^d\)</span> the row vectors of <span class="math inline">\(\tilde A\)</span>. They are generated randomly and independently. Each vector <span class="math inline">\(v_i\)</span> (<span class="math inline">\(i \in [k]\)</span>) satisfies</p>
<ol type="1">
<li>Each entry of <span class="math inline">\(v_i\)</span> is generated independently;</li>
<li><span class="math inline">\(\mathbb{E} \big[v_i [j] \big] = 0, \forall j \in [d]\)</span>, i.e., each entry has mean zero;<br />
</li>
<li><span class="math inline">\(\mathbb{Var} \big[ v_i [j] \big] = 1, \forall j \in [d]\)</span>, i.e., each entry has variance one.</li>
</ol>
<p>For a fixed pair <span class="math inline">\(x, y \in \mathcal{U}\)</span>, define <span class="math inline">\(z = x - y\)</span>. Consider <span class="math display">\[
||A z||^2 = \frac{1}{k} ||\tilde{A } z||^2 = \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2.
\]</span></p>
<p>Our assumption on <span class="math inline">\(v_i : i \in [k]\)</span> show that the <span class="math inline">\(\left&lt; v_i, z \right&gt;^2: i \in [k]\)</span> are independent random variables. We will verify that each of them has expectation <span class="math inline">\(||z||^2\)</span>. For fixed <span class="math inline">\(i \in [k]\)</span>, <span class="math display">\[
\left&lt; v_i, z \right&gt; = \sum_{j \in [d] } v_i [j] \cdot z[j]
\]</span></p>
<p>can be view as the sum of <span class="math inline">\(d\)</span> independently random variables. Hence, by our assumption on <span class="math inline">\(v_i\)</span>, we have <!-- $$
\mathbb{E} [ \left< v_i, z \right> ] = \sum_{j \in [d] } z[j] \cdot  \mathbb{E}[ v_i [j] ] = 0, \\
$$ --></p>
<p><span class="math display">\[
    \mathbb{Var} \big[ \left&lt; v_i, z \right&gt; \big] = \sum_{j \in [d] } \mathbb{Var}\big[ v_i [j] \cdot z[j] \big] = \sum_{j \in [d] } (z[j])^2 \cdot \mathbb{Var} \big[ v_i [j] \big] = ||z||^2.
\]</span></p>
<p>By linearity of expectation, <span class="math display">\[
    \mathbb{E} \big[ \left&lt; v_i, z \right&gt; \big] = \sum_{j \in [d] } \mathbb{E}\big[ v_i [j] \cdot z[j] \big] = \sum_{j \in [d] } z[j] \cdot \mathbb{E} \big[ v_i [j] \big] = 0.
\]</span></p>
<p>It concludes that <span class="math inline">\(\mathbb{E} \big[ \left&lt; v_i, z \right&gt;^2 \big] = \mathbb{Var} \big[ \left&lt; v_i, z \right&gt; \big] - \big( \mathbb{E} \big[ \left&lt; v_i, z \right&gt; \big] \big)^2 = ||z||^2\)</span>.</p>
<p><strong>At this point, <span class="math inline">\(||A z||^2\)</span> is viewed as the average of <span class="math inline">\(k\)</span> independent, mean <span class="math inline">\(||z||^2\)</span> random variables.</strong> By the law of large numbers, the average should concentrate around <span class="math inline">\(||z||^2\)</span>. The detailed analysis of the concentration phenomenon depends on the implementation of <span class="math inline">\(\tilde A\)</span>.</p>
<h1 id="implementation">Implementation</h1>
<p>In this section, we discuss two implementations that fullfil the properties discussed above, in terms of both column and row perspectives.</p>
<h2 id="gaussian">Gaussian</h2>
<p>In the first implementation, each entry of <span class="math inline">\(\tilde A\)</span> is generated independently from standard normal distribution. Let <span class="math inline">\(u_1, ..., u_d\)</span> be the column vectors of <span class="math inline">\(\tilde A\)</span> and <span class="math inline">\(v_1, ..., v_k\)</span> be its row vectors. Hence, <span class="math inline">\(\frac{1}{\sqrt k} u_1, ..., \frac{1}{\sqrt k} u_d\)</span> and <span class="math inline">\(\frac{1}{\sqrt k} v_1, ..., \frac{1}{\sqrt k} v_k\)</span> are the column and row vectors of <span class="math inline">\(A\)</span>, respectively. It satisfies that</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbb{E} \big[ || \frac{1}{\sqrt k} u_i||^2 \big] = \frac{1}{k} \mathbb{Var} \big[ ||u_i||^2 \big] = \frac{k}{k} = 1, \forall i \in [d]\)</span>;</p></li>
<li><p><span class="math inline">\(\mathbb{E} \big[ \left&lt; \frac{1}{\sqrt k} u_i, \frac{1}{\sqrt k} u_j \right&gt; \big] = \frac{1}{k} \sum_{t \in [k] } \mathbb{E} \big[ u_i [t] \big] \cdot \mathbb{E} \big[ u_j[t] \big] = 0, \forall i, j \in [d], i \neq j\)</span>.</p></li>
</ol>
<p>Clearly, by definition, we have <span class="math inline">\(v_i[t] \sim N(0, 1), \forall i \in [k], t \in [d]\)</span>. Hence <span class="math inline">\(A\)</span> satisfies both the required column and row properties.</p>
<p>It is left to analysis the concentration of <span class="math display">\[
    ||A z||^2 = \frac{1}{k} ||\tilde{A } z||^2 = \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2
\]</span> for a given <span class="math inline">\(z \in \R^d\)</span>.</p>
<blockquote>
<p>Theorem. (Chernoff bound for the chi-square distribution). Let <span class="math inline">\(X_i \sim N(0, \sigma^2): i \in [k]\)</span> be independent random variables. Then <span class="math display">\[
\Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \ge (1 + \epsilon) \cdot  \sigma^2 \right] \le \exp \left( - \frac{k}{4} (\epsilon^2 - \epsilon^3) \right), \\
\Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \le (1 - \epsilon) \cdot  \sigma^2 \right] \le \exp \left( - \frac{k}{4} \epsilon^2 \right). \\
\]</span></p>
</blockquote>
<!-- #### Lemma One
Let $t \in R^d$ be a random vector s.t. $t_i \overset{i.i.d}{\thicksim} N(0,1)$ and $x \in R^d$ be any fixed vector. Then 

 1.  $t^T x \thicksim N(0, ||x||^2)$, where $||x||^2 = \sum_{i = 1}^d x_i^2$.
 2.  $\mathbb{E}[(t^Tx)^2] = \mathbb{E}[(\sum_{i = 1}^d t_i x_i)^2]$ = $\sum_{1 \le i, j\le d} x_i x_j \mathbb{E}[t_i t_j]$ =$\sum_{i = 1}^d x_i^2 = ||x||^2$ -->
<p>The proof of the theorem relies on the following lemmas.</p>
<blockquote>
<p>Lemma 1. (Moment Generating Function of Normal Distribution). Let <span class="math inline">\(X \thicksim N(0,\sigma^2)\)</span>, then for <span class="math inline">\(\lambda &lt; \frac{1}{2 \sigma^2}\)</span>, we have <span class="math display">\[
\mathbb{E} \big[ \exp(\lambda X^2) \big] = \frac{1}{ \sqrt{ 1 - 2 \sigma^2 \lambda} }.
\]</span></p>
</blockquote>
<p><em>Proof:</em> <span class="math display">\[
    \begin{aligned}
    \mathbb{E} [ \exp( \lambda X^2 ) ] &amp;= \frac{1}{ \sqrt{2 \pi \sigma } } \int \exp( \lambda X^2 ) \exp(-X^2 / 2 \sigma^2 ) \ dX \\
    &amp;=  \frac{1}{ \sqrt{2 \pi \sigma } } \int \exp \left(- \frac{ (1 - 2 \sigma^2 \lambda ) X^2}{2 \sigma^2 } \right) \ dX  \\
    &amp;= \frac{1}{ \sqrt{1 - 2 \sigma^2 \lambda } }.
    \end{aligned}
\]</span> <span class="math inline">\(\square\)</span></p>
<blockquote>
<p>Lemma 2. Let <span class="math inline">\(\sigma^2 = ||z||^2\)</span>. It holds that <span class="math display">\[
\Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) \le \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).
\]</span></p>
</blockquote>
<p>As <span class="math display">\[
    \ln(1 + \epsilon) = \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3} - \left( \frac{\epsilon^4}{4} - \frac{\epsilon^5}{5} \right) - \left( \frac{\epsilon^6}{6} + \frac{\epsilon^7}{7} \right) - ...\le \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3}, 
\]</span></p>
<p>we get <span class="math display">\[
    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) 
        \le \exp \left( - \frac{k \cdot ( \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} ) }{2} \right) 
        \le \exp \left( - \frac{k}{4} (\epsilon^2 - \epsilon^3) \right).
\]</span></p>
<p><em>Proof:</em> Define <span class="math inline">\(X \sim N(0, \sigma^2)\)</span> be a standard Gaussian random variable and let <span class="math inline">\(X_i = \left&lt; v_i, z \right&gt;, \forall i \in [k]\)</span>. Then, <span class="math inline">\(X_i \sim N(0, \sigma^2) : \forall i \in [k]\)</span> can be viewed as i.i.d copies of <span class="math inline">\(X\)</span>. By lemma 1, for any <span class="math inline">\(\lambda \in (0, \frac{1}{2 \sigma^2} )\)</span>, it holds that</p>
<p><span class="math display">\[
\begin{aligned}
    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)  
    &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2 \ge (1 + \epsilon) \cdot \sigma^2 \right) \\
    &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \ge k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \\
    &amp; = \Pr \left( \exp \left( \lambda \sum_{i \in [k] } X_i^2 \right)\ge \exp \left( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \right) \\
    &amp;\le \frac{\mathbb{E} [ \exp(\lambda X^2 ) ]^k}{ \exp( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 ) } \\
    &amp;= \left( \frac{1}{ \exp( \lambda \cdot (1 + \epsilon) \cdot \sigma^2 ) \cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \lambda} } \right)^k.
\end{aligned}
\]</span></p>
<p>We want to find a <span class="math inline">\(\lambda\)</span> that minimizes the RHS. Let <span class="math display">\[
f = \lambda \cdot (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \ln (1 - 2 \cdot \sigma^2 \cdot  \lambda).
\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(\lambda\)</span>, <span class="math display">\[
\begin{aligned}
    \frac{ \partial f }{ \partial \lambda} 
        &amp;= (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \cdot \frac{1}{1 - 2  \cdot \sigma^2 \cdot \lambda} \cdot (-2  \cdot \sigma^2 ) \\
        &amp;= \sigma^2 \cdot \left( (1 + \epsilon) - \frac{1}{ 1 - 2 \cdot \sigma^2 \cdot \lambda } \right). 
\end{aligned}
\]</span></p>
<p>Hence <span class="math inline">\(f\)</span> increases when <span class="math inline">\(\lambda &lt; \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) }\)</span> and decreases afterwards. It follows that <span class="math display">\[
\begin{aligned}
    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) 
        &amp;\le \left( \frac{1}{ \exp( \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } \cdot (1 + \epsilon) \cdot \sigma^2 ) \cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } } } \right)^k \\
        &amp;= \left( \frac{ \sqrt{1 + \epsilon} }{ \exp( \epsilon / 2) } \right)^k \\
        &amp;= \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p>Lemma 3. Let <span class="math inline">\(\sigma^2 = ||z||^2\)</span>. It holds that <span class="math display">\[
\Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right) \le \exp\left( \frac{k \cdot (\epsilon + \ln (1 - \epsilon) ) }{2} \right).
\]</span></p>
</blockquote>
<p>As <span class="math display">\[
    \ln(1 - \epsilon) = -\epsilon - \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} - \frac{\epsilon^4}{4} - ...\le - \epsilon - \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3}, 
\]</span></p>
<p>we have <span class="math display">\[
    \Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right) \le \exp\left( \frac{k \cdot (\epsilon - \epsilon - \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} ) }{2} \right) \le \exp \left( - \frac{k }{4} \epsilon^2 \right).
\]</span></p>
<p><em>Proof:</em> Similar to the proof of lemma 2, we have <span class="math display">\[
\begin{aligned}
    \Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right) 
        &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2 \le (1 - \epsilon) \cdot \sigma^2 \right) \\
        &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \le k \cdot (1 - \epsilon) \cdot \sigma^2 \right) \\
        &amp; = \Pr \left( \exp \left( - \lambda \sum_{i \in [k] } X_i^2 \right) \ge \exp \left( - \lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2 \right) \right) \\
        &amp;\le \frac{\mathbb{E} [ \exp( -\lambda X^2 ) ]^k}{ \exp( - \lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2 ) } \\
        &amp;= \left( \frac{1}{ \exp( -\lambda \cdot (1 - \epsilon) \cdot \sigma^2 ) \cdot \sqrt{ 1 + 2 \cdot \sigma^2 \cdot \lambda} } \right)^k. 
\end{aligned}
\]</span></p>
<p>Taking <span class="math inline">\(\lambda = \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 - \epsilon) }\)</span>, we get <span class="math display">\[
\begin{aligned}
    \Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right) 
        &amp;\le \left( \frac{1}{ \exp( - \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 - \epsilon) } \cdot (1 - \epsilon) \cdot \sigma^2 ) \cdot \sqrt{ 1 + 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 - \epsilon) } } } \right)^k \\
        &amp;= \left( \sqrt{1 - \epsilon} \exp( \epsilon / 2) \right)^k \\
        &amp;= \exp\left( \frac{k \cdot (\epsilon + \ln (1 - \epsilon) ) }{2} \right).
\end{aligned} 
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<!-- Thus, 
$$
\begin{aligned}
\Pr (\frac{||Ax||^2}{k} \ge (1 + \epsilon) ||x||^2)  
& \le (\frac{1}{e^{s(1 + \epsilon)} \sqrt{1 - 2s}})^k \\\\
& \le (\frac{1 + \epsilon}{e^{\epsilon}})^{k / 2} \\\\
& \le e^{(\ln (1 + \epsilon) - \epsilon) (k  / 2)}  \\\\
& \le e^{(\epsilon - \epsilon^2 / 2 + \epsilon^3 / 3 - \epsilon) ( k / 2)} \\\\
& \le e^{(- \epsilon^2 / 2 + \epsilon^3 / 3) ( k / 2)}  
\end{aligned}
$$ -->
<!-- When $k$ = $4 \ln n \over (\epsilon^2 / 2- \epsilon^3/ 3)$, we get 
$$
\Pr (\frac{||Ax||^2}{k} \notin [ (1 - \epsilon) ||x||^2, (1 + \epsilon) ||x||^2])   \le 2 / n^2
$$

Given lemma three, by union bound $||f(x_i) - f(x_j)||^2 \notin (1 \pm \epsilon) ||x_i - x_j||^2$ occurs with less than $\frac{2}{n^2} * \frac{n (n -1)}{2} = 1 - \frac{1}{n}$ probability. Repeating generating $A$ will boost the success probability to a desired level.  -->
<h2 id="rademacher">Rademacher</h2>
<p>In this implementation, the entries of <span class="math inline">\(\tilde A\)</span> are simply independent Rademacher random variables which equal to <span class="math inline">\(1\)</span> and <span class="math inline">\(-1\)</span> with the same probability <span class="math inline">\(1/2\)</span>. Let <span class="math inline">\(u_1, ..., u_d\)</span> be the column vectors of <span class="math inline">\(\tilde A\)</span> and <span class="math inline">\(v_1, ..., v_k\)</span> be its row vectors. Hence, <span class="math inline">\(\frac{1}{\sqrt k} u_1, ..., \frac{1}{\sqrt k} u_d\)</span> and <span class="math inline">\(\frac{1}{\sqrt k} v_1, ..., \frac{1}{\sqrt k} v_k\)</span> are the column and row vectors of <span class="math inline">\(A\)</span>, respectively. It satisfies that</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbb{E} \big[ || \frac{1}{\sqrt k} u_i||^2 \big] = \frac{1}{k} \mathbb{Var} \big[ ||u_i||^2 \big] = \frac{k}{k} = 1, \forall i \in [d]\)</span>;</p></li>
<li><p><span class="math inline">\(\mathbb{E} \big[ \left&lt; \frac{1}{\sqrt k} u_i, \frac{1}{\sqrt k} u_j \right&gt; \big] = \frac{1}{k} \sum_{t \in [k] } \mathbb{E} \big[ u_i [t] \big] \cdot \mathbb{E} \big[ u_j[t] \big] = 0, \forall i, j \in [d], i \neq j\)</span>.</p></li>
</ol>
<p>Clearly, by definition, we have <span class="math inline">\(v_i[t]\)</span> has mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>, <span class="math inline">\(\forall i \in [k], t \in [d]\)</span>. Hence <span class="math inline">\(A\)</span> satisfies both the required column and row properties.</p>
<p>The proof of the theorem relies on the following lemmas.</p>
<blockquote>
<p>Lemma 1. (Moment Generating Function of Rademacher random variable). Let <span class="math inline">\(X\)</span> be a random variable such that <span class="math display">\[
X = \begin{cases}
\begin{aligned}
&amp; 1,    &amp;\text{w.p.} 0.5,    \\
&amp;-1,    &amp;\text{w.p.} 0.5,    
\end{aligned}
\end{cases}
\]</span> then for <span class="math inline">\(\lambda \in \R\)</span>, we have <span class="math display">\[
\mathbb{E} \big[ \exp(\lambda X^2) \big] = \exp(\lambda ).
\]</span></p>
</blockquote>
<p><em>Proof:</em> <span class="math display">\[
    \begin{aligned}
    \mathbb{E} [ \exp( \lambda X^2 ) ] &amp;= \frac{1}{2} \exp( \lambda ) + \frac{1}{2} \exp(\lambda) = \exp(\lambda)
    \end{aligned}
\]</span> <span class="math inline">\(\square\)</span></p>
<p><span class="math display">\[
\frac{1}{2} \exp( - \sigma \lambda ) + \frac{1}{2} \exp( \sigma \lambda) = \frac{1}{2} \left( \sum_{i = 0}^\infty \frac{ (- \sigma \lambda)^i }{ i! } + \sum_{i = 0}^\infty \frac{ ( \sigma \lambda)^i }{ i! } \right) \\
=  \sum_{i = 0}^\infty \frac{ (\sigma \lambda)^{2i}  }{ (2i)! } 
\le \exp( \sigma^2 \lambda^2 )
\]</span></p>
<blockquote>
<p>Lemma 2. Let <span class="math inline">\(\sigma^2 = ||z||^2\)</span>. It holds that <span class="math display">\[
\Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) \le \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).
\]</span></p>
</blockquote>
<p>As <span class="math display">\[
    \ln(1 + \epsilon) = \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3} - \left( \frac{\epsilon^4}{4} - \frac{\epsilon^5}{5} \right) - \left( \frac{\epsilon^6}{6} + \frac{\epsilon^7}{7} \right) - ...\le \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3}, 
\]</span></p>
<p>we get <span class="math display">\[
    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) 
        \le \exp \left( - \frac{k \cdot ( \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} ) }{2} \right) 
        \le \exp \left( - \frac{k}{4} (\epsilon^2 - \epsilon^3) \right).
\]</span></p>
<p><em>Proof:</em> Define <span class="math inline">\(X\)</span> be a Rademacher random variable and let <span class="math inline">\(X_i = \left&lt; v_i, z \right&gt;, \forall i \in [k]\)</span>. Then, <span class="math inline">\(X_i: \forall i \in [k]\)</span> can be viewed as i.i.d copies of <span class="math inline">\(X\)</span>. By lemma 1, for any <span class="math inline">\(\lambda \in (0, \frac{1}{2 \sigma^2} )\)</span>, it holds that</p>
<p><span class="math display">\[
\begin{aligned}
    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)  
    &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2 \ge (1 + \epsilon) \cdot \sigma^2 \right) \\
    &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \ge k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \\
    &amp; = \Pr \left( \exp \left( \lambda \sum_{i \in [k] } X_i^2 \right)\ge \exp \left( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \right) \\
    &amp;\le \frac{\mathbb{E} [ \exp(\lambda X^2 ) ]^k}{ \exp( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 ) } \\
    &amp;= \left( \frac{ \exp(\lambda^2 \sigma^2 ) }{ \exp( \lambda \cdot (1 + \epsilon) \cdot \sigma^2 )  } \right)^k \\
    &amp;= \exp(- \lambda \cdot \epsilon \cdot \sigma^2 \cdot k)
\end{aligned}
\]</span></p>
<p>We want to find a <span class="math inline">\(\lambda\)</span> that minimizes the RHS. Let <span class="math display">\[
f = \lambda \cdot (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \ln (1 - 2 \cdot \sigma^2 \cdot  \lambda).
\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(\lambda\)</span>, <span class="math display">\[
\begin{aligned}
    \frac{ \partial f }{ \partial \lambda} 
        &amp;= (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \cdot \frac{1}{1 - 2  \cdot \sigma^2 \cdot \lambda} \cdot (-2  \cdot \sigma^2 ) \\
        &amp;= \sigma^2 \cdot \left( (1 + \epsilon) - \frac{1}{ 1 - 2 \cdot \sigma^2 \cdot \lambda } \right). 
\end{aligned}
\]</span></p>
<p>Hence <span class="math inline">\(f\)</span> increases when <span class="math inline">\(\lambda &lt; \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) }\)</span> and decreases afterwards. It follows that <span class="math display">\[
\begin{aligned}
    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) 
        &amp;\le \left( \frac{1}{ \exp( \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } \cdot (1 + \epsilon) \cdot \sigma^2 ) \cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } } } \right)^k \\
        &amp;= \left( \frac{ \sqrt{1 + \epsilon} }{ \exp( \epsilon / 2) } \right)^k \\
        &amp;= \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="reference">Reference</h1>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/09/10/Nonuniform%20Sampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/10/Nonuniform%20Sampling/" class="post-title-link" itemprop="url">Nonuniform Sampling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-10 20:38:54" itemprop="dateCreated datePublished" datetime="2016-09-10T20:38:54+10:00">2016-09-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2016-09-11 19:59:33" itemprop="dateModified" datetime="2016-09-11T19:59:33+10:00">2016-09-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose <span class="math inline">\(S = \{ s_1, s_2, ..., s_m\}\)</span> is a set of samples, with weight <span class="math inline">\(W = \{ w_1, w_2, ..., w_m\}\)</span> and probability <span class="math inline">\(P = \{ p_1, p_2, ..., p_m\}\)</span>.</p>
<p>Now we randomly select an element <span class="math inline">\(X\)</span> from <span class="math inline">\(S\)</span> according to probability <span class="math inline">\(P\)</span>. Define <span class="math inline">\(Y\)</span> the index <span class="math inline">\(X\)</span>. For example, if <span class="math inline">\(X = s_i\)</span>, then <span class="math inline">\(Y = i\)</span>. The expected weight of <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
E[w_Y] = \sum_{i \in [1,m]} w_i P( Y = i) = \sum_{i \in [1,m]} w_i p_i
\]</span></p>
<p>If <span class="math inline">\(P\)</span> is a uniform distribution, i.e., <span class="math inline">\(p_i = 1/m, \ \forall i \in [1,m]\)</span>, then <span class="math display">\[
E[w_Y] = 1 / m * \sum_{i \in [1,m]} w_i
\]</span></p>
<p>Then <span class="math inline">\(m E[w_Y]\)</span> equals just the sum of <span class="math inline">\(W\)</span>. We say that it is an <strong>unbiased estimator</strong> of the sum of <span class="math inline">\(W\)</span>. But what if <span class="math inline">\(P\)</span> is not uniform, i.e., <span class="math inline">\(\exists i,j \ s.t., p_i \neq p_j\)</span>.</p>
<p>One technique to remedy this is to construct a new variable <span class="math inline">\(Z_Y = w_Y / p_Y\)</span>.</p>
<p><span class="math display">\[
E[Z_Y] =  \sum_{i \in [1,m]} w_i / p_i * P(Y = i) = \sum_{i \in [1,m]} w_i
\]</span></p>
<p>The strange thing is that how we could know <span class="math inline">\(P = \{ p_1, p_2, ..., p_m\}\)</span>. If we know it in advance, we need not to do sampling to estimate $ _{i } w_i$.</p>
<p>But in some circumstance, it is possible. To get a random sample <span class="math inline">\(X\)</span> from <span class="math inline">\(S\)</span>, we need to go through some sampling process. Once <span class="math inline">\(X\)</span> is returned, we can calculate the probability of <span class="math inline">\(X\)</span> by examining the sampling process.</p>
<p>[FBKZ16] gives such an example. Consider two relations <span class="math inline">\(R_1 = (A_1, F)\)</span> and <span class="math inline">\(R_2 = (F, A_2)\)</span>, where <span class="math inline">\(R_1.F\)</span> and <span class="math inline">\(R_2.F\)</span> shares the same domain. Let <span class="math inline">\(R = R_1 \bowtie_{R_1.F = R_2.F} R_2\)</span> be the natural join result based on <span class="math inline">\(R_1.F\)</span> and <span class="math inline">\(R_2.F\)</span>. The object is to calculate the sum of attribute <span class="math inline">\(A_2\)</span> on <span class="math inline">\(R = (A_1, F, A_2)\)</span>.</p>
<p>The natural join can be expensive. Is there any way to estimate the join result? Indeed we have.</p>
<p>First sample uniformly at random a tuple <span class="math inline">\((a_1, f)\)</span> from <span class="math inline">\(R_1\)</span>. The probability of a particular tuple chosen is <span class="math inline">\(1/|R_1|\)</span>, where <span class="math inline">\(|R_1|\)</span> is the number of tuples in <span class="math inline">\(R_1\)</span>.</p>
<p>We further assume that there is an index on <span class="math inline">\(R_2.F\)</span>. So we immediately know the size of the set <span class="math inline">\(R_2[f] \doteq \{ (f, a_2) | (f, a_2) \in R_2 \}\)</span>. We choose uniformly at random a tuple <span class="math inline">\((f, a_2)\)</span> from <span class="math inline">\(R_2[f]\)</span> and get the join result <span class="math inline">\((a_1, f, a_2)\)</span>. The probability corresponding to <span class="math inline">\((a_1, f, a_2)\)</span> is <span class="math inline">\(1/ |R_1| * 1 / R_2[f]\)</span>. So <span class="math inline">\(\frac{a_2}{|R_1| |R_2[f]|}\)</span> is an unbiased estimator of <span class="math inline">\(sum(R.A2)\)</span></p>
<p><strong>Reference</strong><br />
[FBKZ16] Feifei. Li, Bin. Wu, Ke. Yi, Zhuoyue. Zhao, Wander Join: Online Aggregation via Random Walks. <em>SIGMOD'16</em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/08/29/SimRank/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/29/SimRank/" class="post-title-link" itemprop="url">SimRank</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-29 20:38:54" itemprop="dateCreated datePublished" datetime="2016-08-29T20:38:54+10:00">2016-08-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-19 21:34:01" itemprop="dateModified" datetime="2020-05-19T21:34:01+10:00">2020-05-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>SimRank</strong><br />
Given a directed graph <span class="math inline">\(G = \left&lt; V, E \right&gt;\)</span>, SimRank score is defined as follows <span class="math display">\[
s(i,j) = 
    \begin{cases}
        \begin{array}{ll}
        1, &amp;\text{ if } i = j \\
        \frac{c}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a,b), &amp;\text{otherwise}
        \end{array}
    \end{cases}
\]</span></p>
<p>where <span class="math inline">\(I(i)\)</span> is the set of in-neighbors of <span class="math inline">\(i \in V\)</span>, and <span class="math inline">\(c \in (0, 1)\)</span>.</p>
<p>By the definition of <em>SimRank</em>, <span class="math inline">\(s(i,j)\)</span> can be interpreted as the value of <span class="math display">\[
E[c^{T_{i,j} } ]
\]</span></p>
<p>where <span class="math inline">\(T_{i,j}\)</span> is the random variable of the first time that a reverse random walk (a random walk that travel along the incoming edges of a vertex randomly at each step) from <span class="math inline">\(i\)</span> and a reverse random walk from <span class="math inline">\(j\)</span> meets. Note that <span class="math display">\[
T_{i,j} = 
    \begin{cases}
        \begin{array}{ll}
        0, &amp;\text{ if } i = j \\
        \frac{1}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} T_{a,b}, &amp;\text{otherwise}
        \end{array}
    \end{cases}
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
E[c^{T_{i,j} } ] 
    &amp;= c^0 \cdot \Pr[T_{i,j} = 0] + \frac{1}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} E[c^{1 + T_{a, b}} ] \\
    &amp;= \mathfrak{1}_{i = j} +  \frac{c}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} E[c^{ T_{a, b}} ] \\
    &amp;= \mathfrak{1}_{i = j} +  \frac{c}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a, b) \\
\end{aligned}
\]</span></p>
<p>However, there is another random walk interpretation as follows:</p>
<ol type="1">
<li><p>First, notice that when <span class="math inline">\(i \neq j\)</span>, [TX16] rewrites the SimRank as <span class="math display">\[
s(i,j) = \frac{ \sqrt c \sqrt c} {|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a,b)
\]</span></p></li>
<li><p>Second, [TX16] defines the <span class="math inline">\(\sqrt c\)</span>-random walk as</p>
<ul>
<li>In any step, the walk stops with probability <span class="math inline">\(1 - \sqrt c\)</span>.</li>
<li>With the other <span class="math inline">\(\sqrt c\)</span> probability, the walk chose the in-neighbors of the current vertex uniformly at random.</li>
</ul></li>
<li><p>Finally <span class="math inline">\(s(i,j)\)</span> is interpreted as</p>
<blockquote>
<p>The probability of two <span class="math inline">\(\sqrt c\)</span> random walks ever meet, such that one of the random walks starts at vertex <span class="math inline">\(i\)</span> and the other starts at <span class="math inline">\(j\)</span>.</p>
</blockquote></li>
</ol>
<p><strong>Monte Carlo</strong><br />
A straightforward solution is to perform Monte Carlo simulation to calculate <span class="math inline">\(s(i,j)\)</span> -- we initialize two random walks from <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> respectively and simulate the two walks until they meet or either walk dies.</p>
<p>We need not to worry about that this process continues forever. Since the expected number of steps of a <span class="math inline">\(\sqrt c\)</span> random walk is given by <span class="math display">\[
\sqrt c + \sqrt c^2 + \sqrt c^3 + ...... = \frac{ \sqrt c}{ 1 - \sqrt c}
\]</span> which is a constant number.</p>
<p><strong>Chernoff Bound</strong><br />
We repeat this process <span class="math inline">\(n_w\)</span> times and use <span class="math inline">\(X_k\)</span> to indicate whether the two walks meet in the k-th trial, i.e., <span class="math inline">\(X_k = 1\)</span> if the two walks meet and <span class="math inline">\(0\)</span> otherwise. It is easy to see that <span class="math inline">\(\{ X_k \}\)</span> is a set of i.i.d random variables with <span class="math inline">\(E[ X_k ] = s(i, j)\)</span>.</p>
<p>Let <span class="math inline">\(\bar{X} \doteq \frac{1}{n_w} \sum_{k = 1}^{n_w} X_k\)</span> and <span class="math inline">\(\mu \doteq s(i,j)\)</span>, So by <em>Chernoff Bound</em>, <span class="math display">\[
P({| \bar{X} - \mu | &gt; (1 + \lambda) \mu }) \le 2 \exp(- \frac{n_w \mu \lambda^2}{3}) = \delta
\]</span> where <span class="math inline">\(\lambda\)</span> is relative error ratio and <span class="math inline">\(\delta\)</span> the error probability we can tolerate.</p>
<p><strong>Time Complexity</strong><br />
The previous section implies <span class="math inline">\(n_w = \frac{3 }{ \mu \lambda ^2}\log \frac{ 2 }{ \delta }\)</span>. By setting <span class="math inline">\(\epsilon = (1 + \lambda) \mu \Leftrightarrow \lambda = \frac{\epsilon}{\mu} - 1\)</span>, we get</p>
<p><span class="math display">\[
n_w = \frac{ 3 }{ \mu ( \frac{\epsilon}{\mu}^2 -  2\frac{\epsilon}{\mu}  + 1 ) }\log \frac{ 2 }{ \delta } 
= O( \frac{ \mu }{ \epsilon^2 }\log \frac{ 1 }{ \delta })
\]</span></p>
<p>Notice that when <span class="math inline">\(\epsilon\)</span> is small, say <span class="math inline">\(0.01\)</span>, <span class="math inline">\(\frac{1}{\epsilon^2}\)</span> is large thus can not be neglected. The problem remains whether we could reduce the number of random walks.</p>
<p><strong>Hitting probability</strong> Another acute observation from [TX16] is that if two walks reach the same vertex, their following behavior follows the same probability distribution (since the walks are memoryless).</p>
<p>Motivated by this, we define <span class="math inline">\(h_i^l(k)\)</span> as the probability such that a walk begins at vertex <span class="math inline">\(i\)</span> and reaches vertex <span class="math inline">\(j\)</span> at the l-th step. <span class="math inline">\(h_i^l(k)\)</span> can be updated iteratively using <span class="math display">\[
h_i^l(k) = \frac{ \sqrt c }{ |I(i) |  } \sum_{j \in I(i)} h_j^{l -1}(k)
\]</span></p>
<p><strong>SimRank and Hitting Probability</strong><br />
Now we set about bridging the gap between SimRank score and hitting probability.</p>
<p>Define <span class="math inline">\(E(i,j)\)</span> as the event that two random walks starting from i and j respectively ever meet. <span class="math inline">\(P(E(i,j)) = s(i,j)\)</span>.</p>
<p><span class="math inline">\(E(i, j, k, l)\)</span> denotes the event that two random walks, starting from <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> respectively, meet at <span class="math inline">\(l\)</span>-th step at <span class="math inline">\(k\)</span> and this is the last time they meet. In other word, these two walk never meet again after <span class="math inline">\(l\)</span>-th step at <span class="math inline">\(k\)</span>. It is easy to see that for different <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>, the <span class="math inline">\(E(i, j, k, l)\)</span>'s are mutually exclusive and constitute a partition of <span class="math inline">\(E(i,j)\)</span>,</p>
<p><span class="math display">\[
E(i,j) = \bigcup_l \bigcup_l E(i, j, k, l)
\]</span></p>
<p>Let <span class="math inline">\(d_k\)</span> be the probability that two walks from k do not meet each other after <span class="math inline">\(0\)</span>-th step. Then <span class="math inline">\(P(E(i, j, k, l)) = h_i^l(k) h_j^l(k) d_k\)</span>. Thus</p>
<p><span class="math display">\[
\begin{aligned}
P( E(i,j) ) &amp;= 
P(\bigcup_l \bigcup_l E(i, j, k, l))  \\
&amp;= \sum_{l = 0}^{\infty}  \sum_{k = 1}^{n} P(E(i, j, k, l)) \\
&amp;= \sum_{l = 0}^{\infty}  \sum_{k = 1}^{n} h_i^l(k) h_j^l(k) d_k
\end{aligned}
\]</span></p>
<p><strong>Computation</strong><br />
Notice that for a given <span class="math inline">\(i\)</span>, there infinity many <span class="math inline">\(h_i^l(k)\)</span>'s we need to kept, which is unfeasible. To get around this, we only keep the <span class="math inline">\(h_i^l(k)\)</span>'s greater than <span class="math inline">\(\epsilon_h\)</span>, where <span class="math inline">\(\epsilon_h = O(\epsilon)\)</span>.</p>
<p>The probability that the <span class="math inline">\(\sqrt c\)</span>-random walk does not to stop before <span class="math inline">\(l\)</span>-th step is <span class="math inline">\(\sqrt c ^l\)</span>, i.e., <span class="math display">\[
\sum_{k = 1}^n h_i^l(k) = \sqrt c^l
\]</span></p>
<p>For a fixed <span class="math inline">\(l\)</span>, at most <span class="math inline">\(\sqrt c ^l / \epsilon_h\)</span> number of <span class="math inline">\(h_i^l(k)\)</span>'s are kept. So the total number of <span class="math inline">\(h_i^l(k)\)</span>'s kept is bounded by <span class="math display">\[
\frac{1}{\epsilon_h} (\sqrt c + \sqrt c^2 + \sqrt c^3 + ......  )= O(\frac{1}{\epsilon_h}) = O(\frac{1}{\epsilon})
\]</span></p>
<p>On the other hand, estimation of <span class="math inline">\(d_k\)</span> could be done by monte carlo simulation. Similar to analysis in <strong>Chernoff Bound</strong> and <strong>Time Complexity</strong> section, we can show that with <span class="math inline">\(O \left( \frac{1}{\epsilon_d^2} \log \frac{1}{\delta_d} \right)\)</span> expected time, <span class="math inline">\(d_k\)</span> could be estimated by <span class="math inline">\(\bar{d_k}\)</span> such that <span class="math inline">\(| \bar{d_k} - d_k| &lt; \epsilon_d\)</span> holds with at least <span class="math inline">\(1 - \delta_d\)</span> probability, where <span class="math inline">\(\epsilon_d = O(\epsilon)\)</span>.</p>
<p><strong>Reference</strong><br />
[TX16] Boyu. Tian and Xiaokui. Xiao SLING: A Near-Optimal Index Structure for SimRank. <em>SIGMOD'16</em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/12/01/Random%20Sampling%20and%20Cuts%20II/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/12/01/Random%20Sampling%20and%20Cuts%20II/" class="post-title-link" itemprop="url">Random Sampling and Cuts II</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-12-01 22:27:54" itemprop="dateCreated datePublished" datetime="2015-12-01T22:27:54+11:00">2015-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-04-07 13:01:58" itemprop="dateModified" datetime="2018-04-07T13:01:58+10:00">2018-04-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>Sampling Theorem</strong></p>
<p>In 1994, Karger gave the following result:</p>
<p><strong>[Sampling Karger]</strong> <em>[Using randomized sparsification to approximate minimum cuts]</em> Given a graph G and a collection of random variables <span class="math inline">\(X_e (X_e \le M)\)</span> associated with all edges e, with probability <span class="math inline">\(1 - O(n^{-d})\)</span> that every cut in the instance <span class="math inline">\(G(X_e)\)</span> generated by putting weight <span class="math inline">\(X_e\)</span> on each e has value with <span class="math inline">\((1 - \epsilon, 1 + \epsilon)\)</span> times its expected value, where <span class="math display">\[
\epsilon = \sqrt{2(d + 2)M\ln n \over c&#39;}
\]</span> and c' is G's minimum expected value.</p>
<p>As an application, if we fix <span class="math inline">\(\epsilon\)</span> and let <span class="math inline">\(p = \Theta({\ln n \over \epsilon^2 c})\)</span>, where c is the minimum cut of G, then G' approximate all its original graph cuts by a factor p with high probability.</p>
<p>The limitation is that either <span class="math inline">\(\epsilon\)</span> or p depends on c. When c is little, we could not reduce many edges.</p>
<p><strong>Improvement:</strong> In 1996, Benczur and Karger improved previous sampling method such that the dependence on c is removed.</p>
<p><strong>New Result</strong>:</p>
<blockquote>
<p>Given a graph G and a parameter <span class="math inline">\(\epsilon\)</span>, there is a new graph G' such that 1. G' has <span class="math inline">\(O({n \ln n \over \epsilon^2})\)</span> edges. 2. every cut in G' has value <span class="math inline">\((1 \pm \epsilon)\)</span> times its original cut value.</p>
</blockquote>
<p>The intuition is that in the dense parts of the graph, each edge can be sample with smaller probability in order to preserve cut value while edge should be sample with higher probability where the graph is sparse.</p>
<p>Before moving on we first formalize the notion of a dense region. We characterize a dense region by <em>Strong Connectivity</em>.</p>
<p><strong>k-Connected</strong></p>
<blockquote>
<p>A graph G is k connected if its minimum cut value is no less than k.</p>
</blockquote>
<p><strong>Vertex-Induced Subgraph</strong></p>
<blockquote>
<p><span class="math inline">\(G&#39;=&lt;V&#39;,E&#39;&gt;\)</span> is a vertex induced subgraph of <span class="math inline">\(G =&lt;V,E&gt;\)</span> if <span class="math inline">\(V&#39;\)</span> is a subset of <span class="math inline">\(V\)</span> and <span class="math inline">\(E&#39;\)</span> is all edges of <span class="math inline">\(E\)</span> whose endpoints are both in <span class="math inline">\(V&#39;\)</span>.</p>
</blockquote>
<p><strong>k-Strong Component</strong></p>
<blockquote>
<p>A k-Strong Component is a maximal k-connected vertex-induced subgraph.</p>
</blockquote>
<p><strong>Strong Connectivity</strong></p>
<blockquote>
<p>The strong connectivity <span class="math inline">\(c_e\)</span> of an edge e is the maximum value of k such that a k-Strong component contains e.</p>
</blockquote>
<h5 id="generating-the-graph">Generating the graph</h5>
<p>Here is how new sampling technique operates</p>
<blockquote>
<p>Given a parameter <span class="math inline">\(\epsilon\)</span>, let <span class="math inline">\(\rho = {16(d + 2)M \ln n \over \epsilon^2}\)</span>, we assign to each edge a random weight <span class="math inline">\(X_e\)</span>, which takes value <span class="math inline">\(1/p\)</span> with probability <span class="math inline">\(p\)</span> and <span class="math inline">\(0\)</span> with probability <span class="math inline">\((1 - p)\)</span>, where <span class="math inline">\(p = \min \{ \rho / c_e, 1 \}\)</span>. When <span class="math inline">\(X_e = 0\)</span>, we eliminate e from the new graph.</p>
</blockquote>
<p>Immediately follows from the definition of <span class="math inline">\(X_e\)</span> we see that <span class="math inline">\(E[X_e] = 1\)</span>.</p>
<p>To check the sampling technique works, we need to verify two things: 1. The number of edge generated is <span class="math inline">\(O(n \ln n / \epsilon^2)\)</span> 2. Each cut does not deviate much from its original value.</p>
<p><strong>Number of edges</strong></p>
<p><strong>Theorem 1</strong> &gt; If an undirected connected graph has k(n - 1) edges, then it must has a k strong component.</p>
<p><em>Proof:</em> We prove it by induction. When k = 1, then the graph itself is a k-strong component. When n = 2, the theorem clearly holds.</p>
<p>Now we consider the general case. Suppose that the graph does not contain a k-strong component. Then there exist a cut <span class="math inline">\(C = &lt;A, B&gt;\)</span> in the graph such that <span class="math inline">\(cut(A,B) &lt; k\)</span>. Now both A and B have less than n nodes. By induction, A has less then k(|A| - 1) edges inside and B has less then k(|B| - 1) edges inside. The total number of edges is less than k(|A| - 1) + k + k(|B| - 1) = k(|A|+|B| - 1) = k(n - 1). A contradiction.</p>
<p><strong>Theorem 2</strong> &gt; <span class="math inline">\(\Sigma {1 \over c_e} &lt; 2(n -1)\)</span></p>
<p><em>Proof:</em> Suppose that <span class="math inline">\(\Sigma {1 \over c_e} \ge 2(n -1)\)</span>, then by theorem 1, there is a 2-strong component. Let e* be the edge with minimum <span class="math inline">\(c_{e^*}\)</span> in that component. Then there is a cut C in the 2-strong component containing e* such that <span class="math inline">\(cut(C) \le c_{e^*}\)</span>. Now</p>
<p><span class="math display">\[
{\underset{e \in C}{\Sigma }} c_e^{-1} \le c_{e^*}^{-1} c_{e^*} = 1 &lt; 2
\]</span></p>
<p>A contradiction.</p>
<p><strong>Error analysis</strong></p>
<p>To analyze the errors, we denote <span class="math inline">\(F_i\)</span> the set of edges whose strong connectivity are between <span class="math inline">\([2^i, 2^{i+1}]\)</span> for <span class="math inline">\(i \ge 0\)</span>. Let <span class="math inline">\(G_i = &lt;V, F_i \cup F_{i + 1} \cup F_{i + 2}...&gt;\)</span>.</p>
<p>In <span class="math inline">\(G_i\)</span>, each edge is sampled in the following way <span class="math display">\[
X_e = \begin{cases}
1, if \ e \notin F_i \\
         \begin{Bmatrix}
           {1 \over p} with \ probability \ p, \\
           0 \  with  \ probability \ 1 - p
    \end{Bmatrix} \ if \ e \in F_i
\end{cases}
\]</span></p>
<p>Then, by sampling theorem, the sampled graph <span class="math inline">\(G_i&#39;\)</span> is within <span class="math inline">\((1 \pm \epsilon)G_i\)</span>.</p>
<p>So <span class="math display">\[
\begin{aligned}
G&#39; &amp; = \Sigma_{i = 0}^{\log m} F_i&#39; \\ 
&amp; = \Sigma G_i&#39; - \Sigma G_{i + 1} \\
&amp; = \Sigma (1 \pm \epsilon)G_i - \Sigma G_{i + 1} \\
&amp; = (1 \pm \epsilon)G_0 \pm \epsilon\Sigma G_{i + 1} \\
&amp; \in G \pm \epsilon \log m \ G
\end{aligned}
\]</span></p>
<p>But there is a log m term in the final result. We can eliminate the log m term with more sophisticated techniques.</p>
<p>The random variable <span class="math inline">\(X_e\)</span> assigned to e in <span class="math inline">\(G_i\)</span> is revised as follows: <span class="math display">\[
X_e = \begin{cases}
2^{i - j}, if \ e \notin F_i \ and \ e \in F_j \\
         \begin{Bmatrix}
           {1 \over p} with \ probability \ p, \\
           0 \  with  \ probability \ 1 - p
    \end{Bmatrix} \ if \ e \in F_i
\end{cases}
\]</span></p>
<p>It can be shown that every cut in <span class="math inline">\(G_i\)</span> is at least <span class="math inline">\(2^i\)</span> heavy.</p>
<p>So <span class="math display">\[
\begin{aligned}
G&#39; &amp; = \Sigma_{i = 0}^{\log m} F_i&#39; \\ 
&amp; = \Sigma G_i&#39; - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = \Sigma (1 \pm \epsilon)G_i - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = \Sigma F_i + (1 \pm \epsilon)\Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = G \pm 2 \epsilon \ \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \\
&amp; = G\pm 2 \epsilon \ G
\end{aligned} 
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description">你生之前悠悠千載已逝，未來還會有千年沉寂的期待</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
