<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/24/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/24/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/05/18/The-Number-of-Distinct-Elements/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/18/The-Number-of-Distinct-Elements/" class="post-title-link" itemprop="url">The Number of Distinct Elements</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-18 16:09:20" itemprop="dateCreated datePublished" datetime="2019-05-18T16:09:20+10:00">2019-05-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-21 21:01:48" itemprop="dateModified" datetime="2020-06-21T21:01:48+10:00">2020-06-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In many network applications, data comes as a stream <span class="math inline">\(S = \{x_1, x_2, ..., x_n \}\)</span>, where each <span class="math inline">\(x_i\)</span> (<span class="math inline">\(1 \le i \le n\)</span>) belongs to the domain <span class="math inline">\(D\)</span>. Suppose each element in <span class="math inline">\(D\)</span> appear at least once in <span class="math inline">\(S\)</span>, we want to evaluate the size of <span class="math inline">\(D\)</span>, i.e., the number of distinct elements in <span class="math inline">\(S\)</span> (denoted as <span class="math inline">\(N = |D|\)</span>).</p>
<p>Potential solutions include</p>
<ol type="1">
<li>We can sort the elements in <span class="math inline">\(S\)</span> and count the number of distinct elements. This requires <span class="math inline">\(O(n \log n)\)</span> time (with quicksort) and <span class="math inline">\(O(n)\)</span> space.<br />
</li>
<li>We can use hashing to count the distinct elements. This can be done in <span class="math inline">\(O(n)\)</span> time and <span class="math inline">\(O(n)\)</span> space with two level hashing.</li>
</ol>
<p>In real world application, <span class="math inline">\(n\)</span> could be extremely large. Therefore, the demand for <span class="math inline">\(O(n)\)</span> space is expansive or even impossible. Can we do this with <span class="math inline">\(O(1)\)</span> space? If we allow some error, this is possible.</p>
<h2 id="solution-1">Solution 1</h2>
<p>The tool we resort to is a set of hash functions <span class="math inline">\(\mathcal{H} = \{ h | h: U \rightarrow [0, 1] \}\)</span> such that<br />
<span class="math display">\[
\begin{aligned}
&amp;\Pr_{h \in \mathcal{H} } [h(x) \in [a, b]] = \frac{1}{b - a}, &amp;\forall x \in U \wedge [a, b] \subset [0, 1]
\end{aligned}
\]</span></p>
<p>That is, given an element <span class="math inline">\(x \in U\)</span>, if we choose an <span class="math inline">\(h\)</span> from <span class="math inline">\(U\)</span> randomly, then <span class="math inline">\(h\)</span> maps <span class="math inline">\(x\)</span> to a number between <span class="math inline">\([0, 1]\)</span> uniformly at random. Further, we assume that <span class="math inline">\(h\)</span> maps different elements in <span class="math inline">\(U\)</span> to <span class="math inline">\([0, 1]\)</span> independently (Remark: to what extend is this possible?).</p>
<p>Further, denote <span class="math inline">\(h(S) = \{ h(x_1), h(x_2), ..., h(x_n)\}\)</span> the image of <span class="math inline">\(S\)</span> under a function <span class="math inline">\(h\)</span>. We claim that, if <span class="math inline">\(h\)</span> is selected randomly from <span class="math inline">\(\mathcal{H}\)</span>, then the minimum expected value of <span class="math inline">\(h(S)\)</span> is <span class="math inline">\(1 / (N + 1)\)</span>: <span class="math display">\[
\underset{h \in \mathcal{H} }{E} [\min h(S)] = \int_{z = 0}^1 z \Pr_{h \in \mathcal{H}} [\min h(S) = z] \ dz = \frac{1}{N + 1}
\]</span></p>
<p>To see this, observe that the probability that <span class="math inline">\(\Pr_{h \in \mathcal{ H } }[\min h(S) \ge z] = (1 - z)^N​\)</span>, therefore <span class="math inline">\(\Pr_{h \in \mathcal{ H } }[\min h(S) \le z] = 1 - (1 - z)^N​\)</span> and the density function is given as <span class="math display">\[
p_{h \in \mathcal{H  } }[\min h(S) = z] = \frac{\partial \Pr_{h \in \mathcal{H}  }[\min h(S) \le z] }{ \partial z} = N (1 - z)^{N - 1}
\]</span></p>
<p>Taking the expectation over <span class="math inline">\(z = 0\)</span> to <span class="math inline">\(1\)</span> gives the desired result.</p>
<p>Another way to prove this is to introduce an additional element <span class="math inline">\(x_{n + 1}\)</span> which is different from any other element in <span class="math inline">\(D\)</span> and let <span class="math inline">\(S&#39; = S \cup \{ x_{n + 1} \}\)</span>. What is the probability that <span class="math inline">\(h(x_{n + 1})\)</span> is the smallest in <span class="math inline">\(h(S&#39;)\)</span>, i.e., <span class="math inline">\(\min h(S&#39;) = h(x_{n + 1})\)</span>? Conditioning on the minimum value of <span class="math inline">\(\{ h(x_1), h(x_2), ..., h(x_n) \}\)</span> (<span class="math inline">\(\min h(S)\)</span>) being some <span class="math inline">\(z\)</span> (<span class="math inline">\(z \in [0, 1]\)</span>), the probability becomes <span class="math display">\[
\begin{aligned}
&amp;\Pr[\min h(S&#39;) = h(x_{n + 1}) \mid \min h(S) = z] \\
= &amp;\Pr[h(x_{n + 1}) \in [0, z] \mid \min h(S) = z] \\
= &amp;z
\end{aligned}
\]</span></p>
<p>Integrating over all possible values of <span class="math inline">\(z\)</span>, we have <span class="math display">\[
\Pr[\min h(S&#39;) = h(x_{n + 1})] = \int_{z = 0}^1 z \Pr[\min h(S) = z] \ dz
\]</span></p>
<p>This is exactly the expectation of <span class="math inline">\(\min h(S)\)</span>. Moreover, by symmetry, <span class="math inline">\(\Pr[\min h(S&#39;) = h(x_{n + 1})] = \frac{1}{N + 1}\)</span>. Therefore, it follows that <span class="math inline">\(E[\min h(S)] = \frac{1}{N + 1}\)</span>.</p>
<p>It is natural to ask how accurate our estimation is. On the one hand, by Markov inequality, <span class="math display">\[
 \Pr_{h \in \mathcal{H} } [\min h(S) \ge 2 \frac{1}{N + 1}] \le \frac{E[\min h(S)]}{2 \frac{1}{N + 1} } = \frac{1}{2}
\]</span></p>
<p>By repeating the algorithm <span class="math inline">\(k\)</span> times, the failure probability drops to <span class="math inline">\(\frac{1}{2^k}\)</span>. However, Markov inequality can not give a lower bound of <span class="math inline">\(\min h(S)\)</span>. What comes to our rescue is Chebyshev's inequality. First, note that <span class="math display">\[
\begin{aligned}
\underset{h \in \mathcal{H} }{E} [ (\min h(S))^2 ]
&amp;= \int_{z = 0}^1 N z^2(1 - z)^{N - 1} dz \\
&amp;= \int_{z = 0}^1 N [(z - 1)^2  + 2(z - 1) + 1] (1 - z)^{N - 1}dz \\
&amp;= -\frac{N}{N + 2}(1 - z)^{N + 2} \mid_{0}^1 + \frac{2N}{N + 1} (1 - z)^{N + 1} \mid_{0}^1 - \frac{N}{N} (1 - z)^{N} \mid_{0}^1  \\  
&amp;= \frac{N}{N + 2} - \frac{2N}{N + 1} + \frac{N}{N} \\
&amp;= \frac{-(N - 1)(N + 2) + N^2 + N}{(N +2)(N + 1)} \\
&amp;= \frac{2}{(N +2)(N + 1)}
\end{aligned}
\]</span></p>
<p>It follows that <span class="math inline">\(\underset{h \in \mathcal{H} }{Var} [ (\min h(S))^2 ] = \frac{2}{(N + 1)(N +2)} - \frac{1}{(N + 1)^2} \le \frac{1}{(N + 1)^2}\)</span>. However, this alone is not enough to give an accurate estimate.</p>
<p><em>Question to ponder: try to apply Chebyshev's inequality directly on this single estimate. Can we get a meaningful lower bound for <span class="math inline">\(\min h(S)\)</span>?</em></p>
<p>Instead, we repeat the algorithm <span class="math inline">\(k\)</span> times, and take the average over the <span class="math inline">\(\min h(S)\)</span>'s. Let <span class="math inline">\(\overline X\)</span> denote this average value. Then <span class="math inline">\(\underset{h \in \mathcal{H} }{Var} [ \overline X ] \le \frac{1}{k (N + 1)^2}\)</span>. Given a parameter <span class="math inline">\(0 &lt; l &lt; 1\)</span> (to be determined later),<br />
<span class="math display">\[
\Pr[ |\overline{X} - \frac{1}{N + 1} | \ge \frac{l}{N + 1}] \le  (\frac{1}{k(N + 1)^2}) / (\frac{l^2}{(N + 1)^2}) = \frac{1}{k l^2}
\]</span></p>
<p>That is, <span class="math inline">\(\overline{X} \in \frac{1}{N + 1}[1 - l, 1 + l]\)</span> with probability <span class="math inline">\(\frac{1}{k l^2}\)</span>. That is <span class="math inline">\(n \in \frac{1}{\overline{X} } [1 - l , 1 + l]\)</span> with probability <span class="math inline">\(\frac{1}{k l^2}\)</span>.</p>
<p><strong>Remark 1</strong>: there exists other analysis for the concentration behavior when we repeat the experiment for <span class="math inline">\(k\)</span> times. Note that for <span class="math inline">\(N \ge 1\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\Pr_{h \in \mathcal{ H } }[\min h(S) \ge (1 + \epsilon) \frac{1}{N + 1}]  
&amp;= (1 - \frac{1 + \epsilon}{N + 1})^N \le \exp \left( -\frac{1 + \epsilon}{N + 1}N \right) \le \exp \left( -\frac{1 + \epsilon}{2} \right) = \frac{1}{\sqrt e} e^{-\epsilon / 2}\\
\Pr_{h \in \mathcal{ H } }[\min h(S) \le (1 - \epsilon) \frac{1}{N + 1}]  
&amp;= 1 - (1 -  \frac{1 - \epsilon}{N + 1})^N \le 1 - \exp \left( - \frac{ 1 - \epsilon }{2(N + 1)}N \right) \le 1 - \exp \left( - \frac{ 1 - \epsilon }{2} \right) = 1 - \frac{1}{\sqrt e} e^{ \epsilon / 2}
\end{aligned}
\]</span></p>
<p>We claim that if we take the <span class="math inline">\(\frac{k}{\sqrt e}​\)</span>-th smallest element, our estimate <span class="math inline">\(\overline X​\)</span> satisfies</p>
<p><span class="math display">\[
\left| \overline X - \frac{1}{N + 1} \right| \le \epsilon \frac{1}{N + 1}
\]</span></p>
<p>with high probability.</p>
<p>Denote <span class="math inline">\(z_1, z_2, ..., z_k\)</span> a set of i.i.d Bernoulli variables with mean <span class="math inline">\(\frac{1}{\sqrt e} e^{-\epsilon / 2}\)</span>. Then by Chernoff bound,<br />
<span class="math display">\[
\begin{aligned}
\Pr[ \sum z_i \ge \frac{k}{\sqrt e} ]
&amp;= \Pr[ \sum z_i \ge (1 + (e^{\epsilon / 2} - 1) ) \frac{k}{\sqrt e} e^ {-\epsilon / 2}] \\
&amp;\le \exp \left( -(e^{\epsilon / 2} - 1)^2 \frac{k}{\sqrt e} e^ {-\epsilon / 2} / 3 \right) \\
&amp;= \exp \left( -(e ^\epsilon - 2 e^{\epsilon / 2} + 1) e^ {-\epsilon / 2} \frac{k}{\sqrt e}  / 3 \right) \\
&amp;= \exp \left( -( e^{\epsilon / 2} - 2 + e^ {-\epsilon / 2}) \frac{k}{\sqrt e}  / 3 \right) \\
&amp;\le \exp \left( - \frac{ \epsilon^2 }{4} \frac{k}{\sqrt e}  / 3 \right) \\
&amp; = \delta
\end{aligned}
\]</span></p>
<p>It suffices to set <span class="math inline">\(k = \frac{12 \sqrt e}{\epsilon^2} \ln \frac{1}{\delta}\)</span>.</p>
<p>By symmetry, we can prove the inequality of the other hand.</p>
<p><strong>Remark 2:</strong> Suppose that we have a function <span class="math inline">\(h\)</span> that maps each of the <span class="math inline">\(N\)</span> elements to a non-negative real number according to exponential distribution <span class="math inline">\(\exp(-1)\)</span> independently (the same element is mapped to the same number), then the minimum of these real numbers serves also as an unbiased estimator of <span class="math inline">\(\frac{1}{N}\)</span>. To verify this, <span class="math display">\[
\Pr[ \min h(S) \le z] = 1 - \exp(-zN) 
\]</span></p>
<p>Therefore, <span class="math display">\[
p[\min h(S) = z] = \frac{\partial \Pr[ \min h(S) \le z]}{\partial z} = N \exp(-Nz)
\]</span> and <span class="math display">\[
E[\min h(S)] = \int_{0}^\infty Nz \exp(-Nz) dz = -\int_{0}^\infty z \ d\exp(-Nz) = -z \exp(-Nz) \mid_{0}^\infty + \int_0^\infty \exp(-Nz) dz = \frac{1}{N}
\]</span> Further, <span class="math display">\[
\Pr[\min h(S) \ge (1 + \epsilon)  \frac{1}{N}] = \exp( - 1 - \epsilon) = \frac{1}{e}e^{-\epsilon} \\
\Pr[\min h(S) \le (1 - \epsilon)  \frac{1}{N}] = 1 - \exp( - 1 + \epsilon) = 1 - \frac{1}{e}e^{\epsilon}
\]</span> Similarly, we can show that if we repeat the experiment <span class="math inline">\(k = O(\frac{\ln 1 / \delta }{\epsilon^2 } )\)</span> times, with probability at least <span class="math inline">\(1 - \delta\)</span>, the <span class="math inline">\(\frac{1}{e}\)</span> smallest elements is an <span class="math inline">\(1 \pm \epsilon\)</span> estimator of <span class="math inline">\(\frac{1}{N}\)</span>.</p>
<h2 id="solution-2">Solution 2</h2>
<p>Suppose we only want to know whether <span class="math inline">\(N \ge t​\)</span> or <span class="math inline">\(N \le t / 2​\)</span> for a given integer <span class="math inline">\(t​\)</span>, is this possible?</p>
<p>The answer is yes and the solution is amazingly easy. We hash each element in <span class="math inline">\(D\)</span> uniformly at random into the range <span class="math inline">\([1, 2, 3, ..., t]\)</span>. If <span class="math inline">\(N \ge t\)</span>, the probability that no elements is assigned the value <span class="math inline">\(t\)</span> is given by <span class="math display">\[
(1 - 1 / t)^N \le (1 - 1 / t)^t \le 1/e
\]</span> The second inequality holds since <span class="math inline">\((1 - 1 / t) \le e^{1 / t}\)</span>.</p>
<p>On the other hand, when <span class="math inline">\(N \le t / 2\)</span>, the probability that no element is assigned the value <span class="math inline">\(t\)</span> is greater than <span class="math display">\[
(1 - 1 / t)^N \ge (1 - 1 / t)^{t / 2} \ge 1 / e
\]</span> The second inequality holds since <span class="math inline">\((1 - 1 / t) \ge e^{- 2 / t} = 1 - \frac{2}{t} + \frac{1}{2!}(\frac{2}{t})^2 - \frac{1}{3!}(\frac{2}{t})^3 + ...\)</span> when <span class="math inline">\(1 / t \le 1/ 2\)</span>.</p>
<p>We can boost the probability to some defined threshold <span class="math inline">\(1 - \delta\)</span> by repeating <span class="math inline">\(k = \log 1 / \delta\)</span> times and check whether the majority of the answers are "yes". Note that the expected number of "yes" is more than <span class="math inline">\((1 - 1 / e)k\)</span> and by chernoff bound the probability that less than <span class="math inline">\(0.5 k\)</span> "yes" are returned is given by <span class="math inline">\(\delta\)</span>.</p>
<p>Now, to estimate <span class="math inline">\(N\)</span>, we can set up <span class="math inline">\(\log n\)</span> values of <span class="math inline">\(t: 1, 2, 4, 8, ..., n\)</span> and test for each value of <span class="math inline">\(t\)</span> whether <span class="math inline">\(N \ge t\)</span> or <span class="math inline">\(N \le t / 2\)</span>. We can find an interval <span class="math inline">\([t_1, t_2]\)</span>, such that <span class="math inline">\(N \in [t_1, t_2]\)</span> and <span class="math inline">\(N / 2 \le t_1 \le t_2 \le 2\cdot N\)</span>. The space complexity is <span class="math inline">\(O(\log n \log 1 / \delta)\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/05/17/Jaccard-Similarity/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/17/Jaccard-Similarity/" class="post-title-link" itemprop="url">Jaccard Similarity</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-17 21:41:06" itemprop="dateCreated datePublished" datetime="2019-05-17T21:41:06+10:00">2019-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-05-19 00:54:26" itemprop="dateModified" datetime="2019-05-19T00:54:26+10:00">2019-05-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The Jaccard similarity measures the fraction of shared elements between set. In particular, given set <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the Jaccard similarity between them is defined as<br />
<span class="math display">\[
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
\]</span> The naive calculation of <span class="math inline">\(J(A, B)\)</span> has time complexity <span class="math inline">\(O(|A \cup B|)\)</span> by scanning all elements in the two sets once. Here we discuss how randomization might save time.</p>
<h2 id="lemma-one">Lemma One</h2>
<p>If we select an item <span class="math inline">\(x\)</span> uniformly at random from <span class="math inline">\(|A \cup B|\)</span>, then <span class="math inline">\(\Pr[x \in A \cap B] = \frac{|A \cap B|}{|A \cup B|}\)</span>.</p>
<p>Define the random variable<br />
<span class="math display">\[
X = \begin{cases}
1, \ if \ x \in \ A \cap B \\
0, \ otherwise
\end{cases}
\]</span></p>
<p>Then we see that <span class="math inline">\(X\)</span> is an unbiased estimator of <span class="math inline">\(J(A, B)\)</span>, i.e., <span class="math inline">\(E[X] = J(A, B)\)</span>.</p>
<p>Suppose that we have a sequence of i.i.d. copies of <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(X_1, X_2, ..., X_k\)</span>, then the average <span class="math inline">\(\mu = \frac{1}{k} \sum_{i = 1}^k X_i\)</span> is also an unbiased estimator of <span class="math inline">\(J(A, B)\)</span>. Moreover, by law of large number, <span class="math inline">\(\mu \rightarrow J(A, B)\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span>. The problem is, how many <span class="math inline">\(X_i\)</span>'s are enough.</p>
<p>The answer depends on a few factors:</p>
<ol type="1">
<li>Whether the value of the required error is relative to <span class="math inline">\(J(A, B)\)</span> or independent to <span class="math inline">\(J(A, B)\)</span>.</li>
<li>What is the tolerance of failure probability.</li>
</ol>
<p>In the following discussion we consider only the case of finding an estimator <span class="math inline">\(\mu\)</span> such that <span class="math inline">\(J(A, B) \in [\mu \pm \epsilon]\)</span> with probability at least <span class="math inline">\(1 - \delta\)</span>, where both <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span> are given parameters. Then the convergence behavior with respect to the number of samples <span class="math inline">\(k\)</span> can be captured by Hoeffding inequality:<br />
<span class="math display">\[
\Pr[ |\mu - J(A, B)| \ge \epsilon] \le 2\exp \left( -\frac{2\epsilon^2}{k} \right) = \delta
\]</span></p>
<p>Solving the equation gives<br />
<span class="math display">\[
k = \frac{2}{\epsilon^2}\log \frac{2}{\delta}
\]</span></p>
<p>Or equivalent, when we interpret <span class="math inline">\(\epsilon\)</span> as the width of confidence interval, <span class="math display">\[
\epsilon = \sqrt \frac{\log \frac{2}{\delta} }{k}
\]</span> which has order <span class="math inline">\(O(\sqrt \frac{1}{k})\)</span>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/05/Multiplicative-Weight-Updates/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/05/Multiplicative-Weight-Updates/" class="post-title-link" itemprop="url">Multiplicative Weight Updates</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-05 18:05:10" itemprop="dateCreated datePublished" datetime="2019-04-05T18:05:10+11:00">2019-04-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-14 17:48:44" itemprop="dateModified" datetime="2020-12-14T17:48:44+11:00">2020-12-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Consider a problem that might arises from many application scenarios. We have a sequence of <span class="math inline">\(T\)</span> binary variables indexed by time <span class="math inline">\(X_1, X_2, ..., X_T\)</span>, where <span class="math inline">\(X_t \in \{0, 1\}, \forall t \in [T]\)</span>. We try to predict the value of <span class="math inline">\(X_t\)</span>, such that we win <span class="math inline">\(1\)</span> dollar if our guess is correct and <span class="math inline">\(0\)</span> dollar otherwise.</p>
<p>We don't make decision on our own. Instead, we resort a group of <span class="math inline">\(n\)</span> experts. At each time <span class="math inline">\(t\)</span>, each expert <span class="math inline">\(i\)</span> gives a prediction <span class="math inline">\(p_i^t\)</span> of <span class="math inline">\(X_t\)</span>. Based on the experts' prediction, we make prediction our prediction <span class="math inline">\(p_A^t\)</span>. The value of <span class="math inline">\(X_t\)</span> is then revealed and we make a mistake if our prediction is wrong. The goal to minimize the number of mistakes.</p>
<h3 id="how-good-can-we-do">How Good Can We Do</h3>
<p>The first question is how good can we do? Define</p>
<ol type="1">
<li><span class="math inline">\(L_i^t:\)</span> the indicator variable of whether the expert <span class="math inline">\(i\)</span> makes a mistake at round <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(L_i = \sum_{t \in [T] } L_i^t:\)</span> the total number of mistakes expert <span class="math inline">\(i\)</span> makes, when <span class="math inline">\(T\)</span> is fixed.</li>
<li><span class="math inline">\(L_A:\)</span> the one made by our algorithm.</li>
</ol>
<p>Ideally, we would like achieve as few mistakes as<br />
<span class="math display">\[
    \sum_{t \in [T] } \min_{i \in [n]} L_i^t. 
\]</span></p>
<p>However, this is impossible. To see this, suppose <span class="math inline">\(X_t\)</span> is a sequence of independent Bernoulli random variable with probability <span class="math inline">\(0.5\)</span> equal to <span class="math inline">\(1\)</span>. No matter what strategy we use, the expected number of mistakes is always <span class="math inline">\(T / 2\)</span>. On the other hand, <span class="math inline">\(\sum_{t \in [T] } \min_{i \in [n]} L_i^t\)</span> could be <span class="math inline">\(0\)</span>.</p>
<p>Despite the negative result, there is hope to achieve as few mistakes as the best expert (up to some constant), that is<br />
<span class="math display">\[
O \left( \min_{i \in [n]} \sum_{t \in [T] } L_i^t \right) = O \left( \min_{i \in [n] } L_i \right). 
\]</span></p>
<h3 id="follow-the-majority">Follow the Majority</h3>
<p>We start with an easy case where there exists an expert who always gives the correct guess. Under this assumption, we can achieve <span class="math display">\[
\min_{i \in [n] } L_i + \log n
\]</span></p>
<p>mistakes. We simply keep track of a set of experts who do not make any mistake so far. Each time we make a decision, we follow the majority of the set.</p>
<blockquote>
<p>Algorithm 1. <strong>Follow the Majority</strong></p>
<ol type="1">
<li>Let <span class="math inline">\(E_0 \leftarrow \{ 1, 2, ..., n \}\)</span> be the set of all experts.<br />
</li>
<li>For <span class="math inline">\(t \leftarrow 1\)</span> to <span class="math inline">\(T\)</span><br />
</li>
<li><span class="math inline">\(\qquad\)</span> Let <span class="math inline">\(S\)</span> be the subset of <span class="math inline">\(E_{t - 1}\)</span> that predicts 0 at time <span class="math inline">\(t\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> Let <span class="math inline">\(\bar S\)</span> be the subset of <span class="math inline">\(E_{t - 1}\)</span> that predicts 1 at time <span class="math inline">\(t\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> If <span class="math inline">\(|S| &gt; |\bar S|\)</span>, then predict 0 (<span class="math inline">\(p_A^t \leftarrow 0\)</span>);<br />
</li>
<li><span class="math inline">\(\qquad\qquad\qquad\quad\)</span> else predict 1 (<span class="math inline">\(p_A^t \leftarrow 1\)</span>).<br />
</li>
<li><span class="math inline">\(\qquad\)</span> Reveal <span class="math inline">\(X_t\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> If <span class="math inline">\(X_t = 0\)</span>, then <span class="math inline">\(E_t \leftarrow S\)</span>;<br />
</li>
<li><span class="math inline">\(\qquad\qquad\qquad\)</span> else <span class="math inline">\(E_t \leftarrow \bar S\)</span>.</li>
</ol>
</blockquote>
<p><em>Theorem. If there is a perfect expert, the algorithm </em>Follow the Majority* makes at most <span class="math inline">\(\log n\)</span> mistakes.*<br />
<em>Proof.</em> Let <span class="math inline">\(M\)</span> be the number of mistakes the algorithm makes. Each time it makes a mistake, the size of <span class="math inline">\(E_{t}\)</span> shrink by half. Hence, <span class="math display">\[
1 \le |E_T| \le |E_1| \frac{1}{2^M}  = \frac{n}{2^M}. 
\]</span></p>
<p>The first inequality follows from the existence of perfect expert. It concludes that <span class="math display">\[
M \le \log n.  
\]</span> <span class="math inline">\(\square\)</span></p>
<h3 id="follow-the-majority-with-reset">Follow the Majority With Reset</h3>
<p>We continue with the case without a perfect expert. By modifying the <em>Follow the Majority</em> a little, we can achieve <span class="math display">\[
\left( \min_{i \in [n] } L_i  + 1 \right) \cdot \log n
\]</span> mistakes. As before, we maintain a set of experts who do not make any mistake so far and we follow the majority of the set to make decisions.</p>
<blockquote>
<p>Algorithm 2. <strong>Follow the Majority With Reset</strong></p>
<ol type="1">
<li>Let <span class="math inline">\(E_0 \leftarrow \{ 1, 2, ..., n \}\)</span> be the set of all experts.<br />
</li>
<li>For <span class="math inline">\(t \leftarrow 1\)</span> to <span class="math inline">\(T\)</span><br />
</li>
<li><span class="math inline">\(\qquad\)</span> Let <span class="math inline">\(S\)</span> be the subset of <span class="math inline">\(E_{t - 1}\)</span> that predicts 0 at time <span class="math inline">\(t\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> Let <span class="math inline">\(\bar S\)</span> be the subset of <span class="math inline">\(E_{t - 1}\)</span> that predicts 1 at time <span class="math inline">\(t\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> If <span class="math inline">\(|S| &gt; |\bar S|\)</span>, then predict 0 (<span class="math inline">\(p_A^t \leftarrow 0\)</span>);<br />
</li>
<li><span class="math inline">\(\qquad\qquad\qquad\quad\)</span> else predict 1 (<span class="math inline">\(p_A^t \leftarrow 1\)</span>).<br />
</li>
<li><span class="math inline">\(\qquad\)</span> Reveal <span class="math inline">\(X_t\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> If <span class="math inline">\(X_t = 0\)</span>, then <span class="math inline">\(E_t \leftarrow S\)</span>;<br />
</li>
<li><span class="math inline">\(\qquad\qquad\qquad\)</span> else <span class="math inline">\(E_t \leftarrow \bar S\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> If <span class="math inline">\(E_t = \emptyset\)</span>, then reset <span class="math inline">\(E_t \leftarrow \{1, 2, ..., n\}\)</span>.</li>
</ol>
</blockquote>
<p><em>Theorem. If there isn't a perfect expert, the algorithm </em>Follow the Majority* makes at most <span class="math inline">\(\left( \min_{i \in [n] } L_i + 1 \right) \cdot \log n\)</span> mistakes.*<br />
<em>Proof.</em> Let <span class="math inline">\(M\)</span> be the number of mistakes the algorithm makes. As analysed before, between two consecutive resets of <span class="math inline">\(E_t\)</span>, the algorithm makes at most <span class="math inline">\(\log n\)</span> mistakes while the best expert makes at least one mistake. Therefore, <span class="math display">\[
\frac{M}{ \log n } \le \min_{i \in [n] } L_i + 1\implies M \le \left( \min_{i \in [n]}  L_i  + 1\right) \cdot \log n
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="follow-the-weighted-majority">Follow the Weighted Majority</h3>
<p>One drawback of the previous approach is that, the algorithm forgets the relative performance of the experts each time it resets <span class="math inline">\(E_t\)</span>. To fix this, we assign each expert <span class="math inline">\(i\)</span> a weight <span class="math inline">\(w_i^t\)</span> and we follow the weighted majority at time <span class="math inline">\(t\)</span>. This reduces the number of mistakes to <span class="math display">\[
    2.41 \cdot ( \min_{i \in [n]} L_i + \log n).
\]</span></p>
<blockquote>
<p>Algorithm 3. <strong>Follow the Weighted Majority</strong></p>
<ol type="1">
<li>Set <span class="math inline">\(w_i \leftarrow 1, \forall i \in [n].\)</span><br />
</li>
<li>For <span class="math inline">\(t \leftarrow 1\)</span> to <span class="math inline">\(T\)</span><br />
</li>
<li><span class="math inline">\(\qquad\)</span> Let <span class="math inline">\(S\)</span> be the set of experts who predict 0.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> Let <span class="math inline">\(\bar S\)</span> be the set of experts who predict 1.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> If <span class="math inline">\(\sum_{i \in S} w_i &gt; \sum_{i \in \bar S} w_i\)</span>, then predict 0 (<span class="math inline">\(p_A^t \leftarrow 0\)</span>);<br />
</li>
<li><span class="math inline">\(\qquad\qquad\qquad\qquad\qquad\qquad\)</span> else predict 1 (<span class="math inline">\(p_A^t \leftarrow 1\)</span>).<br />
</li>
<li><span class="math inline">\(\qquad\)</span> Reveal <span class="math inline">\(X_t\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> For each expert <span class="math inline">\(i\)</span> who predicts wrong</li>
<li><span class="math inline">\(\qquad\qquad\)</span> <span class="math inline">\(w_i \leftarrow w_i / 2\)</span></li>
</ol>
</blockquote>
<p><strong><em>Analysis.</em></strong> Let <span class="math inline">\(w_i^t\)</span> be the weight of player <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span> and define <span class="math inline">\(W^t = \sum_{i = 1}^n w_i^t\)</span>. Each time we make a mistake, the experts who make wrong the prediction have sum of weight <span class="math inline">\(\ge \frac{1}{2} W^t\)</span>. Since their weights halve, we have <span class="math display">\[
W^{t + 1} \le W^t ( 1 - \frac{1}{2}\frac{1}{2}) = \frac{3}{4} W^t.
\]</span></p>
<p>Let <span class="math inline">\(M\)</span> be the number of mistakes the algorithm makes. It follows that <span class="math inline">\(\forall i \in [n]\)</span>, <span class="math display">\[
 \left( \frac{1}{2} \right)^{L_i} = w_i^T \le W^T \le \left( \frac{3}{4} \right)^M W_0 = \left( \frac{3}{4} \right)^M n,  
\]</span></p>
<p>which implies <span class="math display">\[
M \le  \frac{1}{ \log \frac{4}{3} } ( L_i + \log n) \le 2.41 \cdot (L_i + \log n).
\]</span></p>
<p><em>Remark:</em> it is not necessary to halve an expert's weight when it makes a mistake. We can decrease it by any factor <span class="math inline">\((1 - \epsilon)\)</span> for <span class="math inline">\(\epsilon \in (0, 1)\)</span>. In such case, we get <span class="math display">\[
\begin{aligned}
    &amp;\qquad (1- \epsilon)^{L_i} \le \left( 1 - \frac{1 - (1 - \epsilon)}{2} \right)^M n \\
    &amp;\implies L_i \cdot \ln (1 -\epsilon) \le M \ln \left( 1 - \frac{\epsilon}{ 2 } \right) + \ln n \\
    &amp;\implies M \le \frac{ L_i \ln \frac{1}{1 - \epsilon} + \ln n}{ \ln \frac{ 2 }{ 2 - \epsilon } }.
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(x = \frac{1}{1 - \epsilon} &gt; 0\)</span>, then <span class="math inline">\(1 - \epsilon = \frac{1}{x}\)</span> and <span class="math inline">\(\frac{2 - \epsilon }{ 2 } = \frac{x + 1}{2x}\)</span>. Define <span class="math display">\[
y = \frac{ L_i \ln x + \ln n}{ \ln 2x - \ln ( x + 1  ) }.
\]</span></p>
<p>Then <span class="math display">\[
\begin{aligned}
    y&#39; \ge 0 
        &amp;\implies  \frac{L_i}{x} (\ln 2x - \ln ( x + 1  ) ) - \left( \frac{1}{x} - \frac{1}{x + 1} \right) (L_i \ln x + \ln n ) \ge 0 \\
        &amp;\implies  L_i(x + 1) (\ln 2x - \ln ( x + 1  ) ) - (L_i \ln x + \ln n ) \ge 0 \\
        &amp;\implies  (x + 1) (\ln 2x - \ln ( x + 1  ) ) - \ln x \ge \frac{\ln n}{L_i}
\end{aligned}
\]</span></p>
<p>It is not easy to compute a closed-form solution for <span class="math inline">\(x\)</span>. But observe that the function <span class="math inline">\((x + 1) (\ln 2x - \ln ( x + 1 ) ) - \ln x\)</span> equals to <span class="math inline">\(0\)</span> for <span class="math inline">\(x = 1\)</span> and is increasing for <span class="math inline">\(x &gt; 1\)</span>. Hence, there exists some <span class="math inline">\(x &gt; 1\)</span>, s.t., the value of the functions equal to <span class="math inline">\(\frac{ \ln n }{ L_i }\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/MultiplicativeWeightUpdate/MultiplicativeWeightUpdate.png?raw=true" /></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="multiplicative-weight-updates">Multiplicative Weight Updates</h3>
<p>We are in good shape so far. Let's extend the problem to more general settings. In this cases, the experts interact with the environment in round-robin fashion. At each round <span class="math inline">\(t\)</span>,</p>
<ol type="1">
<li>The expert <span class="math inline">\(i\)</span> performs action, termed <span class="math inline">\(a_i^t\)</span>.</li>
<li>The environment returns the loss of performing <span class="math inline">\(a_i^t\)</span>, denoted as <span class="math inline">\(l_i^t \in [0, 1]\)</span>.</li>
</ol>
<p>Although there is no restriction of the experts' actions and their losses, we can come up with some strategy the expected loss of which is almost as good as the best expert: <span class="math display">\[
    \min_{i \in [n] } L_i + 2 \sqrt{ T \ln n }, 
\]</span></p>
<p>where <span class="math inline">\(L_i \doteq \sum_{t \in [T] } l_i^t\)</span> is the loss of expert <span class="math inline">\(i\)</span>. This implies that the average expected loss converges at a rate of <span class="math inline">\(\sqrt \frac{1}{T}\)</span> to the best one.</p>
<blockquote>
<p>Algorithm 4. <strong>Multiplicative Weight Update</strong></p>
<ol type="1">
<li>Set <span class="math inline">\(w_i \leftarrow 1, \forall i \in [n].\)</span><br />
</li>
<li>For <span class="math inline">\(t \leftarrow 1\)</span> to <span class="math inline">\(T\)</span><br />
</li>
<li><span class="math inline">\(\qquad\)</span> Let <span class="math inline">\(W = \sum_{i \in [n] } w_i\)</span>.<br />
</li>
<li><span class="math inline">\(\qquad\)</span> Follow action <span class="math inline">\(a_i^t\)</span> with probability <span class="math inline">\(w_i / W\)</span>.</li>
<li><span class="math inline">\(\qquad\)</span> For each expert <span class="math inline">\(i\)</span>:</li>
<li><span class="math inline">\(\qquad\qquad\)</span> <span class="math inline">\(w_i \leftarrow w_i \cdot (1 - \epsilon l_i^t)\)</span></li>
</ol>
</blockquote>
<p><strong><em>Analysis.</em></strong> Let <span class="math inline">\(l_A^t\)</span> be the loss of the algorithm at round <span class="math inline">\(t\)</span> and <span class="math inline">\(L_A \doteq \sum_{t \in [T] } l_A^t\)</span>. Let <span class="math inline">\(w_i^t\)</span> be the weight of expert <span class="math inline">\(i\)</span> and <span class="math inline">\(W^t\)</span> be the sum of weights at round <span class="math inline">\(t\)</span>. Then <span class="math display">\[
\mathbb{E} [ l_A^t ] = \sum_{i \in [n] } \frac{ \epsilon w_i^t l_i^t }{ W^t }. 
\]</span></p>
<p>By the update rule of the <span class="math inline">\(w_i\)</span>'s, we see <span class="math display">\[
W^{t + 1} = \sum_{i \in [n] } w_i^t (1 - \epsilon l_i^t) = W^t ( 1 - \epsilon \mathbb{E} [ l_A^t ] ) \le W^t \exp( - \epsilon \mathbb{E} [ l_A^t ] ). 
\]</span></p>
<p>By induction, we can write <span class="math display">\[
W^T \le W^1 \exp( - \sum_{t \in [T] } \epsilon \mathbb{E} [ l_A^t ] ) = n \cdot \exp( - \epsilon \mathbb{E} [ L_A ] ).
\]</span></p>
<p>To upper bound <span class="math inline">\(\mathbb{E} [ L_A ]\)</span>, we try to lower bound the value of <span class="math inline">\(n \cdot \exp( - \epsilon \mathbb{E} [ L_A ] )\)</span>. We uses that <span class="math inline">\(\forall i \in [n]\)</span>, <span class="math display">\[
    \prod_{t \in [T] } (1 - \epsilon l_i^t) = w_i^T \le W^T. 
\]</span></p>
<p>Using the fact that <span class="math inline">\(\ln(1 - \epsilon ) \ge \epsilon - \epsilon^2\)</span> for <span class="math inline">\(0 &lt; \epsilon &lt; 0.5\)</span>, we know that <span class="math inline">\(\forall i \in [n]\)</span>,</p>
<p><span class="math display">\[
\exp \left( - \sum_{i \in [T] } \epsilon l_i^t  - \sum_{i \in [T] } (\epsilon l_i^t)^2 \right) \le n \cdot \exp ( - \epsilon \mathbb{E} [ l_A^t ] ). 
\]</span></p>
<p>Taking the log, we get <span class="math display">\[
\mathbb{E} [ l_A^t ] \le \frac{1}{\epsilon } \ln n  + L_i + \epsilon \sum_{i \in [T] } (l_i^t)^2. 
\]</span></p>
<p>There are various ways to upper bound <span class="math inline">\(\sum_{i \in [T] } (l_i^t)^2\)</span>. Naively, this is at most <span class="math inline">\(T\)</span>. Hence, <span class="math display">\[
\mathbb{E} [ l_A^t ] \le \frac{1}{\epsilon } \ln n  + L_i + \epsilon T.  
\]</span></p>
<p>Setting <span class="math inline">\(\epsilon = \sqrt{ \frac{ \ln n}{T} }\)</span>, we get <span class="math display">\[
\mathbb{E} [ l_A^t ] \le L_i + 2 \sqrt{ T \ln n}.
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="last-updated-date-dec-14th-2020.">Last Updated Date: <strong>Dec 14th, 2020</strong>.</h4>
<h3 id="reference">Reference</h3>
<p>[1] Aaron Roth, “The Polynomial Weights Algorithm”, NETS 412: Algorithmic Game Theory,</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/23/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><span class="space">&hellip;</span><a class="page-number" href="/page/42/">42</a><a class="extend next" rel="next" href="/page/25/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">126</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
