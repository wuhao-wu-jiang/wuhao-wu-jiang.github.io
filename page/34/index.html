<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/34/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/34/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/18/Determinant,%20Cross%20Product%20and%20Cramer's-Rule/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/18/Determinant,%20Cross%20Product%20and%20Cramer's-Rule/" class="post-title-link" itemprop="url">Determinant, Cross Product and Cramer's-Rule</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-18 23:36:52" itemprop="dateCreated datePublished" datetime="2020-02-18T23:36:52+11:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-21 19:01:14" itemprop="dateModified" datetime="2020-04-21T19:01:14+10:00">2020-04-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="determinant"><strong><em>Determinant</em></strong></h3>
<p>When studying linear algebra, the determinant function <span class="math inline">\(\det(\cdot)\)</span> is a horrifying subject. We try to give a friendly approach to it in this article.</p>
<p>I think it is the study of the volume of the parallelotope specified by <span class="math inline">\(v_1, v_2, ... ,v_n \in \mathbb{R}^n\)</span> that gives it away. The parallelotope is defined as the set <span class="math display">\[
\left\{ \sum_{i \in [n]} \lambda_i v_i :  \lambda_i \in [0, 1], \forall i \in [n] \right\}
\]</span></p>
<p>Let's denote <span class="math inline">\(f(v_1, v_2, ... ,v_n) : \mathbb{R}^{n \times n} \rightarrow \mathbb{R}\)</span> the singed volume function, i.e., <span class="math inline">\(|f(v_1, v_2, ... ,v_n)|\)</span> gives the volume of the parallelotope. We will show how to determine the sign of <span class="math inline">\(f\)</span> later. At this point, we do not know how <span class="math inline">\(\det(\cdot)\)</span> is related to the volume. Therefore, we use the notation <span class="math inline">\(f\)</span> instead.</p>
<p>We show how to derive the formula of <span class="math inline">\(f\)</span> by its properties. The two key properties associated with volume are</p>
<ol type="1">
<li><p><span class="math inline">\(f\)</span> is multi-linear. Specially, it is a linear function in any dimension when other dimensions are fixed:</p>
<ul>
<li><p><span class="math inline">\(f(v_1, v_2, ..., cv_i, ... ,v_n)=cf(v_1, v_2, ... ,v_n)\)</span> for <span class="math inline">\(\forall i \in [n]\)</span> and <span class="math inline">\(c \in \mathbb{R}\)</span>. Geometrically, if we scale the <span class="math inline">\(i\)</span>-th vector <span class="math inline">\(v_i\)</span> by a factor <span class="math inline">\(c\)</span>, then the volume of the parallelotope should change by the same factor. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity1.jpg" /> In the above example, when <span class="math inline">\(v_2\)</span> is fixed, the area of the parallelogram is determined by its height, i.e., the projection of <span class="math inline">\(v_1\)</span> to the orthogonal direction of <span class="math inline">\(v_2\)</span>. When <span class="math inline">\(v_1\)</span> doubles, the length of its projection doubles.</p></li>
<li><p><span class="math inline">\(f(v_1, v_2, ..., v_i + u_i, ... ,v_n)= f(v_1, v_2, ..., v_i, ... ,v_n) + f(v_1, v_2, ..., u_i, ... ,v_n)\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity2.jpg" /> In the above example, when <span class="math inline">\(v_2\)</span> is fixed, the projection to the orthogonal direction of <span class="math inline">\(v_2\)</span> are additive. In particular, denote <span class="math inline">\(\mathcal{P}(v_1)\)</span> the length of the projection of <span class="math inline">\(v_1\)</span> to the orthogonal direction of <span class="math inline">\(v_2\)</span>. Similarly we define <span class="math inline">\(\mathcal{P}(v_2)\)</span> and <span class="math inline">\(\mathcal{P}(v_1 + v_2)\)</span>. Then<br />
<span class="math display">\[
\mathcal{P}(v_1 + v_2) = \mathcal{P}(v_1) + \mathcal{P}(v_2)
\]</span></p></li>
<li><p>Note that the projection of two vector can cancel each other in some cases, in the sense that they points to different direction. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/Linearity4.jpg" /> In the above example, if we write <span class="math inline">\(v_1 = v_{1, 1} e_1 + v_{1, 2} e_2\)</span>, then <span class="math inline">\(\mathcal{P} (v_{1, 1} e_1)\)</span> and <span class="math inline">\(\mathcal{P} (v_{1,2} e_2)\)</span> points to opposite directions. This implies that we should give volume a sign. For example, we can define the direction pointed by purple vector points to be positive. The volume of the parallelogram increases if a vector's projection moves along this direction and decrease otherwise.</p></li>
</ul></li>
<li><p>If <span class="math inline">\(v_1, v_2, ..., v_n\)</span> are linearly independent, then the parallelotope is contained in a subspace with dimension smaller than <span class="math inline">\(n\)</span> and should have volume zero. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Determinant/NonIndependent.jpg" /> Intuitively, the parallelotope collapses to a lower dimension space. Examples include colinear vectors in two dimension case and the third vector lying in the hyperplane spanned by the other two in the three dimension case.<br />
Finally, we emphases an important case here: <span class="math display">\[
f(..., v_i,..., v_j, ... , ) = 0, \textbf{ if } v_i = v_j, \forall i, j \in [n], i \neq j
\]</span></p></li>
</ol>
<p>If we write <span class="math display">\[
v_i = v_{i, 1} e_1 + v_{i, 2} e_2 + ... + v_{i, n} e_n
\]</span></p>
<p>for all <span class="math inline">\(i \in [n]\)</span>, then by linearity <span class="math display">\[
\begin{aligned}
f(v_1, v_2, ..., v_n) 
    &amp;= f( v_{1, 1} e_1 + v_{1, 2} e_2 + ... + v_{1, n} e_n, \\
    &amp;\qquad v_{2, 1} e_1 + v_{2, 2} e_2 + ... + v_{2, n} e_n, \\
    &amp;\qquad ...\\
    &amp;\qquad v_{n, 1} e_1 + v_{n, 2} e_2 + ... + v_{n, n} e_n) \\
    &amp;= \sum_{k_1, k_2, ..., k_n \in [n]} v_{1, k_1} v_{2, k_2} ... v_{n, k_n} f(e_{k_1}, e_{k_2}, ..., e_{k_n}) 
\end{aligned}
\]</span></p>
<p>Note that if <span class="math inline">\(k_i = k_j\)</span> for <span class="math inline">\(i \neq j\)</span>, then <span class="math inline">\(f(e_{k_1}, e_{k_2}, ..., e_{k_n}) = 0\)</span>. Therefore, <span class="math display">\[
f(v_1, v_2, ..., v_n)  = \sum_{ k  \text{ is a permutation} } v_{1, k_1} v_{2, k_2} ... v_{n, k_n} f(e_{k_1}, e_{k_2}, ..., e_{k_n})
\]</span></p>
<p>We are almost done. It is left to determined the value of <span class="math inline">\(f(e_{k_1}, e_{k_2}, ..., e_{k_n})\)</span>, where <span class="math inline">\(k = (k_1, k_2, ..., k_n)\)</span> is a permutation of <span class="math inline">\((1, 2, ...., n)\)</span>. Here we claim that is purely determined by the value of <span class="math inline">\(f(e_1, e_2, ..., e_n)\)</span>. By conversion, we define <span class="math display">\[
f(e_1, e_2, ..., e_n) = 1
\]</span> Then we have the key lemma:</p>
<p><strong><em>Lemma</em></strong> <span class="math display">\[
f(e_{k_1}, e_{k_2}, ..., e_{k_n}) = -1
\]</span></p>
<p>if <span class="math inline">\({k_1}, {k_2}, ..., {k_n}\)</span> can be recovered to <span class="math inline">\(1, 2, ..., n\)</span> by odd number of neighboring swaps and <span class="math display">\[
f(e_{k_1}, e_{k_2}, ..., e_{k_n}) = 1
\]</span> if <span class="math inline">\({k_1}, {k_2}, ..., {k_n}\)</span> can be recovered to <span class="math inline">\(1, 2, ..., n\)</span> by even number of neighboring swaps.</p>
<p><em>Proof.</em> The key is to prove that we swap two neighboring vectors, then the sign of the volume change: <span class="math display">\[
f(e_{k_1}, e_{k_2}, ..., e_{k_{i } }, e_{k_{i + 1} }, ..., e_{k_n}) = -f(e_{k_1}, e_{k_2}, ..., e_{k_{i + 1 } }, e_{k_{i } }, ..., e_{k_n})
\]</span></p>
<p>This is because <span class="math display">\[
\begin{aligned}
    f(e_{k_1}, e_{k_2}, ..., e_{k_{i } } + e_{k_{i + 1} }, e_{k_{i } } + e_{k_{i + 1} }, ..., e_{k_n})  
        &amp;=
        f(e_{k_1}, e_{k_2}, ..., e_{k_{i } }, e_{k_{i + 1} }, ..., e_{k_n})\\
        &amp;\quad + 
        f(e_{k_1}, e_{k_2}, ..., e_{k_{i + 1 } }, e_{k_{i } }, ..., e_{k_n})
        \\
        &amp;= 0
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="cross-product"><strong><em>Cross Product</em></strong></h3>
<p>If we fix <span class="math inline">\(v_2, v_3, ..., v_n\)</span>, then <span class="math inline">\(\det(x, v_2, ..., v_n): \R^n \rightarrow \R\)</span> is a linear function. Then <span class="math display">\[
\begin{aligned}
    \det(x, v_2, ..., v_n) &amp;= \sum_{i = 1}^n \det(x_i e_i, v_2, ..., v_n) \\
    &amp;= \sum_{i = 1}^n x_i \det(e_i, v_2, ..., v_n)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is the i-th coordinator of <span class="math inline">\(x\)</span>. If we write <span class="math display">\[
d = \left&lt; det(e_1, v_2, ..., v_n), det(e_2, v_2, ..., v_n), ..., det(e_n, v_2, ..., v_n)\right&gt;
\]</span></p>
<p>Then <span class="math display">\[
\det(x, v_2, ..., v_n) = d \cdot x
\]</span></p>
<p>Indeed, in a similar manner, we can prove that for any <span class="math inline">\(h: \R^{n \times n} \rightarrow \R^n\)</span> linear function, there exists a vector <span class="math inline">\(\vec h\)</span>, such that <span class="math display">\[
h(x) = \vec h \cdot x
\]</span></p>
<p>Conventionally, <span class="math inline">\(d\)</span> is called the cross product of <span class="math inline">\(v_2, ..., v_n\)</span> is written as <span class="math display">\[
d = v_2 \times v_3 \times ... \times v_n
\]</span></p>
<p>It has some interesting properties:</p>
<ol type="1">
<li><p><span class="math inline">\(d\)</span> is perpendicular to the subspace spanned by <span class="math inline">\(v_2, ..., v_n\)</span>, as <span class="math inline">\(d \cdot v_i = \det(v_i, v_2, ..., v_n) = 0\)</span> for <span class="math inline">\(2 \le i \le n\)</span>.</p></li>
<li><p>For any <span class="math inline">\(x \in \R^n\)</span>, <span class="math inline">\(d \cdot x\)</span> gives the signed volume of the parallelotope spanned by <span class="math inline">\(x, v_2, ..., v_n\)</span>.</p>
<ul>
<li>When <span class="math inline">\(n = 2\)</span>, the length of <span class="math inline">\(d\)</span> equals to that of <span class="math inline">\(v_2\)</span>.</li>
<li>When <span class="math inline">\(n = 3\)</span>, the length of <span class="math inline">\(d\)</span> equals to the area of the parallelogram spanned by <span class="math inline">\(v_2, v_3\)</span>. Combined with the fact that <span class="math inline">\(d\)</span> is perpendicular to <span class="math inline">\(v_2, v_3\)</span>, we see why <span class="math inline">\(d \cdot x\)</span> gives the volume of the parallelotope spanned by <span class="math inline">\(x, v_2, v_3\)</span>: it takes the projection of <span class="math inline">\(x\)</span> to the direction that is perpendicular to <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span>, scaled by a factor of the area spanned by <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span>.<br />
</li>
</ul></li>
<li><p>The direction given of <span class="math inline">\(d\)</span> is given by right hand rule, when <span class="math inline">\(n = 2\)</span> or <span class="math inline">\(n = 3\)</span>.</p></li>
</ol>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="cramers-rule"><strong><em>Cramer's Rule</em></strong></h3>
<p>Cramer's rule is a formula for solving linear equations. In particular, given an invertible matrix <span class="math inline">\(A \in \R^{n \times n}\)</span>, and vector <span class="math inline">\(b \in \R^n\)</span>, then the solution for the equation <span class="math display">\[
Ax = b
\]</span></p>
<p>is given by (<span class="math inline">\(\forall i \in [n]\)</span>), <span class="math display">\[
x_i = \frac{\det(A_i) }{\det (A)}
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is the i-th coordinate of <span class="math inline">\(x\)</span> and <span class="math inline">\(A_i\)</span> is the matrix formed by replacing <span class="math inline">\(A\)</span>'s the <span class="math inline">\(i\)</span>-th column by column vector <span class="math inline">\(b\)</span>.</p>
<p>To understand the formula, we view <span class="math inline">\(A\)</span> as a mapping <span class="math inline">\(T: \R^n \rightarrow \R^n\)</span> that takes <span class="math inline">\(e_i (i \in [n])\)</span> to <span class="math inline">\(A[:,i]\)</span>, i.e., <span class="math display">\[
T(e_i) = A[:, i]
\]</span></p>
<p>where <span class="math inline">\(A[:, i]\)</span> is the i-th column of <span class="math inline">\(A\)</span>. The image of the hypercube <span class="math display">\[
\left\{ \sum_{i \in [n]} \lambda_i e_i :  \lambda_i \in [0, 1], \forall i \in [n] \right\}
\]</span></p>
<p>is given as <span class="math display">\[
\left\{ \sum_{i \in [n]} \lambda_i A[:, i] :  \lambda_i \in [0, 1], \forall i \in [n] \right\}
\]</span></p>
<p>Original the hypercube has volume 1. After the transformation it has volume <span class="math inline">\(\det(A)\)</span>. One interesting fact about linear transformation is that it preserves volumes ratio. Define <span class="math inline">\(\mathbb{V}(\cdot)\)</span> the volume of a parallelotope.</p>
<p><em>Fact.</em> For any parallelotope <span class="math inline">\(P\)</span>, its volume under transformation <span class="math inline">\(T\)</span> is given by <span class="math display">\[
\det(A) \mathbb{V}(P) = \mathbb{V}(T(P))
\]</span></p>
<p>Now, we rewrite the Cramer's rule as <span class="math display">\[
\det(A) x_i = \det(A_i)
\]</span></p>
<p>Its meaning is clear: there is a parallelotope <span class="math inline">\(P\)</span> whose volume is <span class="math inline">\(x_i\)</span> and after the transformation it has volume <span class="math inline">\(\det(A_i)\)</span>.</p>
<p>It is left to verify that the parallelotope indeed exists. We observe that <span class="math inline">\(\det(A_i)\)</span> can be viewed as the volume of the following parallelotope formed by <span class="math display">\[
\left\{ \sum_{j \in [n], j \neq i} \lambda_j A[:, j] +\lambda_i b_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\}
\]</span></p>
<p>But this parallel0tope is the image of <span class="math display">\[
\left\{ \sum_{j \in [n], j \neq i} \lambda_j e_j +\lambda_i x_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\}
\]</span></p>
<p>as <span class="math display">\[
\begin{aligned}
    &amp; T \left( \left\{ \sum_{j \in [n], j \neq i} \lambda_j e_j +\lambda_i x_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\} \right) \\
    &amp;= \left\{ \sum_{j \in [n], j \neq i} \lambda_j T(e_j) +\lambda_i T(x_i):  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\} \\
    &amp;= \left\{ \sum_{j \in [n], j \neq i} \lambda_j A[:, j] +\lambda_i b_i:  \lambda_j \in [0, 1], \forall j, \lambda_i \in [0, 1] \right\}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/02/07/Simplex/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/Simplex/" class="post-title-link" itemprop="url">Simplex</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-07 10:03:19" itemprop="dateCreated datePublished" datetime="2020-02-07T10:03:19+11:00">2020-02-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-17 17:41:28" itemprop="dateModified" datetime="2021-07-17T17:41:28+10:00">2021-07-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Simplex is the first systematically method for solving linear program via a sequence of Gaussian eliminations. It was invented by George Dantzig in 1947 [1]. Algebraically, it converts the linear program from one slack form into to an equivalent one whose objective value does not decrease and possibly increase. It keeps going until the optimal is reached. Geometrically, it is a greedy search strategy that moves from a vertex of the feasible region to another one, with the objective value at the new vertex as least as good as or possibly improved over the previous one. It is an exponential algorithm, although most of time it does a lot better. In this blog we walk through a simple example to illustrate this.</p>
<p>In this case we are operating a factory with two products. Both product have 1 unit of profit. Making the products needs to use two kind of machine. Product one uses 1 hour of machine one and product two uses 2 hours of machine one. Further, product one uses 2 hours of machine two and product two uses 1 hour of machine two. Both machines operate at most 6 hours each day. We want to maximize our profit: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; x_1 + x_2 \\
&amp;s.t.   &amp; x_1 + 2x_2 \le 6 \\
&amp;       &amp; 2x_1 + x_2 \le 6 \\
&amp;       &amp;   x_1, x_2 \ge 0
\end{aligned}
\]</span></p>
<p>It can be represented concisely in matrix form: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; c^T x \\
&amp;s.t.   &amp; A x \le b \\
&amp;       &amp; x \ge 0
\end{aligned}
\]</span></p>
<p>where <span class="math display">\[
\begin{aligned}
A = \left[ 
    \begin{matrix}
        1 &amp; 2 \\
        2 &amp;  1
    \end{matrix} 
    \right], 
\quad 
b = \left[ 
    \begin{matrix}
        6 \\
        6 
    \end{matrix} 
    \right], 
\quad 
c = \left[ 
    \begin{matrix}
        1 \\
        1 
    \end{matrix} 
    \right], 
\quad 
x = \left[ 
    \begin{matrix}
        x_1 \\
        x_2 
    \end{matrix} 
    \right],
\end{aligned}
\]</span></p>
<p><em>We call the set of points that satisfy the constraints of the optimization problem the feasible region</em> <span class="math display">\[
P = \{x : Ax \le b, x \ge 0\}
\]</span></p>
<p>It is obvious that the optimal is obtained at <span class="math inline">\((2, 2)\)</span>, which gives the objective value <span class="math inline">\(4\)</span>. It is a <em>vertex</em> of the feasible region.</p>
<h4 id="slack-form">Slack Form</h4>
<p>In general, it might not be obvious what the optimal point is. But if the elements of <span class="math inline">\(b\)</span> are non-negative, the origin is a feasible point, from which we can begin our search. To make the search easier, we are going to introduce an additional number of variables, the slack variables <span class="math inline">\(s_1, s_2\)</span> and <span class="math inline">\(z\)</span> that correspond to the constraints and objective value. <span class="math display">\[
\begin{aligned}
z =     &amp;   &amp; x_1       &amp;+x_2   \\
s_1 =   &amp;6  &amp; -x_1      &amp;-2x_2  \\
s_2 =   &amp;6  &amp; -2x_1     &amp;-x_2   \\
\end{aligned}
\]</span></p>
<p>rewrite the optimization into <em>slack form</em>: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; x_1   &amp;+x_2       &amp;       &amp;           &amp;= z\\
&amp;s.t.   &amp; x_1   &amp;+2x_2      &amp;+s_1   &amp;           &amp;= 6 \\
&amp;       &amp; 2x_1  &amp;+x_2       &amp;       &amp;+s_2       &amp;= 6 \\
&amp;       &amp; x_1,  &amp;x_2,       &amp;s_1    &amp;,s_2       &amp;\ge 0
\end{aligned}
\]</span></p>
<p>They are called slacks variables because they correspond how much slack you have in the inequalities in the original problem. For this slack form, we also call <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> basic variables, and the original variables non-basic variables.</p>
<p>Now the feasible region <span class="math inline">\(F\)</span> is considered as the intersection of the two subspaces <span class="math display">\[
F = \{x : Ax = b \} \cap \{ x \ge 0 \}
\]</span></p>
<p>We can represent the LP in <em>slack form</em> even more concisely in a table: <span class="math display">\[
\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\ 
    \hline 
    1   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  0   &amp;&amp; \mid 0 \\
\end{aligned}
\]</span></p>
<p>where the last column denotes the objective function. This example has a trivial starting point : <span class="math inline">\((0, 0, 6, 6)\)</span>. It is called a basic solution in which all non-basic variables are set to zero. It can be read from the tabular form: the identity matrix correspond to basic variables, and <span class="math inline">\((s_1, s_2) = b\)</span>.</p>
<h4 id="pivoting">Pivoting</h4>
<p>Look at the original LP: <span class="math display">\[
\begin{aligned}
&amp;\max   &amp; x_1   &amp;+x_2       &amp;       &amp;           &amp;= z\\
&amp;s.t.   &amp; x_1   &amp;+2x_2      &amp;+s_1   &amp;           &amp;= 6 \\
&amp;       &amp; 2x_1  &amp;+x_2       &amp;       &amp;+s_2       &amp;= 6 \\
&amp;       &amp; x_1,  &amp;x_2,       &amp;s_1    &amp;,s_2       &amp;\ge 0
\end{aligned}
\]</span></p>
<p>As we would like to maximize the objective, we would like to increase the value of <span class="math inline">\(x_1\)</span> or <span class="math inline">\(x_2\)</span> as much as possible. Suppose we just increase one variable at a time. For example, we can increase variable <span class="math inline">\(x_1\)</span> and keep the value of <span class="math inline">\(x_2\)</span> to be <span class="math inline">\(0\)</span>.</p>
<p>How much can we increase <span class="math inline">\(x_1\)</span>? After increasing <span class="math inline">\(x_1\)</span>, we need to maintain the inequality constraints: <span class="math display">\[
x_1 + s_1 = 6 \\
2 x_1 + s_2 = 6
\]</span></p>
<p>The values of <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> decrease as <span class="math inline">\(x_1\)</span> increases. By they can be drop below <span class="math inline">\(0\)</span>. The maximum possible increases of <span class="math inline">\(x_1\)</span> is bounded by <span class="math inline">\(6 / 2 = 3\)</span>. If we set <span class="math inline">\(x_1 = 3\)</span>, <span class="math inline">\(s_1 = 6 - x_1 = 3\)</span> and <span class="math inline">\(s_2 = 6 - 2x_1 = 0\)</span>.</p>
<p>Note that <span class="math inline">\(x_1\)</span> increases from <span class="math inline">\(0\)</span> to <span class="math inline">\(3\)</span> and <span class="math inline">\(s_2\)</span> decreases from <span class="math inline">\(6\)</span> to <span class="math inline">\(0\)</span>. This process is called <em>pivoting</em>.</p>
<p>How do we perform pivoting in the tabular form? By the replacement we have just performed -- normalizing the second row, and using it to eliminate all other elements in the first column.</p>
<p>Recall the original table: <span class="math display">\[
\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\ 
    \hline 
    1   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  0   &amp;&amp; \mid 0 \\
\end{aligned}
\]</span></p>
<p>We divide the second row by a factor of 2 to normalize it, and subtract the first and third row by the second row <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  3/2 &amp;&amp;  1   &amp;&amp; -1/2 &amp;&amp; \mid 3 \\
    1   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp;  1/2 &amp;&amp; \mid 3 \\ 
    \hline 
    0   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp; -1/2 &amp;&amp; \mid -3 \\
\end{aligned}
\]</span></p>
<p>Observe the first two rows of the tabular form: <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  3/2 &amp;&amp;  1   &amp;&amp; -1/2 &amp;&amp; \mid 3 \\
    1   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp;  1/2 &amp;&amp; \mid 3 \\ 
\end{aligned}
\]</span> If we set <span class="math inline">\(x_2 = 0\)</span> and <span class="math inline">\(s_2 = 0\)</span>, the constraints they implies becomes <span class="math display">\[
s_1 = 3 \\
x_1 = 3 
\]</span> Again the unity columns correspond to non-zero elements and we can read the solution from the tabular form.</p>
<h4 id="correctness-of-pivoting">Correctness of Pivoting</h4>
<p>Before we continue our search on the tabular form, we need to show that pivoting is correct in the sense that, after pivoting, the tabular form represents the same optimization.</p>
<p>In particular, we need to guarantee that the feasible regions are the same and the objective function are equivalent defined on the feasible regions.</p>
<p>We focus on the extended matrix that represents the constraints: <span class="math display">\[
\bar A = [A, b] = 
\left[\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\
\end{aligned}\right]
\]</span> <em>FACT 1.</em> Doing a row replacement on <span class="math inline">\(\bar A\)</span> does not change <span class="math inline">\(F\)</span>, where row replacement is defined as <span class="math display">\[
\bar A[i: ] \leftarrow \bar A[i: ] + k \cdot \bar A[j: ]
\]</span> i.e., adding <span class="math inline">\(k\)</span> times the <span class="math inline">\(j\)</span>-th row to the <span class="math inline">\(i\)</span>-th row, for <span class="math inline">\(i \neq j, k \in R\)</span>.</p>
<p>To see this, denote <span class="math inline">\(F\)</span> and <span class="math inline">\(F&#39;\)</span> the sets before and after the replacement respectively. <span class="math inline">\(F = F&#39;\)</span> implies that each point <span class="math inline">\(x\)</span> belonging to <span class="math inline">\(F\)</span> belongs to <span class="math inline">\(F&#39;\)</span>, and vice-versa. Specifically, the row replacement corresponds to left multiplying <span class="math inline">\(\bar A\)</span> by a matrix</p>
<p><span class="math display">\[
\begin{aligned}
E_{(i,j), k} \doteq 
    \left[ \begin{matrix}
        1 &amp; \\ 
          &amp; 1 \\
          &amp;     &amp; \quad...      \\
          &amp;     &amp; k     &amp; 1     \\
          &amp;     &amp;       &amp;       &amp;    ...  \\
          &amp;     &amp;       &amp;       &amp;           &amp;    1
    \end{matrix} \right]
\end{aligned}
\]</span></p>
<p>which is a diagonal matrix with an additional <span class="math inline">\(k\)</span> in the <span class="math inline">\((i, j)\)</span> position. It is invertible with inverse matrix <span class="math display">\[
\begin{aligned}
E_{(i,j), -k} \doteq 
    \left[ \begin{matrix}
        1 &amp; \\ 
          &amp; 1 \\
          &amp;     &amp; \quad...      \\
          &amp;     &amp; -k    &amp; 1     \\
          &amp;     &amp;       &amp;       &amp;    ...  \\
          &amp;     &amp;       &amp;       &amp;           &amp;    1
    \end{matrix} \right]
\end{aligned}
\]</span></p>
<p>The previous claim becomes <span class="math inline">\(\{ x : E_{(i, j), k} A x = E_{(i, j), k} b \} = \{ x : A x = b \}\)</span>.</p>
<p>But wait. Why do we subtract the last row from the second one? <em>Fact 1</em> has justified the replacement operation on <span class="math inline">\(\bar A\)</span>, but not on the objective coefficients <span class="math inline">\(c\)</span> (the last row in the tabular form). To illustrate the meaning of the operation, recall that <span class="math display">\[
x_1 + x_2 + 0 \cdot s_1 + 0 \cdot s_2 = z
\]</span> Hence <span class="math inline">\(z\)</span> is the value of the objective function. After normalizing the second row, it becomes <span class="math display">\[
x_1 + 1 / 2 x_2 +  0 s_1 + 1/2 s_2 = 3 \\
\]</span></p>
<p>The constraint holds for any feasible point of the linear programming. Therefore, <span class="math display">\[
\begin{aligned}
z - 3 
    &amp;= (x_1 + x_2 + 0 s_1 + 0 s_2)  - (x_1 + 1 / 2 x_2 +  0 s_1 + 1/2 s_2) \\ 
    &amp;= 0x_1 + 1/2x_2 + 0s_1 - 1/2 s_2 \\
    &amp;\Leftrightarrow \\
    z 
    &amp;= 0x_1 + 1/2x_2 + 0s_1 - 1/2 s_2 + 3 
\end{aligned}
\]</span></p>
<p>The last <span class="math inline">\(-3\)</span> in the table is the inverse constant in the objective functions after the replacement of variable.</p>
<h4 id="back-to-our-search">Back to our search</h4>
<p>To improve the current solution further, we note that the coefficient of <span class="math inline">\(x_2\)</span> is positive. We can increase the value of <span class="math inline">\(x_2\)</span>. The maximum increase is given by <span class="math inline">\(\min \{ 3 / (1 / 2), 3 / (3/ 2) \} = 2\)</span>, determined by the first row: <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  3/2 &amp;&amp;  1   &amp;&amp; -1/2 &amp;&amp; \mid 3 \\
    1   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp;  1/2 &amp;&amp; \mid 3 \\ 
    \hline 
    0   &amp;&amp;  1/2 &amp;&amp;  0   &amp;&amp; -1/2 &amp;&amp; \mid -3 \\
\end{aligned}
\]</span> To make the change explicitly, we perform elimination by the first row <span class="math display">\[
\begin{aligned}
    0   &amp;&amp;  1   &amp;&amp;  2/3 &amp;&amp; -1/3 &amp;&amp; \mid 2 \\
    1   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp;  2/3 &amp;&amp; \mid 2 \\ 
    \hline 
    0   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp; -1/3 &amp;&amp; \mid -4 \\
\end{aligned}
\]</span> which give solution <span class="math inline">\(x = (2, 2, 0, 0)\)</span> and objective value <span class="math inline">\(4\)</span>. It is indeed optimal solution for the problem, as all coefficients of the optimization are non-positive.</p>
<h4 id="optimality-condition-and-strong-duality">Optimality Condition and Strong Duality</h4>
<p>We investigate the result deeper by viewing the initial tabular form in abbreviation as <span class="math display">\[
\left[ \begin{matrix}
A \quad I \mid b \\ c^T  \quad 0 \mid 0
\end{matrix} \right]
\]</span> The series of <span class="math inline">\(m\)</span> Gaussian elimination can be interpreted as left multiplying a matrix: <span class="math display">\[
\begin{aligned}
\left[ \begin{matrix}
    &amp; R     &amp; 0 \\
    &amp; -y^T  &amp; 1 \\
\end{matrix} \right] \doteq E_m...E_3 E_2 E_1
\end{aligned}
\]</span> where each <span class="math inline">\(E_j\)</span> ( <span class="math inline">\(1 \le j \le m\)</span>) matrix is invertible and represents a Gaussian elimination, i.e., if we left multiple a matrix by <span class="math inline">\(E_j\)</span>, it is equivalent to perform Gaussian elimination on the matrix. The matrix product <span class="math inline">\(E_m...E_3 E_2 E_1\)</span> has the form <span class="math display">\[
\begin{aligned}
\left[ \begin{matrix}
    &amp; R     &amp; 0 \\
    &amp; -y^T  &amp; 1 \\
\end{matrix} \right] 
\end{aligned}
\]</span> because we never use the last row to eliminate the other rows.</p>
<p>Now: <span class="math display">\[
\begin{aligned}
\left[ \begin{matrix}
    &amp; R     &amp; 0 \\
    &amp; -y^T  &amp; 1 \\
\end{matrix} \right]
\left[ \begin{matrix}
    &amp; A &amp; I \mid    b \\ 
    &amp; c^T  &amp; 0 \mid 0
\end{matrix} \right] 
=
\left[ \begin{matrix}
    &amp; RA            &amp; R     &amp; \mid &amp; Rb \\ 
    &amp; -y^TA+c^T     &amp; -y^T  &amp;\mid &amp; -y^T b
\end{matrix} \right]
\end{aligned}
\]</span> In our example, <span class="math display">\[
\begin{aligned}
    1   &amp;&amp;  2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp; \mid 6 \\
    2   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  1   &amp;&amp; \mid 6 \\ 
    \hline 
    1   &amp;&amp;  1   &amp;&amp;  0   &amp;&amp;  0   &amp;&amp; \mid 0 \\
\end{aligned}
\Rightarrow ... \Rightarrow
\begin{aligned}
    0   &amp;&amp;  1   &amp;&amp;  2/3 &amp;&amp; -1/3 &amp;&amp; \mid 2 \\
    1   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp;  2/3 &amp;&amp; \mid 2 \\ 
    \hline 
    0   &amp;&amp;  0   &amp;&amp; -1/3 &amp;&amp; -1/3 &amp;&amp; \mid -4 \\
\end{aligned}
\]</span> we have <span class="math display">\[
R = \begin{aligned}
\left[ \begin{matrix}
    2 / 3 &amp; -1 / 3 \\
    -1 / 3 &amp; 2 /3 
\end{matrix} \right] 
\end{aligned}
\qquad 
y^T = [ -1 / 3, -1  / 3]
\]</span></p>
<p>The optimality condition of simplex algorithm implies that <span class="math display">\[
\begin{aligned}
-y^T A + c^T &amp;\le 0 \\
y^T &amp;\le 0 \\
y^T b &amp;= c^T x
\end{aligned}
\]</span></p>
<p>This is the optimal solution for the dual program of the primal: <span class="math display">\[
\min \ b^T y, \qquad s.t.,  A^T y \ge c, \quad y^T \ge 0
\]</span></p>
<p>If simplex algorithm stops, it implies strong duality holds:</p>
<ul>
<li>The optimal value of the primal program equals to the optimal one of the dual program, assuming that both values exist and are bounded.</li>
</ul>
<h4 id="geometric-interpretation">Geometric Interpretation</h4>
<p>In the beginning we claim that the simplex algorithm move from one vertex to another. We prove it rigorously in this section. First we need a few definitions:</p>
<h5 id="vertex">Vertex</h5>
<p><em>A vertex is a point <span class="math inline">\(v \in F\)</span>, such that <span class="math inline">\(\nexists v_1, v_2 \in F, v_1 \neq v_2\)</span> and <span class="math inline">\(\lambda \in (0, 1)\)</span>, s.t., <span class="math inline">\(v = \lambda v_1 + (1 - \lambda) v_2\)</span>.</em></p>
<p>In other words, <span class="math inline">\(v\)</span> is a vertex of <span class="math inline">\(F\)</span> if it is not contained in a line segment of <span class="math inline">\(P\)</span>.</p>
<p><strong>Theorem.</strong> If LP has an optimal solution, then it has an optimal solution that is a vertex of its feasible set.</p>
<p><em>Proof.</em> Denote <span class="math inline">\(v\)</span> the optimal solution of the LP with maximum number of zero components. If <span class="math inline">\(v\)</span> is not a vertex, then <span class="math inline">\(\exists v_1, v_2 \neq v, v_1, v_2 \in F\)</span> and <span class="math inline">\(\lambda &gt; 0\)</span>, such that <span class="math inline">\(\lambda v_1 + (1 - \lambda) v_2 = v\)</span>.</p>
<p>As <span class="math inline">\(v\)</span> is optimal, we have <span class="math inline">\(c^T v \ge c^T v_1\)</span> and <span class="math inline">\(c^T v \ge c^T v_2\)</span>. By <span class="math inline">\(\lambda v_1 + (1 - \lambda) v_2 = v\)</span>, it concludes that <span class="math inline">\(c_T v = c^T v_1 = c^T v_2\)</span>.</p>
<p>Define <span class="math inline">\(I = \{ i : v[i] &gt; 0 \}\)</span>. As <span class="math inline">\(v_1 \ge 0, v_2 \ge 0\)</span> and <span class="math inline">\(\lambda v_1 + (1 - \lambda) v_2 = v\)</span> so that for <span class="math inline">\(i \neq I\)</span>, <span class="math inline">\(v_1[i] = v_2[i] = v[i] = 0\)</span>.</p>
<p>But <span class="math inline">\(A_I v_1[I] = A_I v_2[I] = b\)</span>. Let <span class="math inline">\(x = \epsilon (v_1 - v_2) + v\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(x\)</span> is feasible, for small enough <span class="math inline">\(\epsilon\)</span>.</li>
<li><span class="math inline">\(x\)</span> is optimal, since <span class="math inline">\(c^T x= \epsilon c^T (v_1 - v_2) + c^T v = c^T v\)</span>.</li>
<li><span class="math inline">\(x_i = 0\)</span> for <span class="math inline">\(i \notin I\)</span>.</li>
</ol>
<p>We can find some <span class="math inline">\(\epsilon\)</span> (either positive or false) to make <span class="math inline">\(x_i = 0\)</span> for some <span class="math inline">\(i \in I\)</span>. Now <span class="math inline">\(x\)</span> contains more zeros than <span class="math inline">\(v\)</span>, a contradiction. <span class="math inline">\(\square\)</span></p>
<h5 id="basic-solution">Basic Solution</h5>
<p><em>A solution <span class="math inline">\(x\)</span> of <span class="math inline">\(Ax = b\)</span> is called a basic solution if <span class="math inline">\(\{A_i : x_i \neq 0 \}\)</span> (columns in <span class="math inline">\(A\)</span> that correspond to non-zero components in <span class="math inline">\(x\)</span> ) are linearly independent.</em></p>
<h5 id="basic-feasible-solution">Basic Feasible Solution</h5>
<p><em>A basic solution <span class="math inline">\(x\)</span> is called basic feasible solution if <span class="math inline">\(x \ge 0\)</span>, i.e., <span class="math inline">\(x \in \{ x : Ax = b\} \cap \{ x \ge 0 \}\)</span>.</em></p>
<p>With respect to the feasible region of LP in slack form, the <em>vertices</em> have have following property:</p>
<p><em>Lemma:</em>. A point <span class="math inline">\(v \in F = \{x : Ax = b\} \cap \{ x: x \ge 0\}\)</span> is a vertex of <span class="math inline">\(F\)</span> if and only if it is a basic feasible solution. Here <span class="math inline">\(A \in R^{m \times n}, x \in R^n, b \in R^{m}\)</span> and <span class="math inline">\(rank(A) = m\)</span> (otherwise some row constraints of the <span class="math inline">\(Ax = b\)</span> are redundant).</p>
<p><em>Proof</em>: Denote <span class="math inline">\(S\)</span> the set of indices of positive components in <span class="math inline">\(v\)</span> and <span class="math inline">\(A_S\)</span> the set of</p>
<p><em>Only if:</em> If <span class="math inline">\(A_S\)</span> does not have full column rank, then <span class="math inline">\(\exists u \in R^{|S|}\)</span>, such that <span class="math inline">\(A_S u = 0\)</span>. Then for small enough <span class="math inline">\(\epsilon \in R, \epsilon &gt; 0\)</span>, both <span class="math inline">\(v + \epsilon u\)</span> and <span class="math inline">\(v - \epsilon u\)</span> are feasible point in <span class="math inline">\(F\)</span>. Now <span class="math inline">\(v = 0.5(v + \epsilon u) + 0.5(v - \epsilon u)\)</span>, a contradiction.</p>
<p><em>If:</em> As columns of <span class="math inline">\(A_S\)</span> are linearly independent, the only solution of to <span class="math inline">\(A_S x_S = 0\)</span> is given by <span class="math inline">\(x_S = 0\)</span>. Hence, <span class="math inline">\(v_S\)</span> is now the unique solution to <span class="math inline">\(A_S v_S = b\)</span>. If <span class="math inline">\(v\)</span> is not a vertex, <span class="math inline">\(\exists v_1, v_2, \lambda\)</span>, such that <span class="math inline">\(v = \lambda v_1 + (1 - \lambda) v_2\)</span>. But in this case we must have <span class="math inline">\(v_1 = v_2 = v\)</span>, as <span class="math inline">\((v_1)_S = (v_2)_S = v_S\)</span> and <span class="math inline">\((v_1)_{\bar S} = (v_2)_{\bar S} = v_{\bar S} = 0\)</span>. A contradiction.</p>
<p><span class="math inline">\(\blacksquare\)</span>.</p>
<p><em>Corollary:</em> The simplex algorithm moves from vertex to vertex.</p>
<p><em>Proof:</em> The positive components in the solution correspond to the columns in the identity matrix in the tabular form of simplex, which are linearly independent.</p>
<h4 id="correctness-of-simplex-to-finish">Correctness of Simplex (TO FINISH)</h4>
<p>Though not shown in the example, there are two issue associated with simplex algorithm.</p>
<ol type="1">
<li>How do we find an initial solution? If there are negative component in <span class="math inline">\(b\)</span>, then <span class="math inline">\(x = 0 \wedge s = b\)</span> is not a feasible solution.</li>
<li>How do we guarantee that simplex terminates?</li>
</ol>
<p>In the previous example, at each step we choose to increase the variable with largest coefficient. In some rare cases, this results in a loop of the algorithm.</p>
<p>One method for avoiding looping is called Bland's rule:</p>
<ol type="1">
<li>If there is a positive coefficient in the objective function, choose the one with largest coefficient.</li>
<li>If there are multiple rows with tight constraints, choose the one with largest coefficient.</li>
</ol>
<p><em>Theorem.</em> Blands' rule guarantees that the algorithm stops.</p>
<p><em>Proof.</em> Suppose that on the contrary, the algorithm loops. Denote <span class="math inline">\(B_1, B_2, ..., B_k\)</span> the set of basis in the loop. In each base, there is entering variable and one leaving variable. Every variable that leaves the base must enter another base later. We called these variable <em>fickle variables</em>. Denote <span class="math inline">\(x_t\)</span> the fickle variable with the largest index.</p>
<ol type="1">
<li><p><span class="math inline">\(D\)</span> the dictionary that <span class="math inline">\(x_t\)</span> leaves the base, and <span class="math inline">\(x_e\)</span> the variable that enters the base.</p></li>
<li><p><span class="math inline">\(D&#39;\)</span> the dictionary that <span class="math inline">\(x_t\)</span> enters the base again. <span class="math display">\[
D = 
\begin{aligned}
 I   &amp;&amp;  A_N &amp;&amp; \mid b \\
 \hline 
 0   &amp;&amp;  c_N &amp;&amp; \mid \alpha \\
\end{aligned}
\Rightarrow ... \Rightarrow
D&#39; = 
\begin{aligned}
 R   &amp;&amp;  R A_N   &amp;&amp; \mid R b \\
 \hline 
 y^T &amp;&amp;  y^T A_N + c_N   &amp;&amp; \mid y^T b + \alpha \\
\end{aligned}
\]</span> We have <span class="math display">\[
c_e&#39; = c_e + y^T A_N[:, e]
\]</span></p></li>
<li><p><span class="math inline">\(x_e\)</span> is the entering variable in <span class="math inline">\(D\)</span>, therefore <span class="math inline">\(c_e &gt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(x_t\)</span> is the entering variable in <span class="math inline">\(D&#39;\)</span>, and <span class="math inline">\(e &lt; t\)</span>, by the definition of <span class="math inline">\(x_t\)</span>, we have <span class="math inline">\(x_e&#39; \le 0\)</span>.</p></li>
<li><p>Hence, <span class="math inline">\(y^T A_N[:, e] = c_e&#39; - c_e &lt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(\exists i, s.t., y_i A_{i, e} &lt; 0\)</span>. Then <span class="math inline">\(y_i \neq 0\)</span>, and <span class="math inline">\(x_i\)</span> is fickle. By definition, <span class="math inline">\(i &lt; t\)</span>. But <span class="math inline">\(x_i\)</span> is not entering <span class="math inline">\(D&#39;\)</span>, we know that <span class="math inline">\(y_i &lt; 0\)</span>.</p></li>
<li><p><span class="math inline">\(A_{i, e} &gt; 0\)</span>. But since <span class="math inline">\(x_i\)</span> is fickle, we have <span class="math inline">\(b_i = 0\)</span>. In this case, <span class="math inline">\(x_i\)</span> is chosen in preference to <span class="math inline">\(x_t\)</span>. A contradiction.</p></li>
</ol>
<h1 id="reference">Reference</h1>
<p>[1]. Dantzig, George B. "Origins of the simplex method." A history of scientific computing. 1990. 141-151.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/30/Weak%20duality,%20Complementary%20Slackness,%20Strong%20Duality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/30/Weak%20duality,%20Complementary%20Slackness,%20Strong%20Duality/" class="post-title-link" itemprop="url">Dual Program, Weak Duality, Complementary Slackness, Strong Duality</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-30 11:47:54" itemprop="dateCreated datePublished" datetime="2020-01-30T11:47:54+11:00">2020-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-24 20:49:41" itemprop="dateModified" datetime="2021-07-24T20:49:41+10:00">2021-07-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="dual-program-and-weak-duality">Dual Program and Weak Duality</h1>
<p>Every linear program can be converted into Canonical form <span class="math display">\[
    \begin{array}{lr}
        \max &amp; c^T x \\
        \text{s.t. } &amp; Ax \le b \\
            &amp;x \ge 0
    \end{array}
\]</span></p>
<p>where <span class="math inline">\(c, x \in \mathbb{R}^n\)</span>, <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(b \in \mathbb{R}^m\)</span>.</p>
<p>Consider a conic combination (<span class="math inline">\(y \in \mathbb{R}^m\)</span> and <span class="math inline">\(y \ge 0\)</span>) of the linear constraints: <span class="math display">\[
    y^T (Ax) \le y^T b. 
\]</span></p>
<p>If <span class="math inline">\(y\)</span> further satisfies the constraint <span class="math display">\[
    c^T \le y^T A,
\]</span></p>
<p>combining with the fact that <span class="math inline">\(x \ge 0\)</span>, we get <span class="math display">\[
    c^T x \le (y^T A) x = y^T (Ax) \le y^T b. 
\]</span></p>
<p>Hence <span class="math inline">\(y^T b\)</span> is an upper bound of the optimal value of the original program. We want the bound to be as tight as possible. This motivates another linear program: <span class="math display">\[
    \begin{array}{lr}
        \min            &amp; y^T b \\
        \text{s.t. }    &amp; y^T A \ge c^T \\
                        &amp; y \ge 0
    \end{array}
\]</span></p>
<p>We call the original program the <strong>primal</strong> and the derived program its <strong>dual</strong>. The philosophy behind the dual construction is to combine the primal constraints to obtain a bound of its optimal value.</p>
<blockquote>
<p><strong>Corollary.</strong> If the primal is unbounded, then the dual is infeasible. If the dual is unbounded, then the primal is infeasible.</p>
</blockquote>
<p>In general, the primal may not be given in Canonical form and can be formulated as</p>
<p><span class="math display">\[
    \begin{array}{lr}
        \max &amp; c^T x + \bar c^T \bar x + \hat c^T \hat x \\
        \text{s.t. } &amp; Ax + \bar A \bar x + \hat A \hat x \le b \\
        &amp; Bx + \bar B \bar x + \hat B \hat x =  \bar b \\ 
        &amp; Cx + \bar C \bar x + \hat C \hat x \ge \hat b \\
        &amp; x \ge 0 \\
        &amp; \hat x \le 0
    \end{array}
\]</span></p>
<p>Note that the <span class="math inline">\(\bar x\)</span>'s are unconstrained variables. Driven by the same philosophy, we hope to derive an upper bound by using the constraints: <span class="math display">\[
\begin{aligned}
    c^T x + \bar c^T \bar x + \hat c^T \hat x 
        &amp; \le y^T (Ax + \bar A \bar x + \hat A \hat x) \\
        &amp; + \bar y^T (Bx + \bar B \bar x + \hat B \hat x) \\
        &amp; + \hat y^T (Cx + \bar C \bar x + \hat C \hat x) \\
        &amp;\le y^T b + \bar y^T \bar b + \hat y^T \hat b
\end{aligned}
\]</span></p>
<ol type="1">
<li><p>To guarantee the second inequality, we require that <span class="math inline">\(y^T \ge 0\)</span> (since the constraint <span class="math inline">\(Ax + \bar A \bar x + \hat A \hat x \le b\)</span> holds), <span class="math inline">\(\bar y^T\)</span> be unconstrained (since the constraint <span class="math inline">\(Bx + \bar B \bar x + \hat B \hat x = \bar b\)</span> holds), and <span class="math inline">\(\hat y^T \le 0\)</span> (since the constraint <span class="math inline">\(Cx + \bar C \bar x + \hat C \hat x \ge \hat b\)</span> holds). We see that the types of constraints in the primal determine the signs of variables in the dual.</p></li>
<li><p>The reverse is also true: the constraints in the dual are determined the signs of variables in the primal. The first inequality implies that (by proper rearranging): <span class="math display">\[
 \begin{aligned}
     c^T x + \bar c^T \bar x + \hat c^T \hat x 
         &amp; \le (y^T A + \bar y^T B + \hat y^T C)x \\
         &amp; + (y^T \bar A + \bar y^T \bar B + \hat y^T \bar C) \bar x \\
         &amp; + (y^T \hat A + \bar y^T \hat B + \hat y^T \hat C) \hat x
 \end{aligned}
 \]</span></p>
<p>As <span class="math inline">\(x \ge 0\)</span>, <span class="math inline">\(\bar x\)</span> unconstrained and <span class="math inline">\(\hat x \le 0\)</span>, a sufficient condition for the inequality to become true is <span class="math display">\[
 \begin{aligned}
     c^T      &amp; \le (y^T A + \bar y^T B + \hat y^T C) \\
     \bar c^T &amp; =   (y^T \bar A + \bar y^T \bar B + \hat y^T \bar C) \\
     \hat c^T &amp; \ge (y^T \hat A + \bar y^T \hat B + \hat y^T \hat C) \\
 \end{aligned}
 \]</span></p>
<p>That gives the first set of constraints the dual variables <span class="math inline">\(y\)</span>, <span class="math inline">\(\bar y\)</span> and <span class="math inline">\(\hat y\)</span> need to satisfies. We have just seen that the sign of the primal variable determines the type of constraint in the dual.</p>
<p>Of particular interest is the inequality constraint, which we will discuss in details in the next section.</p></li>
</ol>
<p>The rules of obtaining dual are summarized as follows:</p>
<blockquote>
<p><span class="math display">\[
\begin{array}{llll}    
\hline
\text{Primal} &amp; \max &amp; \min &amp;  \text{Dual} &amp; \\ 
\hline
\text{Constraints}  &amp;  
\begin{array}{l}
\le b_i \\ 
\ge b_i \\ 
= b_i  
\end{array} &amp;
\begin{array}{l} 
\ge 0 \\ 
\le 0 \\ 
\text{Both sides} 
\end{array} &amp;  
\text{Variables} &amp; \\
\hline
\text{Variables} &amp; 
\begin{array}{l}
\ge 0 \\ 
\le 0 \\ 
\text{Both sides} 
\end{array} &amp; 
\begin{array}{l}
\ge c_j \\ 
\le c_i \\ 
= c_j   
\end{array} &amp; 
\text{Constraints}  &amp; \\
\hline
\end{array}
\]</span></p>
</blockquote>
<h1 id="complementary-slackness">Complementary Slackness</h1>
<p>The strong duality states that, if both primal and dual are feasible, then the objective function values of their optimal solutions are equal. It follows that the inequalities in weak duality are saturated <span class="math display">\[
    c^T x = (y^T A) x = y^T (Ax) = y^T b. 
\]</span></p>
<p>Therefore, <span class="math display">\[
    (y^T A - c^T) x = 0,\,
    \\ y^T (b - Ax) = 0.
\]</span></p>
<p>If the primal is in canonical form, then the fact that <span class="math inline">\(y^T A \ge c\)</span>, <span class="math inline">\(x \ge 0\)</span>, <span class="math inline">\(y \ge 0\)</span>, and <span class="math inline">\(b - Ax \ge 0\)</span> implies that <span class="math display">\[
    \begin{array}{lrrr}
        (y^T A - c^T)_i = 0  &amp;\vee &amp; x_i = 0,\,         &amp;\forall \ i \in [n] \\
        y_j = 0             &amp;\vee  &amp; (b - Ax)_j = 0,\,  &amp;\forall \ j \in [m]
    \end{array}
\]</span></p>
<p>where <span class="math inline">\((y^T A - c^T)_i\)</span> is its <span class="math inline">\(i\)</span>-th coordinate of the vector <span class="math inline">\(y^T A - c^T \in \mathbb{R}^{1 \times n}\)</span>, and <span class="math inline">\((b - Ax)_j\)</span> is the <span class="math inline">\(j\)</span>-th coordinate of the vector <span class="math inline">\(b - Ax \in \mathbb{R}^{m \times 1}\)</span>.</p>
<p>This is known as <strong><em>complementary slackness</em></strong>.</p>
<blockquote>
<p><strong>Question to ponder</strong>: what is the geometric interpretation of complementary slackness?</p>
</blockquote>
<!-- In what follows, we prove this again, by avoiding matrix form. 
First, weak duality implies 
$$
    \sum_{i = 1}^n c_i x_i \le \sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j} \right) x_i =  \sum_{j = 1}^m  y_j \left( \sum_{i = 1}^n A_{i, j} x_i \right) \le \sum_{j = 1}^m y_j b_j.
$$

If the inequalities are saturated, 
$$
    \sum_{i = 1}^n c_i x_i = \sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j}  \right) x_i,  \\  
    \sum_{j = 1}^m  y_j \left( \sum_{i = 1}^n A_{i, j} x_i \right)  = \sum_{j = 1}^m b_j y_j,
$$

i.e., 
$$
    \sum_{i = 1}^n \left( \sum_{j = 1}^m y_j A_{i, j} - c_i \right) x_i  = 0, \\  
    \sum_{j = 1}^m  y_j \left( b_j - \sum_{i = 1}^n A_{i, j} x_i \right) = 0.
$$

As $\sum_{j = 1}^m A_{i, j} y_j - c_i \ge 0$, $x_i \ge 0$ for all $i$ and $b_j - \sum_{i = 1}^n A_{i, j} x_i  \ge 0$ and $y_j \ge 0$ for all $j$, we conclude that 

$$
    \begin{array}{lrrr}
        \sum_{j = 1}^m y_j A_{i, j}  - c_i = 0 &\vee &    x_i = 0,\, & \forall i, \\ 
        b_j - \sum_{i = 1}^n A_{i, j} x_i = 0 &\vee &     y_j  = 0,\, & \forall j,
    \end{array}
$$ -->
<h1 id="strong-duality">Strong Duality</h1>
<blockquote>
<p><strong>Theorem.</strong> Exactly one of the following four cases holds: 1. Neither the primal nor the dual is feasible.<br />
2. If both the primal and dual are feasible, then their optimal values are equal. 3. If the primal is feasible and unbounded, and dual is infeasible. 4. If the dual is feasible and unbounded, and primal is infeasible.</p>
</blockquote>
<h2 id="special-case">Special Case</h2>
<p>We first consider the special case such that not every linear program can be converted to this case. We consider the general form in the next section.</p>
<p>The special case is given by <span class="math display">\[
    \begin{array}{lr}
        \min &amp; y^T b, \\
        \text{s.t. } &amp; y^T A = c^T.
    \end{array}
\]</span></p>
<p>The dual form is given by <span class="math display">\[
    \begin{array}{lr}
        \max &amp; c^T x, \\
        \text{s.t. } &amp; Ax = b.
    \end{array}
\]</span></p>
<p>The primal is feasible if <span class="math inline">\(c\)</span> is a linear combination of rows of <span class="math inline">\(A\)</span>, i.e., in the row space of <span class="math inline">\(A\)</span>. The dual is feasible if <span class="math inline">\(b\)</span> is a linear combination of columns of <span class="math inline">\(A\)</span>, i.e., in column space of <span class="math inline">\(A\)</span>, denoted as <span class="math inline">\(C(A)\)</span>.</p>
<p>It if possible that neither the primal nor the dual is feasible.</p>
<blockquote>
<p><strong>Example.</strong> <span class="math display">\[
A = \begin{bmatrix}
1,\,    &amp;0\\
0,\,    &amp;0\\
\end{bmatrix},\,
b=  \begin{bmatrix}
1 \\
1 \\
\end{bmatrix},\,
c=  \begin{bmatrix}
1 \\
1 \\
\end{bmatrix}.
\]</span> Then neither <span class="math inline">\(A x = b\)</span> nor <span class="math inline">\(y^T A = c^T\)</span> is feasible.</p>
</blockquote>
<p>If both primal and dual are feasible, then for all feasible solutions <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, it holds that <span class="math display">\[
    c^T x = (y^T A) x = y^T (Ax) = y^T b.
\]</span></p>
<p>Next, we show that</p>
<blockquote>
<p><strong>Theorem.</strong> 1. If the primal is feasible and dual is infeasible, then the primal is unbounded. 2. If the dual is feasible and primal is infeasible, then the dual is unbounded.</p>
</blockquote>
<p><strong>Proof.</strong> By symmetry, we prove just the former.</p>
<p>If <span class="math inline">\(Ax = b\)</span> is not feasible, then <span class="math inline">\(b\)</span> lies outside the column space of <span class="math inline">\(A\)</span>, denoted as <span class="math inline">\(C(A)\)</span> (the subspace in <span class="math inline">\(\mathbb{R}^m\)</span> spanned by the column vectors of <span class="math inline">\(A\)</span>). Let <span class="math inline">\(N(A^T)\)</span> be the subspace orthogonal to <span class="math inline">\(C(A)\)</span>. We can decompose <span class="math inline">\(b\)</span> into two part: <span class="math display">\[
    b = b_1 + b_2,
\]</span></p>
<p>where <span class="math inline">\(b_1 \in C(A)\)</span> is in the columns space of <span class="math inline">\(A\)</span> and <span class="math inline">\(b_2 \in N(A^T)\)</span> is orthogonal to the columns space of <span class="math inline">\(A\)</span>. By assumption we have <span class="math inline">\(b_2 \neq \vec 0\)</span>.</p>
<p>Suppose <span class="math inline">\(y\)</span> is a feasible solution to <span class="math inline">\(y^T A = c^T\)</span>. Then for all <span class="math inline">\(k \in \mathbb{R}\)</span>, <span class="math inline">\(y + k \cdot b_2\)</span> is still a feasible solution, since <span class="math display">\[
    (y + k \cdot b_2)^T A = y^T A + k \cdot b_2^T A = y^T A = c^T.
\]</span></p>
<p>The new solution has objective <span class="math display">\[
    (y + k \cdot b_2)^T b = y^T b + k \cdot \Vert b_2 \Vert_2^2.
\]</span></p>
<p>When <span class="math inline">\(k \rightarrow - \infty\)</span>, we have <span class="math inline">\(y^T b + k \cdot \Vert b_2 \Vert_2^2 \rightarrow - \infty\)</span>, which implies that the primal is unbounded.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/StrongDualitySpecialCase.jpg" /></p>
<h2 id="general-case">General Case</h2>
<p>We discuss the more general form: <span class="math display">\[
    \begin{array}{lr}
        \min &amp; y^T b, \\
        \text{s.t. } &amp; y^T A \ge c^T.
    \end{array}
\]</span></p>
<p>Every linear program can be converted into this general form.</p>
<p>Its dual is given by <span class="math display">\[
    \begin{array}{lr}
        \max &amp; c^T x, \\
        \text{s.t. } &amp; Ax = b, \\
        &amp; x \ge 0.
    \end{array}
\]</span></p>
<p>By weak duality, <span class="math display">\[
    c^T x \le y^T A x = y^T b
\]</span></p>
<p>To prove strong duality, we need only to prove that</p>
<blockquote>
<p>If both primal and dual are feasible, then there exists feasible <span class="math inline">\(x, y\)</span>, such that <span class="math inline">\(c^T x = y^T A x\)</span>.</p>
</blockquote>
<p>Define the feasible set of the primal as <span class="math display">\[
    F \doteq \{ y \in \mathbb{R}^m : y^T A \ge c^T \}.  
\]</span></p>
<p>Let <span class="math inline">\(y^\star\)</span> be the optimal solution for the primal. We see that <span class="math inline">\(y^\star\)</span> can not be an interior point of the feasible set <span class="math inline">\(F\)</span>. Otherwise, for small enough <span class="math inline">\(k &gt; 0\)</span>, <span class="math inline">\(y^\star - k \cdot b\)</span> belongs to <span class="math inline">\(F\)</span>. But <span class="math inline">\((y^\star - k \cdot b)^T b &lt; y^\star b\)</span>, contradicting <span class="math inline">\(y^\star\)</span> being optimal.</p>
<p>Therefore, <span class="math inline">\(y^\star\)</span> must be on the boundary of the <span class="math inline">\(F\)</span>, and there must exit one or more constraints saturated by <span class="math inline">\(y^\star\)</span>. Define the set of indexes of the constraints that are saturated by <span class="math inline">\(y^\star\)</span> <span class="math display">\[
    S \doteq \{ i \in [m] : (y^\star)^T A_i = c_i \} \subset [m],
\]</span></p>
<p>where <span class="math inline">\(A_i\)</span> is the <span class="math inline">\(i\)</span>-th column of matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(c_i\)</span> is the <span class="math inline">\(i\)</span>-th coordinator of vector <span class="math inline">\(c\)</span>. Let <span class="math inline">\(A_S\)</span> be set of columns indexed by <span class="math inline">\(S\)</span>.</p>
<blockquote>
<p><strong>Lemma.</strong> <span class="math inline">\(b\)</span> is in the conic hull of columns of <span class="math inline">\(A_S\)</span>, i.e., <span class="math inline">\(\exists x_S \in R^{|S|}\)</span>, such that<br />
1. <span class="math inline">\(A_S x_S = b\)</span>.<br />
2. <span class="math inline">\(x_S \ge 0\)</span>.</p>
</blockquote>
<p>Such <span class="math inline">\(x_S\)</span> can be converted into a feasible solution <span class="math inline">\(x\)</span> to the dual (by padding zeros in dimensions other than <span class="math inline">\(S\)</span>). It holds that <span class="math display">\[
    c^T x = (y^\star)^T A x = (y^\star)^T (A_S x_S ) = (y^\star)^T b.
\]</span></p>
<blockquote>
<p><strong>Question to ponder</strong>: prove the existence of optimal solution of the primal, assuming feasibility of the dual.</p>
</blockquote>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/MinimizeYB.png" /></p>
<p><strong>Proof.</strong> We prove that if <span class="math inline">\(b\)</span> is not in the conic hull of <span class="math inline">\(A_S\)</span>, we can more <span class="math inline">\(y^\star\)</span> along some direction that decreases objective while maintaining feasibility.</p>
<p>By <a target="_blank" rel="noopener" href="https://wuhao-wu-jiang.github.io/2019/01/17/Farkas-Lemma/">Farkas' Lemma</a>, if <span class="math inline">\(b\)</span> is not in the conic hull of <span class="math inline">\(A_S\)</span>, <span class="math inline">\(\exists v \in \mathbb{R}^m\)</span>, such that</p>
<ol type="1">
<li><span class="math inline">\(v^T A_S \ge 0\)</span>.</li>
<li><span class="math inline">\(v^T b &lt; 0\)</span>.</li>
</ol>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/FarkasLemma3.png" /></p>
<p>Then we can move the vector <span class="math inline">\(y^\star\)</span> along the direction of <span class="math inline">\(v\)</span>, by adding <span class="math inline">\(k \cdot v\)</span> to <span class="math inline">\(y^\star\)</span> for some <span class="math inline">\(k \ge 0\)</span>. This decreases the objective <span class="math display">\[
    (y^\star + k \cdot v)^T b = (y^\star)^T b + k \cdot v^T b &lt; (y^\star)^T b.
\]</span></p>
<p>How large can <span class="math inline">\(k\)</span> be, so the constraints <span class="math inline">\((y^\star + k \cdot v)^T A \ge c^T\)</span> are maintained.</p>
<ol type="1">
<li><p>If <span class="math inline">\(i \in \bar S\)</span>, then <span class="math inline">\((y^\star)^T A_i &gt; c_i\)</span>. Define <span class="math display">\[
     t_i \doteq \begin{cases}
         \infty,\, &amp;\text{if }\, v^T A_i \ge 0, \\
         \Big( (y^\star)^T A_i - c_i \Big) / \big| v^T A_i \big|,\, &amp;\text{if }\, v^T A_i &lt; 0, \\
     \end{cases}
 \]</span> Then <span class="math inline">\(t_i &gt; 0\)</span> and it is guaranteed that <span class="math display">\[
     (y^\star)^T A_i + t_i \cdot v \ge c_i.
 \]</span> Denote <span class="math inline">\(k \doteq \min_{i \in \bar S} t_i &gt; 0\)</span>.</p></li>
<li><p>If <span class="math inline">\(i \in S\)</span>, as <span class="math inline">\(v^T A_i \ge 0\)</span>, for the value of <span class="math inline">\(k\)</span> defined above, it holds that <span class="math display">\[
     (y^\star + k v)^T A_i = c_i + k v^T A_i \ge c_i,
 \]</span></p></li>
</ol>
<p>It concludes that <span class="math inline">\((y^\star + k v)\)</span> is feasible. But <span class="math inline">\((y^\star + k v)^T b &lt; (y^\star)^T b\)</span>. A contradiction.</p>
<blockquote>
<p><strong>Question to ponder</strong>: connect the above discussion to Lagrange multiplier and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">K.K.T condition</a>.</p>
</blockquote>
<h1 id="reference.">Reference.</h1>
<ol type="1">
<li>David P. Williamson, ORIE 6300 Mathematical Programming I, <a target="_blank" rel="noopener" href="https://people.orie.cornell.edu/dpw/orie6300/Lectures/lec08.pdf">Lecture 8</a><br />
</li>
<li>Boyd, S. and Vandenberghe, L., 2004. Convex optimization. Cambridge university press.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/33/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><span class="page-number current">34</span><a class="page-number" href="/page/35/">35</a><span class="space">&hellip;</span><a class="page-number" href="/page/59/">59</a><a class="extend next" rel="next" href="/page/35/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">177</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
