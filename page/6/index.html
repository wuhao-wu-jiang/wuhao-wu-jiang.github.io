<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/08/24/Projection-of-A-Random-Unit-Vector-on-Its-First-Dimension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/24/Projection-of-A-Random-Unit-Vector-on-Its-First-Dimension/" class="post-title-link" itemprop="url">Projection of A Random Unit Vector on Its First Dimension</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-24 16:27:36" itemprop="dateCreated datePublished" datetime="2018-08-24T16:27:36+10:00">2018-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-30 14:20:30" itemprop="dateModified" datetime="2020-05-30T14:20:30+10:00">2020-05-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $x = &lt;x_1, x_2, …, x_d&gt;$ be an $d$ dimension random unit vector, i.e., a vector generated uniformly at random from the sphere of the unit ball. What is the probability that $|x_1| \ge \frac{1}{n}$ for some given $n \ge 2$?</p>
<p>When $d = 1$, then $x_1 = 1$ or $x_1 = -1$. Therefore $P[|x_1| \ge \frac{1}{n}] = 1$.</p>
<p>When $d = 2$, </p>
<p>$$<br>P[|x_1| \ge \frac{1}{n}] = \frac{4}{2 \pi} \int_{1/n}^1 \sqrt{1 - t^2} d_t = \frac{2}{\pi} \arccos \frac{1}{n}<br>$$</p>
<p>By<br>$$<br>\begin{aligned}<br>\arcsin(z) &amp;= z + \left({\frac {1}{2} } \right) {\frac {z^{3} } {3} } +\left({\frac {1\cdot 3}{2\cdot 4} } \right){\frac {z^{5} } {5} } +\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6} } \right){\frac {z^{7} } {7} } +\cdots \\<br>&amp;=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!} } \cdot {\frac {z^{2n+1} } {2n+1} } \\<br>&amp;=\sum _{n=0}^{\infty }{\frac { {\binom {2n}{n} } z^{2n+1} } {4^{n}(2n+1)} } \qquad |z|\leq 1<br>\end{aligned}<br>$$</p>
<p>We get<br>$$<br>\begin{aligned}<br>\arccos \frac{1}{n} &amp;= \frac{\pi}{2} - \arcsin \frac{1}{n} \\<br>&amp;= \frac{\pi}{2} - (\frac{1}{n} + \left({\frac {1}{2} } \right) {\frac {\frac{1}{n}^{3} } {3} } +\left({\frac {1\cdot 3}{2\cdot 4} } \right){\frac {\frac{1}{n}^{5} } {5} } +\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6} } \right){\frac {\frac{1}{n}^{7} } {7} } +\cdots  ) \\<br>&amp;\ge \frac{\pi}{2} - \frac{1}{n} - \frac{1}{2 n^3}\left( \sum_{i = 0}^\infty 1 / 2^i \right)  \qquad since \ n \ge 2 \\<br>&amp;\ge  \frac{\pi}{2} - \frac{5}{4n}<br>\end{aligned}<br>$$</p>
<p>It follows that<br>$$<br>P[|x_1| \ge \frac{1}{n}] = \frac{2}{\pi} \arccos \frac{1}{n} \ge 1 - \frac{1}{n}<br>$$</p>
<p>For general values of $d$, we would like to give an easier estimation by upper bounding the  following integration </p>
<p>$$<br>\begin{aligned}<br>\begin{matrix}<br>\int_{x_1 = 0}^{1 / n} &amp; \underbrace{\int_{x_2 = 0}^{1} \int_{x_3 = 0}^{1} … \int_{x_d = 0}^{1} }  &amp; 1 \  d_{x_d} d_{x_{d - 1} }  … d_{x_1}<br>\le \frac{1}{n} &amp;\underbrace{\int_{x_2 = 0}^{1} \int_{x_3 = 0}^{1} … \int_{x_d = 0}^{1} }  &amp; 1 \  d_{x_d} d_{x_{d - 1} }  … d_{x_2} \\<br>&amp; x_2^2 + x_3^2 + … + x_d^2 \le 1 - x_1^2 &amp;<br>&amp; x_2^2 + x_3^2 + … + x_d^2 = 1 &amp;<br>\end{matrix}<br>\end{aligned}<br>$$</p>
<p>Using the result that the surface area of a $d - 1$ dimension sphere is given by</p>
<p>$$<br>{\frac {2\pi ^{\frac {d}{2} } }{\Gamma \left({\frac {d}{2} } \right)} }<br>$$</p>
<p>We get<br>$$<br>\begin{aligned}<br>    P[|x_1| \ge \frac{1}{n}]<br>    &amp;= 1 - \frac{1}{n} \frac{ {\frac {2\pi ^{\frac {d}{2} } }{\Gamma \left({\frac {d}{2} } \right)} } }{ {\frac {2\pi ^{\frac {d + 1}{2} } }{\Gamma \left({\frac {d + 1}{2} } \right)} } } \<br>    &amp;= 1 - \frac{1}{n} \frac{ \Gamma \left({\frac {d + 1}{2} } \right)}{ \Gamma \left({\frac {d}{2} } \right) \pi^{\frac {1}{2} } } \<br>    &amp;\ge 1 - \sqrt{ \frac{d}{2\pi} } \frac{e}{n}<br>\end{aligned}<br>$$</p>
<p>where by Stirling’s formula<br>$$<br>\begin{aligned}<br>    \frac{ \Gamma \left({\frac {d + 1}{2} } \right)}{ \Gamma \left({\frac {d}{2} } \right)} &amp;\le (\frac{d + 1}{2})^{\frac {d + 1}{2} - 1 / 2} e^{1 / (12 (\frac {d + 1}{2}))}  / (\frac{d}{2})^{\frac{d}{2} - 1 / 2} \<br>    &amp;= (1 + \frac{1}{d})^{d/2} e^{1 / (6(d + 1))} (\frac{d}{2})^{1 / 2} \<br>    &amp;\le e (\frac{d}{2})^{1 / 2}<br>\end{aligned}<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/08/23/Unbiased%20Estimator%20of%20Sampling%20Variance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/23/Unbiased%20Estimator%20of%20Sampling%20Variance/" class="post-title-link" itemprop="url">Unbiased Estimator of Sampling Variance</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-23 17:30:22" itemprop="dateCreated datePublished" datetime="2018-08-23T17:30:22+10:00">2018-08-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-22 11:44:17" itemprop="dateModified" datetime="2019-09-22T11:44:17+10:00">2019-09-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $X_1, X_2, …, X_n$ be a set of $i.i.d.$ random variables with mean $\mu$ and varaince $\sigma^2$. Define $\overline X = \frac{1}{n} \sum_i X_i$. Then </p>
<p>$$<br>\begin{aligned}<br>\sum_i (X_i - \overline X)^2<br>&amp;= \sum_i (X_i - \mu  - (\overline X - \mu))^2 \<br>&amp;= \sum_i (X_i - \mu)^2 - 2 \sum_i (X_i - \mu)(\overline X - \mu) + n (\overline X - \mu)^2 \<br>&amp;= \sum_i (X_i - \mu)^2 - n (\overline X - \mu)^2<br>\end{aligned}<br>$$</p>
<p>Define $S = \sum_i X_i$. Note that $S$ is a random variable with mean $n\mu$ and variance $n\sigma^2$. Therefore, </p>
<p>$$<br>\begin{aligned}<br>E[ \sum_i (X_i - \overline X)^2 ]<br>&amp;= \sum_i E[(X_i - \mu)^2] - n E[(\overline X - \mu)^2] \<br>&amp;= n \sigma^2 - \frac{1}{n} E \left[ (S - n\mu)^2 \right] \<br>&amp;= (n - 1) \sigma^2<br>\end{aligned}<br>$$</p>
<p>Another way of looking this is that,<br>$$<br>\begin{aligned}<br>\frac{1}{n}\sum_i (X_i - \overline X)^2<br>&amp;= \frac{1}{n} (\sum_{i} X_i^2 - 2 \overline X (\sum_{i} X_i) + n(\overline X)^2) \<br>&amp;= \frac{1}{n}( \sum_{i} X_i^2 - n(\overline X)^2 ) \<br>&amp;= \frac{1}{n} \sum_{i} X_i^2 - \overline X^2 \<br>&amp;= \frac{1}{n} \sum_{i} X_i^2 - \mu^2 - (\overline X^2 - \mu^2)<br>\end{aligned}<br>$$<br>Therefore,<br>$$<br>\begin{aligned}<br>E[\frac{1}{n}\sum_i (X_i - \overline X)^2]<br>&amp;= \sigma^2 - \frac{1}{n} \sigma^2<br>\end{aligned}<br>$$</p>
<p>We can also prove this via linear algebra. Let $X = (X_1, X_2, …, X_n)$ be a $n$-dimension vector. Also, denote $e = (1, 1, …, 1)$. Then </p>
<p>$$<br>\begin{aligned}<br>\overline X &amp;= \frac{1}{n} e^T X  \<br>\sum_{i} (X_i - \overline X)^2 &amp;= (X - \frac{1}{n} e e^T X)^T (X -  \frac{1}{n} e e^T X) \<br>&amp;= X^T (I -  \frac{1}{n} e e^T)^T (I -  \frac{1}{n} e e^T)X \<br>&amp;= X^T (I -  \frac{1}{n} e e^T)(I -  \frac{1}{n} e e^T)X \<br>&amp;= X^T (I -  2 \frac{1}{n} e e^T +  \frac{1}{n^2 } e e^T e e^T)X \<br>&amp;= X^T (I -  2 \frac{1}{n} e e^T +  \frac{1}{n} e e^T)X \<br>&amp;= X^T (I -  \frac{1}{n} e e^T)X \<br>&amp;= Tr[X^T (I -  \frac{1}{n} e e^T)X] \<br>&amp;= Tr[(I -  \frac{1}{n} e e^T) X X^T]<br>\end{aligned}<br>$$</p>
<p>Since $E[X X^T] = \mu^2 e e^T + \sigma^2 I$, </p>
<p>$$<br>\begin{aligned}<br>E[\sum_{i} (X_i - \overline X)^2] &amp;= Tr[(I -  \frac{1}{n} e e^T) E[X X^T]] \<br>&amp;=Tr[(I -  \frac{1}{n} e e^T)(\mu^2 e e^T + \sigma^2 I)] \<br>&amp;=Tr[(ee^T  - ee^T)\mu^2 + (I -  \frac{1}{n} e e^T)\sigma^2] \<br>&amp;=Tr[(I -  \frac{1}{n} e e^T)\sigma^2] \<br>&amp;=Tr[I -  \frac{1}{n} e e^T] \sigma^2<br>\end{aligned}<br>$$</p>
<p>where $n - 1$ is the trace as well as the rank of $I - \frac{1}{n}e e^T$. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/08/02/Empty-Bins/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/02/Empty-Bins/" class="post-title-link" itemprop="url">Empty Bins</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-08-02 14:06:16 / Modified: 17:29:56" itemprop="dateCreated datePublished" datetime="2018-08-02T14:06:16+10:00">2018-08-02</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose we throw $m$ balls into $n$ bins uniformly at random, what is the probability that at least $\epsilon n$ ($0 &lt; \epsilon &lt; 1$) bins remain empty? </p>
<p>When we throw a ball, if there are at least $\epsilon n$ empty bins, we increase the number of non-empty bins with probability at least $\epsilon$. Let $X_1, X_2, …, X_m$ be $m$ i.i.d Bernoulli random variable that takes $1$ with probability $\epsilon$. The event that $\epsilon n$ empty bins remains is upper bounded by<br>$$<br>\Pr [\sum_{i=1}^m X_i \le (1 - \epsilon) n] = \Pr [\sum_{i=1}^m X_i \le \frac{\epsilon m - \epsilon m + n - \epsilon n}{\epsilon m} \epsilon m] = \Pr [\sum_{i=1}^m X_i \le (1 - (1 - \frac{(1 - \epsilon ) n}{\epsilon m}) \frac{}{} \epsilon m]<br>$$</p>
<p>When $\epsilon$ is fixed, by setting $\frac{1}{\lambda} \doteq \frac{n}{m} =\frac{\epsilon}{1 - \epsilon} (1 - \sqrt{\frac{2n }{\epsilon m} })$ (this is solvable since the left hand side is increasing with $1 / \lambda$ while the right hand side is decreasing with $1 / \lambda$), we have $(1 - \frac{(1 - \epsilon) n}{\epsilon m })^2\frac{\epsilon m}{2} =  n$. Then by Chernoff bound we get<br>$$<br>\Pr [\sum_{i=1}^m X_i \le (1 - \epsilon) n] \le  = \exp (-n)<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/22/Connectivity-Parameters/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/22/Connectivity-Parameters/" class="post-title-link" itemprop="url">Connectivity Parameters</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-07-22 11:10:56 / Modified: 13:53:52" itemprop="dateCreated datePublished" datetime="2018-07-22T11:10:56+10:00">2018-07-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given an un-directed graph $G = \langle V, E \rangle$ without self loops and duplicates, </p>
<p><strong>Connectivity</strong>. The connectivity of an edge $e = (u, v)$, denoted as $\lambda_e$,  is the minimum number of edges to remove to disconnect $u$ from $v$. </p>
<p><strong>Strength</strong>. The strength of an edge $e = (u, v)$, denoted as $s_e$, is the maximum positive integer $k$, s.t.,  it belongs to a $k$-connected vertex induced sub-graph. A sub-graph is called $k$-connected if its min-cut is at least $k$. </p>
<h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><ol>
<li>$\sum_{e \in E} 1 / s_e \le n - 1$, where $n = |V|$. </li>
<li>$\sum_{e \in E} 1 / \lambda_e \le n - 1$</li>
</ol>
<p><strong>Proof</strong>. We prove claim 1 by induction on the number of vertices $n$. When $n = 2$, $|E| = 1$, the strength of the only edge is $1$. Therefore $\sum_{e \in E} 1 / s_e = 1 / 1 = 1$. Suppose claim 1 holds for all graphs with $n - 1$ vertices. For a graph with $n$ vertices, consider a particular min-cut $(S, \bar{S}) \in V \times V$. If an edge $e = (u, v)$ is in the min-cut, then its strength is at most $\lambda = G(S, \bar{S})$ (the min-cut value), since we need to remove at most $\lambda$ edges to disconnect $u$ from $v$ in arbitrary vertex induced sub-graph that contains $e$. But the original graph $G$ contains $e$ and has connectivity $\lambda$. It follows that the strength of $e$ is $\lambda$ and </p>
<p>$$<br>\begin{aligned}<br>\sum_{e \in E} \frac{1}{s_e}<br>&amp;= \sum_{e \in G_S} \frac{1}{s_e} + \sum_{e \in G_{\bar{S} } } \frac{1}{s_e} + \sum_{e \in (S, \bar{S}) } \frac{1}{s_e} \<br>&amp;\le |S| - 1 + |\bar{S}| - 1 + \lambda \frac{1}{ \lambda } \<br>&amp;\le n - 1<br>\end{aligned}<br>$$ </p>
<p>Finally, similar argument applies to claim 2. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/21/Graph%20Sparsification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/21/Graph%20Sparsification/" class="post-title-link" itemprop="url">Graph Sparsification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-21 19:52:33" itemprop="dateCreated datePublished" datetime="2018-07-21T19:52:33+10:00">2018-07-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-22 13:11:24" itemprop="dateModified" datetime="2018-07-22T13:11:24+10:00">2018-07-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Given a dense graph $G = \langle V,E \rangle$ and relative error threshold $\epsilon$,  we want to get a spare graph $G’ = \langle V, E’ \rangle$, such that $G’(S, \bar{S}) \in (1 \pm \epsilon) G(S, \bar{S})$ for any cut $(S, \bar{S}) \in V \times V$, where $G’(S, \bar{S})$ stands for the number of edges between $S$ and $S’$ in $G’$. </p>
<p>$G’$ can be constructed as follows: </p>
<blockquote>
<ol>
<li><p>Keep all vertices $V$ in $G’$, </p>
</li>
<li><p>Keep each edge $e \in E$ in $G’$ with probability $p$. If an edge is kept, set its weight to $1/p$. </p>
</li>
</ol>
</blockquote>
<p>We will determine the value of $p$ later. </p>
<p>There are many possible $G’$’s – precisely, $2^{|E|}$ of $G’$’s. At the first glimpse, it not clear what we get out of them. However, we will show that for a large fraction of them satisfy our requirement. In other words, we get a <em>good</em> $G’$ with high probability. </p>
<p>To show this, first observe that each edge has expected weight $p \cdot 1 / p + (1 - p) \cdot 0 = 1$. Therefore, by linearity of expectation, the expected value of each cut in $G’$ equals to its value in $G$, i.e., $E[G’(S, \bar{S})] = G(S, \bar{S})$, for $\forall (S, \bar{S}) \in V \times V$. </p>
<p>By proper choice of $p$, all $G’(S, \bar{S})$ will concentrate around its expectation. Moreover, the more edges a cut $(S, \bar{S}) \in G$ has, the more likely its expectation has a smaller deviation. This motivates that $p$ is set according to the min-cut of $G$. </p>
<p>Throughout discussion below, we denote $n$ as the number of vertices, $m$ the number of edges, and $c^*$ the min-cut value in $G$. </p>
<h4 id="Two-Important-Results"><a href="#Two-Important-Results" class="headerlink" title="Two Important Results"></a>Two Important Results</h4><p>The first one is the famous combinatorial ramification of Karger Algorithm.</p>
<p><strong>Theorem 1</strong>. The number of cuts whose weight are within $\alpha c^*$ is upper bounded by $n^{2 \alpha} \frac{2^{2\alpha}}{ (2\alpha)! } \frac{e}{2 \pi \sqrt 2}$, where $\alpha \ge 1, 2 \alpha \in N^+$.</p>
<p> Define $f(x)$ ($x \in R, x \ge 1$) the number of cuts in $G’$ with value $x c^*$, where $x c^*$ is an integer. $F(\alpha) = \sum_{x \le \alpha} f(x)$ is bounded by $n^{2\alpha}$.</p>
<p>The second is Chernoff Bound. </p>
<p><strong>Theorem 2</strong>. Let ${ X_i }$ be a set of random variables and $X \doteq \Sigma X_i$, $E[X] = \mu$, then $Pr[|X - \mu| \ge \epsilon \mu] \le 2 \exp(- \epsilon^2 \mu /3)$.</p>
<p>By Chernoff bound, the probability that a min-cut derivate more than $\epsilon c^*$ is bounded by $2 \exp (-\epsilon^2 p c^* / 3)$. Similarly, the  bound for a cut with value $\alpha c^*$ is $2 \exp (-\epsilon^2 \alpha p c^* / 3)$. </p>
<p>Denote $c’ = cp$. It follows by union bound that the probability that some cut deviate more than $\epsilon$ times it expected value is at most $\sum_x f(x) \cdot 2 \exp(-\epsilon^2xc’/3)$.</p>
<p>It can be shown that to maximum this value, we need $F(\alpha)$ takes the value $n^{2 \alpha}$ any for $\alpha \ge 1, 2\alpha \in N^+$. Hence the sum is less than<br>$$<br>2f(1)e^{-\epsilon^2 c’ /3} + 2 \int_{x = 1}^\infty {\partial n^{2x} \over \partial x} e^{-\epsilon^2 x c’/ 3} dx<br>$$</p>
<p>That is<br>$$<br>2 n^2 e^{-\epsilon^2 c’ /3}(1 + {\ln n^2 \over \epsilon^2 c’ /3 - \ln n^2})<br>$$</p>
<p>If we let $p = 3(d + 2) \ln n / (\epsilon^2 c)$ for some parameter d, then the bound becomes $O(n^{-d})$.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/21/Karger%20Algorithm%20I/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/21/Karger%20Algorithm%20I/" class="post-title-link" itemprop="url">Karger Algorithm I</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-21 12:53:14" itemprop="dateCreated datePublished" datetime="2018-07-21T12:53:14+10:00">2018-07-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-09-04 01:30:08" itemprop="dateModified" datetime="2018-09-04T01:30:08+10:00">2018-09-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Karger Algorithm</p>
<p>The Karger algorithm was proposed by David R. Karger [<em>Global Min-cuts in RNC, and Other Ramifications of a Simple Min-Cut Algorithm</em>]. It is a randomized algorithm to find a min-cut of an undirected connected graph. </p>
<p>The idea of the algorithm is simple – randomly pick an edge from the graph, and merge the endpoints of this edge into one. The edges incident on two merged endpoints is now incident on the new endpoint. Repeat this process until two nodes are left on the graph. We show that the probability that the number of edges remained between the two nodes equals the value of min-cut is at least $\frac{2}{n(n - 1)}$. </p>
<p>Formally, suppose we are given an undirected graph $G = (V, E)$ with $n = |V|$ vertices, $m = |E|$ edges, and $c^*=$ <em>size of the min-cut</em>.</p>
<h4 id="Karger-Algorithm"><a href="#Karger-Algorithm" class="headerlink" title="Karger Algorithm"></a>Karger Algorithm</h4><ol>
<li>Select an edge $e = (u,v) \in E$ uniformly with probability $1/|E|$.  </li>
<li>Contract $u$ and $v$ into a new vertex $w$, and all edges – except $e$– incident on $u$ or $v$ are now incident on $w$. In other words, </li>
</ol>
<p>$$<br> \begin{aligned}<br>  V  \leftarrow &amp; V /  {u,v} \cup {w}, \<br>  E  \leftarrow &amp; E /  {(x, y) | (x,y) \in E \ and \ x \in {u, v} }  \<br>   &amp; \cup {(w,y) | \exists  (x,y) \in E \ and \ x \in {u, v} \ and \ y \notin {u,v} }<br> \end{aligned}<br>$$</p>
<ol start="3">
<li>Repeat the above process until |V| = 2.</li>
</ol>
<p>In the end, there is only one cut between the remaining two vertices, whose size we denote as $c$. If follows that $P(c = c^*) = \frac{2}{n ( n - 1)}$. </p>
<p>By repeating the process $\frac{n (n - 1)}{2} \log n$ times, the probability we don’t find the size of min-cut is at most</p>
<p>$$<br>\big( 1 - \frac{2}{n (n - 1)} \big)^{\frac{n (n - 1)}{2} \log n} \le e^{log n} = \frac{1}{n}<br>$$</p>
<p>Now we prove that $P(c = c^*) = \frac{2}{n ( n - 1)}$. </p>
<p>The key observation is that the degree each vertex is at least $c^*$. Therefore there are at least $\frac{c^* n}{2}$ edges for a graph with $n$ vertices. Given a particular min-cut, which has $c^*$ edges, the probability that any of its edges is selected during the contraction operation is at most </p>
<p>$$<br>\frac{c^*}{|E|} \le \frac{c^*}{ \frac{c^* n}{2} } = \frac{2} { n }<br>$$</p>
<p>The second observation is that, if the edges of a min-cut are not selected during contraction, then the min-cut value of the contracted graph remains the same. After $i$ contractions, there are $n - i$ vertices and the probability a min-cut survives the $i + 1$ contraction is most $\frac{2}{n - i}$. After $n - 1$ round, the min-cut remains with probability at least<br>$$<br>(1 - \frac{2}{n})(1 - \frac{2}{n - 1})…(1 - \frac{2}{3}) = \binom{n}{2}^{-1}<br>$$<br>Some interesting corollaries could be derived from this. </p>
<h4 id="Corollary-one"><a href="#Corollary-one" class="headerlink" title="Corollary one"></a>Corollary one</h4><blockquote>
<p>There are $\frac{n (n - 1)}{ 2 }$ min-cuts in the graph, since the events of outputting each min-cut are disjoint. </p>
</blockquote>
<p>By slightly modifying the algorithm, we can give another fact.</p>
<p><strong>Definition</strong>. A $\alpha$-min-cut is a cut with no more than $\alpha c^*$ edges. </p>
<h4 id="Corollary-two"><a href="#Corollary-two" class="headerlink" title="Corollary two"></a>Corollary two</h4><p>For any $\alpha$ s.t. $2 \alpha$ is an integer, the number of $\alpha$-min-cuts is at most $n^{2 \alpha} \frac{2^{2\alpha}}{ (2\alpha)! } \frac{e}{2 \pi \sqrt 2}$, which is less than $n^{2 \alpha}$.</p>
<p><strong>Proof</strong>: We perform the contraction as before but stop when there are $2\alpha$ vertexes left. Then we output one the $2^{2\alpha - 1}$ cuts uniformly at random. </p>
<p>The probability a $\alpha$-min-cut is outputted with probability at least<br>$$<br>\begin{aligned}<br>&amp; (1 - \frac{2 \alpha }{n})(1 - \frac{ 2 \alpha }{ n-1 })…(1 - \frac{2 \alpha }{ 2\alpha + 1 }) \cdot \frac{1}{2^{2 \alpha - 1}}  \<br>&amp;=2 \frac{ (n - 2 \alpha)! (2 \alpha)! }{ 2^{2\alpha} n!  } \<br>&amp;\ge \frac{ 2 } { 2^{2\alpha} }<br>\sqrt{2 \pi (n - 2 \alpha)} \frac{(n - 2\alpha)^{(n - 2\alpha)} }{e ^{(n - 2\alpha)} }<br>\sqrt{2 \pi (2 \alpha)} \frac{(2\alpha)^{(2\alpha)} }{e ^{(2\alpha)} }<br>\frac{1}{e \sqrt n} \frac{e ^{n} }{ n^{n} } \<br>&amp;= \frac{ 2 \cdot 2\pi } { e \cdot 2^{2\alpha} }<br>\sqrt \frac{(n - 2\alpha) 2\alpha}{n}<br>\frac{ (2\alpha)^{(2\alpha)} (n - 2\alpha)^{(n - 2\alpha)} }{ n^n }<br>\end{aligned}<br>$$<br>As $\frac{(n - 2\alpha) 2\alpha}{n}$ is minimized to $\frac{1}{2}$ for $n \ge 2, \ 1 \le 2 \alpha \le n - 1$, the above inequality becomes<br>$$<br>\ge \frac{ \sqrt{2} \cdot 2\pi \cdot (2\alpha)^{(2\alpha)}} { e \cdot 2^{2\alpha} }<br>\frac{ 1 }{ n^{(2\alpha)} }<br>$$<br>The number of $\alpha$-min-cuts is bounded by<br>$$<br>n^{2 \alpha} \frac{2^{2\alpha}}{ (2\alpha)! } \frac{e}{2 \pi \sqrt 2}<br>$$<br><strong>Remark</strong>:<br>For $k \in N$, we have<br>$$<br>\sqrt{2 \pi k} \cdot \frac{k^k}{e^k} \le k! \le e \sqrt k \cdot \frac{k^k}{e^k}<br>$$</p>
<h4 id="Corollary-three"><a href="#Corollary-three" class="headerlink" title="Corollary three"></a>Corollary three</h4><p>The number of $\alpha$-min-cuts is at most $e^{7/6} n^{2\alpha} \frac{2^{ 2 \alpha} }{( 2 \alpha + 1 )^ {2 \alpha} }$ for any $\alpha \ge 1$. </p>
<p><strong>Proof</strong>: We perform the contraction as before but stop when there are $\lceil 2\alpha \rceil$ vertexes left. Then we output one the $2^{\lceil 2\alpha \rceil - 1}$ cuts uniformly at random. </p>
<p>The probability a $\alpha$-min-cut is outputted with probability at least<br>$$<br>\begin{aligned}<br>&amp; (1 - \frac{2 \alpha}{n }) (1 - \frac{2 \alpha}{n - 1 }) … (1 - \frac{2 \alpha}{\lceil 2 \alpha \rceil + 1 }) 2^{ - (\lceil 2 \alpha \rceil -1)} \<br>&amp;= \frac{(n - 2 \alpha) (n - 1 - 2 \alpha) … (\lceil 2 \alpha \rceil + 1 - 2 \alpha)}{ n (n - 1) …(\lceil 2 \alpha \rceil + 1)} 2^{ - (\lceil 2 \alpha \rceil -1)} \<br>&amp;= \frac{\Gamma(n - 2 \alpha + 1)}{ \Gamma(\lceil 2 \alpha \rceil + 1 - 2 \alpha) } \frac{\Gamma(\lceil 2 \alpha \rceil + 1)}{\Gamma(n + 1)} 2^{ - (\lceil 2 \alpha \rceil -1)} \<br>&amp;\ge \sqrt{2\pi} \frac{1}{\sqrt {n - 2 \alpha + 1} }(\frac{n - 2 \alpha + 1}{e})^ {n - 2 \alpha + 1} \<br>&amp;\ \ \cdot \frac{1}{\sqrt {2 \pi} e^{1 / 12 (\lceil 2 \alpha \rceil + 1 - 2 \alpha ) } } \sqrt{\lceil 2 \alpha \rceil + 1 - 2 \alpha)} (\frac{e}{\lceil 2 \alpha \rceil + 1 - 2 \alpha})^ {\lceil 2 \alpha \rceil + 1 - 2 \alpha} \<br>&amp;\ \ \cdot \sqrt{2\pi} \frac{1}{ \sqrt{\lceil 2 \alpha \rceil + 1} }(\frac{ \lceil 2 \alpha \rceil + 1 }{e})^ {\lceil 2 \alpha \rceil + 1} \<br>&amp;\ \ \cdot \frac{1}{\sqrt {2 \pi} e^{1 / 12 (n + 1) } } \sqrt{(n + 1)} (\frac{e}{n + 1})^ {n + 1} 2^{ - (\lceil 2 \alpha \rceil -1)} \<br>&amp;= \frac{1}{e^{2 / 12}} \frac{\sqrt{\lceil 2 \alpha \rceil + 1 - 2 \alpha} }{ \sqrt {n - 2 \alpha + 1} } \frac{\sqrt{(n + 1)} }{\sqrt{\lceil 2 \alpha \rceil + 1}}<br>e^{(\lceil 2 \alpha \rceil + 1 - 2 \alpha ) - (n - 2 \alpha + 1) + (n + 1) - (\lceil 2 \alpha \rceil + 1)} \<br>&amp;\ \ \cdot (\frac{n - 2 \alpha + 1}{n + 1})^ {n - 2 \alpha + 1} (\frac{1}{n + 1})^{2\alpha} \cdot<br>\frac{( \lceil 2 \alpha \rceil + 1 )^ {\lceil 2 \alpha \rceil + 1} }{ (\lceil 2 \alpha \rceil + 1 - 2 \alpha)^ {\lceil 2 \alpha \rceil + 1 - 2 \alpha}} 2^{ - (\lceil 2 \alpha \rceil -1)} \<br>&amp;\ge  \frac{1}{e^{1 / 6}} (\frac{1}{n + 1})^{2\alpha} \cdot<br>\frac{( \lceil 2 \alpha \rceil + 1 )^ {\lceil 2 \alpha \rceil + 1 / 2} }{ (\lceil 2 \alpha \rceil + 1 - 2 \alpha)^ {\lceil 2 \alpha \rceil + 1 / 2 - 2 \alpha}} 2^{ - (\lceil 2 \alpha \rceil -1)} \<br>&amp;\ge \frac{1}{e^{1 / 6}} (\frac{1}{n + 1})^{2\alpha} \cdot<br>\frac{( \lceil 2 \alpha \rceil + 1 )^ {2 \alpha} }{2^{ 2 \alpha} } \<br>&amp;= \frac{1}{e^{1 / 6}} \frac{1}{n^{2\alpha}} \frac{1}{(1 + 1/n)^{2\alpha}}<br>\frac{( \lceil 2 \alpha \rceil + 1 )^ {2 \alpha} }{2^{ 2 \alpha} } \<br>&amp;\ge \frac{1}{e^{7/6} } \frac{1}{n^{2\alpha}} \frac{( \lceil 2 \alpha \rceil + 1 )^ {2 \alpha} }{2^{ 2 \alpha} } \<br>&amp;\ge \frac{1}{e^{7/6} } \frac{1}{n^{2\alpha}} \frac{( 2 \alpha + 1 )^ {2 \alpha} }{2^{ 2 \alpha} }<br>\end{aligned}<br>$$</p>
<p>To bound the inequality, we need the stirling formula for Gamma functions. </p>
<p><strong>Lemma</strong>. $\forall x \ge 0$,<br>$$<br>\Gamma(x + 1) = x \Gamma(x) \<br>\sqrt{2\pi} \frac{1}{\sqrt x} (\frac{x}{e})^x<br>\le<br>\Gamma(x)<br>\le<br>\sqrt{2\pi} \frac{1}{\sqrt x} (\frac{x}{e})^x e^{\frac{1}{12 x}}<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/20/Random-Points-on-Shpere/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/20/Random-Points-on-Shpere/" class="post-title-link" itemprop="url">Random Points on Shpere</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-20 10:45:18" itemprop="dateCreated datePublished" datetime="2018-07-20T10:45:18+10:00">2018-07-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-09-16 18:08:01" itemprop="dateModified" datetime="2018-09-16T18:08:01+10:00">2018-09-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>To generate a random unit vector in $R^n$, the most trivial approach is to use spherical coordinates and generate the angles uniformly at random. Here we introduce another way : sample $n$ independent identical random variables from Gaussian distribution $N(0, 1)$, denoted as $x_1, x_2, …, x_n$. Then a unit vector $v$ is obtained by </p>
<p>$$<br>v = \frac{1}{\sum_{i = 1}^n x_i^2 }(x_1, x_2, …, x_n)<br>$$</p>
<p>The key observation is that all points at a fixed distance from the origin is equally generated. To see this, the joints distribution of $x_1, x_2, …, x_n$ is given by </p>
<p>$$<br>p = \frac{1}{(2 \pi)^{n / 2} } \exp \big( - \frac{\sum_{i=1}^n x_i^2 }{ 2 } \big)<br>$$</p>
<p>$p$ is fixed when $\sum_{i = 1}^n x_i^2$ is fixed. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/18/Hashing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/18/Hashing/" class="post-title-link" itemprop="url">Hashing</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-18 15:07:42" itemprop="dateCreated datePublished" datetime="2018-07-18T15:07:42+10:00">2018-07-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-03 00:17:49" itemprop="dateModified" datetime="2020-05-03T00:17:49+10:00">2020-05-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $S = { x_1, x_2, …, x_m }$ be a set of distinct element from domain $U$. A function $h: U \rightarrow [n]$ that takes element from $U$ to a smaller domain $[n]$ is called a hash function, where $[n] = { 0, 1, …, n - 1}$ and $n \le |U|$. In the case where $n &lt; |U|$, there must be some $i \in [n]$, s.t., $|h^{-1}(i)| &gt; 1$. We are interested in the number of total duplicate value in $h(S) = { h(x_1), h(x_2), …, h(x_m) }$. </p>
<p>If both $S$ and $h$ are fixed, there is nothing uncertain. There are two ways to add some randomness to our analysis</p>
<ol>
<li><p>$h$ is fixed, and $S$ follows some distribution on $U$. As an example, let $U = [0, 2^{20} - 1]$ and $h(x) = x \ &amp; \ 1023$ that returns the last $10$ bits of $x$ (in binary form). If $S$ is sampled uniformly at random from $U$, then on average for each $x_i \in S$, there are $\frac{m - 1}{1024}$ elements that have the same value as $x_i$ under $h$. </p>
</li>
<li><p>The assumption that $S$ follows some distribution is too strong. Instead we assume that given $S$, we select a function $h$ uniformly, from a set of functions $\mathcal{H}$. Now we are going to analyze the probability $P[ h(x_i) = h(x_j) ]$ for $i \neq j$. </p>
</li>
</ol>
<p>Let $p &gt; |U|$ be a prime number and<br>$$<br>\mathcal{H} \doteq { h(x) = ax + b \mod p \mod n | 1 \le a &lt; p, 0 \le b &lt; p }<br>$$</p>
<p>It follows that if $h$ is sampled uniformly at random, $P[h(x_i) = h(x_j)] \le 1 / n$ for $i \neq j$. </p>
<h5 id="Lemma-1"><a href="#Lemma-1" class="headerlink" title="Lemma 1."></a>Lemma 1.</h5><p>For $x_i \neq x_j$, and $s, t \in [p], s \neq t$, there is a unique pair $(a,b) \in [p]^+ \times [p]$<br>$$<br>    a x_i + b \mod p = s \<br>    a x_j + b \mod p = t<br>$$<br>where $[p]^+ \doteq {1, 2, …, p-1 }$. </p>
<p><strong>Proof</strong>: Since $[p]$ is a finite filed and $x_i \neq x_j$, the matrix </p>
<p>$$<br>\begin{bmatrix}<br>x_i \mod p, 1 \<br>x_j \mod p, 1<br>\end{bmatrix} \in [p]^{2 \times 2}<br>$$</p>
<p>is invertible. Therefore, </p>
<p>$$<br>\begin{bmatrix}<br>a \<br>b<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>x_i \mod p, 1 \<br>x_j \mod p, 1<br>\end{bmatrix}^{-1}<br>\begin{bmatrix}<br>s \<br>t<br>\end{bmatrix}<br>$$<br>is unique. Moreover, $a \neq 0$. Otherwise $b = s$ and $b = t$, a contradiction to $s \neq t$. </p>
<h5 id="Lemma-2"><a href="#Lemma-2" class="headerlink" title="Lemma 2."></a>Lemma 2.</h5><p>For $x_i \neq x_j$,<br>$$<br>\begin{aligned}<br>P[h(x_i = x_j)]<br>&amp;= \sum_{s, t \in [p], s \neq t, s \mod n = t \mod n} P[ h(x_i) = s \wedge h(x_j) = t] \<br>&amp;= \sum_{s, t \in [p], s \neq t, s = t \mod n} \frac{1}{ (p-1)p } \<br>&amp;= \sum_{s \in [p]} \sum_{t \in [p], t \neq s, t = s \mod n} \frac{1}{ (p-1)p } \<br>&amp;\le \sum_{s \in [p]} \frac{p - 1}{ n } \frac{1}{ (p-1)p } \<br>&amp;= p \frac{p - 1}{ n } \frac{1}{ (p-1)p } \<br>&amp;= \frac{1}{ n }<br>\end{aligned}<br>$$</p>
<h5 id="Lemma-3"><a href="#Lemma-3" class="headerlink" title="Lemma 3."></a>Lemma 3.</h5><p>The expected number of pair $x_i, x_j$, s.t., $x_i \neq x_j \wedge h(x_i) = h(x_j)$ is given by </p>
<p>$$<br>E[\sum_{x_i \neq x_j} 1_{h(x_i) = h(x_j)}] = \sum_{x_i \neq x_j} P[h(x_i = x_j)] = \binom{m}{2} \frac{1}{n} = \frac{m (m - 1)}{ 2n }<br>$$</p>
<p>When $m^2 \le n$, then $E[\sum_{x_i \neq x_j} 1_{h(x_i) = h(x_j)}] \le \frac{1}{2}$. By Markov inequality, </p>
<p>$$<br>P[\exists \quad \text{collision}] \le \frac{1}{ 2 }<br>$$</p>
<p>Lemma 4. </p>
<p>With probability at most $1/2$, some bin contains more than $\frac{m}{n} + \sqrt{2m}$ balls. </p>
<p>For a fixed bin, let $Y$ denote the number of ball in this bin. Further, define<br>$$<br>Y_i = \begin{cases}<br>1, \qquad \text{if the } i \text{-th ball falls into the bin} \<br>0, \qquad \text{otherwise}<br>\end{cases}<br>$$<br>Then $Y = \sum_i Y_i$, and<br>$$<br>E[Y] = \frac{m}{n} \<br>$$<br>By pair-wise independence, we have<br>$$<br>Var[Y] = \sum_i Var[Y_i] = m \frac{1}{n} \left(1 - \frac{1}{n} \right)<br>$$<br>Therefore, by Chebyshev inequality, we have<br>$$<br>\Pr \left[ |Y - E[Y]| \ge \sqrt{2n} \sqrt{ m \frac{1}{n} \left(1 - \frac{1}{n} \right)} \right] = \Pr[|Y - E[Y]| \ge \sqrt{2n} \cdot \sigma] \le \frac{Var[Y]}{2n\sigma^2} = \frac{1}{2n}<br>$$<br>Finally, applying union bound over $n$ bins, we get<br>$$<br>\Pr[\text{some bin gets more than } \sqrt{2m} \text{ balls} ] \le \frac{1}{2}<br>$$</p>
<h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h5><ol>
<li>Sanjeev Arora, Cos 521: Advanced Algorithm Design, Lecture 1: Course Intro and Hashing</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/17/Sampling-Probability/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/17/Sampling-Probability/" class="post-title-link" itemprop="url">Sampling Probability</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-17 21:34:45" itemprop="dateCreated datePublished" datetime="2018-07-17T21:34:45+10:00">2018-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-18 00:19:26" itemprop="dateModified" datetime="2018-07-18T00:19:26+10:00">2018-07-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose we have a set of number ${x_1, x_2, …, x_n }$ and we would like to estimate its sum $s = \sum_i x_i$. However, we can only access the set ${x_1, x_2, …, x_n }$ by sampling an element $x_i, 1 \le i \le n$ randomly. Let $p_i$ be the probability that we get $x_i$. If $p_i$ is known (either before sampling or the moment we get a sampled element, depending on application), then we can still construct an unbiased estimator of $s$. To see this, define </p>
<p>$$<br>y_i = \frac{x_i} {p_i}<br>$$</p>
<p>and $Y$ a random variable that takes value $y_i$ with probability $p_i$. The mean of $Y$ is given by </p>
<p>$$<br>E[Y] = \sum_i p_i y_i = \sum_i p_i \frac{x_i} {p_i} = \sum_i x_i = s.<br>$$</p>
<p>Moreover, the variance of $Y$ is minimized when $p_i = \frac{x_i} { s }$ for all $i$. It follows that $y_i = \frac{ x_i } { x_i / s } = s$ is a constant and $Y$ has variance $0$ (which is the smallest variance possible as its must be non-negative).</p>
<blockquote>
<p>Remark: we can also verify this by formulating this as an optimization problem then using Lagrangian method. </p>
</blockquote>
<p>In some applications, some elements in ${x_1, x_2, …, x_n }$ may have same value. Suppose there are $m$ different value in ${x_1, x_2, …, x_n }$ and define $X_j ( 1 \le j \le m)$ the set of elements with the $j$-th largest value. Denote the sum of probability of elements in $X_j$ as $P_j = \sum_{x \in X_j} p_x$. Also, let $f(x_i)$ denote the group that element $x_i$ belongs to, i.e., $x_i \in X_{ f( x_i ) }$. Consider  </p>
<p>$$<br>y’_i = \frac{ x_i |X_{ f(x_i) }| } { P_{ f(x_i) } }<br>$$</p>
<p>and $Y’$ a random variable that takes value $y’_i$ with probability $p_i$. The mean of $Y’$ is also an unbiased estimator of $s$: </p>
<p>$$<br>\begin{aligned}<br>E[Y’]<br>&amp;= \sum_{i = 1}^n p_i y’<em>i \<br>&amp;= \sum</em>{i = 1}^n p_i \frac{ x_i |X_{ f(x_i) }| } { P_{ f(x_i) } } \<br>&amp;= \sum_{j = 1}^m \sum_{x \in X_j} p_x \frac{ x |X_{ f(x) }| } { P_j } \<br>&amp;= s<br>\end{aligned}<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/07/04/Fabonacci-Number/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/04/Fabonacci-Number/" class="post-title-link" itemprop="url">Fabonacci Number</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-04 11:05:16" itemprop="dateCreated datePublished" datetime="2018-07-04T11:05:16+10:00">2018-07-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-01 23:29:59" itemprop="dateModified" datetime="2020-11-01T23:29:59+11:00">2020-11-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Consider the Fibonacci sequence ${ F_i }_{i \ge 1}$:<br>$$<br>1, 1, 2, 3, 5, 8, 13, 21, ….<br>$$</p>
<p>which is defined recursively as follows:<br>$$<br>\begin{cases}<br>    F_1 = 1 \<br>    F_2 = 1 \<br>    F_{n + 2} = F_{n + 1} + F_{n} \quad  n \ge 1<br>\end{cases}<br>$$</p>
<p>We discuss some techniques for computing the (approximate) closed form formula for $F_n$ for $n \ge 1$. </p>
<h4 id="Method-1"><a href="#Method-1" class="headerlink" title="Method 1"></a><strong><em>Method 1</em></strong></h4><p>We guess that $F_n$ is of closed form as $F_n = z^n$ for some $z \in \R$. As $F_{n + 2} = F_{n + 1} + F_n$,  $z$ needs to satisfies<br>$$<br>z^2 = z + 1<br>$$</p>
<p>The quadratic equation has two roots,  $z_1 = \frac{1 + \sqrt 5}{2}$ and $z_1 = \frac{1 - \sqrt 5}{2}$ respectively. </p>
<p>Let $a, b \in \R$. $F_n = a z_1^n + b z_2^n$ is also a possible solution that satisfies $F_{n + 2} = F_{n + 1} + F_n$. We can compute the exact value of $a$ and $b$ by the constraints $F_1 = F_2 = 1$:<br>$$<br>a z_1 + b z_2 = 1 \<br>a z_1^2 + b z_2^2 = 1<br>$$</p>
<p>i.e.,<br>$$<br>a (1 + \sqrt 5) + b (1 - \sqrt 5) = 2 \<br>a (6 + 2\sqrt 5) + b (6 - 2 \sqrt 5) = 4<br>$$</p>
<p>Therefore $a = 1  / \sqrt 5$, $b = -1 / \sqrt 5$, and<br>$$<br> F_n = \frac{ (\frac{1 + \sqrt 5} {2})^n  - (\frac{1 - \sqrt 5} {2})^n }{\sqrt 5}<br>$$</p>
<p>$\blacksquare$</p>
<h4 id="Method-2"><a href="#Method-2" class="headerlink" title="Method 2"></a><strong><em>Method 2</em></strong></h4><p>Consider the intervals derived from the sequences:<br>$$<br>    1, 1, 2, 3, 5, 8, 13, 21, ….<br>$$</p>
<p>$$<br>    \rightarrow [\frac{1}{1}, \frac{2}{1} ], \quad [ \frac{3}{2} , \frac{5}{3} ], \quad [ \frac{8}{5} , \frac{13}{8} ], \quad [ \frac{21}{13}, \frac{34}{21} ], …</p>
<p>$$</p>
<p>Define $x_n = \frac{F_{n + 1} }{F_n}$ and denote $I_n$ the $n$-th interval. Then<br>$$<br>I_n = [x_{2n - 1}, x_{2n } ]<br>$$</p>
<p>By observation of the first few $I_n$’s, they seems to be nested closed intervals, with shrinking lengths. If the length of $I_n$ converges to $0$, then the limit of $x_n$ exists. Suppose that<br>$$<br>\lim_{n} x_n = \phi<br>$$</p>
<p>Then<br>$$<br>\lim_n F_n = O(\phi^n )<br>$$</p>
<p>We will prove this in what follows. </p>
<p>The key transformation we performed is<br>$$<br>x_{n} = \frac{ F_{n + 1} }{F_{n} } = \frac{ F_{n } + F_{n - 1 } }{ F_{n} } = 1 + \frac{1}{ x_{ n - 1} }.<br>$$</p>
<h5 id="Lemma-1"><a href="#Lemma-1" class="headerlink" title="Lemma 1"></a>Lemma 1</h5><p>$$<br>        x_1 &lt; x_3 &lt; x_5 &lt; x_7 &lt;  … \<br>        x_2 &gt; x_4 &gt; x_6 &gt; x_8 &gt;  … \<br>$$</p>
<p><em>Proof:</em> For $n \ge 1$,<br>$$<br>x_{2n + 2} - x_{2n} = \frac{1}{ x_{2n + 1} } - \frac{1}{ x_{2n - 1} } = \frac{ x_{2n - 1} - x_{2n + 1} }{ x_{2n + 1} x_{2n - 1} }.<br>$$</p>
<p>Similarly,<br>$$<br>x_{2n + 1} - x_{2n - 1} = \frac{1}{ x_{2n} } - \frac{1}{ x_{2n - 2} } = \frac{ x_{2n - 2} - x_{2n} }{ x_{2n} x_{2n - 2} }.<br>$$</p>
<p>As $x_3 - x_1 = 3 /2 - 1 &gt; 0$, and $x_4 - x_2 = 5 / 3 - 2 &lt; 0$, the lemma follows from induction. </p>
<p>$\square$</p>
<h5 id="Lemma-2-For-n-ge-1"><a href="#Lemma-2-For-n-ge-1" class="headerlink" title="Lemma 2. For $n \ge 1$,"></a>Lemma 2. For $n \ge 1$,</h5><p>$$<br>x_{2n} - x_{2n - 1} \ge 0<br>$$</p>
<p><em>Proof:</em> Clearly this holds for $n = 1$. Consider $n \ge 2$,<br>$$<br>x_{2n} - x_{2n - 1} = \frac{1}{x_{2n - 1} } - \frac{1}{x_{2n - 2} }  = - \frac{ x_{2n - 1} - x_{2n - 2}   }{x_{2n - 1} \cdot x_{2n - 2}} =  \frac{ x_{2n - 2} - x_{2n - 3}   }{x_{2n - 1} \cdot x_{2n - 2} \cdot x_{2n - 2} \cdot x_{2n - 3} }<br>$$</p>
<p>The claim follows from induction. </p>
<h5 id="Corollary-2-For-n-ge-2"><a href="#Corollary-2-For-n-ge-2" class="headerlink" title="Corollary 2. For $n \ge 2$,"></a>Corollary 2. For $n \ge 2$,</h5><p>$$<br>\begin{aligned}<br>    I_n &amp;= x_{2n} - x_{2n - 1} \<br>        &amp;= \frac{ x_{2n - 2} - x_{2n - 3}   }{x_{2n - 1} \cdot x_{2n - 2} \cdot x_{2n - 2} \cdot x_{2n - 3} } \<br>        &amp;\le \frac{ x_{2n - 2} - x_{2n - 3}   }{ x_{2n - 1} } \<br>        &amp;\le \frac{ x_{2n - 2} - x_{2n - 3}   }{ x_3 } \<br>        &amp;= \frac{2}{3} (x_{2n - 2} - x_{2n - 3}) \<br>        &amp;= \frac{2}{3} I_{n - 1}<br>\end{aligned}<br>$$</p>
<p>Hence $\lim_n |I_n| = 0$. </p>
<p>$\square$</p>
<p>Combining lemma 1 and 2, we conclude that $\lim x_n = \phi$ exists. As<br>$$<br>\lim x_n = \phi = \lim (1 + \frac{1}{x_{n - 1} } ) = 1 + \frac{1}{\phi }<br>$$</p>
<p>We conclude that $\phi = \frac{ 1 + \sqrt{5} }{2 }$.  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/06/20/Bennett-s-Inequality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/20/Bennett-s-Inequality/" class="post-title-link" itemprop="url">Bennett's Inequality</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-20 21:56:51" itemprop="dateCreated datePublished" datetime="2018-06-20T21:56:51+10:00">2018-06-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-17 15:31:00" itemprop="dateModified" datetime="2020-07-17T15:31:00+10:00">2020-07-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Let $X$ be a random variable such that $X \le M$ and $E[X]= \mu$. Let $X_1, X_2, …, X_n$ be a set of $i.i.d$ copies of $X$ and $\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i$. Then<br>$$<br>\begin{aligned}<br>P { \overline{X} \ge \mu + t}<br>&amp;= P{ e^{\lambda \overline{X} } \ge e^{\lambda (\mu + t)} } \<br>&amp;\le \frac{E[e^{\overline{\lambda X} }]}{e^{\lambda(\mu + t)} } \<br>&amp;= \frac{E[e^{\lambda\sum_{i=1}^n X_i} ]}{e^{\lambda n (\mu + t)} } \<br>&amp;= \frac{ \prod_{i=1}^n E[e^{\lambda X} ]}{e^{\lambda n (\mu + t)} } \<br>&amp;= \frac{ \prod_{i=1}^n  \sum_{j=1}^\infty \frac{\lambda^j E[X^j]}{j!}  }{e^{\lambda n (\mu + t)} } \<br>&amp;= \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \sum_{j=2}^\infty \frac{\lambda^j E[X^{j-2}X^2]}{j!})  }{e^{ \lambda n(\mu + t)} } \<br>&amp;\le \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \sum_{j=2}^\infty \frac{\lambda^j M^{j-2} E[X^2]}{j!})  }{e^{\lambda  n (\mu + t)} } \<br>&amp;\le \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \frac{E[X^2]}{M^2} \sum_{j=2}^\infty \frac{\lambda^j M^{j} }{j!})  }{e^{\lambda   n (\mu + t)} } \<br>&amp;\le \frac{ \prod_{i=1}^n  （1 + \lambda \mu + \frac{E[X^2]}{M^2} (e^{\lambda M} - 1 - \lambda M))  }{e^{\lambda  n (\mu + t)} } \<br>&amp;= \big( \frac{  e^{\lambda \mu + \frac{E[X^2]}{M^2} (e^{\lambda M} - 1 - \lambda M )} }{e^{\lambda   (\mu + t)} } \big)^n \<br>&amp;= \big( e^{ -\lambda t + \frac{E[X^2]}{M^2} (e^{\lambda M} - 1 - \lambda M )}  \big)^n \<br>\end{aligned}<br>$$</p>
<p>By setting the derivative of $\lambda$ to 0, we get<br>$$<br>-t + {E[X^2] \over M^2} ( M e^{\lambda M} - M) = 0 \rightarrow  \lambda = {1 \over M} \ln { {( {t M \over E[X^2] }  + 1 })}<br>$$<br>Therefore,<br>$$<br>\begin{aligned}<br>P { \overline{X} \ge \mu + t}<br>&amp;\le (e^{ - {t \over M} \ln { {( {t M \over E[X^2] }  + 1 } }) + \frac{E[X^2]}{M^2}{ {( {t M \over E[X^2] }  + 1  - 1 - \ln { {( {t M \over E[X^2] }  + 1 } })} })})^n \<br>&amp;= e^{\frac{nE[X^2]}{M^2}{ {( {t M \over E[X^2] }  - ({t M \over E[X^2] }  + 1 ) \ln { {( {t M \over E[X^2] }  + 1 } })} })} \<br>&amp;= e^{\frac{nE[X^2]}{M^2} h(\frac{tM}{E[X^2]})}<br>\end{aligned}<br>$$<br>where  $h(x) = x - (1 + x) \ln (1 + x)$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/05/21/SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/21/SVD/" class="post-title-link" itemprop="url">SVD</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-05-21 10:49:11" itemprop="dateCreated datePublished" datetime="2018-05-21T10:49:11+10:00">2018-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-04 22:50:56" itemprop="dateModified" datetime="2020-01-04T22:50:56+11:00">2020-01-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Singular vector decomposition is one of the highlights of linear algebra. </p>
<p>Before diving into the topic, we have a brief review of linear transformation. Given a matrix $A \in R^{m \times n}$ with rank $r$, we can view it as a linear transformation from $R^n \rightarrow R^m: f(x) = Ax,  \forall x \in R^n$. It takes a vector in the row space, i.e., the subspace spanned by row vectors of $A$, to a vector in the column space, the subspace spanned by column vectors of $A$. </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Linear%20Mapping.jpg"></p>
<p>Denote the row space as $S(A^T) = { \sum_{i = 1}^n \lambda_i A[i,:], \lambda_i \in R, \forall i \in [n] }$, and the subspace perpendicular to $S(A^T)$ as $N(A^T) = { x \in R^m, s.t., Ax = 0}$. Similarly, we can define $S(A)$ the columns space of $A$ and $N(A)$ the space perpendicular to $S(A)$. Note that $S(A^T)$ and $S(A)$ has the same dimension $r$. For any $x \in R^m$, we can write $x = y + z$, such that $y \in S(A^T)$ and $z \in N(A^T)$. The operation $Ax = Ay + Az$, takes the $y$ component to a vector in $S(A)$ and the $z$ component to point $0$, as illustrated by the figure. </p>
<p><em>The goal of singular vector decomposition is to find an orthogonal base $v_1, v_2, …, v_r$ in $S(A^T)$, and an orthogonal base $u_1, u_2, …, u_r$, such that $f(v_i) = A v_i = \sigma_i u_i$ for all $i \in [r]$ and $\sigma_1 \ge \sigma_2 \ge … \ge \sigma_r &gt; 0$.</em></p>
<p>In some sense, there is a one to one correspondence between the orthogonal bases ${ v_1, v_2, …, v_r }$ and ${ u_1, u_2, …, u_r }$.</p>
<p>If we write $V = [v_1, v_2, …, v_r]$, $\Sigma = \left[ \begin{aligned} \begin{matrix} &amp;\sigma_1 \ &amp;&amp;\sigma_2 \ &amp;&amp;&amp; … \ &amp;&amp;&amp;&amp;\sigma_r \end{matrix} \end{aligned} \right]$ and $U = [u_1, u_2, …, u_r]$, then<br>$$<br>A V = U \Sigma<br>$$</p>
<p>As the columns of $V$ are orthogonal base, it is invertible and $V^{-1} = V^T$. Hence<br>$$<br>A = U \Sigma V^{-1} = U \Sigma V^T<br>$$</p>
<p>Expanding the expression gives:<br>$$<br>A = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + … + \sigma_r u_r v_r^T<br>$$</p>
<p>Further,<br>$$<br>A^T A = V \Sigma^2 V^T, A A^T = U \Sigma^2 U^T<br>$$<br>which implies that<br>$$<br>A^T A V = V \Sigma^2, A A^T U = U \Sigma^2<br>$$<br>The $v_i$’s and $u_i$’s are eigenvectors of $A^T A$ and $A A^T$ respectively. </p>
<h2 id="EXISTENCE"><a href="#EXISTENCE" class="headerlink" title="EXISTENCE"></a>EXISTENCE</h2><p><em>Proof Of The Existence of SVD</em>: </p>
<p>The key is to look at $A^T A$. Note that $A^TA$ is semi-positive and rank $r$. As $A^T A x = 0 \Leftrightarrow x A^T A x = 0 \Leftrightarrow Ax = 0$. It follows that $A^T A$ and $A$ has the same null space and therefore the same row space. Further, as $A^T A$ is semi-positive, it has $r$ positive eigenvalue. Denote the eigenvalues of $A^TA$ in decreasing order as $\lambda_1 \ge \lambda_2 \ge  … \ge \lambda_r &gt; 0$ and the corresponding (unit length) eigenvectors as $v_1, v_2, …, v_r$. The $v_i$’s are orthogonal, as</p>
<ul>
<li>For $i \neq j$, $v_i^T A^T A v_j = \lambda_i v_i^T v_j = \lambda_j v_i^T v_j$. As $\lambda_i \neq \lambda_j$, $v_i^T v_j = 0$. </li>
</ul>
<p>They constitute an orthogonal base of the row space of $A^T A$ and the row space $A$. </p>
<p>Now we use the $v_i$’s to search for $u_i$’s as follows: let $\sigma = \sqrt{\lambda_i}$, we claim the<br>$$<br>u_i = \frac{1}{\sigma_i} A v_i, \forall i \in [r]<br>$$<br>are what we want. </p>
<ol>
<li>The $u_i$’s are orthogonal: $u_i^T u_j = \frac{1}{\sigma_i \sigma_j} v_i^T A^T A v_j =   \frac{\sigma_j^2 }{\sigma_i \sigma_j} v_i^T v_j = 0$</li>
<li>The $u_i$’s are unite vectors: $u_i^T u_i = \frac{1}{\sigma_i^2} v_i^T A^T A v_i = 1$</li>
</ol>
<p>Hence $u_i$’s are an orthogonal base. As the columns of $A$ has dimension $r$, the $u_i$’s covers the entire columns space. This completes the proof.<br>$\blacksquare$</p>
<h2 id="GEOMETRIC-INTERPRETATION"><a href="#GEOMETRIC-INTERPRETATION" class="headerlink" title="GEOMETRIC INTERPRETATION"></a>GEOMETRIC INTERPRETATION</h2><p>To understand the geometry of SVD, we first extend the matrix $V$ to an orthogonal base in $R^n$ and $U$ to an orthogonal base in $R^m$. By adding proper zero rows and columns to $\Sigma$, we still have $A = U \Sigma V^T$. </p>
<p>Now, the transformation $f(x)$ is decomposed into three steps:<br>$$<br>x \rightarrow V^Tx \rightarrow \Sigma (V^T x) \rightarrow U(\Sigma V^T x)<br>$$</p>
<p>It suffices to under the effect of multiplying an orthogonal matrix $V$. It is indeed a rotation. To see this, multiply $V$ by $e_i$ gives $v_i$, i.e., $V e_i = v_i$, for all for $i \in [n]$. Therefore, $V$ takes the orthogonal base $I = [e_1, e_2, …, e_n]$ to the orthogonal base $V = [v_1, v_2, …, v_n]$. </p>
<p>Note that the inverse $V^{-1}$ of $V$ is also an orthogonal matrix and hence a rotation. Indeed it rotates the orthogonal base $V = [v_1, v_2, …, v_n]$ back to $I = [e_1, e_2, …, e_n]$. To understand this, observe that $V^{-1} = V^T$. For arbitrary $v_i$, we have $V^T v_i = e_i$. </p>
<p>Therefore, $V^T x$ corresponds to a rotation, in the reverse direction to the one defined by $V$. </p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Rotation.jpg"></p>
<p>Next, the matrix $\Sigma$ scales the $i$-th dimension of $V^T x$ by $\sigma_i$, for all $i \in [n]$. Combined with the first step, if $x = v_i$, then it is first rotation to the direction $e_i$, then scaled by a factor $\sigma_i$. </p>
<p>Finally, $U$ is an orthogonal matrix and corresponds to another rotation, which take $e_i \in R^m$ to a vector $u_i$. </p>
<p><em>Corollary</em>: Define the Frobenius norm of a matrix $A$ as<br>$$<br>||A|<em>F = \sqrt{\sum</em>{i, j} a_{i, j}^2 }<br>$$<br>Then $||A||<em>F^2 = ||\Sigma||^2 = \sum</em>{i = 1}^r \sigma_i^2$. </p>
<p><em>Proof:</em> Note that rotating the vectors do not change their distance from the origin. Recall that $U^T$ and $V$ are both rotations. Hence $||A||_F^2 = ||AV||_F^2 = ||U^T A V||_F^2 = ||U^T U \Sigma V^T V||_F^2= ||\Sigma||_F^2$. </p>
<p>$\square$. </p>
<h2 id="APPLICATIONS"><a href="#APPLICATIONS" class="headerlink" title="APPLICATIONS"></a>APPLICATIONS</h2><p>An important property of the $v_i$ is that </p>
<ol>
<li>$v_1 = \arg\max_{v \in R^n, ||v|| = 1} ||Av||$. </li>
<li>$v_2 = \arg\max_{v \in R^n, ||v|| = 1, v \perp v_1} ||Av||$.</li>
<li>…</li>
<li>$v_r = \arg\max_{v \in R^n, ||v|| = 1, v \perp v_1, v \perp v_2, …, v \perp v_{r - 1}} ||Av||$</li>
</ol>
<p><em>Proof</em>: For any $v \in R^n, ||v|| = 1$, we can write $v = \sum_{i = 1}^n a_i v_i$, where $\sum_{i = 1}^n a_i^2 = 1$, since $v_i$’s are an orthogonal base. Then<br>$$<br>\begin{aligned}<br>||Av||^2<br>    &amp;= x^T A^T A x \<br>    &amp;=  x^T V \Sigma U^T U \Sigma V^T x \<br>    &amp;= ||\Sigma V^T x||^2 \<br>    &amp;= \sum_{i = 1}^r a_i^2 \sigma_i^2<br>\end{aligned}<br>$$<br>which is maximized to $\sigma_1^2$ when $a_1 = 1$ and $a_i = 0, i \ge 2$. Hence (1) is proved. </p>
<p>Similarly, when $||v|| = 1, v \perp v_1, v_2, …, v_k$ for some $k &lt; r$, then $a_1 = a_2 = … a_k = 0$, and $v = \sum_{i = k + 1}^r a_i v_i$.<br>$$<br>\begin{aligned}<br>||Av||^2<br>    &amp;= \sum_{i = k + 1}^r a_i^2 \sigma_i^2<br>\end{aligned}<br>$$<br>which achieves maximum when $a_{k + 1} = 1$ and $a_i = 0, i &gt; k + 1$. In this case $v = v_{k + 1}$. </p>
<h3 id="Best-Fit-k-subspace"><a href="#Best-Fit-k-subspace" class="headerlink" title="Best Fit $k$ subspace"></a>Best Fit $k$ subspace</h3><p>An implication is that, the subspace space $V_k$ spanned by $(v_1, v_2, …, v_k)$, is the best $k$-dimension subspace of $R^n$ that fits the row vectors of $A$, namely, $A[1, :], A[2, :], …, A[m, :]$. Here fits mean the sum of the square of the perpendicular distance from the row vector to their projections on the subspace is minimized. Denote $S$ any subspace of $R^n$, then for $v \in R^n$, we can decompose $v$ into two part: the part $v_S$ that is within $S$ and the part $v_{S_\perp}$ perpendicular to $S$. Then<br>$$<br>||v||^2 = ||v_{S}||^2 + ||v_{S_\perp}||^2<br>$$<br>Minimizing $||v_{S_\perp}||^2$ is equivalent to maximizing $||v_{S}||^2$. The subspace space by $(v_1, v_2, …, v_k)$ is the subspace that maximize the sum of the squares of the projections of row vectors to the subspace.</p>
<p>We prove this by induction. When $k = 1$, the proof holds trivially by the definition of $v_1$. In particular, $A v_1$ is the lengths of the projections of the row to the line that go through $v_1$, since $v_1$ is a unit vector.  Further, the corresponding projected vectors in $R^n$ are<br>$$<br>Av_1 v_1^T  = \sigma_1 u_1 v_1^T<br>$$</p>
<p>For general $k$, by induction hypothesis $V_{k - 1}$ is the best $k-1$ dimension space that fit rows of $A$. Denote $N(V_{k -1})$ the subspace that is perpendicular to $V_{k - 1}$, which has dimension $n - k + 1$. For any $k$ dimensional subspace $S \cap N(V_{k  - 1}) \neq \empty$, since $dim(S) + dim(N(V_{k  - 1})) = k + n - k + 1 \ge n$. Let $s_k \in S \cap N(V_{k  - 1})$ be a unit vector. By the definition of $v_k$, we have<br>$$<br>||Av_k||^2 \ge ||As_k||^2<br>$$<br>Now we can extend $s_k$ to a base of $S$, denoted as $s_1, s_2, …, s_k$. By induction hypothesis, it holds<br>$$<br>\sum_{i = 1}^{k - 1} ||A v_i||^2 \ge \sum_{i = 1}^{k - 1} ||A s_i||^2<br>$$<br>Hence<br>$$<br>\sum_{i = 1}^{k} ||A v_i||^2 \ge \sum_{i = 1}^{k} ||A s_i||^2<br>$$<br>But the former is exactly sum of the squares of the lengths of the projections to $V_k$ and the later is the one to $S$. Finally, we mention that the projection of $A$ to $V_k$ is given by<br>$$<br>A_k = \sum_{i = 1}^k \sigma_i u_i v_i ^T<br>$$</p>
<p>Corollary 1.<br>For any rank $k$ matrix $B$, we have<br>$$<br>||A - A_k||_F \le ||A - B||_F<br>$$</p>
<p>Intuitively, if we view square root of the sum of square distance of row vectors to the origin (note that the origin can be viewed as a zero dimension subspace.)</p>
<p>Corollary 2. Define the 2-norm of a matrix $A$ as<br>$$<br>||A||_2 = \sigma_1 = \max_{v \in R^n, ||v|| = 1} ||Av||<br>$$<br>The $2$-norm is the square root of the maximum sum of squares of the projections of the row vectors to a one dimension subspace. Then an implication of the SVD is that, for any rank $k$ matrix $B$, we have<br>$$<br>||A - A_k||_2 \le ||A - B||_2<br>$$</p>
<p><em>Proof:</em> First note that for $v = \sum_{i = 1}^r a_i v_i$, such that $\sum_{i = 1}^k a_i^2 = 1$,<br>$$<br>||(A - A_k) v|| = \sum_{i = k + 1}^r a_i \sigma_i<br>$$<br>it is maximized when $a_{k + 1} = 1$ and $||A - A_k||<em>2 = \sigma</em>{k + 1}$. Now, as $dim(A_{k + 1}) + dim(N(B^T)) = k + 1 + n - k \ge n$, $\exists v \in R^n$, such that $Bv = 0$ and $v = \sum_{i = 1}^{k + 1} a_i v_i$ and $||v|| = 1$. Now<br>$$<br>||A - B||<em>2 \ge ||(A - B)v|| = ||Av|| = \sqrt{\sum</em>{i = 1}^{k + 1} a_i^2 \sigma_i^2 } \ge \sigma_{k + 1}<br>$$<br>which finishes our proof. </p>
<!-- *Proof 2:* Rewrite $A = \sum_{i = 1}^n \sigma_i u_i v_i^T$. Note that $v_i$'s are an orthogonal base of $R^n$. Therefore, we can rewrite the rows of any matrix $B$ as a linear combination of $v_i$'s.   -->

<h3 id="Computing-SVD"><a href="#Computing-SVD" class="headerlink" title="Computing SVD"></a>Computing SVD</h3><p>If suffices to compute the eigenvectors and eigenvalues of $A^T A$, i.e., $\sigma_i^2$’s and $v_i$’s. In the simplest case, $\sigma_1 &gt; \sigma_2 \ge \sigma_3 \ge … \ge \sigma_r$, then we can take powers of $A^T A$,<br>$$<br>(A^T A)^k = V \Sigma^{2k} V^T = \sum_{i = 1}^r \sigma_i^{2k} v_i v_i^T<br>$$<br>Dividing the matrix by $\sigma_1^{2k}$,<br>$$<br>\frac{1}{\sigma_1^{2k} } (A^T A)^k = V \Sigma^{2k} V^T = v_1 v_1^T + \sum_{i = 2}^r \frac{\sigma_i^{2k}}{\sigma_1^{2k} }  v_i v_i^T<br>$$<br>The second part converges to 0 as $k \rightarrow \infty$. Then for any vector $v$ that is not perpendicular to $v_1$, we have<br>$$<br>\lim_k \frac{1}{\sigma_1^{2k} } (A^T A)^k v =  v_1 (v_1^T v) = (v_1^T v) v_1<br>$$<br>which is a multiple of $v_1$. Normalizing the vector recovers $v_1$. </p>
<p>When there is a tie of the largest eigenvalues, that is $\sigma_1 = \sigma_2 = … = \sigma_t$ for some $t &lt; r$, and suppose that $v$ is not perpendicular to $v_1, v_2, …, v_t$ simultaneously, then<br>$$<br>\lim_k \frac{1}{\sigma_1^{2k} } (A^T A)^k v =  \text{the projection of } v \text { to  }  V_t<br>$$<br>Suppose we are satisfied with this result, we still need to address two problems. </p>
<ol>
<li>We use $\sigma_1^{2k}$ to normalize the resulting vector, which is unknown.</li>
<li>How can we find a vector $v$ that is not perpendicular to all $v_1, .., v_t$ simultaneously, i.e., its projection to $V_t$ is not zero. </li>
</ol>
<!-- The first one is much easier: we use $\sum_{i = 1}^k \sigma_i^{2k}$ as the denominator instead. This can be computed by the Frobenius norm of $A^{2k}$.  Indeed we have the following lemma


Hence 
$$
\lim_k \frac{1}{||(A^T A)^{k} ||_F } (A^T A)^k  =  \lim_k \sum_{i = 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  v_i v_i^T
$$
For the largest $t$ singular values, the terms $\sum_{i = 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } \rightarrow \frac{1}{t}$, and the rest converge to $0$.  -->

<p>We defer the discussion of the first issue. As for the second issue, we tackle the problem for picking a vector $v$ uniformly at random from the $n$-dimension unit ball, and then normalize it by $v / ||v||$, in the hope that it is a “good” vector. </p>
<p><em>Lemma:</em> With probability at least $1 / 2$, that $v^T v_1 \ge 1 /\sqrt{n}$. </p>
<p><img src="https://pic2.zhimg.com/80/v2-a11c6a482d1f70b3ee0052ee132cbef9_hd.jpg"></p>
<p><em>Proof:</em> By symmetry of the $n$-dimension unit ball, it suffices to show that for an arbitrary given fixed direction, the projection of $v$ to that direction is at least $1 / 4 \sqrt n$ with probability $1 / 2$. For convenience, we choose this direction to be $e_1$. </p>
<p>Denote the volume of an $n$-dimensional unit ball as $V(n)$. </p>
<p>Now, we would like to use two cylinders to estimate the probability of $|e_1 ^T  v| / ||v|| \le  \frac{1}{4 \sqrt n}$. </p>
<p>Note that the points ${v }$ inside the ball with $e_1^T v / ||v|| \le \frac{1}{4 \sqrt n}$ is completely contained in a cylinder centered in the equator, with radius one and height $\frac{1}{4 \sqrt n}$  that is parallel to $e_1$. Its volume is given by $\frac{1}{4 \sqrt n} V(n - 1)$. Hence:<br>$$<br>\Pr \left[ |v^T e_1| \le \frac{1}{4 \sqrt n} \right] \le \frac{2 \frac{1}{4 \sqrt n} V(n - 1)}{V(n)}<br>$$<br>On the other hand, we can use a cylinder centered at the equator to lower bound the volume of $n$-dimension unit ball. The cylinder is inside the ball, with height $2\frac{1}{\sqrt n}$ and radius $(1 -  (\frac{1}{ \sqrt n})^2)^{1/ 2}$ . Therefore,<br>$$<br>\begin{aligned}<br>    V(n)<br>    &amp; \ge 2 \frac{1}{\sqrt n} \left(1 - (\frac{1}{\sqrt n})^2 \right)^{(n - 1) / 2} V(n - 1) \<br>    &amp;\ge 2 \frac{1}{\sqrt n} \left( 1 - \frac{1}{n} \right)^{n / 2} V(n - 1)<br>\end{aligned}<br>$$<br>Note that $(1 - \frac{1}{n})^n$ increases with $n$, and for $n \ge 2$, $(1 - \frac{1}{n})^n \ge \frac{1}{4}$ and  $\left( 1 - \frac{1}{n} \right)^{n / 2} \ge \frac{1}{2}$<br>$$<br>\Pr \left[ v^T e_1 \le \frac{1}{4 \sqrt n} \right] \le \frac{\frac{1}{4 \sqrt n} V(n - 1)}{V(n)} \le \frac{1}{2}<br>$$<br>$\square$</p>
<p><em>Theorem</em> After $\frac{1}{\epsilon} \ln \frac{n}{\epsilon}$ iterations, we can obtain a vector whose component that is perpendicular to $V_t$ is at most $\epsilon$ fraction of its square length, with probability at least $\frac{1}{2}$. </p>
<p><em>Proof:</em> If $\sigma_i &lt; (1 - \epsilon) \sigma_1$, then<br>$$<br>\frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } \le \frac{\sigma_i}{\sigma_1}^{2k} \le (1-\epsilon)^{2k}<br>$$<br>Denote $\sigma_1 = \sigma_2 = … = \sigma_t$ for some $t &lt; r$ the largest singular values. By the previous lemma, we pick a $v$ from the unit ball at random, and take $v \leftarrow v  / ||v||$. Denote $v = \sum_{i = 1}^n {a_i} v_i$, then with probability at least $1/ 2$ we have $\sqrt{ \sum_{i = 1}^t a_i^2} \ge \frac{1}{4 \sqrt n}$. On the other hand, $v$ is a unit vector, hence $\sum_{i = 1}^n a_i^2 = 1$. Therefore, $\sum_{i = t + 1}^n a_i^2 \le 1 - \frac{1}{16 n}$</p>
<p>Now<br>$$<br>\begin{aligned}<br>\frac{1}{||(A^T A)^{k} ||<em>F } (A^T A)^k v<br>    &amp;= \sum</em>{i = 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  v_i v_i^T v \<br>    &amp;= \sum_{i = 1}^r \frac{a_i \sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  v_i \<br>    &amp;= \sum_{i = 1}^t \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } a_i v_i  + \sum_{i = t + 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  a_i v_i<br>\end{aligned}<br>$$</p>
<p>For the second term<br>$$<br>\begin{aligned}<br>||\sum_{i = t + 1}^r \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  a_i v_i ||^2<br>    &amp;= \sum_{i = t + 1}^r (\frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} }  a_i)^2 \<br>    &amp;\le \sum_{i = t + 1}^r (\frac{\sigma_i^{2k}}{ \sum_{j = 1}^t \sigma_1^{2k} }  a_i)^2 \<br>    &amp;\le \frac{ (1 - \epsilon)^{4k} }{t^2} \sum_{i = t + 1}^r a_i^2 \<br>    &amp;\le \frac{ (1 - \epsilon)^{4k} }{t^2}<br>\end{aligned}<br>$$</p>
<p>For the first term<br>$$<br>\begin{aligned}<br>||\sum_{i = 1}^t \frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} } a_i v_i ||^2<br>    &amp;= \sum_{i = 1}^t (\frac{\sigma_i^{2k}}{ \sum_{j = 1}^k \sigma_1^{2k} })^2 a_i^2 \<br>    &amp;\ge \sum_{i = 1}^t (\frac{ 1 }{ t + (n - t)(1 - \epsilon)^{2k} })^2 a_i^2 \<br>    &amp;\ge (\frac{ 1 }{ t + n (1 - \epsilon)^{2k} } )^2 \sum_{i = 1}^t a_i^2 \<br>    &amp;\ge (\frac{ 1 }{ t + n (1 - \epsilon)^{2k} } )^2 \frac{1}{16n}<br>\end{aligned}<br>$$</p>
<p>If we take $k = O( \frac{1}{\epsilon} \ln \frac{n}{\epsilon})$, then the second term is at most $\epsilon$ fraction of the first term. </p>
<p>Remark: picking a point uniformly at random can be done as follows: first pick a point $v$ uniformly at random from the high dimension cube: $[-1, 1]^n$, then checks whether the point is inside the ball ($||v|| &lt; 1$). This however, could suffers from efficiency problem as the volume of the ball approaches to $0$ as the number of dimension increases. More efficient sampling method is needed. But it is not the current focus of this article. </p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>GILBERT STRANG, Introduction to Linear Algebra.</li>
<li> Venkatesan Guruswami and Ravi Kannan, Note 2, Singular Value Decomposition, 15-496/15-859X: Computer Science Theory for the Information Age, Spring 2012, CMU</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/04/18/FORA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/18/FORA/" class="post-title-link" itemprop="url">FORA [1]</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-18 22:51:54" itemprop="dateCreated datePublished" datetime="2018-04-18T22:51:54+10:00">2018-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-02 18:57:47" itemprop="dateModified" datetime="2018-07-02T18:57:47+10:00">2018-07-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose that we know the values of $\pi^o(s, t)$ for all $t \in V$ and $r(s, v_i)$ for all $v_i \in V$. We would like to evaluate the value of<br>$$<br>\pi(s, t) = \pi^o(s, t) + \sum_{i = 1}^n r(s, v_i) \pi(v_i, t)<br>$$</p>
<p>where $\pi(v_i, t)$ denotes the probability of a random starts from $v_i$ and terminate at $t$. Let $\tilde \pi(v_i, t)$ be our estimation of $\pi(v_i,t)$, then we have an estimation of $\pi(s, t)$:<br>$$<br>\tilde\pi(s, t) = \pi^o(s, t) + \sum_{i = 1}^n r(s, v_i) \tilde\pi(v_i, t)<br>$$<br>To get $\tilde\pi(v_i, t)$,  we perform $w_i$ (a value to be determined latter) random walks that start from $v_i$ and count the fraction of walks that terminate at $t$.  Formally, let $X_{i, j}$ be a Bernoulli random variable that takes value 1 if the $j$-th walk from $v_i$ terminates at $t$.  Then<br>$$<br>E[X_{i,j}] = \pi(v_i, t)<br>$$<br>Let $\tilde \pi(v_i, t) = \frac{1}{w_i} \sum_{j = 1}^{w_i} X_{i,j}$ , so<br>$$<br>E[r(s, v_i) \tilde \pi(v_i, t)] = \frac{r(s, v_i)}{w_i} \sum_{j = 1}^{w_i} E[X_{i,j}] = r(s, v_i) \pi(v_i, t)<br>$$<br>A naïve use of Chenoff bound to us $w_i = \Omega(\frac{\log (1 / p_{fail} ) }{\epsilon^2 \pi(v_i, t)} )$ (which is even worse than evaluating $\pi(s, t)$ by Monte Carlo directly), such that $r(s, v_i) \tilde \pi(v_i, t)$ takes value within $\big[(1 - \epsilon) r(s, v_i) \pi(v_i, t), (1 + \epsilon) r(s, v_i) \pi(v_i, t)\big]$with probability $1 - p_{fail}$. The problem here is that when $\pi(v_i, t)$ is small, we need large number of random walks to get a good approximation of it. On the other hand, when  $\pi(v_i, t)$ is small, the contribution of $r(s, v_i)\pi(v_i, t)$ to $\sum_{i = 1}^n r(s, v_i) \pi(v_i, t)$ is small, which means that more deviation is allowed on the estimation of $\pi(v_i, t)$. </p>
<blockquote>
<p>What if the amount of deviation on the estimation of $r(s, v_i) \pi(v_i, t)$ we allow is $\pi(s, t)$? How about $\pi(s, t) / n$? More clever way of allocating deviation? Better Chernoff bound? </p>
</blockquote>
<p>[1] solves this by bounding the cumulative deviation of $\sum_{i = 1}^n r(s, v_i) \tilde \pi(v_i, t)$ as a whole. The statistical tool it turns to is a special Chernoff bound: </p>
<h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><blockquote>
<p>Let $Y_1, Y_2, …, Y_w$ be independent random variables with<br>$$<br>Pr[Y_i = 1] = p_i \qquad Pr[Y_i = 0 ]  = 1 - p_i.<br>$$<br>Let $Y = \frac{1}{w} \sum_{i = 1}^w b_i Y_i$ with $b_i &gt; 0$, and $v = \frac{1}{w} \sum_{i = 1}^w b_i^2 p_i$. Then<br>$$<br>Pr[|Y - E[Y]| \ge \lambda ] \le 2 \exp \big(-\frac{\lambda^2 w} {2 v + 2 b \lambda / 3} \big)<br>$$<br>where $b = \max{ b_1, b_2, …, b_w}$</p>
</blockquote>
<p>To apply this bound, we perform in total $w = \sum_{i = 1}^n w_i$ random walks. The first $w_1$ walks starts from $v_1$ , the next $w_2$ walks starts from $v_2$, and so on. Let<br>$$<br>{ Y_{1}, …, Y_{w_1}, Y_{w_1 + 1}, …, Y_{w_1 + w_2}, …, Y_{\sum_{i = 1}^{n -1}w_i + 1}, …, Y_{w} }<br>$$<br>be the corresponding independent indicator random variables that take value 1 if the walks terminate at $t$ and </p>
<p>$$<br>b_j = \frac{w}{w_i} r(s, v_i)<br>$$<br>if the $j$-th walk starts from vertex $v_i$. So<br>$$<br>b = \max {b_1, b_2, …, b_w} = \max_{1 \le i \le n} \frac{w}{w_i} r(s, v_i)<br>$$<br>It can be verified that<br>$$<br>E[Y] = \frac{1}{w} \sum_{i = 1}^w E[Y_i] = \frac{1}{w} \sum_{i = 1}^n \sum_{j = 1}^{w_i} \frac{w}{w_i} r(s, v_i) \pi(v_i, t) = \sum_{i = 1}^n r(s, v_i) \pi(v_i, t)<br>$$<br>To bound the deviation of $\tilde \pi(s, t)$ from $\pi(s, t)$, we notice that<br>$$<br>|\tilde \pi(s, t) - \pi(s, t)| = | \sum_{i = 1}^n r(s, v_i) \tilde \pi(v_i, t)  -  \sum_{i = 1}^n r(s, v_i) \pi(v_i, t)| = |Y - E[Y]|<br>$$<br> So<br>$$<br>Pr[|Y - E[Y]| \ge \epsilon \pi(s, t)] \le 2 \exp ( - \frac{ \epsilon^2 \pi^2(s, t) \ w}{2 v + 2 b \epsilon \pi(s, t) / 3})<br>$$<br>But notice that<br>$$<br>v = \frac{1}{w} \sum_{i = 1}^w b_i^2 p_i \le  \big( \frac{b}{w}  \sum_{i = 1}^w b_i p_i \big) = b \ E[Y] \le b \ \pi(s, t)<br>$$<br>Thus<br>$$<br>\begin{align}<br>Pr[|Y - E[Y]| \ge \epsilon \pi(s, t)]<br>&amp;\le 2 \exp ( - \frac{ \epsilon^2 \pi^2(s, t) \ w}{2 v + 2 b \epsilon \pi(s, t) / 3}) \\<br>&amp;\le 2 \exp ( - \frac{ \epsilon^2 \pi^2(s, t) \ w}{2 b \pi(s, t) + 2 b \epsilon \pi(s, t) / 3}) \\<br>&amp;\le 2 \exp ( - \frac{ \epsilon^2 \pi (s, t) \ w}{b (2   + 2\epsilon  / 3)}) \\<br>&amp;\le p_{fail}<br>\end{align}<br>$$<br>So<br>$$<br>w \ge b  \frac{2 + 2\epsilon / 3}{\epsilon^2 \delta} \log \frac{2}{p_{fail} }<br>$$<br>We would like to minimize $b$ in order to minimize $w$, which can be formulated as an convex optimization problem:</p>
<p>$$<br>\begin{align}<br>\min \quad &amp;b \\<br>s.t.   \quad &amp;b \ge r(s, v_i) / k_i \\<br>&amp;\sum_i k_i = 1 \\<br>&amp;k_i \ge 0<br>\end{align}<br>$$</p>
<p>Here we rewrite $\frac{w_i}{w} = k_i$. One simple way to solve this is to guess that $b$ is minimized when all ${r(s, v_i)}/{k_i}$ equals. This gives $k_i = r(s, v_i) / r_{sum}$. We can prove this is indeed optimal: if this is not the case, there must be some $k_i &lt; r(s, v_i) / r_{sum}$. Then $b \ge r(s, v_i) / k_i &gt; r_{sum}$ .</p>
<p>We can also solve this problem by a heavy mechanism: KKT conditions. First, define<br>$$<br>L(b, k, \lambda, \beta, , \mu) = b + \sum_{i = 1}^n \lambda_i (r(s, v_i) / k_i - b) + \sum_{i = 1}^n \beta_i (0 - k_i) + \mu (\sum_{i = 1}^n k_i - 1)<br>$$<br>Thus, the KKT conditions are<br>$$<br>\begin{align}<br>    &amp; r(s, v_i) / k_i - b \le 0 &amp;\forall i \in [1, n] \\<br>    &amp; -k_i \le 0 &amp;\forall i \in [1, n] \\<br>    &amp; \sum_i k_i  - 1 = 0 \\<br>    &amp; \lambda_i \ge 0, \beta_i \ge 0 &amp;\forall i \in [1, n] \\<br>    &amp; \lambda_i (r(s, v_i) / k_i - b) = 0 &amp;\forall i \in [1, n] \\<br>    &amp;\beta_i (0 - k_i) = 0 &amp;\forall i \in [1, n] \\<br>    &amp; \frac{\partial L}{\partial b}  = 1 - \sum_{i = 1}^n \lambda_i = 0 \\<br>    &amp; \frac{\partial L}{\partial k_i} = -\lambda_i r(s, v_i) / k_i^2 - \beta_i + \mu = 0 &amp;\forall i \in [1, n]\\<br>\end{align}<br>$$<br>So one solution is given by<br>$$<br>\begin{align}<br>&amp;b = r_{sum} \\<br>&amp;k_i = r(s, v_i) / r_{sum} &amp; \forall i \in [1, n] \\<br>&amp;\lambda_i = r(s, v_i) / r_{sum}^2 &amp; \forall i \in [1, n] \\<br>&amp;\beta_i = 0 &amp; \forall i \in [1, n] \\<br>&amp;\mu = 1<br>\end{align}<br>$$</p>
<p>which implies<br>$$<br>w \ge b  \frac{2 + 2\epsilon / 3}{\epsilon^2 \delta} \log \frac{2}{p_{fail} } \\<br>w_i = w k_i = \frac{w r(s, v_i)}{r_{sum} }<br>$$</p>
<h4 id="Citations"><a href="#Citations" class="headerlink" title="Citations"></a>Citations</h4><p>[1]  Wang, Sibo, et al. “FORA: Simple and Effective Approximate Single-Source Personalized PageRank.” <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. ACM, 2017.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/11/05/Lebesgue%20Measure/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/11/05/Lebesgue%20Measure/" class="post-title-link" itemprop="url">Lebesgue Measure</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-11-05 21:48:44" itemprop="dateCreated datePublished" datetime="2017-11-05T21:48:44+11:00">2017-11-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-17 16:53:33" itemprop="dateModified" datetime="2020-10-17T16:53:33+11:00">2020-10-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Lebesgue Measure</p>
<p>It is quiet intuitive that intervals $(a,b),(a,b],[a,b),[a,b]$ have length $b - a$. Now we want measure sets of real numbers more general than intervals, such as the set of all rational numbers.   </p>
<p>In order to do so, we need a new model, a new concept of length. Before we show what Lebesgue Measure is, it is natural to have following question in mind:  </p>
<blockquote>
<p><strong>1. What do we mean when we talk about the length of a set?  **<br>    **2. Does every set has a length?</strong><br>    <strong>3. What properties do the sets that have length satisfy?</strong></p>
</blockquote>
<p>One way to think of length of sets is regarding it as a function from sets to real numbers. Translating the above questions into the language of functions, we get  </p>
<blockquote>
<p><strong>1. What is the function?</strong><br>    <strong>2. Especially, what is the domain of the function?</strong><br>    <strong>3. What are the typical properties of the function?</strong>  </p>
</blockquote>
<p>Before we delve into the details of constructing Lebesgue Measure, we shows some important goals of that construction (We forgo <strong>why we need such goals</strong> for the moment. Yet again, keep this question in mind):</p>
<blockquote>
<ol>
<li>Intervals like $(a,b],[a,b],[a,b),(a,b)$ are measurable and have length $b - a$.<br>. If set $E$ is measurable, then its complement $\overline{E}$ is measurable.<br>. Countable unions of measurable sets are measurable.   </li>
</ol>
</blockquote>
<p>One immediate corollary of goal (2) and (3) is that  </p>
<blockquote>
<p> 4 .  Countable intersections of measurable sets are measurable. </p>
</blockquote>
<p>Actually, if we replace (3) with (4), we can derive (3) by combining (2) and (4). This is due to the symmetry inherent in set operation<br>$$<br>\overline{E_1 \cup E_2} = \overline{E_1} \cap \overline{E_2}<br>$$</p>
<p>Another corollary of (1), (2) and (3) is   </p>
<blockquote>
<p>5 . Countable intersections and unions of intervals are measurable.</p>
</blockquote>
<h4 id="Two-Components-for-Lebesgue-Measure"><a href="#Two-Components-for-Lebesgue-Measure" class="headerlink" title="Two Components for Lebesgue Measure"></a>Two Components for Lebesgue Measure</h4><ol>
<li>$m^*(E) \doteq \inf { \sum_n l(I_n) : I_n \ \text{is open interval and } E \subset \cup_n I_n}$, where $l(I_n)$ denotes the length of $I_n$.    </li>
<li>For any set A, a measurable set E must satisfy $m^*(A) \ge m^*(A \cap E) + m^*(A \cap \overline{E})$.  </li>
</ol>
<p>Discussion:<br>(1) is a quiet intuitive rule. It states the length of a set is the least upper bound(supermum) of the sum of length of open intervals whose union covers this set. <strong>But why don’t we use the sum of lengths of intervals that constitute the set as the length of this set?</strong><br>One possible explanation is that there are sets which are not countable unions or intersection intervals. So we have to use open intervals to approximate them. <strong>What do these set look like?</strong> Keep this question in mind.   </p>
<p>(2) The second condition is required so that we find a set of sets that are close under complement, countable union and intersection operation with respect to the length definition of (1). <strong>Yet why (1) alone is not enough?</strong>. We will tough more about this issue when we come to non-Lebusgue measurable sets. </p>
<p>More formally, let<br>$$<br>\begin{aligned}<br>\mathfrak{L} = { &amp;E: \forall A \subset R, \ m^*(A) \ge m^*(A \cap E) + m^*(A \cap \overline{E}), \\<br>&amp; where \ m^*(E) \doteq \inf { \sum_n l(I_n) : I_n \ \text{is open interval and } E \subset \cup_n I_n} }<br>\end{aligned}<br>$$<br>we will show that   </p>
<ol>
<li>$[a,b], [a,b), (a,b], (a,b) \in \mathfrak{L}$, and $m^*([a,b])=m^*([a,b))=m^*((a,b])=m^*((a,b))= b - a$.<br> .$\forall E \in \mathfrak{L}, \overline{E} \in \mathfrak{L}.$<br> .${ E_n}_{n \ge 1} \subset \mathfrak{L}, \cup_n E_n \in \mathfrak{L}$<br> .${ E_n}_{n \ge 1} \subset \mathfrak{L}, \cap_n E_n \in \mathfrak{L}$  </li>
</ol>
<p>Before we move on, we notice the construction of $\mathfrak{L}$ so far involves (a) definition of length (b) constraint on the sets involved. <strong>Is it the only way of constructing $\mathfrak{L}?$</strong> In other words, if we replace (a) or (b) with something else, we get a new set $\mathfrak{L’}$. Will $\mathfrak{L’}$ satisfy the above four properties? <strong>Measure Theory</strong> studies these general topics.</p>
<h5 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h5><p>$\forall E \in \mathfrak{L}, \overline{E} \in \mathfrak{L}.$  </p>
<p>Proof: Straightforward from definition of $\mathfrak{L}$.</p>
<h5 id="Lema"><a href="#Lema" class="headerlink" title="Lema"></a>Lema</h5><p>If $E \subset \cup_n E_n, \text{ then } m^*(E) \le \sum_n m^*(E_n)$.</p>
<p>Proof: If $\sum_n m^*(E_n) = \infty$ then, this is obvious. Otherwise, for each n, $m^*(E_n) \le \infty$, so there exist an set of open intervals ${I_k^n }$, s.t., </p>
<p>$E_n \subset \cup_k I_k^n \ m^*(E_n) + {\epsilon \over 2^n}\ge \sum_k l(I_k^n)$</p>
<p>But ${I_k^n : k \ge 1, n \ge 1}$ is a cover of E, so</p>
<p>$$<br>m^*(E) \le \sum_n \sum_k l(I_k^n) \le \sum_n (m^*(E_n) + {\epsilon \over 2^n}) = \sum_n m^*(E_n) + \epsilon<br>$$<br>As $\epsilon$ takes arbitrary values, we have proved the inequality.</p>
<h5 id="Corollary"><a href="#Corollary" class="headerlink" title="Corollary"></a>Corollary</h5><p>$\text{If } E \subset E_1, \text{ then } m^*(E) \le m^*(E_1)$.</p>
<h5 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem"></a>Theorem</h5><p>${ E_n}_{n \ge 1} \subset \mathfrak{L}, \cup_n E_n \in \mathfrak{L}.$</p>
<p>Proof:  </p>
<p>Let $F_n \doteq E_n - \cup_{i = 1}^{n - 1} E_i$, so ${ F_n }_{n \ge 1}$ are disjoint and $\cup F_n = \cup E_n$.  In the following we denote $F \doteq \cup F_n$</p>
<p>By induction on n, we show that<br>$$<br>\forall A \subset R, m^*(A) \ge \sum_{i = 1}^n m^*(A \cap F_i) + m^*(A \cap \overline{\cup_{i = 1}^n F_i}) \ \ \ \ \ \ \ \ \ \ (1)<br>$$<br>when $n = 1$, this clearly holds.  Suppose the inequality above holds up to n, now we consider $n + 1$. First of all, we notice that,<br>$$<br>m^*(A) \ge  m^*(A \cap F{n + 1}) + m^*(A \cap \overline{F{n + 1}})<br>$$<br>by substituting A in (1) with $A \cap \overline{F_{n + 1}}$,<br>$$<br>\begin{aligned}<br>m^*(A \cap \overline{F{n + 1}}) \ge &amp;\sum_{i = 1}^n m^*(A \cap \overline{F{n + 1}} \cap F_i) + m^*(A \cap \overline{F{n + 1}} \cap \overline{\cup_{i = 1}^n F_i}) \<br>=&amp; \sum_{i = 1}^n m^*(A \cap F_i) + m^*(A  \cap \overline{\cup_{i = 1}^{n + 1} F_i})<br>\end{aligned}<br>$$<br>so, $\forall n \in N$,<br>$$<br>\begin{aligned}<br>m^*(A) &amp;\ge \sum_{i = 1}^n m^*(A \cap F_i) + m^*(A \cap \overline{\cup_{i = 1}^n F_i}) \<br>&amp; \ge m^*(A \cap (\cup_{i = 1}^n F_i)) + m^*(A \cap \overline{\cup_{i = 1}^n F_i})<br>\end{aligned}<br>$$<br>let $n \rightarrow \infty$<br>$$<br>\begin{aligned}<br>m^*(A)  \ge m^*(A \cap F) + m^*(A \cap \overline{F})<br>\end{aligned}<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/15/VC%20Dimension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/15/VC%20Dimension/" class="post-title-link" itemprop="url">VC Dimension</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-15 16:20:33" itemprop="dateCreated datePublished" datetime="2017-02-15T16:20:33+11:00">2017-02-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-18 18:31:54" itemprop="dateModified" datetime="2018-07-18T18:31:54+10:00">2018-07-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Definition</p>
<p>A set system is a tuple $(S, R)$, where $S$ is a set and $R$ is a collection of subsets of $S$. </p>
<h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>Given $(S, R)$, $X \subset S$ is shattered by $R$ if $\forall X’ \subset X$, $\exists R’ \in R$, such that $X’ = S \cap R’$. </p>
<h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p>The <strong>VC</strong> dimension of $(S, R)$ is the maximum cardinality of some $X \subset S$ that is shatterred by $R$. </p>
<p>Note: If we view $(S, R)$ as a classifaction problem, i.e., $S$ is a set of points and $R$ a set of functions, the <strong>VC</strong> dimension d is the maximum size of a subset $X$ in which R could separate arbitrary set points from the rest. In some sense, it measures how complex a separation plane $R$ could form. </p>
<h4 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h4><p>Given a nonegative integer $n$ and $d$, we define $\binom{n}{\le d} = \sum_{i = 0}^{d} \binom{n}{i}$</p>
<h4 id="Lemma"><a href="#Lemma" class="headerlink" title="Lemma"></a>Lemma</h4><p>If $(S, R)$ has $VC$ dimension $d$, then $|R| \le \binom{n}{\le d}$, where $|S| = n$. </p>
<p>Proof: Let $X \subset S$ be the set that has size $d$ and is shatterred by $R$. Define $R|X$ as ${R’ \cap X | R’ \in R }$. It is obvious that $(X, R|X)$ has <strong>VC</strong> dimension $d$. Also, the number of subset of $X$ is given by<br>$$<br>2^d = \binom{d}{\le d} = |\ R|X\ |<br>$$</p>
<p> Now we will show this lemma by induction. Label the elements in $S - X$ in arbitrary order as ${ x_{d + 1}, x_{d + 2}, …, x_{n} }$. Define $X_i$ = $X \cup {x_{d+1}, x_{d + 2}, …, x_{d+i} }$ for $0 \le i \le n - d$, where $X_0 = X$. So $|X_i| = d + i$. We will show that $|R | X_i| \le \binom{d + i} {\le d}$ holds. </p>
<p>Suppose the assumption holds for $R|X_i$, now consider $R|X_{i + 1}$. The elements $r$ in $R|X_{i}$ can be divided into </p>
<ol>
<li> $D_1  = { r | r \in R | X_{i + 1}, r \cup x_{d + 1} \in R|X_{i + 1} }$.</li>
<li> $D_2 = { r | r \notin R | X_{i + 1}, r \cup x_{d + 1} \in R|X_{i + 1} }$.</li>
<li> $D_2 = { r | r \in R | X_{i + 1}, r \cup x_{d + 1} \notin R|X_{i + 1} }$.</li>
</ol>
<p>We can verify that $|R|X_i | = |D_1| + |D_2| + |D_3|$, $|R|X_{i + 1}| = 2|D_1| + |D_2| + |D_3|$.</p>
<p>We show that $|D_1| \le \binom{d + i}{\le  d - 1}$. This is because the <strong>VC</strong> dimension of $(X_i, D_1)$ is at most $d - 1$. Otherwize, suppose $Y \subset X_i$ is shatterred by $D_1$ and $|Y| = d$. It is apparent $Y \cup x_{d + 1}$ is shattered by $R|X_{i + 1}$ in $X_{i + 1}$. A contradition.</p>
<p>So<br>$$<br>\begin{aligned}<br>|R | X_{i + 1}|<br>&amp; = |D_1| + |R | X_{i}| \\<br>&amp; \le \binom{d + i}{\le d - 1} + \binom{d + i}{\le d} \\<br>&amp; = \sum_{j = 0}^{d - 1} \binom{d + i}{j} + \sum_{j = 0}^{d}\binom{d +i}{j} \\<br>&amp; = \sum_{j = 1}^{d} \binom{d + i}{j - 1} + \sum_{j = 1}^{d}\binom{d +i}{j} + \binom{d + i}{0} \\<br>&amp; = \sum_{j = 1}^d \binom{d + i + 1}{j} + \binom{d + i +1}{0} \\<br>&amp; = \sum_{j = 0}^d \binom{d + i + 1}{j} \\<br>&amp; = \binom{d + i + 1}{\le d}<br>\end{aligned}<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/02/TF-IDF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/02/TF-IDF/" class="post-title-link" itemprop="url">TF-IDF</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-02 20:38:54" itemprop="dateCreated datePublished" datetime="2017-02-02T20:38:54+11:00">2017-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-07-02 18:29:42" itemprop="dateModified" datetime="2018-07-02T18:29:42+10:00">2018-07-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>TF-IDF stands for term frequency and inverse document frequency. It evaluates the importance of a word to a document in a corpus, which increases with the number of times it appears in the document but is offset by its frequency in the corpus. More precisely, for TF, we define<br>$$<br>tf(term| document) = \frac{\mbox{the number of times<br>“term” appears in “document”} }{\mbox{total number of words in “document”} }<br>$$</p>
<p>For IDF, we define<br>$$<br>idf(term) = 1 + \log(\frac{\mbox{the total number of documents} }{\mbox{the number of documents this term appears in} } )<br>$$</p>
<p>Now suppose we have a query. Let $Q = {t_1, t_2, …, t_n }$ the set of different term in the query. We convert $Q$ into a vector,<br>$$<br>V(Q) = [tf(t_1 | Q)* idf(t_1), tf(t_2|Q) * idf(t_2),…, tf(t_n|Q) * idf(t_n)]<br>$$</p>
<p>Similarily, for a document $D$, we can get a vector $V(D)$. Now the similarity between $Q$ and $D$ is given by<br>$$<br>\frac{V(Q) \times V(D)}{||V(Q)|| \times ||V(D)||}<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/02/Johnson-Lindenstrauss%20Lemma/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/02/Johnson-Lindenstrauss%20Lemma/" class="post-title-link" itemprop="url">Johnson-Lindenstrauss Lemma</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-02 20:38:54" itemprop="dateCreated datePublished" datetime="2017-02-02T20:38:54+11:00">2017-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-20 12:24:37" itemprop="dateModified" datetime="2021-01-20T12:24:37+11:00">2021-01-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Last modified date 19-Jan-2021.</p>
</blockquote>
<h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>Let $\mathcal{U} \subset \R^d$ be a set of $n$ points. We want to find a linear transformation $A \in \R^{k \times d}$ that embeds the points in $\mathcal{U}$ to $\R^k$ while preserving the pair-wise Euclidean distances. </p>
<p>If $k \ge d$, the problem is trivial. There is an isomorphic mapping between $\R^d$ and a $d$-dimensional subspace in $\R^k$. Let $u_1, …, u_d \in \R^k$ and $e_1, …, e_d \in \R^d$ be unit vectors, such that for $i \in [d]$, $u_i$ ($e_i$) has the $i$-th entry equal to 1 and other entries equal to 0. Then one possible isomorphic mapping is</p>
<p>$$<br>A = u_1 e_1^T + … + u_d e_d^T.<br>$$</p>
<p>If $k &lt; d$, then such an isomorphic embedding is impossible. However, it is feasible if we aim to preserve the pair-wise distance up to a multiplicative error $\epsilon \in (0, 1)$ with high probability: $\forall x, y \in \mathcal{U}$,<br>$$<br>||A x - A y ||^2 \in (1 \pm \epsilon) || x - y ||^2.<br>$$</p>
<p>The smaller $k$ is, the less space we need to store the embedded vectors $A \mathcal{U} = { Ax : x \in \mathcal{U} }$. It remains to study how small $k$ can be? </p>
<blockquote>
<p>Theorem. For any set $\mathcal{U} \subset \R^d$ of $n$-points, there is a matrix $A \in \R^{k \times d}$, such that with probability at least $1 - \frac{1}{n}$,<br>$$<br>    \forall x, y \in \mathcal{U}, ||A x - A y ||^2 \in (1 \pm \epsilon) || x - y ||^2,<br>$$<br>where $k = O\left( \frac{ \log n }{ \epsilon^2} \right)$. </p>
</blockquote>
<p>The value of $k$ depends only on $n$ but not $d$. </p>
<h1 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h1><h2 id="Column-Perspective"><a href="#Column-Perspective" class="headerlink" title="Column Perspective"></a>Column Perspective</h2><p>On the high level, we try to squeeze $d$ (in expectation) orthonormal vectors into a smaller space $\R^k$. We do this by generating a set of $d$ random vectors $u_1, u_2, …, u_d \in \R^k$, such that:  </p>
<ol>
<li>Each coordinate of $u_i$ is generated independently for $i \in [d]$;</li>
</ol>
<p>and in expectation, they are orthonormal:  </p>
<ol start="2">
<li>$\mathbb{E} \big[ ||u_i ||^2 \big] = 1$ for $i \in [d]$;  </li>
<li>$\mathbb{E} \big[ \left&lt; u_i, u_j \right&gt; \big] = 0$ for $i, j \in [d], i \neq j$. </li>
</ol>
<p>Let $e_1, …, e_d \in \R^d$ be unit vectors in $\R^d$ and $e_i$ ($i \in [d]$) has the $i$-th entry equal to 1. Define<br>$$<br>A = \begin{bmatrix}<br>    u_1, u_2, …, u_d<br>\end{bmatrix} = u_1 e_1^T + … + u_d e_d^T.<br>$$</p>
<p>For a fixed pair $x, y \in \mathcal{U}$, define $z = x - y$. Then<br>$$<br>    \begin{aligned}<br>        || A x - A y ||^2<br>            &amp;= || A z ||^2 \<br>            &amp;= || \sum_{ i \in [d] } z[i] \cdot u_i||^2 \<br>            &amp;= \sum_{i \in [d] } (z[i])^2 ||u_i||^2 + \sum_{ i, j \in [d], i \neq j} (z[i] \cdot z[j]) \cdot \left&lt; u_i, u_j \right&gt;.<br>    \end{aligned}<br>$$</p>
<p>Taking expectation of both side, and by assumption of $v_t, t \in [d]$, it holds that<br>$$<br>\mathbb{ E } \big[ ||z||^2 \big] = ||z||^2.<br>$$</p>
<h2 id="Row-Perspective"><a href="#Row-Perspective" class="headerlink" title="Row Perspective"></a>Row Perspective</h2><p>We can also characterize $A$ by the properties of its rows. Let $\tilde A$ be a scaled version of $A$, such that<br>$$<br>A = \frac{1}{ \sqrt k} \tilde{A}.<br>$$</p>
<p>Denote $v_1, v_2, …, v_k \in \R^d$ the row vectors of $\tilde A$. They are generated randomly and independently. Each vector $v_i$ ($i \in [k]$) satisfies </p>
<ol>
<li>Each entry of $v_i$ is generated independently; </li>
<li>$\mathbb{E} \big[v_i [j] \big] = 0, \forall j \in [d]$, i.e., each entry has mean zero;  </li>
<li>$\mathbb{Var} \big[ v_i [j] \big] = 1, \forall j \in [d]$, i.e., each entry has variance one. </li>
</ol>
<p>For a fixed pair $x, y \in \mathcal{U}$, define $z = x - y$. Consider<br>$$<br>||A z||^2 = \frac{1}{k} ||\tilde{A } z||^2 = \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2.<br>$$</p>
<p>Our assumption on $v_i : i \in [k]$ show that the $\left&lt; v_i, z \right&gt;^2: i \in [k]$ are independent random variables. We will verify that each of them has expectation $||z||^2$. For fixed $i \in [k]$,<br>$$<br>\left&lt; v_i, z \right&gt; = \sum_{j \in [d] } v_i [j] \cdot z[j]<br>$$</p>
<p>can be view as the sum of $d$ independently random variables. Hence, by our assumption on $v_i$, we have</p>
<!-- $$
\mathbb{E} [ \left< v_i, z \right> ] = \sum_{j \in [d] } z[j] \cdot  \mathbb{E}[ v_i [j] ] = 0, \\
$$ -->

<p>$$<br>    \mathbb{Var} \big[ \left&lt; v_i, z \right&gt; \big] = \sum_{j \in [d] } \mathbb{Var}\big[ v_i [j] \cdot z[j] \big] = \sum_{j \in [d] } (z[j])^2 \cdot \mathbb{Var} \big[ v_i [j] \big] = ||z||^2.<br>$$</p>
<p>By linearity of expectation,<br>$$<br>    \mathbb{E} \big[ \left&lt; v_i, z \right&gt; \big] = \sum_{j \in [d] } \mathbb{E}\big[ v_i [j] \cdot z[j] \big] = \sum_{j \in [d] } z[j] \cdot \mathbb{E} \big[ v_i [j] \big] = 0.<br>$$</p>
<p>It concludes that $\mathbb{E} \big[ \left&lt; v_i, z \right&gt;^2 \big] = \mathbb{Var} \big[ \left&lt; v_i, z \right&gt; \big] - \big(  \mathbb{E} \big[ \left&lt; v_i, z \right&gt; \big] \big)^2 = ||z||^2$.</p>
<p><strong>At this point, $||A z||^2$ is viewed as the average of $k$ independent, mean $||z||^2$ random variables.</strong> By the law of large numbers, the average should concentrate around $||z||^2$. The detailed analysis of the concentration phenomenon depends on the implementation of $\tilde A$.  </p>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>In this section, we discuss two implementations that fullfil the properties discussed above, in terms of both column and row perspectives. </p>
<h2 id="Gaussian"><a href="#Gaussian" class="headerlink" title="Gaussian"></a>Gaussian</h2><p>In the first implementation, each entry of $\tilde A$ is generated independently from standard normal distribution. Let $u_1, …, u_d$ be the column vectors of $\tilde A$ and $v_1, …, v_k$ be its row vectors. Hence, $\frac{1}{\sqrt k} u_1, …, \frac{1}{\sqrt k} u_d$ and $\frac{1}{\sqrt k} v_1, …, \frac{1}{\sqrt k} v_k$ are the column and row vectors of $A$, respectively. It satisfies that </p>
<ol>
<li><p>$\mathbb{E} \big[  || \frac{1}{\sqrt k} u_i||^2 \big] = \frac{1}{k} \mathbb{Var} \big[ ||u_i||^2 \big] = \frac{k}{k} = 1, \forall i \in [d]$;  </p>
</li>
<li><p>$\mathbb{E} \big[  \left&lt; \frac{1}{\sqrt k} u_i, \frac{1}{\sqrt k} u_j \right&gt; \big] = \frac{1}{k} \sum_{t \in [k] } \mathbb{E} \big[ u_i [t] \big] \cdot \mathbb{E} \big[ u_j[t] \big] = 0, \forall i, j \in [d], i \neq j$.</p>
</li>
</ol>
<p>Clearly, by definition, we have $v_i[t] \sim N(0, 1), \forall i \in [k], t \in [d]$. Hence $A$ satisfies both the required column and row properties. </p>
<p>It is left to analysis the concentration of<br>$$<br>    ||A z||^2 = \frac{1}{k} ||\tilde{A } z||^2 = \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2<br>$$<br>for a given $z \in \R^d$. </p>
<blockquote>
<p>Theorem. (Chernoff bound for the chi-square distribution). Let $X_i \sim N(0, \sigma^2): i \in [k]$ be independent random variables. Then<br>$$<br>    \Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \ge (1 + \epsilon) \cdot  \sigma^2 \right] \le \exp \left( - \frac{k}{4} (\epsilon^2 - \epsilon^3) \right), \<br>    \Pr \left[ \frac{1}{k} \sum_{ i \in [k] } X_i^2 \le (1 - \epsilon) \cdot  \sigma^2 \right] \le \exp \left( - \frac{k}{4} \epsilon^2 \right). \<br>$$</p>
</blockquote>
<!-- #### Lemma One
Let $t \in R^d$ be a random vector s.t. $t_i \overset{i.i.d}{\thicksim} N(0,1)$ and $x \in R^d$ be any fixed vector. Then 

 1.  $t^T x \thicksim N(0, ||x||^2)$, where $||x||^2 = \sum_{i = 1}^d x_i^2$.
 2.  $\mathbb{E}[(t^Tx)^2] = \mathbb{E}[(\sum_{i = 1}^d t_i x_i)^2]$ = $\sum_{1 \le i, j\le d} x_i x_j \mathbb{E}[t_i t_j]$ =$\sum_{i = 1}^d x_i^2 = ||x||^2$ -->

<p>The proof of the theorem relies on the following lemmas. </p>
<blockquote>
<p>Lemma 1. (Moment Generating Function of Normal Distribution). Let $X \thicksim N(0,\sigma^2)$, then for $\lambda &lt; \frac{1}{2 \sigma^2}$, we have<br>$$<br>    \mathbb{E} \big[ \exp(\lambda X^2) \big] = \frac{1}{ \sqrt{ 1 - 2 \sigma^2 \lambda} }.<br>$$</p>
</blockquote>
<p><em>Proof:</em><br>$$<br>    \begin{aligned}<br>    \mathbb{E} [ \exp( \lambda X^2 ) ] &amp;= \frac{1}{ \sqrt{2 \pi \sigma } } \int \exp( \lambda X^2 ) \exp(-X^2 / 2 \sigma^2 ) \ dX \<br>    &amp;=  \frac{1}{ \sqrt{2 \pi \sigma } } \int \exp \left(- \frac{ (1 - 2 \sigma^2 \lambda ) X^2}{2 \sigma^2 } \right) \ dX  \<br>    &amp;= \frac{1}{ \sqrt{1 - 2 \sigma^2 \lambda } }.<br>    \end{aligned}<br>$$<br>$\square$</p>
<blockquote>
<p>Lemma 2. Let $\sigma^2 = ||z||^2$. It holds that<br>$$<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) \le \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).<br>$$</p>
</blockquote>
<p>As<br>$$<br>    \ln(1 + \epsilon) = \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3} - \left( \frac{\epsilon^4}{4} - \frac{\epsilon^5}{5} \right) - \left( \frac{\epsilon^6}{6} + \frac{\epsilon^7}{7} \right) - …\le \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3},<br>$$</p>
<p>we get<br>$$<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)<br>        \le \exp \left( - \frac{k \cdot ( \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} ) }{2} \right)<br>        \le \exp \left( - \frac{k}{4} (\epsilon^2 - \epsilon^3) \right).<br>$$</p>
<p><em>Proof:</em> Define $X \sim N(0, \sigma^2)$ be a standard Gaussian random variable and let $X_i = \left&lt; v_i, z \right&gt;, \forall i \in [k]$. Then, $X_i \sim N(0, \sigma^2) : \forall i \in [k]$ can be viewed as i.i.d copies of $X$. By lemma 1, for any $\lambda \in (0, \frac{1}{2 \sigma^2} )$, it holds that </p>
<p>$$<br>\begin{aligned}<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)<br>    &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2 \ge (1 + \epsilon) \cdot \sigma^2 \right) \<br>    &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \ge k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \<br>    &amp; = \Pr \left( \exp \left( \lambda \sum_{i \in [k] } X_i^2 \right)\ge \exp \left( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \right) \<br>    &amp;\le \frac{\mathbb{E} [ \exp(\lambda X^2 ) ]^k}{ \exp( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 ) } \<br>    &amp;= \left( \frac{1}{ \exp( \lambda \cdot (1 + \epsilon) \cdot \sigma^2 ) \cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \lambda} } \right)^k.<br>\end{aligned}<br>$$</p>
<p>We want to find a $\lambda$ that minimizes the RHS. Let<br>$$<br>f = \lambda \cdot (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \ln (1 - 2 \cdot \sigma^2 \cdot  \lambda).<br>$$ </p>
<p>Taking the derivative with respect to $\lambda$,<br>$$<br>\begin{aligned}<br>    \frac{ \partial f }{ \partial \lambda}<br>        &amp;= (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \cdot \frac{1}{1 - 2  \cdot \sigma^2 \cdot \lambda} \cdot (-2  \cdot \sigma^2 ) \<br>        &amp;= \sigma^2 \cdot \left( (1 + \epsilon) - \frac{1}{ 1 - 2 \cdot \sigma^2 \cdot \lambda } \right).<br>\end{aligned}<br>$$</p>
<p>Hence $f$ increases when $\lambda &lt; \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) }$ and decreases afterwards. It follows that<br>$$<br>\begin{aligned}<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)<br>        &amp;\le \left( \frac{1}{ \exp( \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } \cdot (1 + \epsilon) \cdot \sigma^2 ) \cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } } } \right)^k \<br>        &amp;= \left( \frac{ \sqrt{1 + \epsilon} }{ \exp( \epsilon / 2) } \right)^k \<br>        &amp;= \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).<br>\end{aligned}<br>$$</p>
<p>$\square$</p>
<blockquote>
<p>Lemma 3. Let $\sigma^2 = ||z||^2$. It holds that<br>$$<br>    \Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right) \le \exp\left( \frac{k \cdot (\epsilon + \ln (1 - \epsilon) ) }{2} \right).<br>$$</p>
</blockquote>
<p>As<br>$$<br>    \ln(1 - \epsilon) = -\epsilon - \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} - \frac{\epsilon^4}{4} - …\le - \epsilon - \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3},<br>$$</p>
<p>we have<br>$$<br>    \Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right) \le \exp\left( \frac{k \cdot (\epsilon - \epsilon - \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} ) }{2} \right) \le \exp \left( - \frac{k }{4} \epsilon^2 \right).<br>$$</p>
<p><em>Proof:</em> Similar to the proof of lemma 2, we have<br>$$<br>\begin{aligned}<br>    \Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right)<br>        &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2 \le (1 - \epsilon) \cdot \sigma^2 \right) \<br>        &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \le k \cdot (1 - \epsilon) \cdot \sigma^2 \right) \<br>        &amp; = \Pr \left( \exp \left( - \lambda \sum_{i \in [k] } X_i^2 \right) \ge \exp \left( - \lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2 \right) \right) \<br>        &amp;\le \frac{\mathbb{E} [ \exp( -\lambda X^2 ) ]^k}{ \exp( - \lambda \cdot k \cdot (1 - \epsilon) \cdot \sigma^2 ) } \<br>        &amp;= \left( \frac{1}{ \exp( -\lambda \cdot (1 - \epsilon) \cdot \sigma^2 ) \cdot \sqrt{ 1 + 2 \cdot \sigma^2 \cdot \lambda} } \right)^k.<br>\end{aligned}<br>$$</p>
<p>Taking $\lambda = \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 - \epsilon) }$, we get<br>$$<br>\begin{aligned}<br>    \Pr \left( ||A z||^2 \le (1 - \epsilon) \sigma^2 \right)<br>        &amp;\le \left( \frac{1}{ \exp( - \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 - \epsilon) } \cdot (1 - \epsilon) \cdot \sigma^2 ) \cdot \sqrt{ 1 + 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 - \epsilon) } } } \right)^k \<br>        &amp;= \left( \sqrt{1 - \epsilon} \exp( \epsilon / 2) \right)^k \<br>        &amp;= \exp\left( \frac{k \cdot (\epsilon + \ln (1 - \epsilon) ) }{2} \right).<br>\end{aligned}<br>$$</p>
<p>$\square$</p>
<!-- Thus, 
$$
\begin{aligned}
\Pr (\frac{||Ax||^2}{k} \ge (1 + \epsilon) ||x||^2)  
& \le (\frac{1}{e^{s(1 + \epsilon)} \sqrt{1 - 2s}})^k \\\\
& \le (\frac{1 + \epsilon}{e^{\epsilon}})^{k / 2} \\\\
& \le e^{(\ln (1 + \epsilon) - \epsilon) (k  / 2)}  \\\\
& \le e^{(\epsilon - \epsilon^2 / 2 + \epsilon^3 / 3 - \epsilon) ( k / 2)} \\\\
& \le e^{(- \epsilon^2 / 2 + \epsilon^3 / 3) ( k / 2)}  
\end{aligned}
$$ -->

<!-- When $k$ = $4 \ln n \over (\epsilon^2 / 2- \epsilon^3/ 3)$, we get 
$$
\Pr (\frac{||Ax||^2}{k} \notin [ (1 - \epsilon) ||x||^2, (1 + \epsilon) ||x||^2])   \le 2 / n^2
$$

Given lemma three, by union bound $||f(x_i) - f(x_j)||^2 \notin (1 \pm \epsilon) ||x_i - x_j||^2$ occurs with less than $\frac{2}{n^2} * \frac{n (n -1)}{2} = 1 - \frac{1}{n}$ probability. Repeating generating $A$ will boost the success probability to a desired level.  -->


<h2 id="Rademacher"><a href="#Rademacher" class="headerlink" title="Rademacher"></a>Rademacher</h2><p>In this implementation, the entries of $\tilde A$ are simply independent Rademacher random variables which equal to $1$ and $-1$ with the same probability $1/2$. Let $u_1, …, u_d$ be the column vectors of $\tilde A$ and $v_1, …, v_k$ be its row vectors. Hence, $\frac{1}{\sqrt k} u_1, …, \frac{1}{\sqrt k} u_d$ and $\frac{1}{\sqrt k} v_1, …, \frac{1}{\sqrt k} v_k$ are the column and row vectors of $A$, respectively. It satisfies that </p>
<ol>
<li><p>$\mathbb{E} \big[  || \frac{1}{\sqrt k} u_i||^2 \big] = \frac{1}{k} \mathbb{Var} \big[ ||u_i||^2 \big] = \frac{k}{k} = 1, \forall i \in [d]$;  </p>
</li>
<li><p>$\mathbb{E} \big[  \left&lt; \frac{1}{\sqrt k} u_i, \frac{1}{\sqrt k} u_j \right&gt; \big] = \frac{1}{k} \sum_{t \in [k] } \mathbb{E} \big[ u_i [t] \big] \cdot \mathbb{E} \big[ u_j[t] \big] = 0, \forall i, j \in [d], i \neq j$.</p>
</li>
</ol>
<p>Clearly, by definition, we have $v_i[t]$ has mean $0$ and variance $1$,  $\forall i \in [k], t \in [d]$. Hence $A$ satisfies both the required column and row properties. </p>
<p>The proof of the theorem relies on the following lemmas. </p>
<blockquote>
<p>Lemma 1. (Moment Generating Function of Rademacher random variable). Let $X$ be a random variable such that<br>$$<br>    X = \begin{cases}<br>        \begin{aligned}<br>            &amp; 1,    &amp;\text{w.p.} 0.5,    \<br>            &amp;-1,    &amp;\text{w.p.} 0.5,<br>        \end{aligned}<br>    \end{cases}<br>$$<br>then for $\lambda \in \R$, we have<br>$$<br>    \mathbb{E} \big[ \exp(\lambda X^2) \big] = \exp(\lambda ).<br>$$</p>
</blockquote>
<p><em>Proof:</em><br>$$<br>    \begin{aligned}<br>    \mathbb{E} [ \exp( \lambda X^2 ) ] &amp;= \frac{1}{2} \exp( \lambda ) + \frac{1}{2} \exp(\lambda) = \exp(\lambda)<br>    \end{aligned}<br>$$<br>$\square$</p>
<p>$$<br>\frac{1}{2} \exp( - \sigma \lambda ) + \frac{1}{2} \exp( \sigma \lambda) = \frac{1}{2} \left( \sum_{i = 0}^\infty \frac{ (- \sigma \lambda)^i }{ i! } + \sum_{i = 0}^\infty \frac{ ( \sigma \lambda)^i }{ i! } \right) \<br>=  \sum_{i = 0}^\infty \frac{ (\sigma \lambda)^{2i}  }{ (2i)! }<br>\le \exp( \sigma^2 \lambda^2 )<br>$$</p>
<blockquote>
<p>Lemma 2. Let $\sigma^2 = ||z||^2$. It holds that<br>$$<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right) \le \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).<br>$$</p>
</blockquote>
<p>As<br>$$<br>    \ln(1 + \epsilon) = \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3} - \left( \frac{\epsilon^4}{4} - \frac{\epsilon^5}{5} \right) - \left( \frac{\epsilon^6}{6} + \frac{\epsilon^7}{7} \right) - …\le \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3},<br>$$</p>
<p>we get<br>$$<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)<br>        \le \exp \left( - \frac{k \cdot ( \frac{\epsilon^2}{2} - \frac{\epsilon^3}{3} ) }{2} \right)<br>        \le \exp \left( - \frac{k}{4} (\epsilon^2 - \epsilon^3) \right).<br>$$</p>
<p><em>Proof:</em> Define $X$ be a Rademacher random variable and let $X_i = \left&lt; v_i, z \right&gt;, \forall i \in [k]$. Then, $X_i: \forall i \in [k]$ can be viewed as i.i.d copies of $X$. By lemma 1, for any $\lambda \in (0, \frac{1}{2 \sigma^2} )$, it holds that </p>
<p>$$<br>\begin{aligned}<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)<br>    &amp; = \Pr \left( \frac{1}{k} \sum_{i \in [k] } \left&lt; v_i, z \right&gt;^2 \ge (1 + \epsilon) \cdot \sigma^2 \right) \<br>    &amp; = \Pr \left( \sum_{i \in [k] } X_i^2 \ge k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \<br>    &amp; = \Pr \left( \exp \left( \lambda \sum_{i \in [k] } X_i^2 \right)\ge \exp \left( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 \right) \right) \<br>    &amp;\le \frac{\mathbb{E} [ \exp(\lambda X^2 ) ]^k}{ \exp( \lambda \cdot k \cdot (1 + \epsilon) \cdot \sigma^2 ) } \<br>    &amp;= \left( \frac{ \exp(\lambda^2 \sigma^2 ) }{ \exp( \lambda \cdot (1 + \epsilon) \cdot \sigma^2 )  } \right)^k \<br>    &amp;= \exp(- \lambda \cdot \epsilon \cdot \sigma^2 \cdot k)<br>\end{aligned}<br>$$</p>
<p>We want to find a $\lambda$ that minimizes the RHS. Let<br>$$<br>f = \lambda \cdot (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \ln (1 - 2 \cdot \sigma^2 \cdot  \lambda).<br>$$ </p>
<p>Taking the derivative with respect to $\lambda$,<br>$$<br>\begin{aligned}<br>    \frac{ \partial f }{ \partial \lambda}<br>        &amp;= (1 + \epsilon) \cdot \sigma^2 + \frac{1}{2} \cdot \frac{1}{1 - 2  \cdot \sigma^2 \cdot \lambda} \cdot (-2  \cdot \sigma^2 ) \<br>        &amp;= \sigma^2 \cdot \left( (1 + \epsilon) - \frac{1}{ 1 - 2 \cdot \sigma^2 \cdot \lambda } \right).<br>\end{aligned}<br>$$</p>
<p>Hence $f$ increases when $\lambda &lt; \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) }$ and decreases afterwards. It follows that<br>$$<br>\begin{aligned}<br>    \Pr \left( ||A z||^2 \ge (1 + \epsilon) \sigma^2 \right)<br>        &amp;\le \left( \frac{1}{ \exp( \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } \cdot (1 + \epsilon) \cdot \sigma^2 ) \cdot \sqrt{1 - 2 \cdot \sigma^2 \cdot \frac{ \epsilon }{ 2 \cdot \sigma^2 \cdot (1 + \epsilon) } } } \right)^k \<br>        &amp;= \left( \frac{ \sqrt{1 + \epsilon} }{ \exp( \epsilon / 2) } \right)^k \<br>        &amp;= \exp \left( - \frac{k \cdot (\epsilon - \ln (1 + \epsilon) ) }{2} \right).<br>\end{aligned}<br>$$</p>
<p>$\square$</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/09/10/Nonuniform%20Sampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/10/Nonuniform%20Sampling/" class="post-title-link" itemprop="url">Nonuniform Sampling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-10 20:38:54" itemprop="dateCreated datePublished" datetime="2016-09-10T20:38:54+10:00">2016-09-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2016-09-11 19:59:33" itemprop="dateModified" datetime="2016-09-11T19:59:33+10:00">2016-09-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Suppose $S = { s_1, s_2, …, s_m}$ is a set of samples, with weight $W = { w_1, w_2, …, w_m}$ and probability $P = { p_1, p_2, …, p_m}$. </p>
<p>Now we randomly select an element $X$ from $S$ according to probability $P$. Define $Y$ the index $X$. For example, if $X = s_i$, then $Y = i$. The expected weight of $X$ is given by</p>
<p>$$<br>E[w_Y] = \sum_{i \in [1,m]} w_i P( Y = i) = \sum_{i \in [1,m]} w_i p_i<br>$$</p>
<p>If $P$ is a uniform distribution, i.e., $p_i = 1/m, \ \forall i \in [1,m]$, then<br>$$<br>E[w_Y] = 1 / m * \sum_{i \in [1,m]} w_i<br>$$</p>
<p>Then $m E[w_Y]$ equals just the sum of $W$. We say that it is an <strong>unbiased estimator</strong> of the sum of $W$. But what if $P$ is not uniform, i.e., $\exists i,j \ s.t., p_i \neq p_j$.</p>
<p>One technique to remedy this is to construct a new variable $Z_Y = w_Y / p_Y$. </p>
<p>$$<br>E[Z_Y] =  \sum_{i \in [1,m]} w_i / p_i * P(Y = i) = \sum_{i \in [1,m]} w_i<br>$$</p>
<p>The strange thing is that how we could know $P = { p_1, p_2, …, p_m}$. If we know it in advance, we need not to do sampling to estimate $ \sum_{i \in [1,m]} w_i$. </p>
<p>But in some circumstance, it is possible. To get a random sample $X$ from $S$, we need to go through some sampling process. Once $X$ is returned, we can calculate the probability of $X$ by examining the sampling process. </p>
<p>[FBKZ16] gives such an example.  Consider two relations $R_1 = (A_1, F)$ and $R_2 = (F, A_2)$, where $R_1.F$ and $R_2.F$ shares the same domain.  Let $R = R_1 \bowtie_{R_1.F = R_2.F} R_2$ be the natural join result based on $R_1.F$ and $R_2.F$. The object is to calculate the sum of attribute $A_2$ on $R = (A_1, F, A_2)$. </p>
<p>The natural join can be expensive. Is there any way to estimate the join result? Indeed we have. </p>
<p>First sample uniformly at random a tuple $(a_1, f)$ from $R_1$. The probability of a particular tuple chosen is $1/|R_1|$, where $|R_1|$ is the number of tuples in $R_1$. </p>
<p>We further assume that there is an index on $R_2.F$. So we immediately know the size of the set $R_2[f] \doteq { (f, a_2) | (f, a_2) \in R_2 }$. We choose uniformly at random a tuple $(f, a_2)$ from $R_2[f]$ and get the join result $(a_1, f, a_2)$. The probability corresponding to $(a_1, f, a_2)$ is $1/ |R_1| * 1 / R_2[f]$. So $\frac{a_2}{|R_1| |R_2[f]|}$ is an unbiased estimator of $sum(R.A2)$</p>
<p><strong>Reference</strong><br>[FBKZ16] Feifei. Li, Bin. Wu, Ke. Yi, Zhuoyue. Zhao, Wander Join: Online Aggregation via Random Walks. <em>SIGMOD’16</em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/08/29/SimRank/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/29/SimRank/" class="post-title-link" itemprop="url">SimRank</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-29 20:38:54" itemprop="dateCreated datePublished" datetime="2016-08-29T20:38:54+10:00">2016-08-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-19 21:34:01" itemprop="dateModified" datetime="2020-05-19T21:34:01+10:00">2020-05-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>SimRank</strong><br>Given a directed graph $G = \left&lt; V, E \right&gt;$, SimRank score is defined as follows<br>$$<br>s(i,j) =<br>    \begin{cases}<br>        \begin{array}{ll}<br>        1, &amp;\text{ if } i = j \<br>        \frac{c}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a,b), &amp;\text{otherwise}<br>        \end{array}<br>    \end{cases}<br>$$</p>
<p>where $I(i)$ is the set of in-neighbors of $i \in V$, and $c \in (0, 1)$. </p>
<p>By the definition of <em>SimRank</em>, $s(i,j)$ can be interpreted as the value of<br>$$<br>E[c^{T_{i,j} } ]<br>$$</p>
<p>where $T_{i,j}$ is the random variable of the first time that a reverse random walk (a random walk that travel along the incoming edges of a vertex randomly at each step) from $i$ and a reverse random walk from $j$ meets. Note that<br>$$<br>T_{i,j} =<br>    \begin{cases}<br>        \begin{array}{ll}<br>        0, &amp;\text{ if } i = j \<br>        \frac{1}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} T_{a,b}, &amp;\text{otherwise}<br>        \end{array}<br>    \end{cases}<br>$$</p>
<p>Therefore,<br>$$<br>\begin{aligned}<br>E[c^{T_{i,j} } ]<br>    &amp;= c^0 \cdot \Pr[T_{i,j} = 0] + \frac{1}{|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} E[c^{1 + T_{a, b}} ] \<br>    &amp;= \mathfrak{1}<em>{i = j} +  \frac{c}{|I(i)| |I(j)|} \sum</em>{a \in I(i), b \in I(j)} E[c^{ T_{a, b}} ] \<br>    &amp;= \mathfrak{1}<em>{i = j} +  \frac{c}{|I(i)| |I(j)|} \sum</em>{a \in I(i), b \in I(j)} s(a, b) \<br>\end{aligned}<br>$$</p>
<p>However, there is another random walk interpretation as follows:</p>
<ol>
<li><p>First, notice that when $i \neq j$, [TX16] rewrites the SimRank as<br>$$<br>s(i,j) = \frac{ \sqrt c \sqrt c} {|I(i)| |I(j)|} \sum_{a \in I(i), b \in I(j)} s(a,b)<br>$$</p>
</li>
<li><p>Second, [TX16] defines the $\sqrt c$-random walk as </p>
<ul>
<li>In any step, the walk stops with probability $1 - \sqrt c$.</li>
<li>With the other $\sqrt c$ probability, the walk chose the in-neighbors of the current vertex uniformly at random.</li>
</ul>
</li>
<li><p>Finally $s(i,j)$ is interpreted as </p>
<blockquote>
<p>The probability of two $\sqrt c$ random walks ever meet, such that one of the random walks starts at vertex $i$ and the other starts at $j$.</p>
</blockquote>
</li>
</ol>
<p><strong>Monte Carlo</strong><br>A straightforward solution is to perform Monte Carlo simulation to calculate $s(i,j)$ – we initialize two random walks from $i$ and $j$ respectively and simulate the two walks until they meet or either walk dies.</p>
<p>We need not to worry about that this process continues forever. Since the expected number of steps of a $\sqrt c$ random walk  is given by<br>$$<br>\sqrt c + \sqrt c^2 + \sqrt c^3 + …… = \frac{ \sqrt c}{ 1 - \sqrt c}<br>$$<br>which is a constant number. </p>
<p><strong>Chernoff Bound</strong><br>We repeat this process $n_w$ times and use $X_k$ to indicate whether the two walks meet in the k-th trial, i.e., $X_k = 1$ if the two walks meet and $0$ otherwise. It is easy to see that ${ X_k }$ is a set of i.i.d random variables with $E[ X_k ] = s(i, j)$.</p>
<p>Let $\bar{X} \doteq \frac{1}{n_w} \sum_{k = 1}^{n_w} X_k$ and $\mu \doteq s(i,j)$, So by <em>Chernoff Bound</em>,<br>$$<br>P({| \bar{X} - \mu | &gt; (1 + \lambda) \mu }) \le 2 \exp(- \frac{n_w \mu \lambda^2}{3}) = \delta<br>$$<br>where $\lambda$ is relative error ratio and $\delta$ the error probability we can tolerate. </p>
<p><strong>Time Complexity</strong><br>The previous section implies $n_w = \frac{3 }{ \mu \lambda ^2}\log \frac{ 2 }{ \delta }$.  By setting $\epsilon = (1 + \lambda) \mu \Leftrightarrow \lambda = \frac{\epsilon}{\mu} - 1$, we get </p>
<p>$$<br>n_w = \frac{ 3 }{ \mu ( \frac{\epsilon}{\mu}^2 -  2\frac{\epsilon}{\mu}  + 1 ) }\log \frac{ 2 }{ \delta }<br>= O( \frac{ \mu }{ \epsilon^2 }\log \frac{ 1 }{ \delta })<br>$$</p>
<p>Notice that when $\epsilon$ is small, say $0.01$, $\frac{1}{\epsilon^2}$ is large thus can not be neglected. The problem remains whether we could reduce the number of random walks.</p>
<p><strong>Hitting probability</strong><br>Another acute observation from [TX16] is that if two walks reach the same vertex, their following behavior follows the same probability distribution (since the walks are memoryless). </p>
<p>Motivated by this, we define $h_i^l(k)$ as the probability such that a walk begins at vertex $i$ and reaches vertex $j$ at the l-th step. $h_i^l(k)$ can be updated iteratively using<br>$$<br>h_i^l(k) = \frac{ \sqrt c }{ |I(i) |  } \sum_{j \in I(i)} h_j^{l -1}(k)<br>$$</p>
<p><strong>SimRank and Hitting Probability</strong><br>Now we set about bridging the gap between SimRank score and hitting probability. </p>
<p>Define $E(i,j)$ as the event that two random walks starting from i and j respectively ever meet. $P(E(i,j)) = s(i,j)$. </p>
<p>$E(i, j, k, l)$ denotes the event that two random walks, starting from $i$ and $j$ respectively, meet at $l$-th step at $k$ and this is the last time they meet. In other word, these two walk never meet again after $l$-th step at $k$. It is easy to see that for different $k$ and $l$, the $E(i, j, k, l)$’s are mutually exclusive and constitute a partition of $E(i,j)$,</p>
<p>$$<br>E(i,j) = \bigcup_l \bigcup_l E(i, j, k, l)<br>$$</p>
<p>Let $d_k$ be the probability that two walks from k do not meet each other after $0$-th step. Then $P(E(i, j, k, l)) = h_i^l(k) h_j^l(k) d_k$. Thus</p>
<p>$$<br>\begin{aligned}<br>P( E(i,j) ) &amp;=<br>P(\bigcup_l \bigcup_l E(i, j, k, l))  \<br>&amp;= \sum_{l = 0}^{\infty}  \sum_{k = 1}^{n} P(E(i, j, k, l)) \<br>&amp;= \sum_{l = 0}^{\infty}  \sum_{k = 1}^{n} h_i^l(k) h_j^l(k) d_k<br>\end{aligned}<br>$$</p>
<p><strong>Computation</strong><br>Notice that for a given $i$, there infinity many $h_i^l(k)$’s we need to kept, which is unfeasible. To get around this, we only keep the $h_i^l(k)$’s greater than $\epsilon_h$, where $\epsilon_h = O(\epsilon)$.</p>
<p>The probability that the $\sqrt c$-random walk does not to stop before $l$-th step is $\sqrt c ^l$, i.e.,<br>$$<br>\sum_{k = 1}^n h_i^l(k) = \sqrt c^l<br>$$ </p>
<p>For a fixed $l$, at most $\sqrt c ^l / \epsilon_h$ number of $h_i^l(k)$’s are kept. So the total number of $h_i^l(k)$’s kept is bounded by<br>$$<br>\frac{1}{\epsilon_h} (\sqrt c + \sqrt c^2 + \sqrt c^3 + ……  )= O(\frac{1}{\epsilon_h}) = O(\frac{1}{\epsilon})<br>$$</p>
<p>On the other hand, estimation of $d_k$ could be done by monte carlo simulation. Similar to analysis in <strong>Chernoff Bound</strong> and <strong>Time Complexity</strong> section, we can show that with $O \left( \frac{1}{\epsilon_d^2} \log \frac{1}{\delta_d} \right)$ expected time, $d_k$ could be estimated by $\bar{d_k}$ such that $| \bar{d_k} - d_k| &lt; \epsilon_d$ holds with at least $1 - \delta_d$ probability, where $\epsilon_d = O(\epsilon)$.</p>
<p><strong>Reference</strong><br>[TX16] Boyu. Tian and Xiaokui. Xiao SLING: A Near-Optimal Index Structure for SimRank. <em>SIGMOD’16</em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/12/01/Random%20Sampling%20and%20Cuts%20II/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="你生之前悠悠千載已逝，未來還會有千年沉寂的期待">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/12/01/Random%20Sampling%20and%20Cuts%20II/" class="post-title-link" itemprop="url">Random Sampling and Cuts II</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-12-01 22:27:54" itemprop="dateCreated datePublished" datetime="2015-12-01T22:27:54+11:00">2015-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-04-07 13:01:58" itemprop="dateModified" datetime="2018-04-07T13:01:58+10:00">2018-04-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>Sampling Theorem</strong></p>
<p>In 1994, Karger gave the following result:</p>
<p><strong>[Sampling Karger]</strong> <em>[Using randomized sparsification to approximate minimum cuts]</em> Given a graph G and a collection of random variables $X_e (X_e \le M)$ associated with all edges e, with probability $1 - O(n^{-d})$ that every cut in the instance $G(X_e)$ generated by putting weight $X_e$ on each e has value with $(1 - \epsilon, 1 + \epsilon)$ times its expected value, where<br>$$<br>\epsilon = \sqrt{2(d + 2)M\ln n \over c’}<br>$$<br>and c’ is G’s minimum expected value.</p>
<p>As an application, if we fix $\epsilon$ and let $p = \Theta({\ln n \over \epsilon^2 c})$, where c is the minimum cut of G, then G’ approximate all its original graph cuts by a factor p with high probability.</p>
<p>The limitation is that either $\epsilon$ or p depends on c. When c is little, we could not reduce many edges.</p>
<p><strong>Improvement:</strong><br>In 1996, Benczur and Karger improved previous sampling method such that the dependence on c is removed.</p>
<p><strong>New Result</strong>: </p>
<blockquote>
<p>Given a graph G and a parameter $\epsilon$, there is a new graph G’ such that </p>
<ol>
<li>G’ has $O({n \ln n \over \epsilon^2})$ edges.</li>
<li>every cut in G’ has value $(1 \pm \epsilon)$ times its original cut value.</li>
</ol>
</blockquote>
<p>The intuition is that in the dense parts of the graph, each edge can be sample with smaller probability in order to preserve cut value while edge should be sample with higher probability where the graph is sparse.</p>
<p>Before moving on we first formalize the notion of a dense region. We characterize a dense region by <em>Strong Connectivity</em>.</p>
<p><strong>k-Connected</strong> </p>
<blockquote>
<p>A graph G is k connected if its minimum cut value is no less than k.</p>
</blockquote>
<p><strong>Vertex-Induced Subgraph</strong></p>
<blockquote>
<p>$G’=&lt;V’,E’&gt;$ is a vertex induced subgraph of $G =&lt;V,E&gt;$ if $V’$ is a subset of $V$ and $E’$ is all edges of $E$ whose endpoints are both in $V’$.</p>
</blockquote>
<p><strong>k-Strong Component</strong></p>
<blockquote>
<p>A k-Strong Component is a maximal k-connected vertex-induced subgraph.</p>
</blockquote>
<p><strong>Strong Connectivity</strong></p>
<blockquote>
<p>The strong connectivity $c_e$ of an edge e is the maximum value of k such that a k-Strong component contains e.</p>
</blockquote>
<h5 id="Generating-the-graph"><a href="#Generating-the-graph" class="headerlink" title="Generating the graph"></a>Generating the graph</h5><p>Here is how new sampling technique operates</p>
<blockquote>
<p>Given a parameter $\epsilon$, let $\rho = {16(d + 2)M \ln n \over \epsilon^2}$, we assign to each edge a random weight $X_e$, which takes value $1/p$ with probability $p$ and $0$ with probability $(1 - p)$, where $p = \min { \rho / c_e, 1 }$. When $X_e = 0$, we eliminate e from the new graph.</p>
</blockquote>
<p>Immediately follows from the definition of $X_e$ we see that $E[X_e] = 1$.</p>
<p>To check the sampling technique works, we need to verify two things:</p>
<ol>
<li>The number of edge generated is $O(n \ln n /  \epsilon^2)$</li>
<li>Each cut does not deviate much from its original value.</li>
</ol>
<p><strong>Number of edges</strong></p>
<p><strong>Theorem 1</strong> </p>
<blockquote>
<p>If an undirected connected graph has k(n - 1) edges, then it must has a k strong component.  </p>
</blockquote>
<p><em>Proof:</em> We prove it by induction. When k = 1, then the graph itself is a k-strong component. When n = 2, the theorem clearly holds.  </p>
<p> Now we consider the general case. Suppose that the graph does not contain a k-strong component. Then there exist a cut $C = &lt;A, B&gt;$ in the graph such that $cut(A,B) &lt; k$. Now both A and B have less than n nodes. By induction, A has less then k(|A| - 1) edges inside and B has less then k(|B| - 1) edges inside. The total number of edges is less than k(|A| - 1) + k + k(|B| - 1) = k(|A|+|B| - 1) = k(n - 1). A contradiction.</p>
<p><strong>Theorem 2</strong></p>
<blockquote>
<p>$\Sigma {1 \over c_e} &lt; 2(n -1)$</p>
</blockquote>
<p><em>Proof:</em> Suppose that $\Sigma {1 \over c_e} \ge 2(n -1)$, then by theorem 1, there is a 2-strong component. Let e* be the edge with minimum $c_{e^*}$ in that component. Then there is a cut C in the 2-strong component containing e* such that $cut(C) \le c_{e^*}$. Now  </p>
<p>$$<br>{\underset{e \in C}{\Sigma }} c_e^{-1} \le c_{e^*}^{-1} c_{e^*} = 1 &lt; 2<br>$$</p>
<p>A contradiction. </p>
<p><strong>Error analysis</strong></p>
<p>To analyze the errors, we denote $F_i$ the set of edges whose strong connectivity are between $[2^i, 2^{i+1}]$ for $i \ge 0$. Let $G_i = &lt;V, F_i \cup F_{i + 1} \cup F_{i + 2}…&gt;$.</p>
<p>In $G_i$, each edge is sampled in the following way<br>$$<br>X_e = \begin{cases}<br>1, if \ e \notin F_i \<br>         \begin{Bmatrix}<br>           {1 \over p} with \ probability \ p, \<br>           0 \  with  \ probability \ 1 - p<br>    \end{Bmatrix} \ if \ e \in F_i<br>\end{cases}<br>$$</p>
<p>Then, by sampling theorem, the sampled graph $G_i’$ is within $(1 \pm \epsilon)G_i$.</p>
<p>So<br>$$<br>\begin{aligned}<br>G’ &amp; = \Sigma_{i = 0}^{\log m} F_i’ \<br>&amp; = \Sigma G_i’ - \Sigma G_{i + 1} \<br>&amp; = \Sigma (1 \pm \epsilon)G_i - \Sigma G_{i + 1} \<br>&amp; = (1 \pm \epsilon)G_0 \pm \epsilon\Sigma G_{i + 1} \<br>&amp; \in G \pm \epsilon \log m \ G<br>\end{aligned}<br>$$</p>
<p>But there is a log m term in the final result. We can eliminate the log m term with more sophisticated techniques.</p>
<p>The random variable $X_e$ assigned to e in $G_i$ is revised as follows:<br>$$<br>X_e = \begin{cases}<br>2^{i - j}, if \ e \notin F_i \ and \ e \in F_j \<br>         \begin{Bmatrix}<br>           {1 \over p} with \ probability \ p, \<br>           0 \  with  \ probability \ 1 - p<br>    \end{Bmatrix} \ if \ e \in F_i<br>\end{cases}<br>$$</p>
<p>It can be shown that every cut in $G_i$ is at least $2^i$ heavy.</p>
<p>So<br>$$<br>\begin{aligned}<br>G’ &amp; = \Sigma_{i = 0}^{\log m} F_i’ \<br>&amp; = \Sigma G_i’ - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \<br>&amp; = \Sigma (1 \pm \epsilon)G_i - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \<br>&amp; = \Sigma F_i + (1 \pm \epsilon)\Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j - \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \<br>&amp; = G \pm 2 \epsilon \ \Sigma_{i = 0}^{\log m} \Sigma_{j = i + 1}^{\log m} 2^{i - j} F_j \<br>&amp; = G\pm 2 \epsilon \ G<br>\end{aligned}<br>$$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description">你生之前悠悠千載已逝，未來還會有千年沉寂的期待</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
