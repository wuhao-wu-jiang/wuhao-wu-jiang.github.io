<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/21/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/21/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/27/Twin-Drive-or-Not/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/27/Twin-Drive-or-Not/" class="post-title-link" itemprop="url">Twin Drive or Not?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-27 16:15:03" itemprop="dateCreated datePublished" datetime="2020-10-27T16:15:03+11:00">2020-10-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-06 11:08:17" itemprop="dateModified" datetime="2020-11-06T11:08:17+11:00">2020-11-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this article, we introduce differential privacy. We start with a story of information leakage.</p>
<h3 id="twin-drive-or-not">Twin Drive or Not</h3>
<p>Celestial Being is a private military organization with superior technology. It has four advanced machines. The most powerful one is the Gundam 00.</p>
<p>The Gundam 00 is about to engage the enemy.</p>
<div style="text-align:center">
<p><img src="https://lh5.googleusercontent.com/-9AOc-O5bdRk/S_mZxjf1DWI/AAAAAAAACvs/uXI0SvAMqWc/s640/5.jpg" width="500" height="250" /></p>
</div>
<p>Due to maintenance, Gundam 00 is not always equipped with two engines (known as <strong><em>twin drive system</em></strong>). With probability <strong><em>0.5</em></strong>, it uses one drive. We use <span class="math inline">\(T = (1, 1)\)</span> to denote the status of equipping with the twin drive system, and <span class="math inline">\(S = (1, 0)\)</span> (or <span class="math inline">\((0, 1)\)</span>) for status of single drive. Let <span class="math inline">\(X\)</span> be a random variable that indicates drive status. Therefore, <span class="math display">\[
\Pr[ X = T ] = 0.5, \\
\Pr[ X = S ] = 0.5.  
\]</span></p>
<div style="text-align:center">
<p><img src="https://knolly.files.wordpress.com/2009/03/00gundamdrive1.jpg" width="500" height="250" /></p>
</div>
<p>Raiser sword is one of Gundam 00 most powerful weapon. The energy level of the Raiser sword, denote as <span class="math inline">\(Y\)</span>, is a random variable in <span class="math inline">\([0, 100]\)</span>, whose distribution, denoted as <span class="math inline">\(\Pr[\cdot \mid X]\)</span>, depends on the drive status. Thus, <span class="math inline">\(\Pr[Y = y \mid X = T]\)</span> (or <span class="math inline">\(\Pr[Y = y \mid X = S]\)</span>) is the probability that <span class="math inline">\(Y\)</span> equals to <span class="math inline">\(y\)</span> conditioned on <span class="math inline">\(X = T\)</span> (<span class="math inline">\(X = S\)</span>). Specifically, <span class="math display">\[
\Pr[\cdot \mid T] \sim B(100, 0.81) \\
\Pr[\cdot \mid S] \sim B(100, 0.09)
\]</span></p>
<p>where <span class="math inline">\(B(n, p)\)</span> denotes a binomial distribution. The expected energy level is <span class="math inline">\(81\)</span> with <em>twin drive system</em>, compared to only <span class="math inline">\(9\)</span> with single drive. This is called "<strong><em>squaring</em></strong>" phenomenon of <em>twin drive</em>.</p>
<div style="text-align:center">
<p><img src="https://vignette.wikia.nocookie.net/gundam/images/8/84/Raiser_sword.png/revision/latest?cb=20101124151429" width="500" height="250" /></p>
</div>
<p>Now suppose that you're the enemy pilot of a mobile suit with just average performance. Before you start dog fighting with Gundam 00, you will be attacked by the 00's long-range raiser sword (you have not seen 00 yet). Luckily, you survive the raiser sword attack. Now you need to make a decision. If 00 is equipped with twin drive, there is zero chance that you can win the dog fight. The best choice is to leave the battle. Otherwise, you can outperform 00 and you would like to engage it.</p>
<div style="text-align:center">
<p><img src="https://blogimg.goo.ne.jp/user_image/79/da/c198b8cf829b5f7f7dd507d40bb39ba5.jpg" width="500" height="250" /></p>
</div>
<p>Since you have just been attacked by the raiser sword, you have an observation of its energy level <span class="math inline">\(y\)</span>. By Bayes' theorem, <span class="math display">\[
\begin{aligned}
\Pr[ X = T \mid Y = y] = \frac{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] }{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] + \Pr[Y = y \mid X = S] \cdot \Pr[X = S] } 
\\
\\
\Pr[ X = S \mid Y = y] = \frac{ \Pr[Y = y \mid X = S] \cdot \Pr[X = S] }{ \Pr[Y = y \mid X = T] \cdot \Pr[X = T] + \Pr[Y = y \mid X = S] \cdot \Pr[X = S] }
\end{aligned}
\]</span></p>
<p>Substituting with <span class="math inline">\(\Pr[X = S] = \Pr[X = T] = 0.5\)</span>, we get <span class="math display">\[
\begin{aligned}
\Pr[ X = T \mid Y = y] = \frac{ \Pr[Y = y \mid X = T]  }{ \Pr[Y = y \mid X = T]  + \Pr[Y = y \mid X = S] } 
\\
\\
\Pr[ X = S \mid Y = y] = \frac{ \Pr[Y = y \mid X = S]  }{ \Pr[Y = y \mid X = T]  + \Pr[Y = y \mid X = S] }
\end{aligned}
\]</span></p>
<p>The distributions <span class="math inline">\(B(100, 0.09)\)</span> and <span class="math inline">\(B(100, 0.81)\)</span> are plotted together below.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/B-100-0.09-vs-B-100-0.81.png?raw=true" /></p>
<p>Immediately, you can draw some conclusions based on single value of <span class="math inline">\(y\)</span>, such as</p>
<ol type="1">
<li>If <span class="math inline">\(y = 10\)</span>, it is likely 00 has only one drive.<br />
</li>
<li>If <span class="math inline">\(y = 75\)</span>, it is likely 00 has twin drive.</li>
</ol>
<p>Or you can conclude based on the range of <span class="math inline">\(y\)</span>, such as</p>
<ol type="1">
<li>If <span class="math inline">\(y \in [0, 20]\)</span>, it is likely 00 has single drive.</li>
<li>If <span class="math inline">\(y \in [70, 90]\)</span>, it is likely 00 has twin drive.</li>
</ol>
<p>Before the observation of <span class="math inline">\(y\)</span>, as <span class="math inline">\(\Pr[X = S] = \Pr[X = T] = 0.5\)</span>, you have only random guess over the engine status of 00. After the observation, you might be much more confident about your guess, even though the energy level of the raiser sword is a random variable.</p>
<p>Why does this happen? Because the two distributions are well-separated. They are far from each other. Further, the prior probabilities <span class="math inline">\(\Pr[X = S]\)</span> and <span class="math inline">\(\Pr[X = T]\)</span> play important roles. If you know <span class="math inline">\(\Pr[X = S] = 10^{-10}\)</span>, even if you observe an energy level of <span class="math inline">\(y = 10\)</span>, you had better not engage Gundam 00. You know it could be just a trap!</p>
<h3 id="formal-definition">Formal Definition</h3>
<p>Database managers face similar scenarios in privacy protection. We know define the setting for differential privacy. We view a dataset <span class="math inline">\(D\)</span> as a table of <span class="math inline">\(n\)</span> rows, each of which comes from a domain <span class="math inline">\(\mathcal{X}\)</span>. Hence <span class="math inline">\(D \in \mathcal{X}^n\)</span>. Instead of releasing the dataset directly, the manager runs a randomized algorithm <span class="math inline">\(A: \mathcal{X}^n \rightarrow \mathcal{Y}\)</span> on <span class="math inline">\(D\)</span>, and outputs <span class="math inline">\(Y = A(D) \in \mathcal{Y}\)</span>. Here <span class="math inline">\(\mathcal{Y}\)</span> is called the co-domain of <span class="math inline">\(A\)</span> and does not necessarily equal to <span class="math inline">\(\mathcal{X}^n\)</span>. Note that the output distribution of <span class="math inline">\(A\)</span> may depend on the input <span class="math inline">\(D\)</span>. Further, for simplicity of discussion, we believe a uniform prior distribution over datasets in <span class="math inline">\(\mathcal{X}^n\)</span>.</p>
<p>In the Gundam 00's story, <span class="math inline">\(\mathcal{X} = \{0, 1\}^2\)</span>, and <span class="math inline">\(\mathcal{Y} = [0, 100]\)</span>.</p>
<p>Suppose there is another dataset <span class="math inline">\(D&#39;\)</span> that differs only one row from <span class="math inline">\(D\)</span>. The algorithm <span class="math inline">\(A\)</span> is said to be differentially private if a malicious user is unlikely to distinguish the input to <span class="math inline">\(A\)</span> between <span class="math inline">\(D\)</span> and <span class="math inline">\(D&#39;\)</span>, by merely observing <span class="math inline">\(A\)</span>'s output. Simply put, the conditional distributions of <span class="math inline">\(\Pr[\cdot \mid X = D]\)</span> and <span class="math inline">\(\Pr[\cdot \mid X = D&#39;]\)</span> should be similar, where <span class="math inline">\(X\)</span> is a variable that denotes the input dataset.</p>
<p>We observe in the previous section that, if the distributions are well separated, then we can infer the underlying input with high confidence when the output value takes specific values or lies in certain ranges.</p>
<p>There are many ways to characterize the closeness of two distributions. We introduce the one proposed in [1].</p>
<blockquote>
<p>Algorithm <span class="math inline">\(A\)</span> is called <span class="math inline">\((\epsilon, \delta)\)</span> differentially private, if for any pair of neighboring datasets <span class="math inline">\(D\)</span> and <span class="math inline">\(D&#39;\)</span> (the ones that differ in only one row), and for any (measurable) subset <span class="math inline">\(\mathcal{R} \subset \mathcal{Y}\)</span>, it holds that <span class="math display">\[
\Pr[Y \in \mathcal{R} \mid X = D ] \le \exp(\epsilon) \cdot \Pr[Y \in \mathcal{R} \mid X = D&#39; ]+ \delta
\]</span></p>
</blockquote>
<p>In other words, we can't find a subset <span class="math inline">\(\mathcal{R}\)</span>, with which we can distinguish <span class="math inline">\(D\)</span> and <span class="math inline">\(D&#39;\)</span> with high confidence. In the previous example, such <span class="math inline">\(\mathcal{R}\)</span> exists. E.g., <span class="math inline">\(R = [70, 90]\)</span>. If <span class="math inline">\(Y\)</span> lies in <span class="math inline">\(\mathcal{R}\)</span>, we can infer that Gundam 00 is likely to have twin drive.</p>
<h3 id="hypothesis-testing">Hypothesis Testing</h3>
<p>There is alternative view of <span class="math inline">\((\epsilon, 0)\)</span> differential privacy as hypothesis testing. Suppose that we know the underlying dataset is either <span class="math inline">\(D\)</span> or <span class="math inline">\(D&#39;\)</span> with equal probability. After observing the output of <span class="math inline">\(A\)</span>, we need to decide which hypothesis of the following holds: <span class="math display">\[
H_0: \text{ the dataset if } D \\
H_1: \text{ the dataset if } D&#39;
\]</span></p>
<p>Assume that we adopt a fixed strategy:</p>
<ol type="1">
<li>First we choose a fixed subset <span class="math inline">\(\mathcal{R} \subset \mathcal{Y}\)</span>,</li>
<li>If <span class="math inline">\(Y \in \mathcal{R}\)</span>, we choose to accept <span class="math inline">\(H_0\)</span>,</li>
<li>Otherwise, we accept <span class="math inline">\(H_1\)</span>.</li>
</ol>
<p>There are two kinds of errors we can make. The type I error is the one when the true hypothesis is <span class="math inline">\(H_0\)</span> and we accept <span class="math inline">\(H_1\)</span>. Conversely, type II error is the one when the true hypothesis is <span class="math inline">\(H_1\)</span> and we accept <span class="math inline">\(H_0\)</span>. Let <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> be the probabilities we make type I and II errors respectively. The following theorem holds</p>
<blockquote>
<p>Algorithm A is <span class="math inline">\((\epsilon, 0)\)</span> differentially private then</p>
<ol type="1">
<li><span class="math inline">\(\frac{1}{1 + \exp(\epsilon) } \le q \le \frac{1}{1 + \exp(-\epsilon) }\)</span><br />
</li>
<li><span class="math inline">\(\frac{1}{1 + \exp(\epsilon) } \le p \le \frac{1}{1 + \exp(-\epsilon) }\)</span></li>
</ol>
</blockquote>
<p><em>Proof.</em> By definition, <span class="math display">\[
\begin{aligned}
    q   &amp;= \Pr[ X = D \mid  Y \in \bar{\mathcal{R}} ] \\
        &amp;= \frac{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \Pr[X = D ] }{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \Pr[X = D ] + \Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ] \Pr[X = D&#39; ] } \\
        &amp;= \frac{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]  }{ \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]  + \Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ]  } 
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(A\)</span> is <span class="math inline">\((\epsilon, 0)\)</span> differentially private, it holds that <span class="math display">\[
\Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ] \le \exp(\epsilon) \cdot \Pr[  Y \in \bar{\mathcal{R}} \mid X = D ]
\]</span> and <span class="math display">\[
\Pr[  Y \in \bar{\mathcal{R}} \mid X = D ] \le \exp(\epsilon) \cdot \Pr[  Y \in \bar{\mathcal{R}} \mid X = D&#39; ]
\]</span></p>
<p>therefore, <span class="math display">\[
\frac{1}{1 + \exp(\epsilon) } \le q \le \frac{1}{1 + \exp(-\epsilon) }
\]</span></p>
<p>By symmetry, we also have <span class="math display">\[
\frac{1}{1 + \exp(\epsilon) } \le p \le \frac{1}{1 + \exp(-\epsilon) }
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference">Reference</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013<br />
[2] L. Wasserman and S. Zhou, “A statistical framework for differential privacy,” arXiv:0811.2501 [math, stat], Oct. 2009, Accessed: Oct. 27, 2020.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/26/Norm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/26/Norm/" class="post-title-link" itemprop="url">Norm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-26 22:17:18" itemprop="dateCreated datePublished" datetime="2020-10-26T22:17:18+11:00">2020-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-01 00:49:31" itemprop="dateModified" datetime="2020-11-01T00:49:31+11:00">2020-11-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>A norm <span class="math inline">\(| \cdot |:\mathbb{R}^n \rightarrow \mathbb{R}\)</span> is a function that satisfies the following properties:</p>
<ol type="1">
<li>Positive definite: <span class="math inline">\(|x| = 0 \rightarrow x = \vec 0\)</span>,</li>
<li>Nonnegative:<span class="math inline">\(|x| \ge 0\)</span> for any <span class="math inline">\(x \in \mathbb{R}^n\)</span>,</li>
<li>Absolutely homogeneous: <span class="math inline">\(|k x| = |k| |x|\)</span>, for any <span class="math inline">\(x \in \mathbb{R}^n, k \in \mathbb{R}\)</span>,</li>
<li>Subadditive (triangle inequality): <span class="math inline">\(|x + y| \le |x| + |y|\)</span>, for any <span class="math inline">\(x \in \mathbb{R}^n, y \in \mathbb{R}^n\)</span>.</li>
</ol>
<p>Given properties 1-3, we claim that property 4 is equivalent to</p>
<ol start="5" type="1">
<li>The region <span class="math inline">\(\{ x \in \mathbb{R}^n: |x| \le 1\}\)</span> is convex.</li>
</ol>
<p><em>Proof:</em> The proof is straightforward.</p>
<p><span class="math inline">\(5 \rightarrow 4:\)</span> If <span class="math inline">\(x = \vec 0\)</span> or <span class="math inline">\(\vec y = 0\)</span>, then 4 holds trivially. Otherwise, suppose that <span class="math inline">\(x, y \neq \vec 0\)</span>. Then <span class="math display">\[
\begin{aligned}
    &amp;|x + y| \le |x| + |y| \\
    \leftrightarrow 
    &amp;| \frac{ |x| }{ |x| + |y| } \frac{ x }{ |x| } + \frac{ |x| }{ |x| + |y| } \frac{ y }{ |y| } | \le 1
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(|\frac{ x }{ |x| } | =1\)</span>, <span class="math inline">\(|\frac{ y }{ |y| }| = 1\)</span> and <span class="math inline">\(\frac{ |x| }{ |x| + |y| } + \frac{ |y| }{ |x| + |y| } = 1\)</span>, the second inequality follows exactly from condition 5.</p>
<p><span class="math inline">\(4 \rightarrow 5:\)</span> Let <span class="math inline">\(x, y \in \mathbb{R}^n, |x| \le 1, |y| \le 1\)</span> and <span class="math inline">\(a, b \ge 0\)</span>, <span class="math inline">\(a + b = 1\)</span>. Then <span class="math display">\[
|ax + by| \le |ax| + |by| = a|x| + b|y| = 1
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="induced-norm"><strong>Induced Norm</strong></h3>
<p>To illustrate a deeper connection between property 4 and 5, we first show how a norm can be induced by a convex and symmetric region centered at the origin <span class="math inline">\(O\)</span>. Let the boundary of the region as <span class="math inline">\(E\)</span>. We are going to induce a norm by <span class="math inline">\(E\)</span>, denoted as <span class="math inline">\(|\cdot |_E\)</span>.</p>
<p>First, we define</p>
<ol type="1">
<li><span class="math inline">\(| x |_E = 1, \forall x \in E\)</span>.</li>
</ol>
<p>For any other vector <span class="math inline">\(x \notin E\)</span>, consider the ray initiated from the origin and with the same direction as <span class="math inline">\(x\)</span>. Denote its intersection point with <span class="math inline">\(E\)</span> as <span class="math inline">\(x_E\)</span>. Suppose that <span class="math inline">\(x = a \cdot x_E\)</span> for some <span class="math inline">\(a \in \mathbb{R}_+\)</span>. Then we define</p>
<ol start="2" type="1">
<li><span class="math inline">\(| x |_E = a |x_E|_E = a\)</span>.</li>
</ol>
<p>Indeed, if we write the <span class="math inline">\(|\cdot|_2\)</span> as the <span class="math inline">\(\ell_2\)</span> norm, then the value of <span class="math inline">\(a\)</span> is given by <span class="math inline">\(\frac{ |x|_2 } { |x_E|_2 }\)</span>. Clearly, by definition, <span class="math inline">\(|\cdot|_2\)</span> is positive definite and non-negative. To prove that it is absolutely homogeneous, <span class="math inline">\(\forall k \in \mathbb{R}\)</span>, <span class="math display">\[
|k x|_E = |k ax_E|_E
\]</span></p>
<p>If <span class="math inline">\(k \ge 0\)</span>, we have <span class="math inline">\(|k ax_E|_E = ka |x_E|_E = ka\)</span>. Otherwise, if <span class="math inline">\(k &lt; 0\)</span>, <span class="math display">\[
|k x|_E = |k ax_E|_E = |-k a (-x_E)|_E = -k a |(-x_E)|_E
\]</span></p>
<p>By symmetry of <span class="math inline">\(E\)</span> (with respect to the origin <span class="math inline">\(O\)</span>), <span class="math inline">\(|(-x_E)|_E = |x_E|_E = 1\)</span>. Therefore, <span class="math display">\[
|k x|_E = |k|a 
\]</span></p>
<p>Finally, we have a graphical verification of triangle inequality. Let <span class="math inline">\(v, u \in \mathbb{R}^n\)</span>, which intersect with <span class="math inline">\(E\)</span> at <span class="math inline">\(v_E\)</span> and <span class="math inline">\(u_E\)</span> respectively. If <span class="math inline">\(|v|_E &lt; 1\)</span> (<span class="math inline">\(|u|_E &lt; 1\)</span>), then we can extend it along its direction to get its intersection with <span class="math inline">\(E\)</span>. Let <span class="math inline">\(c = |v|_E\)</span> and <span class="math inline">\(d = |u|_E\)</span>, then</p>
<ol type="1">
<li><span class="math inline">\(v = c \cdot v_E\)</span>,</li>
<li><span class="math inline">\(u = d \cdot u_E\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(w = v + u = c \cdot v_E + d \cdot u_E\)</span>. Let <span class="math inline">\(p\)</span> be the intersection between <span class="math inline">\(w\)</span> and the line segment between <span class="math inline">\(v\)</span> and <span class="math inline">\(u\)</span>. It is easy to verify that <span class="math display">\[
p = \frac{c}{c + d} v_E + \frac{d}{ c + d} u_E
\]</span></p>
<p>By convexity of the region bounded by <span class="math inline">\(E\)</span>, <span class="math display">\[
|p|_E \le 1
\]</span></p>
<p>It concludes that <span class="math inline">\(|w|_E = |c v_E + d u_E |_E \le c + d = |v|_E + |u|_E\)</span>.</p>
<!-- <img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Induced-Norm.png?raw=true" style="zoom: 67%;" /> -->
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/Induced-Norm.png?raw=true" /></p>
<h3 id="application"><strong>Application</strong></h3>
<p>The idea discussed above significantly simplifies the proof of Minkowski inequality.</p>
<p>The <span class="math inline">\(p\)</span>-norm (<span class="math inline">\(p \ge 1\)</span>) on <span class="math inline">\(\mathbb{R}^n\)</span> is given as <span class="math display">\[
| x |_p = \big( \sum_{i = 1}^n |x_i|^p \big)^\frac{1}{p}
\]</span></p>
<p>Clearly, <span class="math inline">\(p\)</span>-norm is positive definite, nonnegative, absolutely homogeneous. It is left to verify triangle inequality, which is known as Minkowski inequality for the case of <span class="math inline">\(p\)</span>-norm. This is equivalent to show that <span class="math display">\[
\{ x \in \mathbb{R}^n : |x |_p \le 1 \}
\]</span></p>
<p>is convex. The trick here is that <span class="math inline">\(|x|_p \le 1\)</span> is equivalent to <span class="math inline">\((|x|_p)^p \le 1\)</span>. Hence, <span class="math display">\[
\{ x \in \mathbb{R}^n : |x |_p \le 1 \} = \{ x \in \mathbb{R}^n : \sum_{i = 1}^n |x_i|^p  \le 1 \}
\]</span></p>
<p>The convexity of this set follows directly from the point-wise convexity of the function <span class="math inline">\(|\cdot |^p\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong><em>Remark:</em></strong> To appreciate how concise the above proof is, we also give one traditional proof here, which consists of three steps.</p>
<h4 id="youngs-inequality">Young's Inequality</h4>
<p>For <span class="math inline">\(a, b &gt; 0\)</span>, <span class="math inline">\(p, q &gt; 0\)</span>, s.t., <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>, it holds that <span class="math display">\[
ab \le \frac{a^p}{p} + \frac{b^q}{q}
\]</span></p>
<p><em>Proof:</em> The inequality is nothing more than convexity of the function <span class="math inline">\(x \rightarrow e^x\)</span>: <span class="math display">\[
ab = \exp(\frac{ p \ln a }{p} + \frac{ q \ln b }{q}) \le \frac{ \exp( \ln a^p ) }{p} + \frac{ \exp( \ln b^q ) }{q} = \frac{a^p}{p} + \frac{b^q}{q} 
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="holders-inequality">Holder's Inequality</h4>
<p>For <span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, <span class="math inline">\(p, q &gt; 0\)</span>, s.t., <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>, it holds that <span class="math display">\[
\sum_{i = 1}^n |x_i| |y_i| \le |x|_p |y|_q
\]</span></p>
<p><em>Proof:</em> The inequality is trivial if <span class="math inline">\(x = \vec 0\)</span> or <span class="math inline">\(y = \vec 0\)</span>. Otherwise, let <span class="math inline">\(u = \frac{x}{ |x|_p }\)</span> and <span class="math inline">\(v = \frac{ y }{ |y|_q }\)</span>. It remains to prove that <span class="math display">\[
\sum_{i = 1}^n |u_i| |v_i| \le 1
\]</span></p>
<p>for <span class="math inline">\(|u|_p = 1\)</span> and <span class="math inline">\(|v|_q = 1\)</span>. WLOG, we assume <span class="math inline">\(u &gt; 0\)</span> and <span class="math inline">\(v &gt; 0\)</span>. Applying Young's Inequality, we have <span class="math display">\[
u_i v_i \le \frac{ u_i^p }{p}  + \frac{v_i^q}{q}.
\]</span> Hence, <span class="math display">\[
u \cdot v = \sum_{i = 1}^n \big( \frac{ u_i^p }{p}  + \frac{v_i^q}{q} \big) = \frac{1}{p} + \frac{1}{q} = 1 .
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h4 id="minkowski-inequality">Minkowski Inequality</h4>
<p>For <span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, <span class="math inline">\(p \ge 1\)</span>, it holds that <span class="math display">\[
|x + y|_p \le |x|_p + |y|_p
\]</span></p>
<p><em>Proof:</em> The inequality is trivial if <span class="math inline">\(x = \vec 0\)</span> or <span class="math inline">\(y = \vec 0\)</span>. Otherwise, assume that <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(y &gt; 0\)</span>. Now</p>
<p><span class="math display">\[
|x + y|_p^p = \sum_{i = 1}^n |x_i + y_i|^p = \sum_{i = 1}^n [ x_i (x_i + y_i)^{p - 1} + y_i (x_i + y_i)^{p - 1} ]
\]</span></p>
<p>Let <span class="math inline">\(q = \frac{p}{p - 1}\)</span>. Then <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>. By Holder's inequality, <span class="math display">\[
\begin{aligned}
    \sum_{i = 1}^n x_i (x_i + y_i)^{p - 1} 
    &amp;\le |x|_p ( \sum_{i = 1}^n (x_i + y_i)^{(p - 1)q} )^{ \frac{1}{q} } \\
    &amp;= |x|_p ( \sum_{i = 1}^n (x_i + y_i)^{ p } )^{ \frac{p - 1}{ p } } \\
    &amp;= |x|_p (|x + y|_p)^{p - 1}
\end{aligned}
\]</span></p>
<p>Similarly, we have <span class="math display">\[
\sum_{i = 1}^n y_i (x_i + y_i)^{p - 1} \le |y|_p (|x + y|_p)^{p - 1}
\]</span></p>
<p>Putting the inequalities together, <span class="math display">\[
|x + y|_p^p \le |x|_p (|x + y|_p)^{p - 1} + |y|_p (|x + y|_p)^{p - 1}
\]</span></p>
<p>We finish the proof by dividing both side with <span class="math inline">\((|x + y|_p)^{p - 1}\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/25/Bit-Guessing-Game/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/25/Bit-Guessing-Game/" class="post-title-link" itemprop="url">Bit Guessing Game</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-25 15:33:16" itemprop="dateCreated datePublished" datetime="2020-10-25T15:33:16+11:00">2020-10-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-03 10:23:55" itemprop="dateModified" datetime="2020-11-03T10:23:55+11:00">2020-11-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The bit guessing game models two players, namely Alice and Bob, where Alice has a secret vector <span class="math inline">\(\vec x \in \{0, 1 \}^n\)</span> which Bob tries to learn. Bob is only allowed to ask Alice the value of <span class="math display">\[
\vec c \cdot \vec x
\]</span></p>
<p>for bit vector <span class="math inline">\(\vec c \in \{0, 1\}^n\)</span> he comes up with. Under this setting, it is trivial for Bob to learn each bit of <span class="math inline">\(x\)</span>: he needs to just query Alice for <span class="math inline">\(\vec e_i \cdot \vec x\)</span>, where <span class="math inline">\(\vec e_i\)</span> is the indicator vector with its <span class="math inline">\(i\)</span>-th dimension equal to 1 and other dimensions equal to 0.</p>
<p>One way for Alice to prevent information leakage is to add some bounded noise <span class="math inline">\(Z\)</span> to the query result and returns <span class="math display">\[
r(\vec c) \doteq \vec c \cdot \vec x + Z
\]</span></p>
<p>For example, <span class="math inline">\(Z\)</span> could be a bounded random variable. Is is still a chance for Bob to learn the vector <span class="math inline">\(\vec x\)</span>?</p>
<p><strong><em>Theorem 1.</em></strong> If Bob is allowed to query <span class="math inline">\(r(\vec c)\)</span> for all <span class="math inline">\(\vec c \in \{0, 1\}^n\)</span>, and suppose that <span class="math inline">\(Z\)</span> is bounded by some real number <span class="math inline">\(R\)</span>, then Bob can have an estimation <span class="math inline">\(\vec y\)</span> such that <span class="math display">\[
|\vec x - \vec y| = \sum_{i = 1}^n | x_i - y_i | \le 4R
\]</span></p>
<p><strong>Remark:</strong> <em>In this case Bob queries for <span class="math inline">\(2^n\)</span> vectors. This result implies that is Bob is able to ask exponential number of questions, then it is hard to prevent information leakage. E.g., suppose <span class="math inline">\(R = n / 400\)</span>, which is a relatively large number when <span class="math inline">\(n\)</span> is large, the theorem says that Bob can recover <span class="math inline">\(99\%\)</span> of bits in <span class="math inline">\(\vec x\)</span>.</em></p>
<p><em>Proof:</em> We say a vector <span class="math inline">\(\vec y\)</span> is consistent with a query result <span class="math inline">\(r(\vec c)\)</span> if <span class="math display">\[
| r(\vec c) - \vec c \cdot \vec y| \le R.
\]</span></p>
<p>The strategy of Bob is to choose any <span class="math inline">\(\vec y\)</span> that is consistent with all <span class="math inline">\(\vec c \in \{0, 1\}^n\)</span>. Such <span class="math inline">\(\vec y\)</span> exits, a special of which is given by <span class="math inline">\(\vec y = \vec x\)</span>. It remains to prove that such an <span class="math inline">\(\vec y\)</span> satisfies the constraint.</p>
<p>Consider the special case of <span class="math inline">\(\vec c = \vec x\)</span>, <span class="math display">\[
|r(\vec x) - \vec x \cdot \vec y| \le R \\
|r(\vec x) - \vec x \cdot \vec x| \le R \\
\]</span></p>
<p>By triangle inequality, it holds <span class="math display">\[
|\vec x \cdot \vec y - \vec x \cdot \vec x| \le 2R.
\]</span></p>
<p>Observe that <span class="math inline">\(\vec x \cdot \vec x \ge \vec x \cdot \vec y\)</span>, which implies <span class="math display">\[ 
|\vec x \cdot \vec y - \vec x \cdot \vec x| = \vec x \cdot \vec x - \vec x \cdot \vec y \le 2R
\]</span></p>
<p>Hence, <span class="math inline">\(\vec y\)</span> and <span class="math inline">\(\vec x\)</span> differ by at most <span class="math inline">\(2R\)</span> bits, on the non-zero dimensions of <span class="math inline">\(\vec x\)</span>.</p>
<p>Let <span class="math inline">\(\bar x\)</span> be the bit-wise complement of <span class="math inline">\(\vec x\)</span>. By similar arguments, we get <span class="math display">\[
|\bar x \cdot \vec y - \bar x \cdot \vec x|\le 2R.
\]</span></p>
<p>and <span class="math inline">\(\vec x\)</span> and <span class="math inline">\(\vec y\)</span> differ by at most <span class="math inline">\(2R\)</span> bits, on the zero dimensions of <span class="math inline">\(\vec x\)</span>. It concludes that <span class="math inline">\(|\vec x - \vec y | \le 4R\)</span>.<br />
<!-- Define $S$ to be the set of non-zero dimensions of $\vec x$: 
$$
S \doteq \{ i \in [n]: x_i > 0 \}
$$

Let $\vec e_S \in \{0, 1\}^n$ ($\vec e_{\bar S} \in \{0, 1\}^n$) be the indicator vector that takes $1$ on dimensions belonging to $S$ ($\bar S$).  It follows that 
$$
|r(\vec e_S) - \vec e_S \cdot \vec y| \le R \\
|r(\vec e_S) - \vec e_S \cdot \vec x| \le R \\
$$ --></p>
<!-- By triangle inequality, it holds 
$$
|\vec e_S \cdot \vec y - \vec e_S \cdot \vec x|\le 2R.
$$   -->
<!-- Similarly, $|\vec e_{\bar S} \cdot \vec y - \vec e_{\bar S} \cdot \vec x| \le 2R$. As $S$ and $\bar S$ is a partition of $[n]$, 
$$
| \vec x - \vec y| = |\vec e_S \cdot \vec y - \vec e_S \cdot \vec x| + |\vec e_{\bar S} \cdot \vec y - \vec e_{\bar S} \cdot \vec x| \le 4R
$$ -->
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>What if Bob asks <span class="math inline">\(O(n)\)</span> questions? The following theorem states that if <span class="math inline">\(Z\)</span> is bounded by <span class="math inline">\(O(\sqrt n)\)</span>, then Bob is able to have a good estimation of <span class="math inline">\(\vec x\)</span>.</p>
<p><strong><em>Theorem 2.</em></strong> If Bob is able to ask <span class="math inline">\(\frac{n \log 2 + \ln n}{\log \frac{3}{2} - \log \lambda }\)</span> queries, and <span class="math inline">\(Z\)</span> is bounded by <span class="math inline">\(\alpha \sqrt n\)</span>, then Bob can obtains an estimation <span class="math inline">\(\vec y\)</span>, such that <span class="math display">\[
|\vec x - \vec y| \le (3e \frac{\alpha}{\lambda } )^2 \cdot n
\]</span></p>
<p><em>Proof:</em> As before, a vector <span class="math inline">\(\vec y\)</span> is said to be consistent with a query result <span class="math inline">\(r(\vec c)\)</span> if <span class="math display">\[
| r(\vec c) - \vec c \cdot \vec y| \le \alpha \sqrt n.
\]</span></p>
<p>It suffices to find a sequence of vectors <span class="math inline">\(\vec c_1, \vec c_2, ..., \vec c_k\)</span> (<span class="math inline">\(k\)</span> is a value to be determined), such that</p>
<ol type="1">
<li><span class="math inline">\(k = O(n)\)</span>,</li>
<li>For any <span class="math inline">\(\vec y\)</span> with <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span>, there exists at least one <span class="math inline">\(\vec c_i\)</span>, such that <span class="math inline">\(\vec y\)</span> is not consistent with <span class="math inline">\(r(\vec c_i)\)</span>, with high probability.</li>
</ol>
<p>If Bob can find a sequence of query vectors with the specified properties, he can rule out any <span class="math inline">\(\vec y\)</span> such that <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span>. Note that not every vector in <span class="math inline">\(\{0, 1\}^n\)</span> will be ruled out. At least <span class="math inline">\(\vec x\)</span> will remain. Bob can output any <span class="math inline">\(\vec y\)</span> that is not ruled out as his guess of <span class="math inline">\(\vec x\)</span>.</p>
<p>It turns out that fining <span class="math inline">\(\vec c_i\)</span>'s is simple: each <span class="math inline">\(\vec c_i\)</span> is sampled independently and uniformly at random from <span class="math inline">\(\{0, 1\}^n\)</span>. It remains to prove that any <span class="math inline">\(\vec y\)</span> with <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span> will be ruled out with high probability.</p>
<p>For a fix <span class="math inline">\(i\)</span>, consider the probability of <span class="math display">\[
\Pr[ | r(\vec c_i) - \vec c_i \cdot \vec y| &gt; \alpha \sqrt n ] = \Pr[ |\vec c_i \cdot (\vec x - \vec y) | &gt; \alpha \sqrt n ].
\]</span></p>
<p>Lower bounding this probability is equivalent to upper bounding the probability <span class="math display">\[
\Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ].
\]</span></p>
<p><em>Lemma.</em> <strong>For a value <span class="math inline">\(l \in \mathbb{N}\)</span></strong>, <span class="math display">\[
\Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] \le \frac{e }{\sqrt {|\vec x - \vec y| } }
\]</span></p>
<p>We leave the proof of the lemma at the end of our discussion. It following by union bound that <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ] 
        &amp;= \sum_{l = -\alpha \sqrt n}^{\alpha \sqrt n} \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] \\
        &amp;\le \frac{2 e \cdot \alpha \sqrt n}{\sqrt {|\vec x - \vec y| } }
\end{aligned}
\]</span></p>
<p>Conditioning on that <span class="math inline">\(|\vec x - \vec y| &gt; (3e \frac{\alpha}{\lambda } )^2 \cdot n\)</span>, this probability is at most <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | \le \alpha \sqrt n ] 
        &amp;\le \frac{2}{3} \lambda
\end{aligned}
\]</span></p>
<p>The probability that this holds simultaneously for all <span class="math inline">\(\vec c_i\)</span> is therefore <span class="math inline">\((\frac{2\lambda }{3})^k\)</span>. Applying union bound over all <span class="math inline">\(\vec y\)</span> such that <span class="math inline">\(|\vec x - \vec y| &gt; (3e\alpha)^2 \cdot n\)</span>, we have an upper bound of failure probability <span class="math display">\[
(\frac{2 \lambda }{3})^k 2^n = \exp( k \log \frac{2 \lambda }{3} + n \log 2)
\]</span></p>
<p>If we take <span class="math inline">\(k = \frac{n \log 2 + \ln n}{\log \frac{3}{2} - \log \lambda }\)</span>, this probability becomes <span class="math inline">\(\frac{1}{n}\)</span>.</p>
<p><em>Proof of the lemma:</em> Without lose of generality, suppose that <span class="math inline">\(l \ge 0\)</span>. Observe that <span class="math inline">\(\vec x - \vec y\)</span> is a vector in <span class="math inline">\(\{-1, 0, 1 \}^n\)</span>. Denote <span class="math inline">\(v = |\vec x - \vec y|\)</span> be the number of non-zero dimensions in <span class="math inline">\(\vec x - \vec y\)</span>. Further, let <span class="math inline">\(v_+\)</span> (<span class="math inline">\(v_-\)</span>) be the number of positive (negative) dimensions in <span class="math inline">\(\vec x - \vec y\)</span>. Hence, <span class="math inline">\(v = v_+ + v_-\)</span>. As <span class="math inline">\(\vec c_i\)</span> is sampled uniformly from <span class="math inline">\(\{0, 1\}^n\)</span>, we have <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] 
    &amp;= \frac{1}{2^{v} } \sum_{j = 0}^{ \min \{ v_+ - l, v_- \} } \binom{v_+}{l + j} \binom{v_-}{j} \\
    &amp;= \frac{1}{2^{v} } \sum_{j = 0}^{ \min \{ v_+ - l, v_- \} } \binom{v_+}{l + j} \binom{v_-}{v_- - j} \\
    &amp;\le \frac{1}{2^{v} } \binom{v_+ + v_- }{l + v_-} \\
    &amp;\le \frac{1}{2^{v} } \binom{v }{ \lfloor v / 2 \rfloor } \\
\end{aligned}
\]</span></p>
<p>By Stirling's approximation, <span class="math display">\[
v! \le e \sqrt v (\frac{v}{e} )^v  \\
\lfloor v / 2 \rfloor! \ge \sqrt{2 \pi v} (\frac{ \lfloor v / 2 \rfloor }{e} )^{\lfloor v / 2 \rfloor }  \\
\lceil v / 2 \rceil ! \ge \sqrt{2 \pi v} ( \frac{ \lceil v / 2 \rceil }{ e } )^{\lceil v / 2 \rceil }   \\
\]</span></p>
<p>Assuming that <span class="math inline">\(\lfloor v / 2\rfloor \ge 1\)</span>, we get <span class="math display">\[
\begin{aligned}
    \Pr[ |\vec c_i \cdot (\vec x - \vec y) | = l ] 
    &amp;\le \frac{1}{2^{v} } \binom{v }{ \lfloor v / 2 \rfloor } \\
    &amp;\le \frac{1}{2^{v} } \frac{e}{2 \pi} \frac{v^{ v + 0.5 } }{ (v / 2)^{ v + 1 } } (\frac{ v / 2 }{ \lfloor v / 2 \rfloor })^{ \lfloor v / 2 \rfloor + 0.5 } (\frac{ v / 2 }{ \lceil v / 2 \rceil })^{ \lceil v / 2 \rceil + 0.5 } \\
    &amp;\le \frac{ 1 }{ \sqrt v } ( \frac{ v / 2 }{ \lfloor v / 2 \rfloor })^{ \lfloor v / 2 \rfloor + 0.5 } \\
    &amp;\le \frac{ 1 }{ \sqrt v } \exp( \frac{ 1 }{ 2 \lfloor v / 2 \rfloor } ( \lfloor v / 2 \rfloor + 0.5 )  ) \\
    &amp;\le \frac{ e }{ \sqrt v }  \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p>The idea behind Theorem 2 is more obvious if we study the scenario where <span class="math inline">\(\vec x \in \{-1, 1\}^n\)</span>. For any estimation <span class="math inline">\(\vec y \in \{-1, 1\}^n\)</span>, if <span class="math inline">\(\vec c\)</span> is sampled uniformly at random from <span class="math inline">\(\{-1, 1\}^n\)</span>, then <span class="math display">\[
\mathbb{E}[ \vec c (\vec x - \vec y)] = 0
\]</span></p>
<p>If <span class="math inline">\(|\vec x - \vec y| = O(\alpha^2 n)\)</span>, by anti-concentration property, the probability <span class="math display">\[
\Pr[ |\vec c (\vec x - \vec y) | \ge \alpha \sqrt n] 
\]</span></p>
<p>equals to some constant. Equivalently, <span class="math inline">\(\Pr[ |\vec c (\vec x - \vec y) | &lt; \alpha \sqrt n]\)</span> is some constant. By sampling <span class="math inline">\(\vec c\)</span> repeatedly, the probability that <span class="math inline">\(|\vec c (\vec x - \vec y) | &lt; \alpha \sqrt n\)</span> holds for all sampled <span class="math inline">\(\vec c\)</span> decreases exponentially. That is why we can rule out any <span class="math inline">\(\vec y\)</span> such that <span class="math inline">\(|\vec x - \vec y| = O(\alpha^2 n)\)</span> with high probability.</p>
<!-- First, observe that 
$$
\mathbb{E}[ \vec c_i \cdot ( \vec x - \vec y) ] = \mathbb{E}[ \vec c_i \cdot ( \vec x - \vec y) ] \ge 0.
$$ -->
<!-- We re-write the event of $\vec c_i$ not consistent with $\vec y$ as 
$$
\begin{array}{ll}
\{ | r(\vec c_i) - \vec c_i \cdot \vec y| > \alpha \sqrt n \} \\
= \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] + \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n \} \\
= \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | + |\mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n \} \\
\supset \{ | \vec c_i \cdot ( \vec x - \vec y) - \mathbb{ E }[ \vec c_i \cdot ( \vec x - \vec y) ] | > \alpha \sqrt n  \} 
\end{array}
$$

and 
$$
\Pr[ \vec c \cdot (\vec x - \vec y) - \mathbb{E}[ \vec c \cdot ( \vec x - \vec y) ] \ge \sqrt n] ] \le \binom{\alpha n}{ \alpha n / 2} \frac{1}{2^{\alpha n} } 
$$ -->
<h3 id="reference">Reference</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/20/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><span class="page-number current">21</span><a class="page-number" href="/page/22/">22</a><span class="space">&hellip;</span><a class="page-number" href="/page/57/">57</a><a class="extend next" rel="next" href="/page/22/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">170</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
