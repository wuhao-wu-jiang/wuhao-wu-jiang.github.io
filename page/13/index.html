<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/13/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/13/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/13/Advanced-Composition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/13/Advanced-Composition/" class="post-title-link" itemprop="url">Advanced Composition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-13 22:26:07" itemprop="dateCreated datePublished" datetime="2020-11-13T22:26:07+11:00">2020-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-15 22:41:53" itemprop="dateModified" datetime="2020-11-15T22:41:53+11:00">2020-11-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Once we have designed some basic differentially private algorithms, it is a natural idea to combine them and analysis the privacy loss. We begin with an illustrative example that sets up the mathematical model step by step.</p>
<p>Image yourself in front of the door of a safe vault protected by a password lock. To open the door, you need the correct password <span class="math inline">\(P\)</span>. If tried with the wrong password, the lock would destroy itself automatically.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/door.png?raw=true" width="400" height="340" /></p>
</div>
<p>Luckily, you know two candidate passwords <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span>, with one of them being the correct one. Further, you notice that the designer of the lock left a collection of boxes near the door, which contain information on how they decide the correct passwords. Obtaining complete information of anyone of them gives you the correct password.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box1.png?raw=true" width="400" height="340" /></p>
</div>
<p>But the boxes are also protected and you don't have legally access to them. However, you can hack into the boxes. Hacking into the box won't give you all its information, but a random message. In particular, each box <span class="math inline">\(B\)</span> is associated with a set <span class="math inline">\(\mathcal{R}_B\)</span>, which is a finite collection of messages (in English). When <span class="math inline">\(B\)</span> is hacked, it returns a message <span class="math inline">\(Y_B\)</span> generated randomly from <span class="math inline">\(\mathcal{R}_B\)</span>, whose distribution depends on the correct password <span class="math inline">\(P\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> be the distribution of <span class="math inline">\(Y_B\)</span> if the correct password is <span class="math inline">\(P_1\)</span>, and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> be the one if the correct password is <span class="math inline">\(P_2\)</span>. If there is a huge difference between <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>, then you might be able to guess the correct password.</p>
<p>E.g., suppose <span class="math inline">\(\mathcal{R}_B =\)</span> { "Dog bites.", "Cat scratches." } and the distributions are given as</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_1}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathcal{D}_{B, P_2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">"Dog bites."</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="even">
<td style="text-align: center;">"Cat scratches."</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.99</td>
</tr>
</tbody>
</table>
<p>Hence, when you get <span class="math inline">\(Y_B=\)</span> "Dog bites.", you prefer <span class="math inline">\(P_1\)</span> over <span class="math inline">\(P_2\)</span> and vice versa.</p>
<p>Anticipating such potential information leakage, the designer of the lock equips the boxes with a defense mechanism, called <span class="math inline">\((\epsilon, 0)\)</span>-mechanism. It guarantees the distributions <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are similar so that it is hard for you to infer the correct password from the message. In particular, <span class="math inline">\(\forall S \subset \mathcal{R}_B\)</span>, if <span class="math inline">\(S\)</span> is measurable, it holds that <span class="math display">\[
\Pr[ Y_B \in S \ | \ P = P_1 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_2 ] \\
\Pr[ Y_B \in S \ | \ P = P_2 ] \le e^\epsilon \cdot \Pr[ Y_B \in S \ | \ P = P_1 ] \\
\]</span></p>
<p>When these inequalities are satisfied, we say that <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close. The inequalities require <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> to have the same support on <span class="math inline">\(\mathcal{R}_B\)</span>. Therefore, throughout our discussion below, we assume that both <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span> assign positive probability to each element in <span class="math inline">\(\mathcal{R}_B\)</span>. Otherwise, we can just replace <span class="math inline">\(\mathcal{R}_B\)</span> with the support of <span class="math inline">\(\mathcal{D}_{B, P_1}\)</span> (which is also the support of <span class="math inline">\(\mathcal{D}_{B, P_2}\)</span>).</p>
<p>Now, hacking one box is unlikely to help you to guess the correct password. You want to hack more boxes, with the hope that the information combined will assist you. Due to resource limit, you can't hack all boxes but only a finite number of them. You choose the first box randomly. Then you choose every new box based on the information obtained from the hacked boxes. The figure below shows an example of hacking five boxes.</p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/AdvancedComposition/box2.png?raw=true" width="400" height="340" /></p>
</div>
<p>Suppose that your resource enables you to hack <span class="math inline">\(n\)</span> boxes and let <span class="math inline">\(\vec Y_n = (Y_1, Y_2, ..., Y_n)\)</span> be the output messages you obtained. Similar to the situation of hacking one box, if the distribution of <span class="math inline">\(\vec Y_n\)</span>, conditioned on <span class="math inline">\(P = P_1\)</span>, is utterly distant from that conditioned on <span class="math inline">\(P = P_2\)</span>, then there could be some cases when you can confidently infer the true password. Conversely, to prevent severe information leakage, the designer needs to ensure there isn't such case, i.e., the two distributions should be similar.</p>
<p>What makes things even more complicated is that, the distribution of <span class="math inline">\(\vec Y_n\)</span> depends not only on the output distributions of the boxes, but also on your strategy of choosing the boxes to hack. Let's use symbol <span class="math inline">\(A\)</span> to denote your strategy. Whatever <span class="math inline">\(A\)</span> is, the designer need to guarantee that the distribution of <span class="math inline">\(\vec Y_n\)</span> conditioned <span class="math inline">\(P = P_1\)</span> should be similar to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Surprisingly, this is in some sense achievable, as long as for each box, its output distribution conditioned on <span class="math inline">\(P = P_1\)</span> is <span class="math inline">\((\epsilon, 0)\)</span> close to that conditioned on <span class="math inline">\(P = P_2\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{R}\)</span> be the set of possible possible messages of all boxes.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> <span class="math inline">\(\forall A\)</span>, <span class="math inline">\(\forall S \subset \mathcal{R}^n\)</span>, it holds that <span class="math inline">\(\forall \delta&#39; \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ \vec Y_n \in S \ | \ P = P_1, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; \\
\Pr[ \vec Y_n \in S \ | \ P = P_2, A] \le e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_1, A] + \delta&#39;
\]</span> where <span class="math inline">\(\epsilon&#39; = k\epsilon(e^\epsilon - 1) + \epsilon \sqrt{2 n \log \frac{1}{\delta&#39;} }\)</span>.</p>
</blockquote>
<p>If we view <span class="math inline">\(\epsilon\)</span> as the privacy loss of a single box, then the theorem states that the privacy loss grows to <span class="math inline">\(O(\sqrt {n} \epsilon )\)</span> is <span class="math inline">\(n\)</span> boxes are hacked.</p>
<p>To prove the theorem, we need a rigorous model for the problem.</p>
<p><strong><em>Definitions.</em></strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathcal{I}\)</span>: the index set.</p></li>
<li><p><span class="math inline">\(\{ B_\alpha : \alpha \in \mathcal{I} \}\)</span>: the collection of boxes.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{R}_{\alpha} : \alpha \in \mathcal{I} \}\)</span>: the ranges of the outputs of the boxes.</p></li>
<li><p><span class="math inline">\(\mathcal{R} \doteq \cup_{\alpha \in \mathcal{I} } R_\alpha\)</span>: the range of any possible output by any box.</p></li>
<li><p><span class="math inline">\(\{ \mathcal{D}_{\alpha, P_1} : \alpha \in \mathcal{I} \}\)</span> (<span class="math inline">\(\{ \mathcal{D}_{\alpha, P_2} : \alpha \in \mathcal{I} \}\)</span>): the set of output distribution when the correct password is <span class="math inline">\(P_1\)</span> (<span class="math inline">\(P_2\)</span>). Without loss of generality, we assume that for a fixed <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, the distribution <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> (<span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span>) assigns positive probability to each element in <span class="math inline">\(\mathcal{R}_\alpha\)</span>. Moreover, <span class="math inline">\(\mathcal{D}_{\alpha, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{\alpha, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close.</p></li>
<li><p><span class="math inline">\(n\)</span>: the number of boxes you can hack.</p></li>
<li><p><span class="math inline">\(\vec i_k \doteq (i_1, i_2, ..., i_k)\)</span>: the index sequence of boxes you have chosen to hack up to time <span class="math inline">\(k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, <span class="math inline">\(\vec i_0\)</span> corresponds to a empty sequence.</p></li>
<li><p><span class="math inline">\(\vec Y_k = (Y_1, Y_2, ..., Y_k):\)</span> the random variables that represent messages outputted by the chosen boxes up to time <span class="math inline">\(k \in [1, n]\)</span>, where <span class="math inline">\(Y_t \in \mathcal{R}_{i_t} \subset \mathcal{R}\)</span> for <span class="math inline">\(t \in [1, k]\)</span>. Therefore, <span class="math inline">\(\vec Y_k\)</span> can be view as random vector in <span class="math inline">\(\mathcal{R}^k\)</span>.</p></li>
<li><p><span class="math inline">\(\vec x_k = (x_1, x_2, ..., x_k) \in \mathcal{R}^k:\)</span> a point in <span class="math inline">\(\mathcal{R}^k\)</span>, where <span class="math inline">\(k \in [0, n]\)</span>. When <span class="math inline">\(k = 0\)</span>, we define <span class="math inline">\(\vec x_0 = \emptyset\)</span>.</p></li>
<li><p><span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt; = \left&lt; (i_1, i_2, i_3, ..., i_k), (x_1, x_2, ..., x_k) \right&gt;:\)</span> the history up to <span class="math inline">\(k \in [0, n]\)</span>, which consists of the chosen indexes and observations up to time <span class="math inline">\(k\)</span>. We use <span class="math inline">\(\vec h_0\)</span> to denote the empty history.</p></li>
<li><p><span class="math inline">\(A:\)</span> your strategy (policy) for choosing boxes. It works as follows: for any fixed history <span class="math inline">\(\vec h_k = \left&lt; \vec i_k, \vec x_k \right&gt;\)</span>, A is associated with a fixed distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span> over <span class="math inline">\(\mathcal{I} \setminus \vec i_k\)</span>, the set of indexes of the unchosen boxes. When inputted with <span class="math inline">\(\vec h_k\)</span>, <span class="math inline">\(A\)</span> returns a random variable <span class="math inline">\(A(\vec h_k)\)</span> that follows the distribution <span class="math inline">\(\mathcal{D}_{A, \vec h_k}\)</span>.</p></li>
</ol>
<p>We will show that, no matter what strategy you use, it is unlikely that you distinguish via the output <span class="math inline">\(\vec Y_n\)</span> whether the correct password is <span class="math inline">\(P_1\)</span> or <span class="math inline">\(P_2\)</span>. This is because whether <span class="math inline">\(P = P_1\)</span> or not, the output distributions of <span class="math inline">\(\vec Y_n\)</span> are similar.</p>
<p><strong><em>Proof of the theorem.</em></strong> We will just prove the first inequality, and the second one follows from symmetry. We consider a bad set in <span class="math inline">\(\mathcal{R}^n\)</span>: <span class="math display">\[
\mathcal{W} \doteq \{ \vec x_n \in \mathcal{R}^n : \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] \ge e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] \}
\]</span></p>
<p>and will use the following lemma.</p>
<blockquote>
<p><strong>Lemma.</strong> <span class="math inline">\(\Pr[ \vec Y_n \in \mathcal{W}\ | \ P = P_1, A] \le \delta&#39;\)</span>.</p>
</blockquote>
<p>Hence, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n \in S \ | \ P = P_1, A] 
        &amp;=  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in S \cap \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le  \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \Pr[ \vec Y_n \in \mathcal{W} \ | \ P = P_1, A] \\
        &amp;\le \Pr[ \vec Y_n \in S \cap \bar{\mathcal{W} } \ | \ P = P_1, A] + \delta&#39; \\
        &amp;= \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] + \delta&#39; \\
        &amp;&lt; \sum_{ \vec x_n \in S \cap \bar{\mathcal{W} } } e^{\epsilon&#39;} \cdot  \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] + \delta&#39; \\
        &amp;=  e^{\epsilon&#39;} \cdot \Pr[ \vec Y_n \in S \ | \ P = P_2, A] + \delta&#39; 
\end{aligned}
\]</span></p>
<p>The first inequality follows from monotonicity of probability, the second one from the lemma, and the final one from the definition of <span class="math inline">\(\mathcal{W}\)</span>.</p>
<p><em>Proof of the lemma.</em> To prove the lemma, we need only to consider those point <span class="math inline">\(\vec x_n\)</span> with <span class="math display">\[
\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0. 
\]</span> Otherwise, <span class="math inline">\(\vec x_n\)</span> contributes to 0 probability to the set <span class="math inline">\(\mathcal{W}\)</span> (whether it belongs to <span class="math inline">\(\mathcal{W}\)</span> or not).</p>
<p>Now, we expand the probability <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A]\)</span>. To obtain the <span class="math inline">\(k\)</span>-th output, there are two steps</p>
<ol type="1">
<li><span class="math inline">\(A\)</span> generates a index <span class="math inline">\(i_k\)</span> of a box based on the known history <span class="math inline">\(\vec h_{k - 1}\)</span>.<br />
</li>
<li>The box <span class="math inline">\(B_{i_k}\)</span> is hacked, and output a random message <span class="math inline">\(x_k\)</span> sampled from its distribution <span class="math inline">\(D_{i_k, P_1}\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(\vec H_k\)</span> be a random variable that represents the history up to <span class="math inline">\(k\)</span>. By chain rule, we have <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A  ]
\end{aligned}
\]</span></p>
<p>By <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] &gt; 0\)</span>, each term in the expansion are positive. Similarly, <span class="math display">\[
\begin{aligned}
    \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] 
    &amp;= \prod_{k = 1}^n \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] \\ 
    &amp;\ \ \cdot \prod_{k = 1}^n \Pr_{ A(\vec h_{k - 1} ) \sim  \mathcal{D}_{A, \vec h_{k - 1} } } [ A(\vec h_{k - 1} ) = i_k \mid \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A  ]
\end{aligned}
\]</span></p>
<p>As both <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> assign positive probability to each point in <span class="math inline">\(\mathcal{R}_{i_k}\)</span>, we know <span class="math inline">\(\Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] &gt; 0\)</span>.</p>
<p>We are ready to consider the ratio: <span class="math display">\[
\begin{aligned}
    \ln \frac{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_1, A] }{ \Pr[ \vec Y_n = \vec x_n \ | \ P = P_2, A] } 
    = \sum_{k = 1}^n \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = x_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }. 
\end{aligned}
\]</span></p>
<p>If we replace <span class="math inline">\(x_k\)</span> by a random variable <span class="math inline">\(X_k \sim \mathcal{D}_{i_k, P_1}\)</span>, then <span class="math display">\[
C_k \doteq \ln \frac{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_1} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_1, A ]  }{ \Pr_{Y_k \sim \mathcal{D}_{i_k, P_2} } [ Y_k = X_k \ | A(\vec h_{k - 1} ) = i_k, \vec H_{k - 1} = \vec h_{k - 1}, \ P = P_2, A ] }
\]</span></p>
<p>is a random variable. As <span class="math inline">\(\mathcal{D}_{i_k, P_1}\)</span> and <span class="math inline">\(\mathcal{D}_{i_k, P_2}\)</span> are <span class="math inline">\((\epsilon, 0)\)</span> close, we have <span class="math display">\[
C_k \le \epsilon.
\]</span></p>
<p>Further, we have</p>
<blockquote>
<p><strong>Fact 1.</strong> For any <span class="math inline">\(\alpha \in \mathcal{I}\)</span>, it holds that <span class="math display">\[
\begin{aligned}
\mathbb{E}_{ X \sim \mathcal{D}_{\alpha, P_1} } \left[ \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = X]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = X]  }  \right] 
&amp;= \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le \sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;+ 
\sum_{x \in \mathcal{R}_\alpha } \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x]  \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } \\
&amp;= \sum_{x \in \mathcal{R}_\alpha } \left[ \underset{ X \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ X = x]  - \underset{ X \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ X = x] \right] \cdot \ln \frac{ \underset{ Y \sim \mathcal{D}_{\alpha, P_1} }{ \Pr } [ Y = x]  } { \underset{ Y \sim \mathcal{D}_{\alpha, P_2} }{ \Pr } [ Y = x]  } \\
&amp;\le (e^\epsilon - 1) \epsilon
\end{aligned}
\]</span></p>
</blockquote>
<p>Therefore, <span class="math inline">\(\mathbb{E} [C_k] \le (e^\epsilon - 1) \epsilon\)</span>.</p>
<blockquote>
<p><strong>Fact 2.</strong> (<strong>Azuma Inequality</strong>). Let <span class="math inline">\(C_1, ...., C_n\)</span> be random variables such that <span class="math inline">\(\forall k \in [n]\)</span>, <span class="math inline">\(\Pr[ |C_k| \le \epsilon ] = 1\)</span>, and for every <span class="math inline">\((c_1, ..., c_{k -1} ) \in \text{Supp} (C_1, ..., C_{k - 1} )\)</span>, we have <span class="math display">\[
\mathbb{E}[ C_i \mid C_1 = c_1, ..., C_{k -1} = c_{k - 1} ] \le \beta,
\]</span> Then for every <span class="math inline">\(z &gt; 0\)</span>, we have <span class="math display">\[
\Pr[ \sum_{k = 1}^n C_i -  n \beta &gt;  z] \le \exp(- \frac{z^2}{ 2 n\epsilon^2 } )
\]</span></p>
</blockquote>
<p>Finally, applying <em>Azuma Inequality</em> with <span class="math inline">\(z = \sqrt{ 2n \log \frac{1}{\delta&#39;} }\)</span> and <span class="math inline">\(\beta = (e^\epsilon - 1) \epsilon\)</span>, we get the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h3 id="reference.">Reference.</h3>
<p>[1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2013</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/12/PAC-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/12/PAC-learning/" class="post-title-link" itemprop="url">PAC Learning Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-12 16:06:55" itemprop="dateCreated datePublished" datetime="2020-11-12T16:06:55+11:00">2020-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-19 15:16:12" itemprop="dateModified" datetime="2020-11-19T15:16:12+11:00">2020-11-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We discuss a toy model of probably approximately correct (PAC) learning [1]. The setting focuses on a <span class="math inline">\(d\)</span>-dimension Boolean hypercube <span class="math inline">\(\{0, 1\}^d\)</span> and a set of functions <span class="math inline">\(\mathcal{H} = \{ h : \{0, 1\}^d \rightarrow \{0, 1 \} \}\)</span>. A function <span class="math inline">\(h \in \mathcal{H}\)</span> is called a hypothesis or a concept, which assigns each vertex in the hypercube a label 0 or 1.</p>
<p>The goal of PAC learning is to learn an unknown hypothesis, denoted as <span class="math inline">\(h^* \in \mathcal{H}\)</span>, from a dataset, which consists of <span class="math inline">\(n\)</span> points <span class="math inline">\(X_1, X_2, ..., X_n \in \{0, 1\}^n\)</span> sampled independently from an unknown distribution <span class="math inline">\(D\)</span> (over the hypercube), as well as their labels <span class="math inline">\(Y_1 = h^*(X_1), Y_2 = h^*(X_2), ..., Y_n = h^*(X_n)\)</span>.</p>
<p>We can state the algorithm for PAC learning in just one sentence.</p>
<blockquote>
<p>Output any hypothesis <span class="math inline">\(h&#39; \in \mathcal{H}\)</span> that is consistent with the sampled dataset, i.e., <span class="math display">\[
h&#39;(X_i) = Y_i, \qquad \forall i \in [n]
\]</span></p>
</blockquote>
<p>Such an hypothesis always exists, as <span class="math inline">\(h^*\)</span> satisfies this constraint. We have the classic PAC learning theorem.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p>The PAC learning theorem states that the probability that <span class="math inline">\(h&#39;\)</span> mislabels a point is bounded by <span class="math inline">\(\sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}\)</span>.</p>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\mathbb{E}[Z_i]\)</span> is an unbiased estimator of <span class="math inline">\(\mu_h\)</span>. Further, denote <span class="math inline">\(\hat \mu_h\)</span> the empirical mean of <span class="math inline">\(Z_i\)</span>'s <span class="math display">\[
\hat \mu_h \doteq \frac{1}{n} \sum_{i \in [n] } Z_i
\]</span></p>
<p>Note that for the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(\hat \mu_{h&#39;} = 0\)</span>.</p>
<p>By Hoeffding inequality, for any <span class="math inline">\(\delta&#39; &gt; 0\)</span>, <span class="math display">\[
\Pr[ \hat \mu_h \le \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} } ] \le \delta&#39;. 
\]</span></p>
<p>By union bound, the probability that <span class="math display">\[
\exists h \in \mathcal{H}, \hat \mu_h \ge \mu_h - \sqrt{ \frac{ \log \frac{1}{\delta&#39;} }{2n} }
\]</span></p>
<p>is bounded by <span class="math inline">\(| \mathcal{H} | \delta&#39;\)</span>. By setting <span class="math inline">\(\delta&#39; = \frac{\delta}{ | \mathcal{H} | }\)</span>, it is guaranteed that <span class="math display">\[
0 = \hat \mu_{h&#39; } \le \mu_{h&#39;} -  \sqrt{ \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over 2n}
\]</span></p>
<p>with probability at most <span class="math inline">\(\delta\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>The previous theorem can be improved with the <span class="math inline">\(\sqrt{ \cdot }\)</span> removed. The key here is that the selected <span class="math inline">\(h&#39;\)</span> is error-free on the samples.</p>
<blockquote>
<p><strong><em>Theorem.</em></strong> Let <span class="math inline">\(X \in \{0, 1\}^n\)</span> be a random point sampled from the unknown distribution, then for a given <span class="math inline">\(\delta \in (0, 1)\)</span>, <span class="math display">\[
\Pr[ h&#39;(X) \neq h^*(X)] \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}
\]</span> with probability at most <span class="math inline">\(\delta\)</span>.</p>
</blockquote>
<p><em>Proof.</em> Consider some fixed <span class="math inline">\(h \in \mathcal{H}\)</span>. For convenience, we rewrite <span class="math display">\[
\Pr[ h(X) \neq h^*(X)] = \mu_h. 
\]</span></p>
<p>We will prove that if <span class="math inline">\(\mu_h \ge { \log |\mathcal{H}| + \log \frac{ 1 }{ \delta } \over n}\)</span>, then <span class="math inline">\(h\)</span>'s probability of being selected is very low.</p>
<p>Denote <span class="math inline">\(Z_i\)</span> the indicator variable of whether <span class="math inline">\(h\)</span> mislabels <span class="math inline">\(X_i\)</span>, i.e., <span class="math display">\[
Z_i = \begin{cases}
    0, \qquad \text{ if } h(X_i) = Y_i \\
    1, \qquad \text{ if } h(X_i) \neq Y_i 
\end{cases}
\]</span></p>
<p>By definition, <span class="math inline">\(\Pr[Z_i = 1] = \mu_h\)</span>. Therefore, <span class="math display">\[
    \begin{aligned}
        \Pr[ Z_1 = 0 \wedge Z_2 = 0 \wedge ... \wedge Z_n = 0] = (1 - \mu_h)^n \le \exp(- \mu_h n) = \frac{\delta}{ |\mathcal{H} | }
    \end{aligned}
\]</span></p>
<p>Taking union bound over all possible <span class="math inline">\(h\)</span> gives the desired result.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Caveat.</em></strong> <em>In both theorem, we see a <span class="math inline">\(\log \mathcal{H}\)</span> term in the failure probability, which is due to the use of union bound and can be roughly considered as the complexity of the hypothesis family. It is tempting to think that this could be avoid as follows.</em></p>
<blockquote>
<p>For the outputted <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(Z_1, ..., Z_n\)</span> are i.i.d samples. Therefore, the probability of all of them being <span class="math inline">\(0\)</span> is give by<br />
<span class="math display">\[
(1 - \mu_{h&#39;} )^n \le \exp( - \mu_{h&#39;} n). 
  \]</span><br />
Bounding this probability by <span class="math inline">\(\delta\)</span> gives <span class="math inline">\(\mu_{h&#39;} \le \frac{ \log \frac{1}{\delta} } {n}\)</span>.</p>
</blockquote>
<p><em>The argument is wrong. For the outputted <span class="math inline">\(h&#39;\)</span>, the <span class="math inline">\(Z_1, ..., Z_n\)</span> are not i.i.d samples, as <span class="math inline">\(h&#39;\)</span> is selected according to <span class="math inline">\(Z_1, ..., Z_n\)</span> and depends on them.</em></p>
<p><em>To make it more concrete, suppose that there is an <span class="math inline">\(h\)</span>, such that <span class="math inline">\(\mu_h = \frac{ \log \frac{1}{\delta} } {n}\)</span>. Consider the following experiment</em></p>
<blockquote>
<ol type="1">
<li>Sample <span class="math inline">\(n\)</span> points independently the distribution <span class="math inline">\(D\)</span>.<br />
</li>
<li>Pick an <span class="math inline">\(h&#39;\)</span> that makes no prediction error on the samples.</li>
</ol>
</blockquote>
<p><em>If we repeat the experiment <span class="math inline">\(N\)</span> times for some positive integer <span class="math inline">\(N\)</span>, then the fixed <span class="math inline">\(h\)</span> makes no mistake in roughly <span class="math inline">\((1 - \delta) N\)</span> of the experiments and constitutes an candidate of <span class="math inline">\(h&#39;\)</span>. However, on the other roughly <span class="math inline">\(\delta N\)</span> experiments, where <span class="math inline">\(h\)</span> makes prediction error, it is no longer considered as the candidate of <span class="math inline">\(h&#39;\)</span>. The selection procedure of <span class="math inline">\(h&#39;\)</span> ignores bad event of <span class="math inline">\(h\)</span> automatically.</em></p>
<p><em>We could also investigate the following example. Let <span class="math inline">\(h^* = h_0\)</span>. There are also other two hypothesis <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span>. Let <span class="math inline">\(S\)</span> be the sample space. It holds that</em> <span class="math display">\[
|h_1(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_1} \\
    0,\ \       \forall x \in S_{h_1}
\end{cases}
\]</span> <span class="math display">\[
|h_2(x) - h^*(x) | = \begin{cases}
    2 \epsilon, \forall x \in S \setminus S_{h_2} \\
    0,\ \       \forall x \in S_{h_2}
\end{cases}
\]</span> <em>Moreover, <span class="math inline">\(\Pr[S_{h_1}] = \Pr[S_{h_2} ] = \delta\)</span>. How, if we get a sample in <span class="math inline">\(S_{h_1}\)</span>, then we can output <span class="math inline">\(h&#39;\)</span> as <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_1\)</span>, since both of them make no error when <span class="math inline">\(x \in S_{h_1}\)</span>. Similarly, if <span class="math inline">\(x \in S_{h_2}\)</span>, we can output either <span class="math inline">\(h_0\)</span> or <span class="math inline">\(h_2\)</span>. We call <span class="math inline">\(S_{h_1}\)</span> and <span class="math inline">\(S_{h_2}\)</span> the bad areas. When <span class="math inline">\(x\)</span> is in the bad area of any hypothesis, we can't distinguish this hypothesis from the true hypothesis. That is why we need to use union bound to bound the total probabilities of these bad areas.</em></p>
<div style="text-align:center">
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/blob/master/PAC-Learning.png?raw=true" width="400" height="340" /></p>
</div>
<h3 id="reference">Reference</h3>
<p>[1] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. 6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/Gaussian-Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/06/Gaussian-Mechanism/" class="post-title-link" itemprop="url">Gaussian Mechanism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-06 23:00:19" itemprop="dateCreated datePublished" datetime="2020-11-06T23:00:19+11:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-09 20:59:47" itemprop="dateModified" datetime="2021-06-09T20:59:47+10:00">2021-06-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="background">Background</h1>
<p>Let <span class="math inline">\((\mathcal{X}, d)\)</span> be a metric space, and <span class="math inline">\(f\)</span> be a function <span class="math inline">\(\mathcal{X} \rightarrow \mathbb{R}^n\)</span>. A Gaussian mechanism adds Gaussian noise to the output of <span class="math inline">\(f\)</span>, such that for all neighboring pairs <span class="math inline">\(x, y \in \mathcal{X}\)</span> with <span class="math inline">\(d(x, y) = 1\)</span>, it is hard to distinguish the outputs, i.e., their outputs distributions are similar.</p>
<blockquote>
<p><strong>Definition.</strong> The sensitivity <span class="math inline">\(\Delta\)</span> of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[
\Delta = \max_{x, y \in \mathcal{X}, d(x, y) = 1 } \Vert f(x) - f(y) \Vert_2, 
\]</span> where <span class="math inline">\(\Vert \cdot \Vert_2\)</span> is the <span class="math inline">\(\ell_2\)</span>-norm of a vector in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Given <span class="math inline">\(f\)</span> and a variance parameter <span class="math inline">\(\sigma^2\)</span>, the output of the Gaussian mechanism is a random variable defines as <span class="math display">\[
g(x) = f(x) + X,
  \]</span> where <span class="math inline">\(X = (X_1, X_2, ..., X_n) \sim N(0, \sigma^2 I)\)</span>.</p>
</blockquote>
<h1 id="privacy-guarantee">Privacy Guarantee</h1>
<blockquote>
<p><strong>Theorem.</strong> For <span class="math inline">\(\delta, \epsilon \in (0, 1)\)</span>, if <span class="math inline">\(\sigma^2 = \frac{ \Delta^2 }{ \epsilon^2 } \Big( 2 \ln \frac{1}{\delta} + \epsilon \Big)\)</span>, then <span class="math inline">\(\forall x, y \in \mathcal{X}\)</span>, s.t., <span class="math inline">\(d(x, y) = 1\)</span>, and for all measurable subset <span class="math inline">\(S \subset \mathbb{R}^n\)</span>, it holds <span class="math display">\[
\Pr[ g(x) \in S] \le e^\epsilon \cdot \Pr[ g(y) \in S] + \delta 
\]</span></p>
</blockquote>
<p>It is obvious that if we set <span class="math inline">\(\sigma \rightarrow \infty\)</span>, then the Gaussian distribution converge to a uniform one and it holds that <span class="math inline">\(\Pr[ g(x) \in S] = \Pr[ g(y) \in S]\)</span>. Therefore, we are interested in how small <span class="math inline">\(\sigma\)</span> can be.</p>
<p>We claim that <span class="math inline">\(\sigma = O(\frac{\Delta}{\epsilon } )\)</span> suffices. This is based on three observations: 1. Most of the probability mass of a Gaussian distribution concentrates on a ball centered at its mean with radius <span class="math inline">\(O(\sigma)\)</span>. 2. The density ratio of any two points within the ball is bounded by a constant. 3. The distance between the distribution center of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> is bounded by <span class="math inline">\(\Delta\)</span>.</p>
<p><strong>Proof.</strong> To ease the notations, we write <span class="math inline">\(g(y) = f(y) + Y\)</span> to distinguish it from <span class="math inline">\(g(x)\)</span>. By definition, <span class="math display">\[g(x) \sim N(f(x), \sigma^2 I), \quad g(y) \sim N( f(y), \sigma^2 I).
\]</span></p>
<p>Let <span class="math inline">\(p_x\)</span> and <span class="math inline">\(p_y\)</span> the density functions of <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(g(y)\)</span> respectively. The sketch of the proof is as follows: we partition <span class="math inline">\(\mathbb{R}^n\)</span> into two parts: <span class="math display">\[
    \mathcal{Y}_1 = \{ t \in \mathcal{R}^n : p_x(t) \le e^\epsilon \cdot p_y(t) \} \\
    \mathcal{Y}_2 = \{ t \in \mathcal{R}^n : p_x(t) &gt; e^\epsilon \cdot p_y(t) \}.
\]</span></p>
<p>If <span class="math inline">\(\Pr[ g(x) \in \mathcal{Y}_2 ] \le \delta\)</span> <span class="math inline">\(\big(\)</span> here we measure the size of <span class="math inline">\(\mathcal{Y}_2\)</span> by the probability measure induced by <span class="math inline">\(g(x)\)</span> <span class="math inline">\(\big)\)</span>, then <span class="math display">\[
\begin{aligned}
    \Pr[ g(x) \in S] 
        &amp;= \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[ g(x) \in S \cap \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \Pr[  g(x) \in \mathcal{Y}_2] \\
        &amp;\le \Pr[ g(x) \in S \cap \mathcal{Y}_1] + \delta \\
        &amp;= \int_{S \cap \mathcal{Y}_1 } p_x(t) \ d t + \delta \\
        &amp;\le \int_{S \cap \mathcal{Y}_1 } e^\epsilon \cdot p_y(t) \ d t + \delta \\
        &amp;= e^\epsilon \cdot \Pr[ g(y) \in S \cap \mathcal{Y}_1] + \delta. \\
        &amp;\le e^\epsilon \cdot \Pr[ g(y) \in S ] + \delta. \\
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(p_y(t) &gt; 0\)</span> for all <span class="math inline">\(t \in \mathbb{R}^n\)</span>, we can rewrite <span class="math display">\[
    \mathcal{Y}_1 = \Big\{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } \le \epsilon \Big\} \\
    \mathcal{Y}_2 = \Big\{ t \in \mathcal{R}^n : \ln \frac{p_x(t) }{ p_y(t) } &gt; \epsilon \Big\}.
\]</span></p>
<p>In literature, the ratio <span class="math inline">\(\ln \frac{p_x(t) }{ p_y(t) }\)</span> is know as <em>privacy loss</em>. Substituting <span class="math inline">\(p_x(t)\)</span> and <span class="math inline">\(p_y(t)\)</span> with their definitions, and letting <span class="math inline">\(t&#39; = t - f(x)\)</span> and <span class="math inline">\(v = f(x) - f(y)\)</span>, we get <span class="math display">\[
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &amp;= \ln \frac{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( -\frac{ \Vert t - f(x)\Vert ^2 }{ 2 \sigma^2 } \Big) }{ \frac{1}{ \sqrt{2 \pi \sigma^2} } \exp \Big( -\frac{ \Vert t - f(y) \Vert ^2 }{ 2 \sigma^2 } \Big)  } \\
        &amp;=  -\frac{ \Vert t&#39; \Vert ^2 - \Vert  t&#39; + v \Vert ^2 }{ 2 \sigma^2 } \\
        &amp;= \frac{ 2 v^T t&#39; + \Vert  v \Vert ^2 }{ 2 \sigma^2 }  . 
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(t \sim N( f(x), \sigma^2 I)\)</span>, it holds that <span class="math inline">\(t&#39; \sim N( 0, \sigma^2 I)\)</span>.</p>
<!-- $$
\begin{aligned}
    \ln \frac{ p_x( t)  }{  p_y(t)} 
        &=  -\frac{ \Vert  t' \Vert ^2 - \Vert  t' + v \Vert ^2 }{ 2 \sigma^2 }    \\
        &= \frac{ 2 v^T t' + \Vert  v \Vert ^2 }{ 2 \sigma^2 }    \\
\end{aligned}
$$ -->
<p>Then <span class="math inline">\(v^T t&#39; \sim N(0, \Vert v \Vert ^2 \sigma^2)\)</span>. Let <span class="math inline">\(Z \sim N(0, 1)\)</span>, then <span class="math inline">\(\ln \frac{ p_x( t) }{ p_y(t)}\)</span> has the same distribution as <span class="math display">\[
    \frac{ 2 \Vert v\Vert  \sigma Z + \Vert v\Vert ^2 }{ 2 \sigma^2  } = \frac{ \Vert v\Vert ^2 }{ 2 \sigma^2  } + \frac{ \Vert v\Vert  }{  \sigma  } Z \sim N \Big( \frac{ \Vert v\Vert ^2 }{ 2 \sigma^2  },  \frac{ \Vert v\Vert ^2 }{  \sigma^2  } \Big).   \\
\]</span></p>
<p>Now, <span class="math display">\[
    \frac{ \Vert v\Vert ^2 }{ 2 \sigma^2  } + \frac{ \Vert v\Vert  }{  \sigma  } Z  \ge \epsilon \\
    \longleftrightarrow  Z  \ge \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  }
\]</span></p>
<p>As <span class="math inline">\(Z \sim N(0, 1)\)</span>, <span class="math inline">\(\forall z \in \mathbb{R}_{\ge 0}\)</span>, <span class="math display">\[
    \Pr[ Z \ge z] \le \exp \Big( -\frac{ z^2 }{2} \Big).
\]</span></p>
<p>Assuming that <span class="math inline">\(\sigma^2 \ge \frac{\Vert v \Vert^2}{ 2 \epsilon }\)</span>. Then <span class="math inline">\(\frac{ \sigma }{ \Vert v\Vert } \epsilon - \frac{ \Vert v\Vert }{ 2 \sigma } \ge 0\)</span>. Replacing <span class="math inline">\(z\)</span> with <span class="math inline">\(\frac{ \sigma }{ \Vert v\Vert } \epsilon - \frac{ \Vert v\Vert }{ 2 \sigma }\)</span>, <span class="math display">\[
    \Pr[ z \ge t ] \le  \exp \Big( -\frac{1}{2} \Big( \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  } \Big)^2 \Big).
\]</span></p>
<p>We would like to bound this probability by some <span class="math inline">\(\delta \in (0, 1)\)</span>, then <span class="math display">\[
\begin{aligned}
    &amp;\exp \Big( -\frac{1}{2} \Big( \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  } \Big)^2 \Big) \le \delta \\
    &amp;\Longrightarrow \frac{  \sigma  }{ \Vert v\Vert  } \epsilon - \frac{ \Vert v\Vert  }{ 2 \sigma  } \ge \sqrt{2 \ln \frac{1}{\delta} } \\
    &amp;\Longrightarrow \frac{\epsilon }{\Vert v\Vert  } \sigma^2 - \sqrt{2 \ln \frac{1}{\delta} } \sigma - \frac{ \Vert v\Vert  }{2} \ge 0
\end{aligned}
\]</span></p>
<p>Finally, we get <span class="math display">\[
    \sigma \ge \frac{ \sqrt{ 2 \ln \frac{1}{\delta} } + \sqrt{ 2 \ln \frac{1}{\delta} + 2 \epsilon  }  }{ 2 \frac{\epsilon }{\Vert v\Vert  } }
\]</span></p>
<p>By concavity of the square root function, it suffices to take <span class="math display">\[
    \sigma = \frac{ \Vert v\Vert  }{ \epsilon } \sqrt{ 0.5 \cdot 2 \ln \frac{1}{\delta} + 0.5 \cdot \Big( 2 \ln \frac{1}{\delta} + 2 \epsilon \Big) } = \frac{ \Vert v\Vert  }{ \epsilon } \sqrt{  2 \ln \frac{1}{\delta} + \epsilon },
\]</span></p>
<p>i.e., <span class="math display">\[
    \sigma^2 = \frac{ \Vert v\Vert ^2 }{ \epsilon^2 } \Big( 2 \ln \frac{1}{\delta} + \epsilon \Big). 
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h1 id="reference">Reference</h1>
<p>[1] G. Kamath, “Lecture 5 — Approximate Diﬀerential Privacy”, CS 860 - Algorithms for Private Data Analy sis.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/51/">51</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">152</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
