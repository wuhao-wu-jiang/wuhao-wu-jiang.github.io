<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/12/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/12/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>WOW</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WOW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/01/Entropy-and-Random-Bits/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/01/Entropy-and-Random-Bits/" class="post-title-link" itemprop="url">Entropy and Random Bits</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-01 00:16:09" itemprop="dateCreated datePublished" datetime="2020-06-01T00:16:09+10:00">2020-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-14 00:15:12" itemprop="dateModified" datetime="2020-07-14T00:15:12+10:00">2020-07-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="entropy">Entropy</h4>
<p>The entropy of a random variable <span class="math inline">\(X\)</span> is defined as <span class="math display">\[
\mathbf{H}[X] = \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right]
\]</span></p>
<p>Here we adopt the convention that <span class="math inline">\(0 \log 0 = 0\)</span>, as <span class="math inline">\(\lim_{x \rightarrow 0^+} x \log x = \lim_{x \rightarrow \infty} \frac{1 }{ x } \log \frac{1 }{ x } = 0\)</span>.</p>
<p>The entropy is oblivious to the specific values <span class="math inline">\(X\)</span> takes and is only sensitive to the probabilities with which <span class="math inline">\(X\)</span> takes these values.</p>
<blockquote>
<p>Example.<br />
1. <span class="math inline">\(X_1 = \begin{cases} e^{1000}, \text{with probability 0.5} \\ 0, \text{with probability 0.5} \end{cases}\)</span><br />
2. <span class="math inline">\(X_2 = \begin{cases} 1, \text{with probability 0.5} \\ -1, \text{with probability 0.5} \end{cases}\)</span></p>
</blockquote>
<p>By definition, both <span class="math display">\[
\mathbf{H}[X_1] = \mathbf{H}[X_2] = 0.5 \log_2 2 + 0.5 \log_2 2 = 1
\]</span></p>
<p>We also notice that the entropy of a random variable may not reflect the whether a random variable is concentrated at its mean or not.</p>
<p>The binary entropy function <span class="math inline">\(\mathbf{H}(p)\)</span> of a Bernoulli random variable <span class="math inline">\(X\)</span> that takes value <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p\)</span> is <span class="math display">\[
\begin{aligned}
    \mathbf{H}(p) = &amp;p \log_2 \frac{1}{p} + (1 - p) \log_2 \frac{1}{1 - p} \\
    = &amp;-p \log_2 p - (1 - p) \log_2 (1 - p)    
\end{aligned}
\]</span></p>
<p>By concavity of <span class="math inline">\(\log_2(\cdot )\)</span>, we know that <span class="math display">\[
\begin{aligned}
    \mathbf{H}(p)
    &amp;\le \log_2 \left( p \frac{1}{p} + (1 - p) \frac{1}{1 - p} \right) 
    &amp;= 1
\end{aligned}
\]</span></p>
<p>The equality holds when <span class="math inline">\(p = 0.5\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/Entropy.png" /></p>
<p>Some interesting values:</p>
<ul>
<li><span class="math inline">\(p = 0.5\)</span>, <span class="math inline">\(\mathbf{H}(p) = 1\)</span></li>
<li><span class="math inline">\(p = 0.2/0.8\)</span>, <span class="math inline">\(\mathbf{H}(p) \approx 0.7\)</span></li>
<li><span class="math inline">\(p = 0.1/0.9\)</span>, <span class="math inline">\(\mathbf{H}(p) \approx 0.5\)</span></li>
</ul>
<p><strong><em>Lemma.</em></strong> Given a discrete range <span class="math inline">\(\mathcal{U}\)</span>, the entropy of a random variable <span class="math inline">\(X\)</span> defined on <span class="math inline">\(\mathcal{U}\)</span> is maximized when <span class="math inline">\(X\)</span> has uniform distribution, i.e., <span class="math display">\[
\mathbf{H} (X) \le \log |\mathcal{U}|
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\begin{aligned}
    \mathbf{H} (X) 
        &amp;= \sum_{x} p(x) \log \frac{1}{p(x) } \\ 
        &amp;\le  \log \sum_{x} p(x) \frac{1}{p(x) } \\
        &amp;= \log |\mathcal{U}|
\end{aligned}
\]</span></p>
<p>The inequality holds with equality only when <span class="math inline">\(p(x) = 1/ |\mathcal{U}|\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Lemma.</em></strong> Let <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> be two distributions and <span class="math inline">\(\lambda \in [0, 1]\)</span>, then <span class="math display">\[
\mathbf{H} (\lambda p(x) + (1 - \lambda) q(x) ) \ge \lambda \mathbf{H}(p(x) ) + (1 - \lambda) \mathbf{H} (q(x) )
\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math display">\[
\begin{aligned}
    \lambda \mathbf{H}(p(x) ) + (1 - \lambda) \mathbf{H} (q(x) ) 
        &amp;= \sum_{x} \left( \lambda p(x) \log \frac{1}{p(x)} + (1 - \lambda) q(x) \log \frac{1}{q(x) }  \right) \\ 
        &amp;= (  \lambda p(x) + (1 - \lambda) q(x) )\sum_{x} \left( \frac{ \lambda p(x) }{  \lambda p(x) + (1 - \lambda) q(x) } \log \frac{1}{p(x)} + \frac{(1 - \lambda) q(x) }{ \lambda p(x) + (1 - \lambda) q(x) } \log \frac{1}{q(x) }  \right) \\
        &amp;\le \sum_{x} ( \lambda p(x) + (1 - \lambda) q(x) ) \log \frac{1}{ \lambda p(x) + (1 - \lambda) q(x) } \\ 
        &amp;= \mathbf{H} ( \lambda p(x) + (1 - \lambda) q(x) ) 
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Lemma.</em></strong> For two independent random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we have <span class="math display">\[
\mathbb{E} \left[ \log_2 \frac{1}{\Pr[X + Y] } \right] = \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right] + \mathbb{E} \left[ \log_2 \frac{1}{\Pr[Y] } \right]
\]</span></p>
<p>i.e., <span class="math display">\[
\mathbf{H}[X + Y] = \mathbf{H}[X] + \mathbf{H}[Y]
\]</span></p>
<p><strong><em>Proof.</em></strong> We prove it for the case of discrete random variables. <span class="math display">\[
\begin{array}{rrl}
    &amp;\mathbb{E} \left[ \log_2 \frac{1}{\Pr[X + Y] }  \right] 
    &amp;= \sum_{x, y} p(x, y) \log_2 \frac{1}{p(x, y)} \\
    &amp;&amp;= \sum_{x, y} p(x) p(y) \left( \log_2 \frac{1}{p(x)} + \log_2 \frac{1}{p(y) } \right) \\
    &amp;&amp;= \sum_{x} p(x) \sum_{y} p(y) \log_2 \frac{1}{p(x)} + \sum_{y} p(y) \sum_{x} p(x) \log_2 \frac{1}{p(y) } \\
    &amp;&amp;= \mathbb{E} \left[ \log_2 \frac{1}{\Pr[X] } \right] + \mathbb{E} \left[ \log_2 \frac{1}{\Pr[Y] } \right]    
\end{array}
\]</span> <span class="math inline">\(\blacksquare\)</span></p>
<p><strong><em>Definition.</em></strong> Given random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the joint entropy <span class="math inline">\(\mathbf{H}(X, Y)\)</span> with a distribution <span class="math inline">\(p(x, y)\)</span> is defined as <span class="math display">\[
    \mathbf{H}(X,Y) = \mathbb{E}\left[ \log \frac{1}{p(x, y)} \right]
   \]</span> When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables, it is equivalent to <span class="math display">\[
    \mathbf{H}(X,Y) = \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(x, y) }.
   \]</span></p>
<p><strong><em>Definition.</em></strong> The conditional entropy <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> with a distribution <span class="math inline">\(p(x, y)\)</span> is defined as <span class="math display">\[
   \begin{aligned}
       \mathbf{H}(Y \mid X) 
        &amp;= \sum_{x \in \mathcal{X} } p(x) \cdot \mathbf{H} (Y \mid X = x) \\
        &amp;= \sum_{x \in \mathcal{X} } p(x) \cdot \sum_{y \in \mathcal{Y} } p(y \mid x) \log \frac{1}{p(y \mid x) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) } \\
        &amp;= \mathbb{E} \left[  \log \frac{1}{p(y \mid x) } \right]
   \end{aligned}
   \]</span></p>
<ul>
<li><strong>Remark.</strong> <em>Conditioned on <span class="math inline">\(X = x\)</span>, <span class="math inline">\(Y\)</span> is a random variable and therefore we can define its Entropy as <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span>. The conditional entropy <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> is the average of the <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span> over the distribution of <span class="math inline">\(X\)</span>. Both <span class="math inline">\(\mathbf{H}(Y \mid X)\)</span> and <span class="math inline">\(\mathbf{H}(Y \mid X = x)\)</span> represent a value. This is different from the conditional expectation: <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span> is a random variable and <span class="math inline">\(\mathbb{E}[Y \mid X = x]\)</span> is a value.</em></li>
</ul>
<p><strong><em>Chain Rule.</em></strong> For random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, it holds that <span class="math display">\[
   \begin{aligned}
       \mathbf{H}(X, Y) 
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(x, y) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) p(x) } \\
        &amp;= \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{p(y \mid x) } + \sum_{x \in \mathcal{X} } \sum_{y \in \mathcal{Y} } p(x, y) \log \frac{1}{ p(x) } \\
        &amp;= \mathbf{H}(Y \mid X) + \mathbf{H}(X)
   \end{aligned}
   \]</span></p>
<p><em>Corollary of</em> <strong><em>Chain Rule.</em></strong> <span class="math display">\[
   \mathbf{H}(X) + \mathbf{H}(Y \mid X) =  
   \mathbf{H}(Y) + \mathbf{H}(X \mid Y)
   \]</span></p>
<p>Or <span class="math display">\[
   \mathbf{H}(X) - \mathbf{H}(X \mid Y)=  
   \mathbf{H}(Y) - \mathbf{H}(Y \mid X)
   \]</span></p>
<h3 id="binomial-distribution">Binomial Distribution</h3>
<p>We study the relationship of binomial distribution <span class="math inline">\(B(n, p)\)</span> and entropy. As a rule of thumb, for reasonably large <span class="math inline">\(n\)</span> (say, <span class="math inline">\(n \ge 100\)</span>) and moderate size of <span class="math inline">\(p\)</span> (e.g., <span class="math inline">\(0.1 \le p \le 0.9\)</span>), the probability is concentrated on the interval <span class="math inline">\([np - \sqrt n, np + \sqrt n]\)</span>.</p>
<p><span class="math inline">\(N = 100, p = 0.5\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.5.png" /></p>
<p><span class="math inline">\(N = 100, p = 0.3\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.3.png" /></p>
<p><span class="math inline">\(N = 100, p = 0.1\)</span>. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=100_P=0.1.png" /></p>
<p><span class="math inline">\(N = 1000, p = 0.1\)</span>. Note that <span class="math display">\[
\Pr[ 100 - \sqrt{1000} \le X \le 100 + \sqrt{1000} ] \approx 0.9993
\]</span> <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/BinomialDist_N=1000_P=0.1.png" /></p>
<p><em><strong>Lemma</strong>.</em> For <span class="math inline">\(n\in \mathbf{Z}^+\)</span>, <span class="math inline">\(k \in [n]\)</span>, it holds that <span class="math display">\[
\binom{n}{k} \ge \frac{1}{n + 1} 2^{n \mathbf{H}(\frac{k}{n} ) }
\]</span></p>
<p><em><strong>Proof</strong>.</em> Define <span class="math inline">\(q = \frac{k}{n}\)</span>. Then <span class="math display">\[
\begin{aligned}
    (q + (1 - q))^n &amp;= \sum_{i = 0}^n \binom{n}{i} q^i (1 - q)^{n - i}
\end{aligned}
\]</span></p>
<p>The summation consists of <span class="math inline">\((n + 1)\)</span> terms. We claim that <span class="math display">\[
\binom{n}{k} q^k (1 - q)^{n - k}
\]</span></p>
<p>is the largest one. For <span class="math inline">\(i \in [n]\)</span>, <span class="math display">\[
\frac{ \binom{n}{i + 1} q^{i + 1} (1 - q)^{n - i - 1} }{ \binom{n}{i } q^{i} (1 - q)^{n - i } } = \frac{n - i }{i + 1} \frac{ q}{1 - q}
\]</span></p>
<p>The ratio is at least <span class="math inline">\(1\)</span> when <span class="math inline">\(\frac{n - i}{i + 1} \ge \frac{1 - q}{ q }\)</span>. We see that when <span class="math inline">\(i = nq - 1 = k - 1\)</span>, it holds that <span class="math display">\[
\frac{n - i}{i + 1} = \frac{n - nq + 1}{ n q} \ge \frac{1 -  q}{ q }
\]</span> and when <span class="math inline">\(i = k\)</span>, <span class="math display">\[
\frac{n - i}{i + 1} = \frac{n - nq}{ n q + 1} &lt; \frac{1 -  q}{ q }
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
    \binom{n}{k} q^k (1 - q)^{n - k} 
        &amp;= \binom{n}{k} 2^{k \log_2 q} 2^{ (n - k )\log_2 (1 - q)} \\
        &amp;= \binom{n}{k} 2^{ n [ q \log_2 q +  (1 - q )\log_2 (1 - q) ]} \\
        &amp;\ge \frac{1}{n + 1}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="extracting-random-bits">Extracting Random Bits</h4>
<p>Consider a uniform random variable <span class="math inline">\(X\)</span> at <span class="math inline">\([0, m - 1]\)</span>. By definition we know that <span class="math display">\[
\mathbf{H}[X] = \sum_{i = 0}^{m - 1} \frac{1}{m} \log_2 \frac{1}{\frac{1}{m} } = \log_2 m
\]</span></p>
<p>Given <span class="math inline">\(X\)</span> as an input, the following algorithm outputs a binary string <span class="math inline">\(s\)</span>, such that each bit of <span class="math inline">\(s\)</span> can be interpreted as the outcome of independently Bernoulli random variables with probability <span class="math inline">\(0.5\)</span>.</p>
<p>On the high level, we divide <span class="math inline">\(m\)</span> into intervals of powers of 2. When <span class="math inline">\(X\)</span> falls into an interval, we return a binary number that is within the range of that interval. Specifically, we rewrite <span class="math display">\[
m = 2^{d_k} + 2^{d_{k - 1} } + ... + 2^{d_1}
\]</span></p>
<p>where <span class="math inline">\(\lfloor \mathbf{H}[X] \rfloor = \lfloor \log_2 m \rfloor = d_k &gt; d_{k - 1} &gt; ... &gt; d_1 \ge 0\)</span> are the indexes of non-zero bits in the binary representation of <span class="math inline">\(m\)</span>. The extraction is as follows:</p>
<blockquote>
<p><strong><em>Algorithm 1.</em></strong></p>
<ol type="1">
<li>for <span class="math inline">\(i \leftarrow k\)</span> <em>down-to</em> <span class="math inline">\(1\)</span> <em>do</em>:<br />
</li>
<li><span class="math inline">\(\quad\)</span> <em>if</em> <span class="math inline">\(X &lt; 2^{d_i}\)</span> <em>then</em>:<br />
</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(\quad\)</span> <em>return a <span class="math inline">\(d_i\)</span>-bit binary representation of</em> <span class="math inline">\(X\)</span>;<br />
</li>
<li><span class="math inline">\(\quad\)</span> <em>else:</em><br />
</li>
<li><span class="math inline">\(\quad\)</span> <span class="math inline">\(\quad\)</span> <span class="math inline">\(X \leftarrow X - 2^{d_i}\)</span>;</li>
</ol>
</blockquote>
<p><strong>Example (<span class="math inline">\(m = 13\)</span>).</strong> <em>It is interesting to note that when <span class="math inline">\(X = 12\)</span>, our proposed extraction method return no random bit, as <span class="math inline">\(0 &lt; 2^0\)</span>. In this case our proposed method has wasted some randomness.</em></p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/Entropy/ExtractingRandomBits.png" /></p>
<p><em>Question to ponder: Come up with some method that uses <span class="math inline">\(n\)</span> i.i.d. <span class="math inline">\(X\)</span>'s to extract binary string with expected length close to <span class="math inline">\(n \mathbf{H}[X]\)</span>.</em></p>
<p>Let <span class="math inline">\(s\)</span> be the binary string returned by Algorithm 1.</p>
<p><strong><em>Theorem 1</em></strong>. <span class="math display">\[
\lfloor \log_2 m \rfloor - 1 \le \mathbf{E}[|s|] \le \log_2 m
\]</span></p>
<p><em>Proof.</em> It is easy to see that <span class="math display">\[
  \mathbf{E}[|s| ] = \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } 
\]</span> which depends on <span class="math inline">\(m\)</span>. To show the upper bound, <span class="math display">\[
\begin{array}{lll}
    \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } - \log_2 m
    = \sum_{i = 1}^k \frac{2^{d_i} }{m } \log_2 \frac{2^{d_i} }{m } \le 0
\end{array}
\]</span> The inequality follows from <span class="math inline">\(\log_2 \frac{2^{d_i} }{m } \le 0\)</span> for <span class="math inline">\(1 \le i \le k\)</span>.</p>
<p>To show the lower bound, define the function <span class="math display">\[
f(m) = \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } 
\]</span></p>
<p>We prove by induction on <span class="math inline">\(m\)</span> that <span class="math inline">\(f(m) \ge \lfloor \log_2 m \rfloor - 1 = d_k - 1\)</span>. The cases holds trivially when <span class="math inline">\(m = 1\)</span>. Now suppose it holds for all integer less than <span class="math inline">\(m\)</span>. We prove that it holds for <span class="math inline">\(m\)</span>.</p>
<p>Case 1. <span class="math inline">\(m = 2^{d_k}\)</span>. Then <span class="math inline">\(f(m) = d_k = \log_2 m\)</span>.</p>
<p>Case 2. <span class="math inline">\(m &gt; 2^{d_k}\)</span>. Now<br />
<span class="math display">\[
\begin{aligned}
    f(m) &amp;= \sum_{i = 1}^k d_i \frac{2^{d_i} }{m } \\
    &amp;= d_k \frac{2^{d_k} }{m} + \left( \sum_{i = 1}^{k - 1} d_i \frac{2^{d_i} }{m - 2^{d_k}  }  \right) \frac{m - 2^{d_k} }{m} \\
\end{aligned}
\]</span></p>
<p>But <span class="math inline">\(\sum_{i = 1}^{k - 1} 2^{d_i} = m - 2^{d_k}\)</span>. It follows by induction <span class="math display">\[
\sum_{i = 1}^{k - 1} d_i \frac{2^{d_i} }{m - 2^{d_k}  } = f(m - 2^{d_k}) \ge d_{k - 1} - 1
\]</span></p>
<p>Finally, <span class="math display">\[
\begin{aligned}
    f(m)
    &amp;\ge d_k \frac{2^{d_k} }{m} + \left( d_{k - 1} -1 \right) \frac{m - 2^{d_k} }{m} \\
    &amp;= d_k +  \left( d_{k - 1}- d_k -1 \right)  \frac{m - 2^{d_k} }{m} \\
    &amp;= d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( 1 - \frac{ 2^{d_k} }{m} \right)\\
    &amp;\ge  d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( 1 - \frac{ 2^{d_k} }{2^{d_k} + 2^{d_{k - 1} } } \right)\\
    &amp;=  d_k -  \left( d_k + 1 - d_{k - 1}  \right) \left( \frac{ 2^{d_{k - 1} } }{2^{d_k} + 2^{d_{k - 1} } } \right)\\
    &amp;=  d_k -  \left( \frac{ d_k  - d_{k - 1} + 1  }{2^{d_k - d_{k - 1} } + 1 } \right)\\     
    &amp;\ge  d_k -  \left( \frac{ d_k  - d_{k - 1} + 1  }{2^{d_k - d_{k - 1} } } \right)\\
    &amp;\ge d_k - 1
\end{aligned}
\]</span></p>
<p>The last inequality holds since <span class="math inline">\(d_{k - 1} \le d_k - 1\)</span> and the function <span class="math inline">\(f(x) = \frac{x + 1}{2^x}\)</span> decreases when <span class="math inline">\(x \ge 1\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h5 id="binary-strings">Binary Strings</h5>
<p>To illustrate the idea of entropy, we study another example of random bit extraction.</p>
<p><strong><em>Problem.</em></strong> <em>Let <span class="math inline">\(t\)</span> be an <span class="math inline">\(n\)</span>-bit binary string each bit of which is interpreted as the outcome of independent bias coin flip that comes up head probability <span class="math inline">\(p \le 0.5\)</span>. Given <span class="math inline">\(t\)</span> as input, we want to output a binary string <span class="math inline">\(s\)</span> (not necessary of length <span class="math inline">\(n\)</span>) of independent unbiased random bits.</em></p>
<p><strong><em>Theorem.</em></strong> There is an extraction algorithm, such that,<br />
1. <span class="math inline">\(\forall \delta \in (0, 1)\)</span>, <span class="math inline">\(\exists N &gt; 0\)</span>, <span class="math inline">\(s.t.\)</span>, <span class="math inline">\(\forall n \ge N\)</span>, <span class="math inline">\(\mathbf{E}[|s|] \ge (1 - \delta) n\mathbf{H}(p)\)</span>.<br />
2. <span class="math inline">\(\forall n &gt; 0\)</span>, <span class="math inline">\(\mathbf{E}[|s|] \le n\mathbf{H}(p)\)</span>.</p>
<p>The theorem states that the expected length of <span class="math inline">\(s\)</span> approximates <span class="math inline">\(n\mathbf{H}(p)\)</span>. Equivalently, <span class="math display">\[
\lim_{n \rightarrow \infty} \frac{\mathbf{E}[|s|]}{n} = \mathbf{H}(p)
\]</span></p>
<p><strong><em>Proof.</em></strong> Define the random variable <span class="math inline">\(Z\)</span> to be the number of ones in <span class="math inline">\(t\)</span>. For any integer <span class="math inline">\(k \ge 0\)</span>, there are <span class="math inline">\(\binom{n}{k}\)</span> strings with <span class="math inline">\(k\)</span> ones. We can map these strings to <span class="math inline">\(\{0, 1, ..., \binom{n}{k} - 1 \}\)</span>. Conditioned on <span class="math inline">\(Z = k\)</span>, the mapping of <span class="math inline">\(t\)</span> is uniform on <span class="math inline">\(\{0, 1, ..., \binom{n}{k} - 1 \}\)</span>. We can use Algorithm 1 to return a binary string <span class="math inline">\(s\)</span>. Now its expected length is <span class="math display">\[
\mathbf{E}[|s|] = \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k]
\]</span></p>
<p>Further, by Hoeffding's inequality, it holds that for <span class="math inline">\(0 &lt; \epsilon &lt; p\)</span>, <span class="math display">\[
\Pr[ |Z - np | \ge n\epsilon] \le \exp\left(- 2 n \epsilon^2 \right)
\]</span></p>
<p>By setting the failure probability <span class="math inline">\(\exp(-2 n \epsilon^2) = \delta / 2\)</span>, we get <span class="math inline">\(\epsilon = \sqrt{\frac{\ln \frac{2}{\delta} }{2n} }\)</span>.</p>
<p>Let <span class="math inline">\(LB = \lceil np - n\epsilon \rceil\)</span> and <span class="math inline">\(UB = \lfloor np + n\epsilon \rfloor\)</span>. Observing that <span class="math inline">\(Z\)</span> is an integer, we have <span class="math display">\[
\begin{array}{rl}  
    \Pr[LB \le Z \le UB] &amp;= \Pr[np - n\epsilon \le Z \le np + n\epsilon] \\
    &amp;\ge 1 - \delta/2
\end{array}
\]</span></p>
<p>As <span class="math inline">\(p \le 0.5\)</span>, it holds that <span class="math display">\[
n = \lceil n/2 - n\epsilon \rceil + \lfloor n/2 + n\epsilon \rfloor \ge \lceil np - n\epsilon \rceil + \lfloor np + n\epsilon \rfloor
\]</span></p>
<p>Therefore, <span class="math display">\[
LB \le UB \le n - LB \\
\]</span></p>
<p>Since <span class="math inline">\(\binom{n}{k}\)</span> increase for <span class="math inline">\(k &lt; n / 2\)</span> and decrease <span class="math inline">\(k &gt; n / 2\)</span>, it holds that <span class="math display">\[
\binom{n}{LB} \le \binom{n}{UB}
\]</span></p>
<p>For <span class="math inline">\(LB \le k \le UB\)</span>, we have <span class="math display">\[
\binom{n}{LB} \le \binom{n}{k} 
\]</span></p>
<p>Define <span class="math inline">\(q = \frac{ LB }{n}\)</span>, <span class="math display">\[
\binom{n}{LB}  \ge \frac{1}{n + 1} 2^{n \mathbf{H}( \frac{ LB }{n} ) } \ge \frac{1}{n + 1} 2^{n\mathbf{H}(p - \epsilon)} 
\]</span></p>
<p>By <strong><em>Theorem 1</em></strong>, we have <span class="math display">\[
\begin{aligned}
    \mathbf{E}[|s|] &amp;= \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
    &amp;\ge \sum_{k = LB}^{UB} \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
    &amp;\ge \sum_{k = LB }^{UB } \left( \left\lfloor \log_2 \binom{n}{k} \right\rfloor - 1 \right) \binom{n}{k} p^k (1 - p)^{n - k}  \\
    &amp;\ge \sum_{k = LB }^{UB } \left( \left\lfloor \log_2  \binom{n}{ LB  } \right\rfloor - 1 \right) \binom{n}{k} p^k (1 - p)^{n - k}  \\
    &amp;=  \left( \left\lfloor \log_2  \binom{n}{ LB  } \right\rfloor - 1 \right) \Pr[LB \le Z \le UB] \\
    &amp;\ge \left( n \mathbf{H}(p - \epsilon)  - \log_2 (n + 1) \right) (1- \delta / 2) \\
    &amp;=  \left( \frac{\mathbf{H}(p - \epsilon)}{\mathbf{H}(p) }  - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) (1- \delta / 2 ) \cdot n \cdot \mathbf{H}(p) \\
    &amp;=  \left( \frac{\mathbf{H} \left(p - \sqrt{\frac{\ln \frac{2}{\delta} }{2n} } \right) }{\mathbf{H}(p) }  - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) (1- \delta / 2 ) \cdot n \cdot \mathbf{H}(p) \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\exists N &gt; 0\)</span>, s.t., <span class="math inline">\(\forall n &gt; N\)</span>, <span class="math inline">\(\left( \frac{\mathbf{H} \left(p - \sqrt{\frac{\ln \frac{2}{\delta} }{2n} } \right) }{\mathbf{H}(p) } - \frac{\log_2 (n + 1) }{ n\mathbf{H}(p) }\right) \ge (1 - \delta / 2)\)</span>, which finishes the proof of lower bound.</p>
<p>As for the upper bound, observe that <span class="math display">\[
\binom{n}{k} p^k ( 1- p)^{n - k} \le 1
\]</span></p>
<p>Therefore, <span class="math inline">\(\binom{n}{k} \le p^{-k} ( 1- p)^{-(n - k) }\)</span>. Thus, we have <span class="math display">\[
\begin{aligned}
       \mathbf{E}[|s|] &amp;= \sum_{k = 1}^n \mathbf{E}\left[ |s| \mid Z = k \right] \Pr[Z = k] \\
       &amp;\le \sum_{k = 1}^n \left( \log_2 \binom{n}{k} \right) \binom{n}{k} p^k (1 - p)^{n - k} \\
       &amp;\le \sum_{k = 1}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k (1 - p)^{n - k} } 
\end{aligned}
\]</span></p>
<p>The last term is exactly the entropy of the string. By independence of the bits, we know that it is equal to <span class="math inline">\(n \mathbf{H}(p)\)</span>.</p>
<p>Remark: we may also prove it algebraically. <span class="math display">\[
\begin{aligned}
       \mathbf{E}[|s|] 
       &amp;\le \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k (1 - p)^{n - k} } \\
       &amp;= \sum_{k = 1}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ p^k } + \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} \log_2 \frac{1}{ (1 - p)^{n - k} } \\
       &amp;= \left( p \log_2 \frac{1}{p} \right) \sum_{k = 1}^n  \binom{n}{k} k p^{k - 1} (1 - p)^{n - k}  \\
       &amp;\quad +  \left( ( 1- p) \log_2 \frac{1}{ 1 - p } \right) \sum_{k = 0}^{n - 1} \binom{n}{k} (n - k) p^k (1 - p)^{n - k - 1} \\
       &amp;= \left( p \log_2 \frac{1}{p} \right) [(x + (1 - p))^n]_{x = p}&#39;  +  \left( (1 - p) \log_2 \frac{1}{ 1 - p } \right) [(p + x)^n]_{x = 1-p}&#39; \\
       &amp;= n \mathbf{H}(p)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<h4 id="reference.">Reference.</h4>
<p>[1] S. Har-Peled, “Chapter 26 Entropy, Randomness, and Information,” p. 6.<br />
[2] M. Mitzenmacher and E. Upfal, Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge: Cambridge University Press, 2005.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/30/Integer-Shortest-Path/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/30/Integer-Shortest-Path/" class="post-title-link" itemprop="url">Integer Shortest Path</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-30 11:15:49" itemprop="dateCreated datePublished" datetime="2020-05-30T11:15:49+10:00">2020-05-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-18 13:01:58" itemprop="dateModified" datetime="2020-06-18T13:01:58+10:00">2020-06-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Single source shortest path is one of most popular the topics taught in a introductory algorithm course. Given a graph <span class="math inline">\(G = \left&lt; V, E \right&gt;\)</span> with <span class="math inline">\(n\)</span> vertices and <span class="math inline">\(m\)</span> edges, the famous Dijkstra's algorithm has runtime <span class="math inline">\(O(m \log n)\)</span> by using a binary heap. When more advanced heap such such Fibonacci heap is deployed, the runtime can be improved to <span class="math inline">\(O(m + n \log n)\)</span>. Most introductory courses are satisfied with the above solutions without going further.</p>
<p>Here we explore the topic by considering a special case where all edge weights are non-negative integers within the set <span class="math inline">\(\{0, 1, 2, ..., C\}\)</span>. We develop new heaps incrementally and we will finally see that Dijkstra algorithm can do substantially better than <span class="math inline">\(O(m + n \log n)\)</span>.</p>
<h4 id="time-om-nc-space-onc.">Time: <span class="math inline">\(O(m + nC)\)</span>, Space: <span class="math inline">\(O(nC)\)</span>.</h4>
<p>As a warm up, we demonstrate an algorithm with runtime <span class="math inline">\(O(m + nC)\)</span> and space overhead <span class="math inline">\(O(nC)\)</span>. Here we exclude the space <span class="math inline">\(O(m + n)\)</span> required for storing the graph to simplify the discussion and comparison between the algorithms.</p>
<p>The idea is very simple. We know that the possible distance from any vertex to the source vertex (denoted as <span class="math inline">\(s\)</span>) is no less than <span class="math inline">\(nC\)</span>. It suffices to keep an array <span class="math inline">\(A_1\)</span> with length <span class="math inline">\(nC\)</span>, such that <span class="math inline">\(A_1[i]\)</span> maintains the set of vertices with temporary distance <span class="math inline">\(i\)</span>. In this context, we call <span class="math inline">\(A_1[i]\)</span> a bucket and the array <span class="math inline">\(A_1\)</span> itself the buckets.</p>
<p>Initially all <span class="math inline">\(A_1[i]\)</span>'s are empty except that <span class="math inline">\(A_1[0]\)</span> contains the source vertex. The algorithm iterates over <span class="math inline">\(A_1\)</span> to find an unmarked vertex with the minimum distance, marked it, update its neighbors' distances and move them to the corresponding buckets.</p>
<p>Each vertex is inserted into <span class="math inline">\(A_1\)</span> once. There are at most <span class="math inline">\(m\)</span> updates of vertices' distances. As the minimum distance of the unmarked vertices is monotonically increasing, we scan the array <span class="math inline">\(A_1\)</span> only once. Summing over the cost we get the <span class="math inline">\(O(m + nC)\)</span> time complexity bound.</p>
<h4 id="time-om-nc-space-oc.">Time: <span class="math inline">\(O(m + nC)\)</span>, Space: <span class="math inline">\(O(C)\)</span>.</h4>
<p>The space usage of the above algorithm can be improve to <span class="math inline">\(O(C)\)</span>. Relabel the vertices as <span class="math inline">\(s = v_0, v_1, v_2, ..., v_n\)</span> according to the order they are marked.</p>
<p>Suppose that Dijkstra's algorithm uses an array <span class="math inline">\(d\)</span> to record the vertices' distances to <span class="math inline">\(s\)</span>. Initially, <span class="math inline">\(d(s) = 0\)</span> and <span class="math inline">\(d(v) = \infty\)</span> for <span class="math inline">\(\forall v \neq s\)</span>. Now, consider the moment that <span class="math inline">\(k\)</span> (<span class="math inline">\(0 \le k &lt; n\)</span>) vertices have been marked. For <span class="math inline">\(i &gt; k\)</span>, the value of <span class="math inline">\(d(v_i)\)</span> is either</p>
<ol type="1">
<li><p><span class="math inline">\(d(v_i) = \infty\)</span>, if <span class="math inline">\(v_i\)</span> is not adjacent to any vertex <span class="math inline">\(v_j\)</span> for <span class="math inline">\(j \le k\)</span>. In this case <span class="math inline">\(v_i\)</span> is not in the array <span class="math inline">\(A_1\)</span>.</p></li>
<li><p><span class="math inline">\(d(v_i) = d(v_j) + w_{i,j}\)</span> for some <span class="math inline">\(j \le k\)</span>, where <span class="math inline">\(w_{i,j}\)</span> is the edge weight between <span class="math inline">\(v_i\)</span> and <span class="math inline">\(v_j\)</span>.</p></li>
</ol>
<p>By the property of Dijkstra's algorithm, at this moment it also holds that <span class="math display">\[
0 = d(s) = d_(v_0) \le d(v_1) \le d(v_2) ... \le d(v_k)
\]</span></p>
<p>Therefore, for <span class="math inline">\(i &gt; k\)</span>, either <span class="math inline">\(v_i\)</span> is not in <span class="math inline">\(A_1\)</span> or <span class="math display">\[
d(v_i) \le d(v_j) + w_{i,j} \le d(v_k) + C
\]</span></p>
<p>All un-marked vertices that are in <span class="math inline">\(A_1\)</span> must be in the range of <span class="math display">\[
A_1[d(v_k) ... d(v_k) + C]
\]</span></p>
<p>In general, let <span class="math inline">\(\mu\)</span> be the distance of the least marked vertex. It suffices to maintain a window of <span class="math inline">\(A_1[\mu...\mu+C]\)</span>. We can implement this by initializing <span class="math inline">\(A_1\)</span> with size <span class="math inline">\(C+1\)</span> and use it in a wrap-around manner.</p>
<h4 id="time-om-n-sqrt-c-space-oc.">Time: <span class="math inline">\(O(m + n \sqrt C)\)</span>, Space: <span class="math inline">\(O(C)\)</span>.</h4>
<p>In previous section, we successfully reduce the size of <span class="math inline">\(A_1\)</span> to <span class="math inline">\(1 + C\)</span>. But the runtime remains <span class="math inline">\(O(m + nC)\)</span>. To motivate the algorithm discussed in this section, we first provide an alternative view of this complexity:</p>
<ol type="1">
<li><p>We update the vertices' distance and move the vertices between buckets, which has cost <span class="math inline">\(O(m)\)</span>;</p></li>
<li><p>At each iteration of Dijkstra's algorithm, in the worst case, we need to scan the entire array of <span class="math inline">\(A_1\)</span> to find an unmarked vertex with minimum distance, which takes <span class="math inline">\(O(C)\)</span> times. This is repeated <span class="math inline">\(n\)</span> times.</p></li>
</ol>
<p>Where could we have wasted our time? Do we really have spend <span class="math inline">\(O(C)\)</span> time to find the unmarked vertex with minimum distance?</p>
<p>We can improve this to <span class="math inline">\(O(\sqrt C)\)</span> by using two-level buckets. We break the buckets in <span class="math inline">\(A_1\)</span> into <span class="math inline">\(\sqrt { C + 1}\)</span> blocks, each of size <span class="math inline">\(\sqrt { C + 1}\)</span> (assume here <span class="math inline">\(\sqrt { C + 1}\)</span> is an integer for simplicity). For each block, we use one flag bit to indicate whether this block is empty. This results in <span class="math inline">\(\sqrt { C + 1}\)</span> flags bits, which are stored in bit array <span class="math inline">\(A_2\)</span>. From now on,</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/1.png" /></p>
<p>Now, to find an unmarked vertex with minimum distance, we scan <span class="math inline">\(A_2\)</span> to find the first non-zero bit then the corresponding bucket in <span class="math inline">\(A_1\)</span>, which takes <span class="math inline">\(O(\sqrt C)\)</span> time. Totaling the runtime, we obtain a bound of <span class="math inline">\(O(m + n \sqrt C)\)</span>.</p>
<p><strong><em>Question to ponder: calculate the size of <span class="math inline">\(\sqrt C\)</span> for some values of <span class="math inline">\(C\)</span> which you consider reasonable in real applications.</em></strong></p>
<h4 id="time-o-m-log_-fracmn-c-space-oc.">Time: <span class="math inline">\(O( m \log_{ \frac{m}{n} } C)\)</span>, Space: <span class="math inline">\(O(C)\)</span>.</h4>
<p>It is natural to extend the idea to <span class="math inline">\(3\)</span>-level buckets. Suppose the block size if <span class="math inline">\(\Delta\)</span>. Then the third level contains a single bit block of size <span class="math inline">\(\Delta\)</span>, the second level contains <span class="math inline">\(\Delta\)</span> bit blocks (with total size <span class="math inline">\(\Delta^3\)</span>), and the first level contains the buckets <span class="math inline">\(A_1\)</span> of size <span class="math inline">\(\Delta^3 = C + 1\)</span>. It follows that <span class="math inline">\(\Delta = (C + 1)^\frac{1}{3}\)</span> and the time complexity is <span class="math inline">\(O(m + n C^\frac{1}{3} )\)</span>.</p>
<p>We may use four level buckets, in which <span class="math inline">\(\Delta = (C + 1)^\frac{1}{4}\)</span> and the time complexity is <span class="math inline">\(O(m + n C^\frac{1}{4})\)</span>.</p>
<p><strong><em>Question to ponder: calculate the size of <span class="math inline">\(C^\frac{1}{3}\)</span> and <span class="math inline">\(C^\frac{1}{4}\)</span> for some values of <span class="math inline">\(C\)</span> which you consider reasonable in real applications.</em></strong></p>
<p>Does the analysis carry for <span class="math inline">\(k\)</span>-level buckets for general value of <span class="math inline">\(k\)</span>? No. We can no longer consider the <span class="math inline">\(k\)</span> as a constant and have to take into consider the overhead of insertion, decrease-key and deletion explicitly.</p>
<p>For a <span class="math inline">\(k\)</span>-level bucket structure with bucket size <span class="math inline">\(\Delta = (1 + C)^\frac{1}{k} \ge 2\)</span>,</p>
<ol type="1">
<li><p>insertion takes <span class="math inline">\(O(k)\)</span> time,</p></li>
<li><p>decrease-key takes <span class="math inline">\(O(k)\)</span> time,</p></li>
</ol>
<p>as we need to maintain the indicators bits in <span class="math inline">\(2...k\)</span> levels. Finally,</p>
<ol start="3" type="1">
<li>delete-min takes <span class="math inline">\(O(k \Delta)\)</span> time,</li>
</ol>
<p>as we need to scan <span class="math inline">\(k\)</span> buckets, each of size <span class="math inline">\(\Delta\)</span>, to find the min element and delete it.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/2.png" /></p>
<p>The time complexity now becomes <span class="math display">\[
O(mk + n k \Delta) = O( mk + nkC^\frac{1}{k})
\]</span></p>
<p>Balancing the two terms, we get <span class="math inline">\(\frac{1}{k} = \frac{ \log \frac{m}{n} }{ \log C}\)</span>, i.e., <span class="math inline">\(k = \log_{\frac{m}{n} } C\)</span>. To summarize, the time is <span class="math display">\[
O(m \log_{\frac{m}{n} } C)
\]</span></p>
<p>and the space overhead is <span class="math display">\[
O\left( \sum_{i = 0}^{k - 1} C / \Delta^i \right) = O(C)
\]</span></p>
<p><strong><em>Question to ponder: calculate the size of <span class="math inline">\(\log_{\frac{m}{n} } C\)</span> for some values of <span class="math inline">\(C\)</span> which you consider reasonable in real applications.</em></strong></p>
<h4 id="time-om-n-fraclog-clog-log-c-space-o-fraclog-clog-log-c-2-.">Time: <span class="math inline">\(O(m + n \frac{\log C}{\log \log C})\)</span>, Space: <span class="math inline">\(O( (\frac{\log C}{\log \log C} )^2 )\)</span>.</h4>
<p>The core idea underlying the k-level bucket structure discussed in the previous section is that every integer <span class="math inline">\(w\)</span> in <span class="math inline">\([0, C]\)</span> can be written as a base-<span class="math inline">\(\Delta\)</span> number with length at most <span class="math inline">\(k\)</span>: <span class="math display">\[
w = \sum_{i = 0}^{k - 1} w_i \cdot \Delta^i
\]</span></p>
<p>where <span class="math inline">\(w_i \in [0, \Delta)\)</span>. For convenience, we write it as <span class="math display">\[
w = (w_{k - 1}, w_{k - 2}, ..., w_0)_\Delta
\]</span></p>
<p>Now the meaning of the <span class="math inline">\(k\)</span>-level buckets is clear. The <span class="math inline">\(k^{th}\)</span> level partitions the <span class="math inline">\(w\)</span>'s according to the value of <span class="math inline">\(w_{k - 1}\)</span>. The <span class="math inline">\((k - 1)^{th}\)</span> level further partitions the <span class="math inline">\(w\)</span>'s according to the value of <span class="math inline">\(w_{k - 2}\)</span>, and so on. The path from the top level to bottom level is uniquely determined by the sequence <span class="math inline">\((w_{k - 1}, w_{k - 2}, ..., w_0)_\Delta\)</span>. Indeed, this is a special case of <strong><em>trie</em></strong>, which has alphabet size <span class="math inline">\(\Delta\)</span> and contains only sequence of length <span class="math inline">\(k\)</span>.</p>
<p>But we may still waste some work. Remember that our goal for designing the data structure is to find the min element. Therefore, it suffices to distinguish the min element from the rest. We should not waste our effort in determining the relative order between the non-min elements.</p>
<p>In the following example, the min elements <span class="math inline">\(v_1, v_2\)</span> fall into the subtree of the first bucket on the <span class="math inline">\(k^{th}\)</span> level while <span class="math inline">\(v_{n - 1}, v_n\)</span> fall into the last one. As <span class="math inline">\(v_{n - 1}, v_n\)</span> are not min elements, it is unnecessary to maintain a subtree to separate them. Worse still, if later decrease-key is applied to either <span class="math inline">\(v_{n -1}\)</span> or <span class="math inline">\(v_n\)</span>, and if the new key has to be moved to a new bucket in the <span class="math inline">\(k^{th}\)</span> level, the effort we have spent in constructing the sub-tree is completely useless.</p>
<p>Instead, we can create an auxiliary set to the last bucket on level <span class="math inline">\(k\)</span>, to pus <span class="math inline">\(v_{n - 1}, v_n\)</span> into the set directly. Similar idea apply to <span class="math inline">\(v_3\)</span> in the example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/3.png" /></p>
<p>The new structure we get is as follows:</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/4.png" /></p>
<p>In general, we may not even has a <span class="math inline">\(k\)</span>-level trie. In the example below, we can already determine the min-element <span class="math inline">\(v_1\)</span> at some level <span class="math inline">\(i &gt; 1\)</span>. Why should we bother to expand the buckets further? Note that this idea is similar to that of trie compression used in <strong><em>Patricia trie</em></strong>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/5.png" /></p>
<p>We can be even more lazy. We expand a bucket only when it is necessary. See the example below as an example. In this example, <span class="math inline">\(C = 15\)</span> and <span class="math inline">\(\Delta = 4\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example1-2.png" /></p>
<p>Now, to delete-min, we need to determine the order of <span class="math inline">\(v_1, v_2\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example3.png" /></p>
<p>Inserting a new element follows the current structure, until it finds a bucket to fall in.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example4.png" /></p>
<p>Delete-min begin the search from the lowest level non-empty bucket block.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example5.png" /></p>
<p>After delete-min, we may need to delete the empty bucket blocks recursively. For each bucket block, we maintain counter for the number of non-empty buckets. This allows us to delete empty bucket blocks in <span class="math inline">\(O(k)\)</span> time.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example6.png" /></p>
<p>We decrease the key of <span class="math inline">\(v_4\)</span> to 9. Note that we do not need to compare <span class="math inline">\(v_4\)</span> and <span class="math inline">\(v_5\)</span> for the moment. Bucket block counter is maintained when <span class="math inline">\(v_4\)</span> is moved to another bucket.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example7.png" /></p>
<p>Delete-min is invoked again. We need to expand the bucket containing <span class="math inline">\(v_5\)</span> and <span class="math inline">\(v_4\)</span> to distinguish them.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example8.png" /></p>
<p>We continue to insert a new element <span class="math inline">\(v_6 = 12\)</span>.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example9.png" /></p>
<p>We decrease the key of <span class="math inline">\(v_6\)</span> to <span class="math inline">\(10\)</span>. It is pushed down by one level.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Example10.png" /></p>
<p>We are now prepared to analyze the amortized cost of these operations:</p>
<ol type="1">
<li><p>Insert: we pay an <span class="math inline">\(O(k)\)</span> amortized cost of insertion. Any subsequent pushed down operation on this element is charged to its insertion. Note that an element can be push-down by at most <span class="math inline">\(k\)</span> levels.</p></li>
<li><p>Decrease-key: we pay an <span class="math inline">\(O(1)\)</span> amortized cost of decrease-key operation. The push-down operation is charged to its insertion.</p></li>
<li><p>Delete-min: to find the min-element, we scan the bucket block in the lowest level to find the first non-empty bucket. This has time <span class="math inline">\(\Delta\)</span>. If the bucket found contains multiple elements, we perform an expansion and push the elements down, the cost of which is charged to insertion of these elements. Finally, we may need to delete empty bucket blocks recursively, which has cost <span class="math inline">\(O(k)\)</span>.</p></li>
</ol>
<p>Therefore, the Dijkstra's algorithm with this structure has runtime <span class="math inline">\(O(m + n(k + \Delta))\)</span>, which is minimized when <span class="math display">\[
k = \Delta = (C + 1)^\frac{1}{k}
\]</span></p>
<p>Solving the equation gives <span class="math inline">\(k = O(\frac{\log C}{\log \log C} )\)</span>.</p>
<h4 id="time-o-m-n-sqrtlog-c-space-osqrtlog-c-cdot-2sqrtlog-c-.">Time: <span class="math inline">\(O( m + n \sqrt{\log C} )\)</span>, Space: <span class="math inline">\(O(\sqrt{\log C} \cdot 2^{\sqrt{\log C} })\)</span>.</h4>
<p>Beginning from the naïve approach, we have improved a lot on both time and space overhead. Amazingly, this is not the end of the story. To motivate what is possibly the inefficient part of <span class="math inline">\(O(m + n \frac{\log C}{\log \log C})\)</span> approach, look at the following example.</p>
<p>Suppose that <span class="math inline">\(C\)</span> is a large number. Initially the structure is empty. We insert an element <span class="math inline">\(v_1\)</span> followed by <span class="math inline">\(v_2\)</span> and they fall into the same bucket in the <span class="math inline">\(k^{th}\)</span> level. Next, we perform a delete-min operation. To distinguish, we expand the buckets all the way down to <span class="math inline">\(1^{st}\)</span> level.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/IntegerShortestPath/Drawback.png" /></p>
<p>But since there are two elements, why don't we compare them directly and return the minimum one? In such case we avoid the unnecessary expansions.</p>
<p>We can extend the idea further. If the number of elements in a bucket is too few, we refuse to expand it. Instead we make of copy of the elements and construct a comparison based priority queue (e.g., Fibonacci heap) on them. Further, note that we are only interested in the possible min-element. Therefore the heap is constructed for only one bucket that contains the min-element.</p>
<p>We use a parameter <span class="math inline">\(t\)</span> (to be determined) to control this number. In particular, if the number is at most <span class="math inline">\(t\)</span>, we construct a comparison based priority queue. We perform expansion only if the number exceeds <span class="math inline">\(t\)</span>.</p>
<ol type="1">
<li><p>Insert: insert an element as before. If the element falls into the bucket that is associated with a priority, then also insert this element into the heap.</p></li>
<li><p>Decrease-key: if the element is in the bucket that is associated with a priority queue, perform decrease-key in the queue. Otherwise, perform decrease-key as before. If the element falls into the bucket that is associated with a priority queue, then also insert this element into the heap.</p></li>
<li><p>Delete-min:</p></li>
</ol>
<blockquote>
<ol type="1">
<li>Find the first non-empty bucket in the lowest level<br />
</li>
<li><strong><em>IF</em></strong> the bucket has more than <span class="math inline">\(t\)</span> elements <strong><em>THEN</em></strong><br />
</li>
<li><span class="math inline">\(\qquad\)</span> Expand the bucket<br />
</li>
<li><span class="math inline">\(\qquad\)</span> <strong><em>GO TO STEP 1</em></strong></li>
<li><strong><em>ELSE</em></strong></li>
<li><span class="math inline">\(\qquad\)</span> <strong><em>IF</em></strong> <span class="math inline">\(\nexists\)</span> queue <strong><em>THEN</em></strong><br />
</li>
<li><span class="math inline">\(\qquad\qquad\)</span> Construct a queue</li>
<li><span class="math inline">\(\qquad\)</span> Perform delete-min in the queue</li>
</ol>
</blockquote>
<p>Let <span class="math inline">\(I(t), D(t), X(t)\)</span> be the time needed to perform insert, decrease-key and delete-min operation in a comparison based priority queue respectively. Then it is obvious that the the structure we propose, the amortized time we need for insert and decrease-key is</p>
<ol type="1">
<li>Insert: <span class="math inline">\(O(k + I(t))\)</span>.</li>
<li>Decrease-key: <span class="math inline">\(O(D(t) + I(t))\)</span>.</li>
</ol>
<p>To analyze the time needed for delete-min is more complicated. It relies on the implementation of the following:</p>
<blockquote>
<p>Find the first non-empty bucket in the lowest level.</p>
</blockquote>
<p>We record the previous deleted min element <span class="math inline">\(\mu\)</span> (initially set to 0). Denote the <span class="math inline">\(\Delta\)</span>-base representation of <span class="math inline">\(\mu\)</span> as <span class="math display">\[
\mu = (\mu_{k - 1}, \mu_{k - 2}, ..., \mu_0)_\Delta
\]</span> Observe that the non-empty bucket block in the lowest level must contain <span class="math inline">\(\mu\)</span>. Denote this level as <span class="math inline">\(i\)</span>. To find the first non-empty bucket, we start scan from <span class="math inline">\(\mu_{i - 1}^{th}\)</span> bucket in the <span class="math inline">\(i^{th}\)</span> level. Therefore, each bucket block is only scanned once. Further, the buck block in the <span class="math inline">\(i^{th}\)</span> level is created from the <span class="math inline">\((i + 1)^{th}\)</span> level because it contains more than <span class="math inline">\(t\)</span> elements. We can charge the scanned cost to these <span class="math inline">\(t\)</span> elements, each with <span class="math inline">\(\frac{\Delta}{t}\)</span>. As there are <span class="math inline">\(k\)</span> level, a element could be charge at most <span class="math inline">\(k\)</span> times. It follows that the amortized cost of delete-min is given by</p>
<ol start="3" type="1">
<li>Delete-min: <span class="math inline">\(O(X(t) + \frac{k \Delta}{t})\)</span></li>
</ol>
<p>For Fibonacci heap, <span class="math inline">\(I(t) = D(t) = 1\)</span> and <span class="math inline">\(X(t) = \log t\)</span>. Therefore, we have runtime <span class="math display">\[
O \left( m + n \left[k + \log t + \frac{k \Delta}{t} \right] \right)
\]</span></p>
<p>To minimize it, we need to set <span class="math inline">\(k = \log t = \frac{k \Delta}{t}\)</span>. It holds that <span class="math inline">\(t = 2^k\)</span> and <span class="math inline">\(t = \Delta = C^\frac{1}{k}\)</span>, which implies that <span class="math inline">\(2^{k} = 2^{ \frac{\log C}{k} }\)</span> and <span class="math inline">\(k = \sqrt {\log C}\)</span>. The time is <span class="math display">\[
O(m + n \sqrt{\log C})
\]</span></p>
<p>and the space overhead is <span class="math inline">\(O(k \Delta) = O(\sqrt{\log C} \cdot 2^{\sqrt{\log C} })\)</span>.</p>
<h3 id="reference">Reference</h3>
<p>[1]. David R. Karger, MIT Advanced Algorithm, 2013.<br />
[2]. B. V. Cherkassky, A. V. Goldberg, and C. Silverstein, “Buckets, Heaps, Lists, and Monotone Priority Queues,” SIAM J. Comput., vol. 28, no. 4, pp. 1326–1346, Jan. 1999</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/26/Splay-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/26/Splay-Tree/" class="post-title-link" itemprop="url">Splay Tree</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-26 16:19:41" itemprop="dateCreated datePublished" datetime="2020-05-26T16:19:41+10:00">2020-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-05 16:12:56" itemprop="dateModified" datetime="2020-06-05T16:12:56+10:00">2020-06-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Splay Tree [1] is a self-binary search tree that achieves <span class="math inline">\(O(\log n)\)</span> amortized time complexity for each operation. Unlike other balanced binary search trees, it does not require to maintain additional information to keep balance. Further, it has good property that more recently accessed nodes are more easily retrieved and achieves certain optimality even for an unknown sequence of operations.</p>
<h2 id="basic-operations">Basic Operations</h2>
<p>The philosophy of splay tree is very simple: just bring the last accessed node <span class="math inline">\(x\)</span> to the root of the tree, via a sequence of <em>rotations</em>. This brings the most recently visited nodes close to the root.</p>
<p>[Remark: For insertion, the last accessed node is the node inserted. For a successful search operation, the last accessed node is the node found. For an unsuccessful search/deletion, it is the predecessor or successor of the search node / deleted node. ]</p>
<p>The rotations that bring a node <span class="math inline">\(x\)</span> to the root is not unique. For example, the simplest one rotates <span class="math inline">\(x\)</span> with its parent until it becomes the root. However, we can easily find counter examples for this naïve approach. The rotations need to be carefully designed and are called <em>splays</em>. At each step the node being splayed is brought up at most 2 levels. It can be divided into three categories. Call <span class="math inline">\(x\)</span> the node being splayed, <span class="math inline">\(y\)</span> its parent and <span class="math inline">\(z\)</span> its grandparent (if they exist).</p>
<ol type="1">
<li>Root case: <span class="math inline">\(x\)</span> is the root. Do nothing.</li>
<li>The <em>Zig</em> case: <span class="math inline">\(y\)</span> is the root. We just perform <span class="math inline">\(rotate(x, y)\)</span>.
<ul>
<li>See the following figure. <img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/Zig.png" /></li>
</ul></li>
<li>Otherwise, <span class="math inline">\(x\)</span> has a grandparent <span class="math inline">\(z\)</span>. It is further divided into two categories:
<ol type="1">
<li><em>Zig-Zig</em> case: both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are left children or right children. We first perform <span class="math inline">\(rotate(y, z)\)</span> then <span class="math inline">\(rotate(x, y)\)</span>.<br />
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZig.png" /></li>
<li><em>Zig-Zag</em> case: <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are different children of their parents, i.e., <span class="math inline">\(x\)</span> is the left child and <span class="math inline">\(y\)</span> is the right child, or vice versa. We first perform <span class="math inline">\(rotate(x, y)\)</span> then <span class="math inline">\(rotate(y, z)\)</span>.<br />
<img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZag.png" /></li>
</ol></li>
</ol>
<p>The splay is performed repeatedly on <span class="math inline">\(x\)</span> until it reaches the root. It is also possible to perform equivalent operations in an top-down approach [1].</p>
<h2 id="time-analysis">Time Analysis</h2>
<p>There is no explicit control over the height of the tree. It is possible that the accessed node has depth <span class="math inline">\(\Omega(\log n)\)</span>. The intuition is that, however, the imbalanced situation is not easy to create and must accumulate from previous operations. The number such operations can not be too to create the imbalance. We can charge the cost of accessing the node with long depth to these operations. Intuitively, each previous operation is required to reserve some additional "energy" for the amount of imbalanced it creates, which is used later to fix the imbalance. When a long path is encountered, there should be enough "energy" to cover the search on the path and the splay of the access node.</p>
<p>It is left to give a quantitative scheme for keeping the "energy" and "using" the "energy". We need a few more definitions for a node <span class="math inline">\(x\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(T(x):\)</span> the subtree rooted at <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(w(x):\)</span> the weight associated with node <span class="math inline">\(x\)</span>. This weight is only used for analysis and is not kept explicitly by splay tree.</li>
<li><span class="math inline">\(s(x)=\sum_{y \in T(x)} w_y:\)</span> the sum of weights of nodes in <span class="math inline">\(T(x)\)</span>, which is referred to as the size of <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(r(x) \doteq \log r(x)\)</span>, the rank of <span class="math inline">\(x\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(T\)</span> be the splay tree. The potential function is defined as <span class="math display">\[
\Phi = \sum_{x \in T} r(x)
\]</span> which records the amount of "energy" in <span class="math inline">\(T\)</span>.</p>
<p>The amortized cost of an operation is defined as <span class="math display">\[
\text{amortized cost} = \text{actual cost} + \Delta \Phi
\]</span> where <span class="math inline">\(\Delta \Phi\)</span> is the change of the potential function.</p>
<p>Now, let <span class="math inline">\(r(x)\)</span> be the rank of <span class="math inline">\(x\)</span> before we performed a single splay operation on <span class="math inline">\(x\)</span> and <span class="math inline">\(r&#39;(x)\)</span> the one after. The <em>Access Lemma</em> states the <span class="math inline">\(\Phi\)</span> changes as follows:</p>
<blockquote>
<p><strong><em>Access Lemma</em></strong> For a single splay operation, the potential change is bounded by <span class="math display">\[ 3(r&#39;(x) - r(x)) - 2 \]</span> for <em>zig-zig</em> and <em>zig-zag</em> case and <span class="math display">\[ r&#39;(x) - r(x) \]</span> for the <em>zig</em> case.</p>
</blockquote>
<p>Before we prove this key lemma, we use it for the following theorem.</p>
<p><strong><em>Theorem.</em></strong> The amortized cost of the splaying a node <span class="math inline">\(x\)</span> to the root is <span class="math inline">\(O(1 + \log \frac{s(root) }{s(x) })\)</span>.</p>
<p><em>Proof.</em> To splay a node <span class="math inline">\(x\)</span> to the root, suppose that we have performed <span class="math inline">\(k\)</span> splays. Further, let <span class="math inline">\(r_i(x)\)</span> be the rank of <span class="math inline">\(x\)</span> after we performed the <span class="math inline">\(i^{th}\)</span> splay on <span class="math inline">\(x\)</span>. We have <span class="math inline">\(r_0(x) = \log s(x)\)</span> and <span class="math inline">\(r_k(x) = \log s(root)\)</span>. Observe that the actual cost of splay operation is <span class="math inline">\(2\)</span> for <em>zig-zig</em> and <em>zig-zag</em> case and 1 for the <em>zig</em> case. Hence, the amortized cost is <span class="math display">\[
\begin{aligned}
\text{amortized cost} 
&amp;\le \sum_{i = 1}^k 3(r_i(x) - r_{i - 1}(x)) +1 \\
&amp;= 3(r_k(x) - r_0(x)) + 1
\end{aligned}
\]</span> The added 1 results from the final possible <em>zig</em> operation.</p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>If we set <span class="math inline">\(w(x) = 1\)</span> for all nodes, then <span class="math inline">\(r_k(x) = \log n\)</span> and <span class="math inline">\(r_1(x) \ge \log 1 = 0\)</span>. Therefore, the amortized cost of splaying <span class="math inline">\(x\)</span> is bounded by <span class="math inline">\(O(\log n)\)</span>.</p>
<h3 id="proof-of-the-access-lemma.">Proof of The Access Lemma.</h3>
<h4 id="zig-zig"><strong><em>Zig-Zig</em></strong></h4>
<ul>
<li><p>Example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZig.png" /></p></li>
</ul>
<p>First, note that <span class="math display">\[
\begin{aligned}
        &amp;   \frac{1}{2} r&#39;(z) + \frac{1}{2} r(x)    \\
    =   &amp;   \frac{1}{2} \log s&#39;(z) + \frac{1}{2} \log s(x)   \\
    \le &amp;   \log \left[ \frac{s&#39;(z)}{2}  + \frac{ s(x) }{2} \right] \\
    &lt;   &amp;   \log \left[ \frac{s(z)}{2}  \right] \\
\end{aligned}
\]</span></p>
<p>The first inequality follows from that the function <span class="math inline">\(\log (\cdot )\)</span> is concave and the second one follows from <span class="math inline">\(s&#39;(z) + s(x) = s(z) - 1 &lt; s(z)\)</span>.</p>
<p>Therefore, <span class="math display">\[
r&#39;(z) + r(x) \le 2 \left[ r(z) - 1 \right]
\]</span></p>
<p>Now consider the potential change of the zig-zig operation: <span class="math display">\[
\begin{array}{rll}
        &amp;   r&#39;(x) + r&#39;(y) + r&#39;(z) - r(x) - r(y) - r(z) \\
    =   &amp;   r&#39;(y) + r&#39;(z) - r(x) - r(y) \\
    \le &amp;   r&#39;(x) + r&#39;(z) - r(x) - r(x) \\
    =   &amp;   [r&#39;(x) - r(x)]  +[r&#39;(z) + r(x)] - 2 r(x) \\
    \le &amp;   [r&#39;(x) - r(x)] +2[r(z) - 1] - 2 r(x) \\
    =   &amp;   3[r&#39;(x) - r(x)] - 2
\end{array}
\]</span></p>
<p>The first equality follows from <span class="math inline">\(r&#39;(x) = r(z)\)</span>, as <span class="math inline">\(s&#39;(x) = s(z)\)</span>. The first inequality holds since <span class="math inline">\(r(x) \le r(y)\)</span>.</p>
<h4 id="zig-zag"><strong><em>Zig-Zag</em></strong></h4>
<ul>
<li><p>Example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/ZigZag.png" /></p></li>
</ul>
<p>First, note that <span class="math display">\[
\begin{aligned}
        &amp;   \frac{1}{2} r&#39;(y) + \frac{1}{2} r&#39;(z)    \\
    =   &amp;   \frac{1}{2} \log s&#39;(y) + \frac{1}{2} \log s&#39;(z)   \\
    \le &amp;   \log \left[ \frac{s&#39;(y)}{2}  + \frac{ s&#39;(z) }{2} \right] \\
    &lt;   &amp;   \log \left[ \frac{s&#39;(x)}{2}  \right] \\
\end{aligned}
\]</span></p>
<p>The first inequality follows from that the function <span class="math inline">\(\log (\cdot )\)</span> is concave and the second one follows from <span class="math inline">\(s&#39;(y) + s&#39;(z) = s&#39;(x) - 1 &lt; s&#39;(x)\)</span>.</p>
<p>Therefore, <span class="math display">\[
r&#39;(y) + r&#39;(z) \le 2 \left[ r&#39;(x) - 1 \right]
\]</span></p>
<p>Now consider the potential change of the zig-zag operation: <span class="math display">\[
\begin{array}{rll}
        &amp;   r&#39;(x) + r&#39;(y) + r&#39;(z) - r(x) - r(y) - r(z) \\
    =   &amp;   r&#39;(y) + r&#39;(z) - r(x) - r(y) \\
    \le &amp;   r&#39;(y) + r&#39;(z) - r(x) - r(x) \\
    \le &amp;   2 \left[ r&#39;(x) - 1 \right] - 2 r(x) \\
    =   &amp;   2[r&#39;(x) - r(x)] - 2
\end{array}
\]</span></p>
<p>The first equality follows from <span class="math inline">\(r&#39;(x) = r(z)\)</span>, as <span class="math inline">\(s&#39;(x) = s(z)\)</span>. The first inequality holds since <span class="math inline">\(r(x) \le r(y)\)</span>.</p>
<h4 id="zig"><strong><em>Zig</em></strong></h4>
<ul>
<li><p>Example.</p>
<p><img src="https://github.com/wuhao-wu-jiang/BlogImgs/raw/master/SplayTree/Zig.png" /></p></li>
</ul>
<p>Now consider the potential change of the zig-zag operation: <span class="math display">\[
\begin{array}{rll}
        &amp;   r&#39;(x) + r&#39;(y)  - r(x) - r(y)  \\
    =   &amp;   r&#39;(y)  - r(x)  \\
    \le &amp;   r&#39;(x)  - r(x) 
\end{array}
\]</span></p>
<p><span class="math inline">\(\blacksquare\)</span></p>
<p>We now show the implementation of basic BST operations and their amortized cost. Throughout the analysis we assume that <span class="math inline">\(w(x) = 1\)</span>.</p>
<h3 id="cost-of-insertion">Cost of Insertion</h3>
<p>Inserting a node <span class="math inline">\(x\)</span> is the same as that in a BST. Then we splay node <span class="math inline">\(x\)</span> to the root. We charge the cost of going down the tree by the cost of splaying <span class="math inline">\(x\)</span>, which has amortized cost <span class="math inline">\(O(\log n)\)</span>. It is left to analyze the change of <span class="math inline">\(\Phi\)</span>. Let <span class="math inline">\(root = x_1, x_2, ..., x_k, x_{k + 1} = x\)</span> be the root to <span class="math inline">\(x\)</span> path right after inserting <span class="math inline">\(x\)</span>. The insertion of <span class="math inline">\(x\)</span> increases <span class="math inline">\(s(x_1), s(x_2), ..., s(x_{k })\)</span> by 1, therefore, <span class="math display">\[
\begin{array}{llr}
\Delta \Phi &amp;= \sum_{i = 1}^k \log \frac{s(x_i) + 1 }{s(x_i)} \\
            &amp;= \log \prod_{i = 1}^k \frac{s(x_i) + 1 }{s(x_i)} \\
            &amp;\le \log \frac{s(x_1) + 1}{ s(x_k ) }
\end{array}
\]</span> The last inequality follows from that <span class="math inline">\(s(x_i) \ge s(x_{i + 1}) + 1\)</span> for <span class="math inline">\(i \in [k]\)</span>. Therefore, <span class="math inline">\(\Delta \Phi = O(\log n)\)</span> and the overall amortized cost of insertion is <span class="math inline">\(O(\log n)\)</span>.</p>
<p>Remark: <span class="math inline">\(key(x_{k})\)</span> is either the predecessor or successor of <span class="math inline">\(key(x)\)</span>.</p>
<h3 id="cost-of-search">Cost of Search</h3>
<p>Whether the search is successful or not, we splay the last accessed node to the root and charge the cost of going down the tree to the splay-operations. Therefore, the amortized cost is <span class="math inline">\(O(\log n)\)</span>.</p>
<p>Remark: in case of unsuccessful search, we find either the predecessor or successor of the search key.</p>
<h3 id="cost-of-deletion">Cost of Deletion</h3>
<p>As in a BST, deletion is a little bit tricky. Let <span class="math inline">\(x\)</span> be the node to delete and <span class="math inline">\(y\)</span> be its parent (if exists). There are 3 cases</p>
<ul>
<li><p>If <span class="math inline">\(x\)</span> is a leaf node, then we just delete it and splay <span class="math inline">\(y\)</span> to the root.</p></li>
<li><p>If <span class="math inline">\(x\)</span> has only one child, we replace <span class="math inline">\(x\)</span> with its child as the child of <span class="math inline">\(y\)</span>. Then we delete <span class="math inline">\(x\)</span> and splay <span class="math inline">\(y\)</span> to the root.</p></li>
</ul>
<p>The final case uses the first two cases as a sub-routine. We first discuss the amortized cost of the first two cases. The cost of going down to <span class="math inline">\(x\)</span> can be charged to the cost of splaying <span class="math inline">\(y\)</span> to the root, which has amortized cost <span class="math inline">\(O(\log n)\)</span>. Deleting the node decreases the potential function, which will only lower the overall amortized cost. Therefore, the amortized cost of deletion is bounded by <span class="math inline">\(O(\log n)\)</span>.</p>
<ul>
<li>If <span class="math inline">\(x\)</span> has two children, we find the predecessor node <span class="math inline">\(pred(x)\)</span> of <span class="math inline">\(x\)</span>, which is the node with largest key in <span class="math inline">\(x\)</span>'s left sub-tree. Note that <span class="math inline">\(pred(x)\)</span> has at most one child. Then we delete the node <span class="math inline">\(pred(x)\)</span>. This reduces to case 1 or case 2. Finally we replace the <span class="math inline">\(key(x)\)</span> with <span class="math inline">\(key(pred(x))\)</span>.</li>
</ul>
<p>In the following sections we analyze the properties of the splay tree on a sequence <span class="math inline">\(S\)</span> of <span class="math inline">\(m\)</span> operations.</p>
<h2 id="static-optimality">Static Optimality</h2>
<p>Suppose that <span class="math inline">\(S\)</span> consists of only successful search operations. Let $p_i &gt; 0 $ be the frequency that the <span class="math inline">\(i^{th}\)</span> element is accessed. Define a static binary search tree to be the one whose structure is fixed. In particular, it can not perform rotations.</p>
<p><strong><em>Theorem.</em></strong> (Static Optimality) The total cost of <span class="math inline">\(S\)</span> performed on the splay tree is at most a constant times the minimum possible cost of <span class="math inline">\(S\)</span> performed a static binary search tree, plus <span class="math inline">\(O(m )\)</span>.</p>
<p><em>Proof.</em> Let <span class="math inline">\(T^*\)</span> be the static binary search tree that minimizes the cost of <span class="math inline">\(S\)</span>. For the <span class="math inline">\(i^{th}\)</span> element, let <span class="math inline">\(l_i\)</span> be the depth of <span class="math inline">\(i\)</span> in <span class="math inline">\(T^*\)</span>, i.e. the number of nodes on the path from <span class="math inline">\(i\)</span> to the root, so <span class="math inline">\(l(root) = 1\)</span>. The total cost in <span class="math inline">\(T^*\)</span> is given by <span class="math inline">\(\sum_{i = 1}^n p_i \cdot m \cdot l_i\)</span>. We are going to prove that the cost for splay tree is <span class="math display">\[
O(m + \sum_{i = 1}^n p_i \cdot m \cdot l_i)
\]</span> Suppose that <span class="math inline">\(T^*\)</span> has maximum depth <span class="math inline">\(d_{max}(T^*)\)</span>. We claim that for any <span class="math inline">\(k &lt; d_{max}(T^*)\)</span>, <span class="math inline">\(T^*\)</span> must contain exactly <span class="math inline">\(2^k\)</span> nodes with depth <span class="math inline">\(k\)</span>. Otherwise, we can cut some leaf node from its parent and pad it into the <span class="math inline">\(i^{th}\)</span> level. This will only decrease the access cost, contradicting <span class="math inline">\(T^*\)</span> being the optimal static binary tree. Then, <span class="math inline">\(d_{max}(T^*) \le \lfloor \log_2 n \rfloor + 1\)</span> and <span class="math display">\[
\sum_{i = 1}^n 3^{-l_i} \le \frac{1}{2} \sum_{k = 1}^{ \lfloor \log_2 n \rfloor + 1 } (\frac{2}{3})^k \le 1
\]</span> To investigate the time for splay tree, we choose a different weight than earlier and just set <span class="math inline">\(w_i = 3^{-l_i}\)</span>. Now the amortized cost of accessing <span class="math inline">\(i^{th}\)</span> element is given by <span class="math display">\[
O(1 + \log_2 \frac{s(root)}{s(i)}) = O(1 + \log_2 \frac{1}{3^{-l_i}}) = O(1 + l_i)
\]</span> Let <span class="math inline">\(c_k\)</span> be the actual cost of the <span class="math inline">\(k^{th}\)</span> operation in <span class="math inline">\(S\)</span> and <span class="math inline">\(a_k\)</span> be the amortized cost. Further, let <span class="math inline">\(\Phi_k\)</span> be the potential of the splay tree after the <span class="math inline">\(k^{th}\)</span> operation. Then <span class="math display">\[
\sum_{k = 1}^m a_k = \sum_{k = 1}^m (c_k + \Phi_k - \Phi_0) = \sum_{k = 1}^m c_k + \Phi_m - \Phi_0
\]</span> Hence, the actual cost on this sequence is bounded by <span class="math display">\[
\begin{aligned}
\sum_{k = 1}^m c_k &amp;= \Phi_0 - \Phi_m + \sum_{k = 1}^m a_k \\
&amp;=  \Phi_0 - \Phi_m + \sum_{i = 1}^m p_i \cdot m \cdot \log 3^{l_i} \\
&amp;=  \Phi_0 - \Phi_m + \sum_{i = 1}^m p_i \cdot m \cdot (l_i \log 3 + 1)  \\
&amp;= m + \Phi_0 - \Phi_m + \sum_{i = 1}^m p_i \cdot m \cdot l_i \cdot \log 3 
\end{aligned}
\]</span> It is left to bound <span class="math inline">\(\Phi_0 - \Phi_m\)</span>. Let <span class="math inline">\(r^0(i)\)</span> be the rank of <span class="math inline">\(i^{th}\)</span> before and <span class="math inline">\(r^m(i)\)</span> be the rank after the operations. It holds that <span class="math inline">\(r^0(i) \le \log_2 1 = 0\)</span> and <span class="math inline">\(r^m(i) \ge \log_2 w(i)\)</span>. Now, <span class="math display">\[
\begin{aligned}
\Phi_0 - \Phi_m &amp;= \sum_{i = 1}^n \left( r^0(i) - r^m(i) \right) \\
&amp;\le \sum_{i  = 1}^n \log 3^{l_i} \\
&amp;=  \sum_{i =1}^n l_i \log 3 \\
\end{aligned}
\]</span> Under the assumption that each item is accessed at least once, we know <span class="math inline">\(p_i \cdot m \ge 1\)</span> and <span class="math inline">\(\Phi_0 - \Phi_m \in O(\sum_{i = 1}^m p_i \cdot m \cdot l_i \cdot \log 3 )\)</span>.</p>
<p><strong><em>Remark</em></strong>. Of course, we could bound <span class="math display">\[
\sum_{i =1}^n l_i \log 3 \le (1 + 2 \cdot 2 + 3 \cdot 2^2 + 4 \cdot 2^3 + ... + (\lfloor \log_2 n \rfloor + 1) \cdot 2^{ \lfloor \log_2 n \rfloor }) \log 3
\]</span> Let <span class="math inline">\(\Sigma=1 + 2 \cdot 2 + 3 \cdot 2^2 + 4 \cdot 2^3 + ... + (\lfloor \log_2 n \rfloor + 1) \cdot 2^{ \lfloor \log_2 n \rfloor }\)</span>, then <span class="math display">\[
2 \cdot \Sigma - \Sigma = (\lfloor \log_2 n \rfloor + 1) \cdot 2^{ \lfloor \log_2 n \rfloor + 1} - \sum_{i = 0}^{\lfloor \log_2 n \rfloor - 1} 2^i \in O(n \log n)
\]</span></p>
<p>Note that <span class="math inline">\(n \log n\)</span> is a lower bound of the access cost of the optimal static binary search tree, under the assumption that each item is accessed at least once.</p>
<p><strong><em>Remark</em></strong>:</p>
<p>In general, <span class="math display">\[
\sum_{i = 0}^k x^i = \frac{x^{k + 1} - 1}{x - 1}
\]</span> Applying <span class="math inline">\(x \cdot \frac{\partial}{\partial x}\)</span> to both sides, <span class="math display">\[
\begin{aligned}
\sum_{i = 1}^k i x^{i} 
&amp;= x \cdot \frac{(k + 1) x^k (x - 1) - (x^{k + 1} - 1)}{(x - 1)^2} \\
&amp;= x \cdot \frac{(k + 1) (x^{k +1} - x^k) - (x^{k + 1} - 1)}{(x - 1)^2} \\
&amp;= x \cdot \frac{ k x^{k +1} - (k + 1) x^k  + 1}{(x - 1)^2} \\
&amp;= x \cdot \left( \frac{k x^k }{x - 1} - \frac{ x^k  - 1}{(x - 1)^2} \right)\\
\end{aligned}
\]</span></p>
<h2 id="working-set-property">Working Set Property</h2>
<p>Suppose that <span class="math inline">\(S = \{x_1, x_2, ..., x_m\}\)</span> and let <span class="math inline">\(t(k)\)</span> be the number of different nodes accessed before <span class="math inline">\(x_k\)</span> since the last access of node <span class="math inline">\(x_k\)</span>, or since the beginning of the sequence if <span class="math inline">\(x_k\)</span> is the first access.</p>
<p><strong><em>Theorem</em>.</strong> The overall cost of <span class="math inline">\(S\)</span> on the splay tree is bounded by <span class="math display">\[
O(m + n \log n + \sum_{k = 1}^n \log t(k) )
\]</span> <strong><em>Proof.</em></strong> We maintain <span class="math inline">\(w(x) = \frac{1}{(j + 1)^2}\)</span>, if the sequence accesses <span class="math inline">\(j\)</span> different nodes since the last accessed of node <span class="math inline">\(x\)</span>, or <span class="math inline">\(w(x) = \frac{1}{n^2}\)</span> if it is never accessed before. By definition of <span class="math inline">\(t(k)\)</span>, we have <span class="math inline">\(w(x_k) = \frac{1}{(t(x_k) + 1)^2}\)</span>. Now <span class="math display">\[
W \doteq \sum_{x \in T}^n w(x) \in O(1)
\]</span> For a node <span class="math inline">\(x_k\)</span> , if it is accessed for the first time, the amortized cost is <span class="math display">\[
O(1 + \log \frac{W}{w(x_k)} ) = O(\log n) = O(\log t(k))
\]</span> Summing over all nodes <span class="math inline">\(x\)</span> in the splay tree <span class="math inline">\(T\)</span>, we get <span class="math inline">\(O(n \log n)\)</span>.</p>
<p>Otherwise, if an item <span class="math inline">\(x_k\)</span> is accessed before, the amortized cost of accessing <span class="math inline">\(x_k\)</span> is <span class="math display">\[
O\left( 1 + \log \frac{W}{w(x_k) } \right) = O \left( 1 + \log t(k) \right)
\]</span> After each access of a node <span class="math inline">\(x_k\)</span> and splaying it to the root, we modify the weights of the nodes as follows: for all node <span class="math inline">\(x\)</span> with <span class="math inline">\(t(x) &lt; t(x_k)\)</span> (the nodes accessed after the last access of <span class="math inline">\(x_k\)</span>), we set <span class="math display">\[
w(x) = \frac{1}{(t(x) + 1)^2}
\]</span> This guarantees that <span class="math inline">\(w(x_k) = {1} / {(1 + t(k))^2}\)</span> for <span class="math inline">\(k \in [m]\)</span>.</p>
<p>The weight of the root does not change but the weight of other nodes may decrease after the reassignment. Therefore, the overall potential decreases. The amortized cost of the this access only decreases and is still bounded by <span class="math inline">\(O(1 + \log t(k))\)</span>.</p>
<p>Hence <span class="math display">\[
\sum_{k = 1}^m a_k \in O(n \log n + m + \sum_{k = 1}^m \log t(k) )
\]</span> Finally, it is easy to see that <span class="math inline">\(\Phi_0 - \Phi_m \in O(n \log n)\)</span>. Therefore, <span class="math display">\[
\sum_{k = 1}^m c_k \in O(n \log n + m + \sum_{k = 1}^m \log t(k) )
\]</span> <span class="math inline">\(\blacksquare\)</span></p>
<h2 id="static-finger-property">Static Finger Property</h2>
<p>The static finger property states that splay tree achieves some kind of locality. Suppose we order the nodes in the tree in increasing order according to their keys and labels them as <span class="math inline">\(1, 2, ..., n\)</span>. For a node <span class="math inline">\(x\)</span>, we use <span class="math inline">\(\pi(x)\)</span> to denote its order.</p>
<p><strong><em>Theorem.</em></strong> For any fixed <span class="math inline">\(i \in [n]\)</span>, the total cost of <span class="math inline">\(S\)</span> on the splay tree is bounded by <span class="math display">\[
O(n \log n + m + \sum_{k = 1}^m \log (|\pi(x_k) - i| + 1) )
\]</span> <strong><em>Proof.</em></strong> For <span class="math inline">\(x \in T\)</span>, we set <span class="math inline">\(w(x) = \frac{1}{(|pi(x) - i| + 1)^2}\)</span>. Then <span class="math display">\[
W \doteq \sum_{x \in T} \in O(1)
\]</span> and the amortized cost of accessing <span class="math inline">\(x_k\)</span> is <span class="math display">\[
O(1 + \log \frac{W}{w(x_k)}) = O(1 + \log (|\pi(x_k) - i| + 1))
\]</span> The theorem then follows from <span class="math inline">\(\Phi_0 - \Phi_m = O(n \log n)\)</span>.</p>
<p><span class="math inline">\(\blacksquare\)</span>.</p>
<h2 id="reference">Reference</h2>
<p>[1]. Sleator, D.D. and Tarjan, R.E., 1983. A data structure for dynamic trees. Journal of computer and system sciences, 26(3), pp.362-391.</p>
<p>[2]. Jelani Nelson, Lecture 7, CS 224: Advanced Algorithms.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/45/">45</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">133</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WOW</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
