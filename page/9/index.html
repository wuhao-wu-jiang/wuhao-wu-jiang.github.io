<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Helvetica:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.24.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="WOW">
<meta property="og:url" content="http://example.com/page/9/index.html">
<meta property="og:site_name" content="WOW">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="WOW">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/9/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/9/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>WOW</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">WOW</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">你生之前悠悠千載已逝<br>未來還會有千年沉寂的期待</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">WOW</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">200</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/06/Report-Noisy-Max-and-Exponential-Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/06/Report-Noisy-Max-and-Exponential-Mechanism/" class="post-title-link" itemprop="url">Report Noisy Max and Exponential Mechanism</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-06 11:12:45" itemprop="dateCreated datePublished" datetime="2022-03-06T11:12:45-05:00">2022-03-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-17 22:29:00" itemprop="dateModified" datetime="2022-10-17T22:29:00-04:00">2022-10-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Let <span class="math inline">\(d \in \mathbb{N}^+\)</span> be a
positive integer, and <span class="math inline">\(\vec{u} \in
\mathbb{N}^d\)</span> be a vector.</p>
<blockquote>
<p><strong>Definition.</strong> Two vectors <span
class="math inline">\(\vec{u}, \vec{v} \in \mathbb{N}^d\)</span> are
neighboring, denoted as <span class="math inline">\(\vec{u} \sim
\vec{v}\)</span>, if <span class="math inline">\(\Vert \vec{u} - \vec{v}
\Vert_\infty = 1\)</span>.</p>
</blockquote>
<p>We are interested in the following research problem.</p>
<blockquote>
<p><strong>Problem.</strong> Design an <span
class="math inline">\(\epsilon\)</span>-differentially private algorithm
to report <span class="math display">\[
\underset{i \in [d]}{\arg\max} \, u_i.
\]</span></p>
</blockquote>
<h1 id="laplacian-mechanism">Laplacian Mechanism</h1>
<p>Since <span class="math inline">\(\Vert \vec{u} - \vec{v}
\Vert_\infty = 1\)</span>, it is possible that <span
class="math inline">\(\Vert \vec{u} - \vec{v} \Vert_1 = d\)</span>. A
naive approach would be adding independent Laplacian noise <span
class="math inline">\(X_i \sim \mathcal{Lap}( \epsilon / d)\)</span> to
each coordinate of <span class="math inline">\(\vec{u}\)</span>, and
then return <span class="math display">\[
    \mathcal{M}(\vec{u}) \doteq \underset{ i \in [d] }{\arg \max} \big(
u_i + X_i \big).
\]</span></p>
<blockquote>
<p><strong>Theorem (Privacy Guarantee).</strong> The algorithm is <span
class="math inline">\(\epsilon\)</span>-differentially private.</p>
</blockquote>
<p><strong>Proof.</strong></p>
<p>This follows directly from Laplacian mechanism.</p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Theorem (Utility Guarantee).</strong> With probability at
least <span class="math inline">\(1 - \beta\)</span>, it holds that
<span class="math display">\[
u_{ \mathcal{M}(\vec{u}) } \ge \left( \max_{i \in [d] } u_i \right) -
\frac{d}{\epsilon} \ln \frac{d}{\beta}.
\]</span> <strong>Proof.</strong> For each <span class="math inline">\(i
\in [d]\)</span>, <span class="math display">\[
\Pr \left[ |X_i| \ge \frac{d}{\epsilon} \ln \frac{d}{\beta} \right] \le
\frac{\beta}{d}.
\]</span> Via union bound, <span class="math display">\[
\Pr \left[ \exists i \in [d] : |X_i| \ge \frac{d}{\epsilon} \ln
\frac{d}{\beta} \right] \le \beta.
\]</span> <span class="math inline">\(\square\)</span></p>
</blockquote>
<h1 id="report-noisy-max">Report Noisy Max</h1>
<h2 id="laplacian-noise">Laplacian Noise</h2>
<p>It is possible to add Laplacian noise <span class="math inline">\(X_i
\sim \mathcal{Lap}(2 / \epsilon)\)</span> to each coordinate of <span
class="math inline">\(\vec{u}\)</span>, and then return <span
class="math display">\[
    \mathcal{M}(\vec{u}) \doteq \underset{ i \in [d] }{\arg \max} \big(
u_i + X_i \big).
\]</span></p>
<blockquote>
<p><strong>Theorem (Privacy Guarantee).</strong> The algorithm is <span
class="math inline">\(\epsilon\)</span>-differentially private.</p>
</blockquote>
<p><strong>Proof.</strong> We prove that for each <span
class="math inline">\(k \in [d]\)</span>, and each <span
class="math inline">\(\vec{u}, \vec{v} \in \mathbb{N}^d\)</span> s.t.,
<span class="math inline">\(\vec{v} \sim \vec{u}\)</span>, it holds that
<span class="math display">\[
    e^{-\epsilon} \cdot \Pr[ \mathcal{M}(\vec{v}) = k] \le
    \Pr[ \mathcal{M}(\vec{u}) = i ]
    \le
    e^\epsilon \cdot \Pr[ \mathcal{M}(\vec{v}) = k].
\]</span> By symmetry, we prove the claim for just <span
class="math inline">\(k = n\)</span>.</p>
<p>For each <span class="math inline">\(\vec{u} \in
\mathbb{N}^d\)</span> and each sequence <span
class="math inline">\(\vec{s} = (s_1, \ldots, s_{n - 1} ) \in
\mathbb{R}^{n - 1}\)</span>, define <span class="math display">\[
    z (\vec{u}, \vec s) = \max_{j \in [n - 1] } \big(  u_j + s_j \big).
\]</span></p>
<p>Let <span class="math inline">\(X_j \sim \mathcal{Lap}(\epsilon / 2),
j \in [n]\)</span>, and denote <span class="math inline">\(\vec X_{n -
1} \doteq (X_1, \ldots, X_{n - 1})\)</span>. The event <span
class="math inline">\(\mathcal{M}(\vec{u}) = n\)</span> happens if and
only if<br />
<span class="math display">\[
    z( \vec{u} , \vec X_{n - 1} ) &lt; u_n + X_n.
\]</span></p>
<p>Fix a sequence <span class="math inline">\(\vec s = (s_1, \ldots,
s_{n - 1} ) \in \mathbb{R}^{n - 1}\)</span>: <span
class="math display">\[
    \begin{aligned}
        \Pr\big[ \mathcal{M}(\vec{u}) = n \Vert \vec X_{n - 1} = \vec s
\big]
            &amp;= \Pr \big[ X_n &gt; z( \vec{u} , \vec{s} ) - u_n \big]
\\
            &amp;= \begin{cases}
                    \begin{aligned}
                        \frac{1}{2} \exp \left( -\frac{\epsilon}{2}
\cdot | z( \vec{u} , \vec{s} ) - u_n| \right),
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{u} , \vec{s} ) - u_n &gt;
0, \\
                    \begin{aligned}
                        1 - \frac{1}{2} \exp \left( -\frac{\epsilon}{2}
\cdot | z( \vec{u} , \vec{s} ) - u_n| \right),
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{u} , \vec{s} ) - u_n \le 0.
            \end{cases}
    \end{aligned}
\]</span></p>
<p>Similarly, <span class="math display">\[
    \begin{aligned}
        \Pr\big[ \mathcal{M}(\vec{v}) = n \Vert \vec X_{n - 1} = \vec s
\big]
            &amp;= \Pr \big[ X_n &gt; z( \vec{v} , \vec{s} ) - v_n \big]
\\
            &amp;= \begin{cases}
                    \begin{aligned}
                        \frac{1}{2} \exp \left( -\frac{\epsilon}{2}
\cdot | z( \vec{v} , \vec{s} ) - v_n | \right),
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{v} , \vec{s} ) - v_n &gt;
0, \\
                    \begin{aligned}
                        1 - \frac{1}{2} \exp \left( -\frac{\epsilon}{2}
\cdot | z( \vec{v} , \vec{s} ) - v_n| \right),
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{v} , \vec{s} ) - v_n \le 0.
            \end{cases}
    \end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(\Delta \doteq z( \vec{u} , \vec{s} )
- u_n - (z( \vec{v} , \vec{s} ) - v_n)\)</span>. The assumption <span
class="math inline">\(\Vert \vec{u} - \vec{v} \Vert_\infty = 1\)</span>
implies that <span class="math inline">\(|\Delta| \le 2\)</span>. WLOG,
assume that <span class="math inline">\(z( \vec{u} , \vec{s} ) - u_n \le
z( \vec{v} , \vec{s} ) - v_n\)</span>. Then <span
class="math display">\[
    \Pr \big[ X_n &gt; z( \vec{v} , \vec{s} ) - v_n \big]
        = \Pr \big[ X_n + \Delta &gt; z( \vec{u} , \vec{s} ) - u_n
\big].
\]</span></p>
<p>Define <span class="math inline">\(Y_n \doteq X_n + \Delta\)</span>,
which is also a Laplacian random variable with distribution <span
class="math inline">\(\mathcal{L}ap (\Delta, 2 / \epsilon)\)</span>. It
follows that <span class="math inline">\(Y_n\)</span> is <span
class="math inline">\((\epsilon, 0)\)</span>-close to <span
class="math inline">\(X_n\)</span>, and <span class="math display">\[
    \begin{aligned}
        \frac{
            \Pr\big[ \mathcal{M}(\vec{u}) = n \Vert \vec X_{n - 1} =
\vec s \big]
        }{
            \Pr \big[ \mathcal{M}(\vec{v}) = n \Vert \vec X_{n - 1} =
\vec s \big]
        }
            =
            \frac{
                \Pr \big[ X_n &gt; z( \vec{u} , \vec{s} ) - u_n \big]
            }{
                \Pr \big[ Y_n &gt; z( \vec{u} , \vec{s} ) - u_n \big]
            }
            \le e^\epsilon.
    \end{aligned}
\]</span></p>
<p><strong>Remark 1.</strong> We can also verifies this via the close
form expressions.</p>
<p>If <span class="math inline">\(0 \le z( \vec{u} , \vec{s} ) -
u_n\)</span>, clearly we have <span class="math display">\[
    \begin{aligned}
        \frac{\Pr\big[ \mathcal{M}(\vec{u}) = n \Vert \vec X_{n - 1} =
\vec s \big] }{ \Pr \big[ \mathcal{M}(\vec{v}) = n \Vert \vec X_{n - 1}
= \vec s \big] }
            \le \exp(\epsilon).
    \end{aligned}
\]</span></p>
<p>Consider <span class="math inline">\(z( \vec{v} , \vec{s} ) - v_n \le
0\)</span>. Let <span class="math inline">\(a \doteq |z( \vec{u} ,
\vec{s} ) - u_n|\)</span>, <span class="math inline">\(|b \doteq z(
\vec{v} , \vec{s} ) - v_n|\)</span>, and <span class="math inline">\(c =
1 / 2 \cdot \exp \left( - {\epsilon} / {2} \cdot b \right) \in [0, 1 /
2]\)</span>. Then <span class="math inline">\(a, b \ge 0\)</span>, <span
class="math inline">\(a - b \le 2\)</span>, and <span
class="math display">\[
    \begin{aligned}
        \frac{\Pr\big[ \mathcal{M}(\vec{u}) = n \Vert \vec X_{n - 1} =
\vec s \big] }{ \Pr \big[ \mathcal{M}(\vec{v}) = n \Vert \vec X_{n - 1}
= \vec s \big] }
            &amp;=        
            \frac{
                1 - \frac{1}{2} \exp \left( -\frac{\epsilon}{2} \cdot a
\right)
            }{
                1 - \frac{1}{2} \exp \left( -\frac{\epsilon}{2} \cdot b
\right)       
            } \\
            &amp;=       
            \frac{
                1 - c \cdot \exp \left( -\frac{\epsilon}{2} \cdot (a -
b) \right)
            }{
                1 - c       
            } \\
            &amp;\le       
            \frac{
                1 - c \cdot \exp \left( -\epsilon \right)
            }{
                1 - c       
            } \\                           
            &amp;=       
            \frac{1}{e^\epsilon}
            \frac{
                e^\epsilon - 1 + 1 - c
            }{
                1 - c       
            } \\
            &amp;=       
            \frac{1}{e^\epsilon}
            \left(
                \frac{
                    e^\epsilon - 1
                }{
                    1 - c       
                }  
                + 1
            \right) \\
            &amp;\le       
            \frac{1}{e^\epsilon}
            \left(
                \frac{
                    e^\epsilon - 1
                }{
                    1 - 1 / 2       
                }  
                + 1
            \right) \\                                 
            &amp;=       
            \frac{1}{e^\epsilon}
            \left(
                    2 e^\epsilon - 1
            \right) \\                   
            &amp;\le \exp(\epsilon).
    \end{aligned}
\]</span></p>
<p>Finally, consider <span class="math inline">\(z( \vec{u} , \vec{s} )
- u_n &lt; 0, z( \vec{v} , \vec{s} ) - v_n &gt; 0\)</span>. Let <span
class="math inline">\(a \doteq |z( \vec{u} , \vec{s} ) - u_n|\)</span>,
<span class="math inline">\(|b \doteq z( \vec{v} , \vec{s} ) -
v_n|\)</span>, and <span class="math inline">\(c = 1 / 2 \cdot \exp
\left( - {\epsilon} / {2} \cdot b \right) \in [0, 1 / 2]\)</span>. Then
<span class="math inline">\(a, b \ge 0\)</span>, <span
class="math inline">\(a + b \le 2\)</span>, and <span
class="math display">\[
    \begin{aligned}
        \frac{\Pr\big[ \mathcal{M}(\vec{u}) = n \Vert \vec X_{n - 1} =
\vec s \big] }{ \Pr \big[ \mathcal{M}(\vec{v}) = n \Vert \vec X_{n - 1}
= \vec s \big] }
            &amp;=        
            \frac{
                1 - \frac{1}{2} \exp \left( -\frac{\epsilon}{2} \cdot a
\right)
            }{
                \frac{1}{2} \exp \left( -\frac{\epsilon}{2} \cdot b
\right)       
            } \\
            &amp;=       
            \frac{
                1 - c \cdot \exp \left( -\frac{\epsilon}{2} \cdot (a -
b) \right)
            }{
                c       
            } \\
            &amp;\le       
            \frac{
                1 - c \cdot \exp \left( -\epsilon \right)
            }{
                c       
            } \\                           
            &amp;=       
            \frac{1}{c} -
            \frac{
                1
            }{
                e^\epsilon       
            } \\
            &amp;\le       
            2 - \exp(-\epsilon) \\
            &amp;\le \exp(\epsilon).
    \end{aligned}
\]</span></p>
<p>Integrating over <span class="math inline">\(\vec X_{n - 1} = \vec
s\)</span> for all possible <span class="math inline">\(\vec s\)</span>
finishes the proof.</p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Theorem (Utility Guarantee).</strong> With probability at
least <span class="math inline">\(1 - \beta\)</span>, it holds that
<span class="math display">\[
u_{ \mathcal{M}(\vec{u}) } \ge \left( \max_{i \in [d] } u_i \right) -
\frac{1}{\epsilon} \ln \frac{d}{\beta}.
\]</span> <strong>Proof.</strong> For each <span class="math inline">\(i
\in [d]\)</span>, <span class="math display">\[
\Pr \left[ |X_i| \ge \frac{1}{\epsilon} \ln \frac{d}{\beta} \right] \le
\frac{\beta}{d}.
\]</span> Via union bound, <span class="math display">\[
\Pr \left[ \exists i \in [d] : |X_i| \ge \frac{1}{\epsilon} \ln
\frac{d}{\beta} \right] \le \beta.
\]</span> <span class="math inline">\(\square\)</span></p>
</blockquote>
<h2 id="exponential-noise">Exponential Noise</h2>
<p>It is possible to add Exponential noise <span
class="math inline">\(X_i \sim \mathcal{Exp}(2 / \epsilon)\)</span> to
each coordinate of <span class="math inline">\(\vec{u}\)</span>, and
then return <span class="math display">\[
    \mathcal{M}(\vec{u}) \doteq \underset{ i \in [d] }{\arg \max} \big(
u_i + X_i \big).
\]</span></p>
<p>Following the same vein, we have <span class="math display">\[
    \begin{aligned}
        \Pr\big[ \mathcal{M}(\vec{u}) = n \Vert \vec X_{n - 1} = \vec s
\big]
            &amp;= \Pr \big[ X_n &gt; z( \vec{u} , \vec{s} ) - u_n \big]
\\
            &amp;= \begin{cases}
                    \begin{aligned}
                        \exp \left( -\frac{\epsilon}{2} \cdot | z(
\vec{u} , \vec{s} ) - u_n| \right),
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{u} , \vec{s} ) - u_n &gt;
0, \\
                    \begin{aligned}
                        1 ,
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{u} , \vec{s} ) - u_n \le 0.
            \end{cases}
    \end{aligned}
\]</span></p>
<p>And <span class="math display">\[
    \begin{aligned}
        \Pr\big[ \mathcal{M}(\vec{v}) = n \Vert \vec X_{n - 1} = \vec s
\big]
            &amp;= \Pr \big[ X_n &gt; z( \vec{v} , \vec{s} ) - v_n \big]
\\
            &amp;= \begin{cases}
                    \begin{aligned}
                        \exp \left( -\frac{\epsilon}{2} \cdot | z(
\vec{v} , \vec{s} ) - v_n | \right),
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{v} , \vec{s} ) - v_n &gt;
0, \\
                    \begin{aligned}
                        1 ,
                    \end{aligned}    
                    &amp;\text{ if } z( \vec{v} , \vec{s} ) - v_n \le 0.
            \end{cases}
    \end{aligned}
\]</span></p>
<p>It is straightforward to check that <span class="math display">\[
    \begin{aligned}
        \frac{\Pr\big[ \mathcal{M}(\vec{u}) = n \Vert \vec X_{n - 1} =
\vec s \big] }{ \Pr \big[ \mathcal{M}(\vec{v}) = n \Vert \vec X_{n - 1}
= \vec s \big] }
            \le \exp(\epsilon).
    \end{aligned}
\]</span></p>
<h1 id="exponential-mechanism">Exponential Mechanism</h1>
<p>The exponential mechanism is defined by its output
distribution:<br />
<span class="math display">\[
    \Pr[\mathcal{M}(\vec{u}) = i] \propto \exp \left( \epsilon
\cdot   u_i  / 2 \right), \quad \forall i \in [d].
\]</span></p>
<p>Normalizing these probabilities give <span class="math display">\[
    \Pr[\mathcal{M}(\vec{u}) = i]
        = \frac{
            \exp \left( \epsilon \cdot u_i / 2 \right)
        }{
            \sum_{j \in [d] } \exp \left( \epsilon \cdot u_j / 2 \right)
        },
        \quad \forall i \in [d].
\]</span></p>
<blockquote>
<p><strong>Theorem (Privacy Guarantee).</strong> The algorithm is <span
class="math inline">\(\epsilon\)</span>-differentially private.</p>
</blockquote>
<p><strong>Proof.</strong> Fix a <span class="math inline">\(\vec{u} \in
\mathbb{N}^d\)</span>. We prove that for each <span
class="math inline">\(i \in [d]\)</span>, and each <span
class="math inline">\(\vec{v} \sim \vec{u}\)</span>, it holds that <span
class="math display">\[
    e^{-\epsilon} \cdot \Pr[ \mathcal{M}(\vec{v}) = i] \le \Pr[
\mathcal{M}(\vec{u}) = i ] \le e^\epsilon \cdot \Pr[
\mathcal{M}(\vec{v}) = i].
\]</span> By symmetry, we prove the claim for just <span
class="math inline">\(i = n\)</span>.</p>
<p><span class="math display">\[
    \begin{aligned}
        \frac{\Pr[ \mathcal{M}(\vec{u}) = n] }{\Pr[ \mathcal{M}(\vec{v})
= n] }
        &amp;= \frac{
            \exp \left( \epsilon \cdot u_n / 2 \right)
        }{
            \exp \left( \epsilon \cdot v_n / 2 \right)
        }
        \cdot \frac{
            \sum_{j \in [d] } \exp \left( \epsilon \cdot  v_j / 2
\right)
        }{
            \sum_{j \in [d] } \exp \left( \epsilon \cdot u_j / 2 \right)
        } \\
        &amp;\le
        \exp( \epsilon / 2 )
        \cdot \exp( \epsilon / 2 ) \\
        &amp;= \exp( \epsilon ).
    \end{aligned}
\]</span></p>
<p>The inequality follows since <span class="math display">\[
    \begin{aligned}
        u_i - v_i \le \Vert \vec{u} - \vec{v} \Vert_\infty \le 1,
        \implies \exp( \epsilon (u_i - v_i) / 2) \le \exp( \epsilon  /
2), \, \forall i \in [d].
    \end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Theorem (Utility Guarantee).</strong> With probability at
least <span class="math inline">\(1 - \beta\)</span>, it holds that
<span class="math display">\[
u_{ \mathcal{M}(\vec{u}) } \ge \left( \max_{i \in [d] } u_i \right) -
\frac{2}{\epsilon} \cdot \ln \frac{|S|}{|S_{OPT}| \cdot \beta}.
\]</span> <strong>Proof.</strong> For each <span class="math inline">\(t
\in \mathbb{R}\)</span>, the un-normalized probability mass assigned to
the set <span class="math inline">\(S \doteq \{ i \in [d] : u_i \le t
\}\)</span> is at most <span class="math inline">\(|S| \cdot \exp \left(
\epsilon \cdot t / 2\right)\)</span>.</p>
</blockquote>
<p>On the other hand, the un-normalized probability mass assigned to the
set <span class="math inline">\(S_{OPT} \subseteq [d]\)</span>
consisting of optimal solutions is <span class="math inline">\(|S_{OPT}|
\cdot \exp \left( \epsilon \cdot \left( \max_{i \in [d] } u_i \right) /
2\right)\)</span>.</p>
<p><span class="math display">\[
    \begin{aligned}
        \Pr \left[ u_{ \mathcal{M}(\vec{u}) } \le t \right]
        &amp;\le
        \frac{
            |S| \cdot \exp \left( \epsilon \cdot t / 2\right)
        }
        {
            |S_{OPT}| \cdot \exp \left( \epsilon \cdot \left( \max_{i
\in [d] } u_i \right) / 2 \right)
        } \\
        &amp;\le \frac{|S|}{|S_{OPT}|} \exp \left( \epsilon \left( t -
\max_{i \in [d] } u_i \right) / 2 \right).
    \end{aligned}
\]</span></p>
<p>To bound the last term by <span class="math inline">\(\beta\)</span>,
we can set <span class="math display">\[
    t = \left( \max_{i \in [d] } u_i \right) - \frac{2}{\epsilon} \cdot
\ln \frac{|S|}{|S_{OPT}| \cdot \beta}.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h2 id="gumbel-noise">Gumbel Noise</h2>
<p>A random variable <span class="math inline">\(X\)</span> follows the
Gumbel distribution <span class="math inline">\(\mathcal{Gumbel}(\mu,
b)\)</span> if its density is given by <span class="math display">\[
    p(x)
        \doteq \frac{1}{b} \cdot \exp \left( - (x - \mu) / b - e^{- (x -
\mu) / b}  \right)
        = \frac{1}{b} \cdot e^{ - (x - \mu) / b } \cdot \exp \left(-
e^{- (x - \mu) / b} \right), \forall x \in \mathbb{R}.
\]</span> It follows that the cumulative distribution of <span
class="math inline">\(X\)</span> is <span class="math display">\[
    \begin{aligned}
        \Pr[ X \le t ]
            &amp;= \int_{-\infty}^t \frac{1}{b} \cdot e^{ - (x - \mu) /
b } \cdot \exp \left(- e^{- (x - \mu) / b} \right) \, d x \\
            &amp;= - \int_{-\infty}^t \exp \left(- e^{- (x - \mu) / b}
\right) \, d e^{ -  (x - \mu) / b } \\
            &amp;= \int^{\infty}_{\exp(- (t - \mu) / b)} \exp \left(- y
\right) \, d y \\
            &amp;= \exp \left(- e^{ - (t - \mu) / b } \right).
    \end{aligned}
\]</span></p>
<blockquote>
<p><strong>Theorem.</strong> If we add Gumbel noise <span
class="math inline">\(X_i \sim \mathcal{Gumbel}(0, 2 /
\epsilon)\)</span> to each coordinate of <span
class="math inline">\(\vec{u}\)</span>, and then return <span
class="math display">\[
\mathcal{M}(\vec{u}) \doteq \underset{ i \in [d] }{\arg \max} \big( u_i
+ X_i \big),  
\]</span> the output has the same distribution as the one of exponential
mechanism.</p>
</blockquote>
<p><strong>Proof.</strong> Let <span class="math inline">\(b = 2 /
\epsilon\)</span>, and <span class="math inline">\(Z_i \doteq u_i +
X_i\)</span>. Then <span class="math inline">\(Z_i \sim
\mathcal{Gumbel}(u_i, b)\)</span>, and <span class="math display">\[
    \begin{aligned}
        \Pr[ \mathcal{M}(\vec{u}) = n]
            &amp;= \int_{-\infty}^\infty \left( p(z_n) \cdot \prod_{i
\in [n - 1]} \Pr[ Z_i \le z_n ] \right) \, d z_n \\
            &amp;= \int_{-\infty}^\infty \left(
                    \frac{1}{b} \cdot e^{ - (z_n - u_n) / b } \cdot \exp
\left(- e^{- (z_n - u_n) / b} \right)
                    \cdot
                    \prod_{i \in [n - 1]} \exp \left(- e^{- (z_n - u_i)
/ b } \right)
                \right) \, d z_n  \\
            &amp;= e^{ u_n / b } \cdot \int_{-\infty}^\infty \left(
                    \frac{1}{b} \cdot e^{ - z_n / b } \cdot \exp \left(-
e^{- z_n / b} \cdot \sum_{i \in [n ]}  e^{ u_i / b } \right)
                \right) \, d z_n  \\
            &amp;= - e^{ u_n / b } \cdot \int_{\infty}^0
                    \exp \left( - y \cdot \sum_{i \in [n ]}  e^{ u_i / b
} \right)
                \, d y
            &amp; (y \doteq e^{-z_n/b} ) \\
            &amp;= \frac{ \exp \left( u_n / b \right) }{
                \sum_{i \in [n]} \exp \left( u_i / b \right)
            }.
    \end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="reference">Reference</h1>
<p>[1] <em>C. Dwork and A. Roth, “The Algorithmic Foundations of
Differential Privacy,”</em></p>
<p>[2] <a
target="_blank" rel="noopener" href="https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/"><em>The
Gumbel-Max Trick for Discrete Distributions</em></a></p>
<p>[3] <em>D. Durfee and R. M. Rogers, “Practical Differentially Private
Top-k Selection with Pay-what-you-get Composition,” in Advances in
Neural Information Processing Systems, 2019, vol. 32. Accessed: Sep. 13,
2022.</em></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/11/Approximate-Densest-Subgraph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/11/Approximate-Densest-Subgraph/" class="post-title-link" itemprop="url">Approximate Densest Subgraph</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-02-11 22:49:24" itemprop="dateCreated datePublished" datetime="2022-02-11T22:49:24-05:00">2022-02-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-02-12 01:32:34" itemprop="dateModified" datetime="2022-02-12T01:32:34-05:00">2022-02-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Let <span class="math inline">\(G = (V, E)\)</span> be an undirected
graph with <span class="math inline">\(|V| = n\)</span> vertices and
<span class="math inline">\(|E| = m\)</span> edges.<br />
For each <span class="math inline">\(S \subset V\)</span>, denote <span
class="math inline">\(E(S)\)</span> the set of edges in the subgraph
induced by <span class="math inline">\(S\)</span>, i.e., <span
class="math display">\[
    E(S) \doteq \{ (u, v) \in E : u, v \in S \}.
\]</span> The density of the subgraph induced by <span
class="math inline">\(S\)</span> is <span class="math display">\[
    d_G(S) \doteq \frac{|E(S)|}{|S|}.
\]</span></p>
<blockquote>
<p><strong>Problem.</strong> Find <span class="math inline">\(S^*
\subset V\)</span>, s.t. <span class="math inline">\(d_G(S^*)\)</span>
is maximized.</p>
</blockquote>
<h1 id="greedy-algorithm">Greedy Algorithm</h1>
<blockquote>
<p><strong>Definition.</strong> Given <span class="math inline">\(S
\subset V\)</span>, for each <span class="math inline">\(u \in
S\)</span>, define the set of edges incident to <span
class="math inline">\(u\)</span> in <span
class="math inline">\(S\)</span> as <span class="math display">\[
E(u, S) \doteq \{ (u, v) \in E(S) \}.
\]</span></p>
<p><strong>Definition.</strong> A function <span
class="math inline">\(w: E \rightarrow V\)</span> is called an edge
assignment, if it maps each edge <span class="math inline">\((u, v) \in
E\)</span> to one of its endpoints <span class="math inline">\(\{u, v
\}\)</span>.</p>
</blockquote>
<p>There is a linear time, 2-approximation greedy algorithm [1] which
works as follows.</p>
<blockquote>
<p><strong>Greedy Algorithm.</strong> 1. <span class="math inline">\(S_1
\leftarrow V\)</span>;<br />
2. for <span class="math inline">\(i \in [n]\)</span> do<br />
3. <span class="math inline">\(\qquad\)</span> <span
class="math inline">\(u_i \leftarrow \arg\min_{u \in S_i} |E(u,
S_i)|\)</span>;<br />
4. <span class="math inline">\(\qquad\)</span> <span
class="math inline">\(S_{i + 1} \leftarrow S_i \setminus \{ u_i
\}\)</span>;<br />
5. <span class="math inline">\(\qquad\)</span> for <span
class="math inline">\(e = (u_i, v) \in E(u_i, S_i)\)</span> do<br />
6. <span class="math inline">\(\qquad\)</span> <span
class="math inline">\(\qquad\)</span> <span class="math inline">\(w(e)
\leftarrow u_i\)</span>;<br />
7. <span class="math inline">\(k \leftarrow \arg\max_{i \in n]} |E(u_i,
S_i)|\)</span>;<br />
8. return <span class="math inline">\(S_k\)</span>;</p>
</blockquote>
<h1 id="approximation-guarantee">Approximation Guarantee</h1>
<blockquote>
<p><strong>Definition.</strong> Given an edge assignment <span
class="math inline">\(w\)</span>, for each <span class="math inline">\(u
\in V\)</span>, denote the set of edges assigned to <span
class="math inline">\(u\)</span> as <span class="math display">\[
A_w(u) \doteq \{ e = (u, v) \in E : w(e) = u \}.
\]</span></p>
</blockquote>
<p>The approximation guarantee relies on the following lemma.</p>
<blockquote>
<p><strong>Lemma.</strong> For edge assignment <span
class="math inline">\(w\)</span> and each <span class="math inline">\(S
\subset V\)</span>, it holds that <span class="math display">\[
\max_{u \in V} |A_w(u)| \ge d_G(S).
\]</span></p>
</blockquote>
<p><strong>Proof.</strong> Observe that all edges in <span
class="math inline">\(E(S)\)</span> are assigned to vertices in <span
class="math inline">\(S\)</span>. It follows that <span
class="math display">\[
    \max_{u \in V} |A_w(u)|
        \ge \max_{u \in S} |A_w(u)|
        \ge \frac{|E(S)|}{|S|} = d_G(S).
\]</span> <span class="math inline">\(\square\)</span></p>
<blockquote>
<p><strong>Theorem.</strong> The solution <span
class="math inline">\(S_k\)</span> satisfies <span
class="math display">\[
d_G(S_k) \ge \frac{1}{2} \cdot d_G (S^*).
\]</span></p>
</blockquote>
<p><strong>Proof.</strong><br />
Let <span class="math inline">\(w\)</span> be the assignment constructed
by the greedy algorithm, and denote <span class="math display">\[
    d_{\max} \doteq \max_{u \in V} |A_w(u)| = \max_{i \in [n]} |E(u_i,
S_i)| = |E(u_k, S_k)|.
\]</span> Via the previous lemma, it holds that <span
class="math display">\[
    d_G(S^*) \le d_{\max} = |E(u_k, S_k)|.
\]</span></p>
<p>Further, as <span class="math inline">\(u_k\)</span> is the vertices
with minimum degree in the subgraph induced by <span
class="math inline">\(S_k\)</span>, <span class="math inline">\(|E(u_k,
S_k)|\)</span> should be upper bounded by the average degree in <span
class="math inline">\(S_k\)</span>: <span class="math display">\[
    d_G(S^*) \le |E(u_k, S_k)| \le 2 \cdot  \frac{|E(S_k)|}{ |S_k| } = 2
\cdot d_G(S_k).
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="running-time">Running Time</h1>
<blockquote>
<p><strong>Theorem.</strong> The algorithm can be implemented in <span
class="math inline">\(O(m + n)\)</span> time.</p>
</blockquote>
<p><strong>Proof.</strong></p>
<p>We can maintain the degrees of the vertices in <span
class="math inline">\(S_i, i \in [n]\)</span> dynamically.</p>
<ol type="1">
<li><p>At the beginning of the algorithm, set <span
class="math inline">\(d(u) = |E(u, V)| = |E(u, S_i)|\)</span> for each
<span class="math inline">\(u \in V\)</span>. We maintain the invariant
that <span class="math inline">\(d(u) = |E(u, S_i)|\)</span> at the
beginning of each iteration <span class="math inline">\(i \in
[n]\)</span>.</p></li>
<li><p>At the <span class="math inline">\(i^{(th)}\)</span> iteration,
when <span class="math inline">\(u_i\)</span> is removed from <span
class="math inline">\(S_i\)</span> to construct <span
class="math inline">\(S_{i + 1}\)</span>, we decrease <span
class="math inline">\(d(v)\)</span> by <span
class="math inline">\(1\)</span>, if <span class="math inline">\((u_i,
v) \in E(S_i)\)</span>.</p></li>
</ol>
<p>It is easy to see that the invariant is maintained, and the total
update time is <span class="math inline">\(O(m)\)</span>.</p>
<p>Next, we design a data structure that supports the extraction of
<span class="math inline">\(u_i\)</span>, the vertex with minimum degree
in <span class="math inline">\(S_i\)</span>, in <span
class="math inline">\(O(1)\)</span> time. We keep an array with size
<span class="math inline">\(n\)</span>.</p>
<ul>
<li><p>Each vertex <span class="math inline">\(u\)</span> is kept in the
<span class="math inline">\(d(u)\)</span>-th bucket of the array. When
<span class="math inline">\(d(u)\)</span> is updated, we update the
bucket of <span class="math inline">\(u\)</span> correspondingly. The
total update cost is <span class="math inline">\(O(m)\)</span>.</p></li>
<li><p>At the beginning of the algorithm, we search bucket <span
class="math inline">\(0, 1, \ldots\)</span> , until we find the first
non-empty bucket <span class="math inline">\(j \in [n]\)</span>. Then we
extract a vertex <span class="math inline">\(u\)</span> from the bucket
<span class="math inline">\(j\)</span>, and updates the <span
class="math inline">\(d(v)\)</span> for which <span
class="math inline">\(v\)</span> is neighbor of <span
class="math inline">\(u\)</span> in the induced subgraph. The update
decreases <span class="math inline">\(d(v)\)</span> by <span
class="math inline">\(1\)</span>. Therefore, the bucket <span
class="math inline">\(\max \{ 0, j - 1 \}\)</span> can become non-empty
at the next iteration, from which we should try to find the next vertex
with minimum degree. There are <span class="math inline">\(n\)</span>
iteration, and we can move backward at the array by at most <span
class="math inline">\(n\)</span> steps. It follows that the total
searching cost in the array is <span
class="math inline">\(O(n)\)</span>.</p></li>
</ul>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="reference">Reference</h1>
<p>[1] <em>M. Charikar, “Greedy Approximation Algorithms for Finding
Dense Components in a Graph,” 2000</em>.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/08/Sparse-Vector/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="WOW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WOW">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | WOW">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/08/Sparse-Vector/" class="post-title-link" itemprop="url">Sparse Vector</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-02-08 17:59:19" itemprop="dateCreated datePublished" datetime="2022-02-08T17:59:19-05:00">2022-02-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-11 16:42:55" itemprop="dateModified" datetime="2024-04-11T16:42:55-04:00">2024-04-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Let <span class="math inline">\((\mathcal{X}, \Vert \cdot
\Vert)\)</span> be metric space.</p>
<blockquote>
<p><strong>Definition.</strong> Two elements <span
class="math inline">\(D, D&#39; \in \mathcal{X}\)</span> are
neighboring, denoted as <span class="math inline">\(D \sim
D&#39;\)</span>, if <span class="math inline">\(\Vert D&#39;, D&#39;
\Vert = 1\)</span>.<br />
<strong>Definition.</strong> A function <span class="math inline">\(q:
\mathcal{X} \rightarrow \mathbb{R}\)</span> has sensitivity <span
class="math inline">\(\Delta \in \mathbb{R}^+\)</span>, if <span
class="math display">\[
\Delta = \sup_{ D, D&#39; \in \mathcal{X}, D \sim D&#39; } | q(D) -
q(D&#39;) |.
\]</span></p>
</blockquote>
<h1 id="problem">Problem</h1>
<blockquote>
<p>Let <span class="math inline">\(q_i: \mathcal{X} \rightarrow
\mathbb{R}, i \in \mathbb{N}^+\)</span> be a sequence of functions with
sensitivity <span class="math inline">\(\Delta\)</span>. Let <span
class="math inline">\(t \in \mathbb{R}\)</span> be a threshold. Design
an <span class="math inline">\(\epsilon\)</span>-differentially private
algorithm, that reports the first <span class="math inline">\(i \in
\mathbb{N}^+\)</span> for which <span
class="math inline">\(q_i(D)\)</span> is near or above <span
class="math inline">\(t\)</span>.</p>
</blockquote>
<h1 id="mechanism-one">Mechanism One</h1>
<blockquote>
<p><strong>Definition.</strong> For each <span class="math inline">\(b
\in \mathbb{R}^+\)</span>, let <span
class="math inline">\(\mathcal{Lap}(b)\)</span> be the Laplacian
distribution with parameter <span class="math inline">\(b\)</span>.</p>
</blockquote>
<p>The algorithm is described as follows.</p>
<blockquote>
<p><strong>Algorithm <span
class="math inline">\(\mathcal{A}\)</span>.</strong><br />
1. <span class="math inline">\(\hat t \leftarrow t + Z\)</span> where
<span class="math inline">\(Z \sim \mathcal{Lap}(3 \Delta /
\epsilon)\)</span><br />
2. <strong>for</strong> <span class="math inline">\(i \in
\mathbb{N}^+\)</span> <strong>do</strong><br />
3. <span class="math inline">\(\qquad\)</span> <span
class="math inline">\(\hat q_i(D) \leftarrow q_i(D) + X_i\)</span>,
where <span class="math inline">\(X_i \sim \mathcal{Lap}( 3 \Delta /
\epsilon)\)</span><br />
4. <span class="math inline">\(\qquad\)</span> <strong>if</strong> <span
class="math inline">\(\hat q_i(D) \ge \hat t\)</span>
<strong>then</strong><br />
5. <span class="math inline">\(\qquad\qquad\)</span>
<strong>output</strong> <span class="math inline">\(\bot\)</span><br />
6. <span class="math inline">\(\qquad\qquad\)</span>
<strong>halt</strong><br />
7. <span class="math inline">\(\qquad\)</span>
<strong>else</strong><br />
8. <span class="math inline">\(\qquad\qquad\)</span>
<strong>output</strong> <span class="math inline">\(\top\)</span></p>
</blockquote>
<p>Note that it is necessary to add noise to <span
class="math inline">\(t\)</span>, to obtain <span
class="math inline">\(\hat{t}\)</span>.</p>
<p>Otherwise, assume that <span class="math inline">\(\mathcal{A} (D) =
\mathcal{A} (D&#39;) = \underbrace{\top \ldots \top}_{ n \text{
occurrences} } \bot\)</span> for some <span class="math inline">\(n \in
\mathbb{N}\)</span> and neighboring datasets <span
class="math inline">\(D\)</span> and <span
class="math inline">\(D&#39;\)</span>.</p>
<p>Assume further that <span class="math inline">\(t = 0\)</span>, <span
class="math inline">\(q_i(D) = \Delta\)</span> and <span
class="math inline">\(q_i(D&#39;) = 0\)</span> for all <span
class="math inline">\(i \in \mathbb{N}\)</span>. Then <span
class="math display">\[
    \begin{align*}
         \Pr[ \mathcal{A} (D) = \underbrace{\top \ldots \top}_{ n \text{
occurrences} } \bot ]
            &amp;= \left( \frac{1}{2} \right)^{n + 1} \exp \left( -
\frac{\epsilon \cdot n }{3} \right) \left( 1 - \exp \left( -
\frac{\epsilon}{3} \right)\right). \\
         \Pr[ \mathcal{A} (D&#39;) = \underbrace{\top \ldots \top}_{ n
\text{ occurrences} } \bot ]
            &amp;= \left( \frac{1}{2} \right)^{n + 1}.
    \end{align*}
\]</span></p>
<h2 id="privacy-guarantee">Privacy Guarantee</h2>
<p>Let <span class="math inline">\(D&#39;\)</span> be a neighboring
dataset of <span class="math inline">\(D\)</span>.</p>
<blockquote>
<p><strong>Theorem.</strong> For each <span class="math inline">\(i \in
\mathbb{N}^+\)</span>, let <span class="math inline">\(s_i \doteq
\underbrace{\top \ldots \top}_{ i - 1 \text{ occurrences} }
\bot\)</span>. Then <span class="math display">\[
e^{-\epsilon}
\le
\frac{\Pr[ \mathcal{A}(D) = s_i ]}{\Pr[ \mathcal{A}(D&#39;) = s_i ]}
\le
e^{\epsilon}.
\]</span></p>
</blockquote>
<p><strong>Proof 1.</strong> <!-- 
$$
    \begin{aligned}
        \Pr[ \mathcal{A}(D) = s_i ] 
            &= \Pr[ \wedge_{\ell = 1}^{i - 1} \,\, q_\ell (D) + X_\ell < t + Z \, \wedge \, q_i (D) + X_i \ge t + Z ] \\
            &= \E_{z \sim \mathcal{Lap}(3 \Delta / \epsilon)} \left[ \Pr \left[ \wedge_{\ell = 1}^{i - 1} \,\, q_\ell (D) + X_\ell < t + z \, \wedge \, q_i (D) + X_i \ge t + z \right] \right] \\
            &= \E_{z \sim \mathcal{Lap}(3 \Delta / \epsilon)} \left[ \prod_{\ell = 1}^{i - 1} \Pr \left[ q_\ell (D) + X_\ell < t + z \right] \cdot \Pr \left[ q_i (D) + X_i \ge t + z \right] \right] \\
    \end{aligned}
$$ 
--></p>
<p>Fix an <span class="math inline">\(i \in \mathbb{N}^+\)</span>. For
each dataset <span class="math inline">\(D\)</span> and each <span
class="math inline">\(\vec x_{i - 1} = (x_1, \ldots, x_{i - 1} ) \in
\mathbb{R}^{i - 1}\)</span>, define <span class="math display">\[
    M(D, \vec x_{i - 1}) = \max_{j \in [i - 1]} \Big( q_j(D) + x_j
\Big).
\]</span></p>
<p>Denote <span class="math inline">\(\vec X_{i - 1} \doteq (X_1,
\ldots, X_{i - 1})\)</span>, and the event <span
class="math inline">\(\mathcal{A}(D) = s_i\)</span> happens if and only
if<br />
<span class="math display">\[
    M(D, \vec X_{i - 1}) &lt; \hat t \le \hat q_i(D).
\]</span></p>
<p>Fix a sequence <span class="math inline">\(\vec x_{i - 1} = (x_1,
\ldots, x_{i - 1} ) \in \mathbb{R}^{i - 1}\)</span>. Let <span
class="math inline">\(\mu\)</span> be a shorthand of <span
class="math inline">\(\mathcal{Lap}(3 \Delta / \epsilon)\)</span>, and
<span class="math inline">\(F\)</span> be its cumulative function. Since
the random variables <span class="math inline">\(Z, \vec X_{i - 1},
X_i\)</span> are independent, via Fubini's theorem, <span
class="math display">\[
    \begin{aligned}
        \Pr \big[ \mathcal{A}(D) = s_i \Vert \vec X_{i - 1} = \vec x_{i
- 1} \big]
            &amp;= \Pr \big[ M(D, \vec x_{i - 1}) &lt; \hat t \le \hat
q_i(D) \big] \\
            &amp;= \Pr \big[ M(D, \vec x_{i - 1}) &lt; t + Z \le q_i(D)
+ X_i \big] \\
            &amp;= \mathbb{E}_{z \sim \mu} \left[ \mathbb{I} \Big[ M(D,
\vec x_{i - 1}) &lt; t + z \Big] \cdot \big( 1 - F ( t + z - q_i(D) )
\big) \right] \\
            &amp;= \mathbb{E}_{z \sim \mu} \left[ \mathbb{I} \Big[ M(D,
\vec x_{i - 1}) &lt; t + z \Big] \cdot F \big(  - t - z + q_i(D) \big)
\right] \\
            &amp;= \int_{M(D, \vec x_{i - 1}) - t}^\infty p(z) \cdot F
\big(  - t - z + q_i(D) \big) \, d z \\
            &amp;= \int_{M(D&#39;, \vec x_{i - 1}) - t}^\infty p\big( z
+ \kappa \big) \cdot F \big(  - t - z - \kappa + q_i(D) \big) \, d z,
    \end{aligned}
\]</span> where <span class="math inline">\(\kappa \doteq M(D, \vec x_{i
- 1}) - M(D&#39;, \vec x_{i - 1})\)</span>.</p>
<!-- 
Since $M(D, \vec x_{i - 1}) \ge M(D', \vec x_{i - 1}) - \Delta$, and $q_i(D) \le q_i(D') + \Delta$, we have 
$$
    \begin{aligned}
        \Pr \big[ \mathcal{A}(D) = s_i \Vert \vec X_{i - 1} = \vec x_{i - 1} \big] 
            &= \mathbb{E}_{z \sim \mu} \left[ \mathbb{I} \Big[ M(D, \vec x_{i - 1}) < t + z \Big] \cdot F \big(  - t - z + q_i(D) \big) \right] \\
            &\le \mathbb{E}_{z \sim \mu} \left[ \mathbb{I} \Big[ M(D', \vec x_{i - 1}) - \Delta < t + z \Big] \cdot F \big(  - t - z + q_i(D') + \Delta \big) \right] \\
    \end{aligned}
$$ 
-->
<p>Similarly, the conditional probability density of <span
class="math inline">\(\Pr\big[ \mathcal{A}(D&#39;) = s_i \big]\)</span>
is given by <span class="math display">\[
    \begin{aligned}
        \Pr \big[ \mathcal{A}(D&#39;) = s_i \Vert \vec X_{i - 1} = \vec
x_{i - 1} \big]
            &amp;= \int_{M(D&#39;, \vec x_{i - 1}) - t}^\infty p(z)
\cdot F \big(  - t - z + q_i(D&#39;) \big) \, d z.
    \end{aligned}
\]</span></p>
<p>Comparing <span class="math inline">\(p(z)\)</span> and <span
class="math inline">\(p(z + \kappa)\)</span>, <span
class="math inline">\(F \big(  - t - z - \kappa + q_i(D) \big)\)</span>
and <span class="math inline">\(F \big(  - t - z + q_i(D&#39;)
\big)\)</span> finishes the proof.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Proof 2.</strong></p>
<p>Fix an <span class="math inline">\(i \in \mathbb{N}^+\)</span>. For
each dataset <span class="math inline">\(D\)</span> and each <span
class="math inline">\(\vec x_{i - 1} = (x_1, \ldots, x_{i - 1} ) \in
\mathbb{R}^{i - 1}\)</span>, define <span class="math display">\[
    M(D, \vec x_{i - 1}) = \max_{j \in [i - 1]} \Big( q_j(D) + x_j
\Big).
\]</span></p>
<p>Denote <span class="math inline">\(\vec X_{i - 1} \doteq (X_1,
\ldots, X_{i - 1})\)</span>, and the event <span
class="math inline">\(\mathcal{A}(D) = s_i\)</span> happens if and only
if<br />
<span class="math display">\[
    M(D, \vec X_{i - 1}) &lt; \hat t \le \hat q_i(D).
\]</span></p>
<p>Fix a sequence <span class="math inline">\(\vec x_{i - 1} = (x_1,
\ldots, x_{i - 1} ) \in \mathbb{R}^{i - 1}\)</span>. Since the random
variables <span class="math inline">\(Z, \vec X_{i - 1}, X_i\)</span>
are independent, <!-- 
Conditioned on $\vec X_{i - 1} = \vec x_{i - 1}$, the probability density of $\Pr\big[ \mathcal{A}(D) = s_i \big]$ is given by
--> <span class="math display">\[
    \begin{aligned}
        \Pr \big[ \mathcal{A}(D) = s_i \Vert \vec X_{i - 1} = \vec x_{i
- 1} \big]
            &amp;= \Pr \big[ M(D, \vec x_{i - 1}) &lt; \hat t \le \hat
q_i(D) \big] \\
            &amp;= \Pr \big[ M(D, \vec x_{i - 1}) &lt; t + Z \le q_i(D)
+ X_i \big] \\
            &amp;= \int \int p(z) \cdot p(x_i) \cdot \mathbb{I} \Big[
M(D, \vec x_{i - 1}) &lt; t + z \le q_i(D) + x_i \Big] \, d x_i \, dz,
    \end{aligned}
\]</span> where <span class="math inline">\(p(z)\)</span> and <span
class="math inline">\(p(x_i)\)</span> are probability densities of <span
class="math inline">\(Z\)</span> and <span
class="math inline">\(X_i\)</span>, respectively. Similarly, the
conditional probability density of <span class="math inline">\(\Pr\big[
\mathcal{A}(D&#39;) = s_i \big]\)</span> is given by <span
class="math display">\[
    \begin{aligned}
        \Pr \big[ \mathcal{A}(D&#39;) = s_i \Vert \vec X_{i - 1} = \vec
x_{i - 1} \big]
            &amp;= \Pr \big[ M(D&#39;, \vec x_{i - 1}) &lt; \hat t \le
\hat q_i(D&#39;) \big] \\
            &amp;= \Pr \big[ M(D&#39;, \vec x_{i - 1}) &lt; t + Z \le
q_i(D&#39;) + X_i \big] \\
            &amp;= \int \int  p(z&#39;) \cdot p(x_i&#39;) \cdot
\mathbb{I} \Big[ M(D&#39;, \vec x_{i - 1}) &lt; t + z&#39; \le
q_i(D&#39;) + x_i&#39; \Big] \, d x_i&#39; \, dz&#39;.
    \end{aligned}
\]</span></p>
<p>Suppose that the following equations holds: <span
class="math display">\[
    \begin{aligned}
        t + z&#39; - M(D&#39;, \vec x_{i - 1}) &amp;= t + z - M(D, \vec
x_{i - 1}), \\
        q_i(D&#39;) + x_i&#39; - (t + z&#39;) &amp;= q_i(D) + x_i - (t +
z).
    \end{aligned}
\]</span></p>
<p>Then <span class="math display">\[
    \mathbb{I} \Big[ M(D&#39;, \vec x_{i - 1}) &lt; t + z&#39; \le
q_i(D&#39;) + x_i&#39; \Big]
    = \mathbb{I} \Big[ M(D, \vec x_{i - 1}) &lt; t + z \le q_i(D) + x_i
\Big].
\]</span></p>
<p>But this implies that <span class="math display">\[
    \begin{aligned}
        |z&#39; - z|    &amp;= |M(D&#39;, \vec x_{i - 1}) - M(D, \vec
x_{i - 1})| \le \Delta \\
        |x_i&#39; - x_i|    &amp;= |z&#39; - q_i(D&#39;) - z + q_i(D)|
\\
                    &amp;= |M(D&#39;, \vec x_{i - 1}) - M(D, \vec x_{i -
1}) + q_i(D) - q_i(D&#39;)| \\
                    &amp;\le 2 \cdot \Delta.
    \end{aligned}
\]</span></p>
Further, observe that $$
<span class="math display">\[\begin{vmatrix}
        \begin{aligned}    
        
                \frac{d x_i&#39;}{d_{x_i}},     &amp;\frac{d
x_i&#39;}{d_z} \\
                \frac{d z&#39;}{d_{x_i}},     &amp;\frac{d z&#39;}{d_z}
            
        \end{aligned}
    \end{vmatrix}\]</span>
=
<span class="math display">\[\begin{vmatrix}
        1,     &amp;0 \\
        0,     &amp;1
    \end{vmatrix}\]</span>
<p>= 1, $$</p>
<p>Therefore, by integration by substitution, <span
class="math display">\[
    \begin{aligned}
        \frac{\Pr \big[ \mathcal{A}(D) = s_i \Vert \vec X_{i - 1} = \vec
x_{i - 1} \big]}{\Pr \big[ \mathcal{A}(D&#39;) = s_i \Vert \vec X_{i -
1} = \vec x_{i - 1} \big] }
            &amp;= \frac{
                \int \int p(z) \cdot p(x_i) \cdot \mathbb{I} \Big[ M(D,
\vec x_{i - 1}) &lt; t + z \le q_i(D) + x_i \Big] \, d x_i \, dz
            }
            {
                \int \int  p(z&#39;) \cdot p(x_i&#39;) \cdot \mathbb{I}
\Big[ M(D&#39;, \vec x_{i - 1}) &lt; t + z&#39; \le q_i(D&#39;) +
x_i&#39; \Big] \, d x_i&#39; \, dz&#39;
            } \\
            &amp;= \frac{
                \int \int p(z) \cdot p(x_i) \cdot \mathbb{I} \Big[ M(D,
\vec x_{i - 1}) &lt; t + z \le q_i(D) + x_i \Big] \, d x_i \, dz
            }
            {
                \int \int  p(z&#39;) \cdot p(x_i&#39;) \cdot \mathbb{I}
\Big[ M(D, \vec x_{i - 1}) &lt; t + z \le q_i(D) + x_i \Big] \, d x_i \,
dz
            } \\
            &amp;\le \exp \Big( \Delta \cdot
\frac{\epsilon}{3\Delta}\Big) \cdot \exp \Big( 2 \cdot \Delta \cdot
\frac{\epsilon}{3\Delta} \Big) \\
            &amp;\le \exp(\epsilon).
    \end{aligned}
\]</span></p>
<p>Integrating over <span class="math inline">\(\vec X_{i - 1} = \vec
x_{i - 1}\)</span> for all possible <span class="math inline">\(\vec
x_{i - 1}\)</span> proves that <span class="math inline">\(\Pr\big[
\mathcal{A}(D) = s_i \big] \le e^\epsilon \cdot \Pr\big[
\mathcal{A}(D&#39;) = s_i \big]\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="mechanism-two">Mechanism Two</h1>
<blockquote>
<p><strong>Definition.</strong> For each <span class="math inline">\(b
\in \mathbb{R}^+\)</span>, let <span
class="math inline">\(\mathcal{Exp}(b)\)</span> be the exponential
distribution with parameter <span class="math inline">\(b\)</span>.</p>
</blockquote>
<p>The algorithm is described as follows.</p>
<blockquote>
<p><strong>Algorithm <span
class="math inline">\(\mathcal{A}\)</span>.</strong><br />
1. <span class="math inline">\(\hat t \leftarrow t + Z\)</span> where
<span class="math inline">\(Z \sim \mathcal{Exp}(3 \Delta /
\epsilon)\)</span><br />
2. <strong>for</strong> <span class="math inline">\(i \in
\mathbb{N}^+\)</span> <strong>do</strong><br />
3. <span class="math inline">\(\qquad\)</span> <span
class="math inline">\(\hat q_i(D) \leftarrow q_i(D) + X_i\)</span>,
where <span class="math inline">\(X_i \sim \mathcal{Exp}( 3 \Delta /
\epsilon)\)</span><br />
4. <span class="math inline">\(\qquad\)</span> <strong>if</strong> <span
class="math inline">\(\hat q_i(D) \ge \hat t\)</span>
<strong>then</strong><br />
5. <span class="math inline">\(\qquad\qquad\)</span>
<strong>output</strong> <span class="math inline">\(\bot\)</span><br />
6. <span class="math inline">\(\qquad\qquad\)</span>
<strong>halt</strong><br />
7. <span class="math inline">\(\qquad\)</span>
<strong>else</strong><br />
8. <span class="math inline">\(\qquad\qquad\)</span>
<strong>output</strong> <span class="math inline">\(\top\)</span></p>
</blockquote>
<h2 id="privacy-guarantee-1">Privacy Guarantee</h2>
<p>Let <span class="math inline">\(D&#39;\)</span> be a neighboring
dataset of <span class="math inline">\(D\)</span>.</p>
<blockquote>
<p><strong>Theorem.</strong> For each <span class="math inline">\(i \in
\mathbb{N}^+\)</span>, let <span class="math inline">\(s_i \doteq
\underbrace{\top \ldots \top}_{ i - 1 \text{ occurrences} }
\bot\)</span>. Then <span class="math display">\[
e^{-\epsilon}
\le
\frac{\Pr[ \mathcal{A}(D) = s_i ]}{\Pr[ \mathcal{A}(D&#39;) = s_i ]}
\le
e^{\epsilon}.
\]</span></p>
</blockquote>
<p>THe analysis is very similar to the one for report noisy max.
Informally speaking, we are applying report noisy max twice, one for
<span class="math inline">\(\hat t\)</span> being max among <span
class="math inline">\((\hat t, \hat q(D_1), \ldots, \hat q(D_{i -
1}))\)</span>, one for <span class="math inline">\(\hat q(D_i)\)</span>
being max among <span class="math inline">\((\hat t, \hat q(D_1),
\ldots, \hat q(D_{i}))\)</span>.</p>
<p><strong>Proof.</strong></p>
<p>Fix an <span class="math inline">\(i \in \mathbb{N}^+\)</span>. For
each dataset <span class="math inline">\(D\)</span> and each <span
class="math inline">\(\vec x_{i - 1} = (x_1, \ldots, x_{i - 1} ) \in
\mathbb{R}^{i - 1}\)</span>, define <span class="math display">\[
    M(D, \vec x_{i - 1}) = \max_{1 \le j \le i - 1} \Big( q_j(D) + x_j
\Big).
\]</span></p>
<p>Denote <span class="math inline">\(\vec X_{i - 1} \doteq (X_1,
\ldots, X_{i - 1})\)</span>, and The event <span
class="math inline">\(\mathcal{A}(D) = s_i\)</span> happens if and only
if<br />
<span class="math display">\[
    M(D, \vec X_{i - 1}) &lt; \hat t \le \hat q_i(D).
\]</span></p>
<p>Fix a sequence <span class="math inline">\(\vec x_{i - 1} = (x_1,
\ldots, x_{i - 1} ) \in \mathbb{R}^{i - 1}\)</span>. Since the random
variables <span class="math inline">\(Z, \vec X_{i - 1}, X_i\)</span>
are independent, <span class="math display">\[
    \begin{aligned}
        \Pr \big[ \mathcal{A}(D) = s_i \Vert \vec X_{i - 1} = \vec x_{i
- 1} \big]
            &amp;= \Pr \big[ M(D, \vec x_{i - 1}) &lt; \hat t \le \hat
q_i(D) \big] \\
            &amp;= \Pr \big[ M(D, \vec x_{i - 1}) &lt; t + Z \le q_i(D)
+ X_i \big] \\
            &amp;= \int_{M(D, \vec x_{i - 1}) - t}^\infty \left( p(z)
\cdot \int_{t + z - q_i (D)}^\infty p(x_i) \, d x_i \right) \, dz \\
            &amp;= \int_{M(D, \vec x_{i - 1}) - t}^\infty
                p(z) \cdot
                \exp \left( -\frac{\epsilon}{3 \Delta} \cdot \max \big\{
0, t + z - q_i (D) \big\}
            \right) \, dz
    \end{aligned}
\]</span> where <span class="math inline">\(p(z)\)</span> and <span
class="math inline">\(p(x_i)\)</span> are probability densities of <span
class="math inline">\(Z\)</span> and <span
class="math inline">\(X_i\)</span>, respectively. For now, let <span
class="math display">\[
    \kappa \doteq M(D, \vec x_{i - 1}) - M(D&#39;, \vec x_{i - 1}) \in
[- \Delta, \Delta] .
\]</span> Following similar vein, the conditional probability density of
<span class="math inline">\(\Pr\big[ \mathcal{A}(D&#39;) = s_i
\big]\)</span> is given by <span class="math display">\[
    \begin{aligned}
        \Pr \big[ \mathcal{A}(D&#39;) = s_i \Vert \vec X_{i - 1} = \vec
x_{i - 1} \big]
            &amp;= \Pr \big[ M(D&#39;, \vec x_{i - 1}) &lt; \hat t \le
\hat q_i(D&#39;) \big] \\
            &amp;= \Pr \big[ M(D&#39;, \vec x_{i - 1}) &lt; t + Z \le
q_i(D&#39;) + X_i \big] \\
            &amp;= \int_{M(D&#39;, \vec x_{i - 1}) - t}^\infty \left(
p(z) \cdot \int_{t + z - q_i (D&#39;)}^\infty p(x_i) \, d x_i \right) \,
dz \\
            &amp;= \int_{M(D&#39;, \vec x_{i - 1}) - t}^\infty
                p(z) \cdot
                \exp \left( -\frac{\epsilon}{3 \Delta} \cdot \max \big\{
0, t + z - q_i (D&#39;) \big\}
            \right) \, dz \\
            &amp;\stackrel{y \doteq z + \kappa}{=} \int_{M(D, \vec x_{i
- 1}) - t}^\infty
                p(y - \kappa) \cdot
                \exp \left( -\frac{\epsilon}{3 \Delta} \cdot \max \big\{
0, t + y - \kappa - q_i (D&#39;) \big\}
            \right) \, dy
    \end{aligned}
\]</span></p>
<p>Comparing the integrands we have <span class="math display">\[
    \begin{aligned}
        \frac{\Pr \big[ \mathcal{A}(D) = s_i \Vert \vec X_{i - 1} = \vec
x_{i - 1} \big]}{\Pr \big[ \mathcal{A}(D&#39;) = s_i \Vert \vec X_{i -
1} = \vec x_{i - 1} \big] }
            &amp;\le \exp(\epsilon).
    \end{aligned}
\]</span></p>
<p>Integrating over <span class="math inline">\(\vec X_{i - 1} = \vec
x_{i - 1}\)</span> for all possible <span class="math inline">\(\vec
x_{i - 1}\)</span> proves that <span class="math inline">\(\Pr\big[
\mathcal{A}(D) = s_i \big] \le e^\epsilon \cdot \Pr\big[
\mathcal{A}(D&#39;) = s_i \big]\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="utility-guarantee">Utility Guarantee</h1>
<p>Suppose that the algorithm outputs <span
class="math inline">\(\top\)</span> at the <span
class="math inline">\(i\)</span>-th query, we measure the empirical
error by <span class="math inline">\(\max \{0, q_i(D) - t \}\)</span>,
i.e., if <span class="math inline">\(q_i(D)\)</span> does not exceed
<span class="math inline">\(t\)</span>, then we regards the empirical
error as <span class="math inline">\(0\)</span>. Similarly, for an
output <span class="math inline">\(\bot\)</span>, the empirical error is
measured by <span class="math inline">\(\max \{0, t - q_i(D)
\}\)</span>.</p>
<blockquote>
<p><strong>Theorem.</strong> Fix a sequence of <span
class="math inline">\(k\)</span> queries <span
class="math inline">\(q_1, \ldots, q_k\)</span> to be performed by the
algorithm. With probability at most <span
class="math inline">\(\beta\)</span>, the maximum empirical error
exceeds <span class="math display">\[
\frac{6 \cdot \Delta}{\epsilon}\ln \frac{k + 1}{\beta}.
\]</span></p>
</blockquote>
<p><strong>Proof.</strong> <span class="math display">\[
    \Pr \left[ |Z| \ge \frac{2 \cdot \Delta}{\epsilon}\ln \frac{k +
1}{\beta} \right]
        \le \exp \left( 2 \cdot \ln \frac{k + 1}{\beta} \right)
        = \frac{\beta}{k + 1}.
\]</span> Similarly, for each <span class="math inline">\(i \in
[k]\)</span>, <span class="math display">\[
    \Pr \left[ |X_i| \ge \frac{2 \cdot \Delta}{\epsilon}\ln \frac{k +
1}{\beta} \right]
        \le \exp \left( 2 \cdot \ln \frac{k + 1}{\beta} \right)
        = \frac{\beta}{k + 1}.
\]</span></p>
<p>By union bound, with probability at most <span
class="math inline">\(\beta\)</span>, it holds that <span
class="math display">\[
    |Z| + \max_{i \in [k]} |X_i| \ge \frac{6 \cdot \Delta}{\epsilon}\ln
\frac{k + 1}{\beta}.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="reference">Reference</h1>
<p>[1] <em>C. Dwork and A. Roth, “The Algorithmic Foundations of
Differential Privacy,”</em></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/67/">67</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">WOW</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
